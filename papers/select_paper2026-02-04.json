[
  {
    "date": "2026-02-04",
    "title": "What's in a Benchmark? The Case of SWE-Bench in Automated Program Repair",
    "authors": "Matias Martinez, Xavier Franch",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04449v1",
    "source": "arXiv",
    "abstract": "The rapid progress in Automated Program Repair (APR) has been fueled by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a benchmark designed to evaluate repair systems using real issues mined from popular open-source Python repositories. Its public leaderboards-SWE-Bench Lite and Verified-have become central platforms for tracking progress and comparing solutions. In this paper, we present the first comprehensive study of these two leaderboards, examining who is submitting solutions, the products behind the submissions, the LLMs employed, and the openness of the approaches. We analyze 79 entries submitted to Lite leaderboard and 133 to Verified. Our results show that most entries on both leaderboards originate from industry, particularly small companies and large publicly traded companies. These submissions often achieve top results, although academic contributions-typically open source-also remain competitive. We also find a clear dominance of proprietary LLMs, especially Claude family, with state-of-the-art results on both leaderboards currently achieved by Claude 4 Sonnet. These findings offer insights into the SWE-Bench ecosystem that can guide greater transparency and diversity in future benchmark-driven research.",
    "title_zh": "基准中包含什么？以自动化程序修复中的 SWE-Bench 为例",
    "abstract_zh": "自动化程序修复（APR）领域的快速发展得益于人工智能技术的进步，尤其是大型语言模型（LLMs）和基于智能体的系统的发展。SWE-Bench 是一个旨在通过从流行的开源 Python 仓库中挖掘真实问题来评估修复系统性能的基准测试。其公开排行榜——SWE-Bench Lite 和 Verified——已成为追踪进展和比较解决方案的核心平台。本文首次对这两个排行榜进行了全面研究，深入分析了提交者身份、背后的开发团队、所使用的 LLM 模型以及方法的开放程度。我们分析了提交至 Lite 排行榜的 79 个条目以及提交至 Verified 排行榜的 133 个条目。研究结果表明，两个排行榜上的大多数提交均来自工业界，尤其是中小型公司和大型上市公司。这些工业界提交通常取得顶尖成绩，但学术界贡献（通常为开源项目）也保持了较强的竞争力。此外，我们发现专有大型语言模型占据明显主导地位，尤其是 Claude 系列模型，目前两个排行榜上的最先进成果均由 Claude 4 Sonnet 实现。这些发现为理解 SWE-Bench 生态系统提供了重要洞见，有助于未来以基准测试为导向的研究实现更高的透明度与多样性。"
  },
  {
    "date": "2026-02-04",
    "title": "Fine-Grained Activation Steering: Steering Less, Achieving More",
    "authors": "Zijian Feng, Tianjiao Li, Zixiao Zhu, Hanzhang Zhou, Junlang Qian, Li Zhang, Jia Jim Deryl Chua, Lee Onn Mak, Gee Wah Ng, Kezhi Mao",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04428v1",
    "source": "arXiv",
    "abstract": "Activation steering has emerged as a cost-effective paradigm for modifying large language model (LLM) behaviors. Existing methods typically intervene at the block level, steering the bundled activations of selected attention heads, feedforward networks, or residual streams. However, we reveal that block-level activations are inherently heterogeneous, entangling beneficial, irrelevant, and harmful features, thereby rendering block-level steering coarse, inefficient, and intrusive. To investigate the root cause, we decompose block activations into fine-grained atomic unit (AU)-level activations, where each AU-level activation corresponds to a single dimension of the block activation, and each AU denotes a slice of the block weight matrix. Steering an AU-level activation is thus equivalent to steering its associated AU. Our theoretical and empirical analysis show that heterogeneity arises because different AUs or dimensions control distinct token distributions in LLM outputs. Hence, block-level steering inevitably moves helpful and harmful token directions together, which reduces efficiency. Restricting intervention to beneficial AUs yields more precise and effective steering. Building on this insight, we propose AUSteer, a simple and efficient method that operates at a finer granularity of the AU level. AUSteer first identifies discriminative AUs globally by computing activation momenta on contrastive samples. It then assigns adaptive steering strengths tailored to diverse inputs and selected AU activations. Comprehensive experiments on multiple LLMs and tasks show that AUSteer consistently surpasses advanced baselines while steering considerably fewer activations, demonstrating that steering less achieves more.",
    "title_zh": "细粒度激活调控：少干预，多成效",
    "abstract_zh": "激活控制（Activation Steering）作为一种成本效益高的方法，正逐渐成为调整大语言模型（LLM）行为的主流范式。现有方法通常在模块层级进行干预，通过操控选定的注意力头、前馈网络或残差流所捆绑的激活值来实现控制。然而，我们发现模块级激活本质上具有高度异质性，混杂了有益、无关甚至有害的特征，导致模块级控制方式粗糙、低效且具有侵入性。为探究其根本原因，我们首次将模块激活分解为细粒度的原子单元（Atomic Unit, AU）层级激活——其中每个AU层级的激活对应模块激活的一个维度，而每个AU则代表模块权重矩阵的一个切片。因此，对AU层级激活的控制等价于对相应AU的控制。\n\n我们的理论与实证分析表明，异质性产生的根源在于：不同的AU或维度分别控制LLM输出中不同token的分布。因此，模块级控制不可避免地同时移动有益与有害的token方向，从而降低控制效率。若将干预范围限制在有益的AU上，则可实现更精准、更高效的控制。基于这一发现，我们提出了AUSteer——一种在AU层级操作的简单而高效的控制方法。AUSteer首先通过在对比样本上计算激活动量，全局识别出具有判别性的AU；随后，根据输入内容和选定的AU激活情况，自适应地分配控制强度。在多个大语言模型和任务上的全面实验表明，AUSteer始终显著优于现有先进基线方法，同时仅需操控更少的激活值，充分证明了“少即是多”的控制理念：控制得更少，反而效果更好。"
  },
  {
    "date": "2026-02-04",
    "title": "SPEAR: An Engineering Case Study of Multi-Agent Coordination for Smart Contract Auditing",
    "authors": "Arnab Mallick, Indraveni Chebolu, Harmesh Rana",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04418v1",
    "source": "arXiv",
    "abstract": "We present SPEAR, a multi-agent coordination framework for smart contract auditing that applies established MAS patterns in a realistic security analysis workflow. SPEAR models auditing as a coordinated mission carried out by specialized agents: a Planning Agent prioritizes contracts using risk-aware heuristics, an Execution Agent allocates tasks via the Contract Net protocol, and a Repair Agent autonomously recovers from brittle generated artifacts using a programmatic-first repair policy. Agents maintain local beliefs updated through AGM-compliant revision, coordinate via negotiation and auction protocols, and revise plans as new information becomes available. An empirical study compares the multi-agent design with centralized and pipeline-based alternatives under controlled failure scenarios, focusing on coordination, recovery behavior, and resource use.",
    "title_zh": "SPEAR：智能合约审计中多智能体协作的工程案例研究",
    "abstract_zh": "我们提出SPEAR，一个用于智能合约审计的多智能体协调框架，该框架在现实的安全分析工作流中应用了成熟的多智能体系统（MAS）模式。SPEAR将审计建模为由专业化智能体协同完成的任务：规划智能体利用风险感知启发式方法对合约进行优先级排序，执行智能体通过合约网协议分配任务，修复智能体则采用以程序化为核心的修复策略，自主恢复因脆弱生成物导致的问题。各智能体通过符合AGM修正原则的本地信念更新机制保持信息同步，借助协商与拍卖协议进行协调，并在获取新信息时动态调整计划。一项实证研究在受控故障场景下，将多智能体设计与集中式及流水线式方案进行对比，重点考察了协调能力、恢复行为以及资源使用效率。"
  },
  {
    "date": "2026-02-04",
    "title": "ProxyWar: Dynamic Assessment of LLM Code Generation in Game Arenas",
    "authors": "Wenjun Peng, Xinyu Wang, Qi Wu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04296v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) have revolutionized automated code generation, yet the evaluation of their real-world effectiveness remains limited by static benchmarks and simplistic metrics. We present ProxyWar, a novel framework that systematically assesses code generation quality by embedding LLM-generated agents within diverse, competitive game environments. Unlike existing approaches, ProxyWar evaluates not only functional correctness but also the operational characteristics of generated programs, combining automated testing, iterative code repair, and multi-agent tournaments to provide a holistic view of program behavior. Applied to a range of state-of-the-art coders and games, our approach uncovers notable discrepancies between benchmark scores and actual performance in dynamic settings, revealing overlooked limitations and opportunities for improvement. These findings highlight the need for richer, competition-based evaluation of code generation. Looking forward, ProxyWar lays a foundation for research into LLM-driven algorithm discovery, adaptive problem solving, and the study of practical efficiency and robustness, including the potential for models to outperform hand-crafted agents. The project is available at https://github.com/xinke-wang/ProxyWar.",
    "title_zh": "代理战争：在游戏竞技场中对大语言模型代码生成的动态评估",
    "abstract_zh": "大型语言模型（LLMs）已彻底改变了自动化代码生成领域，然而其在真实世界中的实际效果评估仍受限于静态基准测试和简单的度量指标。我们提出了ProxyWar——一种新颖的框架，通过将LLM生成的智能体嵌入多样且具有竞争性的游戏环境中，系统性地评估代码生成的质量。与现有方法不同，ProxyWar不仅评估代码的功能正确性，还考察生成程序的运行特性，结合自动化测试、迭代式代码修复以及多智能体对战机制，全面揭示程序的行为表现。在一系列前沿代码生成模型与游戏环境上的应用表明，基准测试得分与动态环境中的实际表现之间存在显著差异，暴露出以往被忽视的局限性以及改进空间。这些发现凸显了采用更丰富、基于竞赛的评估方式来衡量代码生成能力的必要性。展望未来，ProxyWar为研究LLM驱动的算法发现、自适应问题求解，以及实际效率与鲁棒性的分析奠定了基础，甚至揭示了模型超越人工精心设计智能体的潜力。该项目代码已开源，地址为：https://github.com/xinke-wang/ProxyWar。"
  },
  {
    "date": "2026-02-04",
    "title": "Landscape-aware Automated Algorithm Design: An Efficient Framework for Real-world Optimization",
    "authors": "Haoran Yin, Shuaiqun Pan, Zhao Wei, Jian Cheng Wong, Yew-Soon Ong, Anna V. Kononova, Thomas Bäck, Niki van Stein",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04529v1",
    "source": "arXiv",
    "abstract": "The advent of Large Language Models (LLMs) has opened new frontiers in automated algorithm design, giving rise to numerous powerful methods. However, these approaches retain critical limitations: they require extensive evaluation of the target problem to guide the search process, making them impractical for real-world optimization tasks, where each evaluation consumes substantial computational resources. This research proposes an innovative and efficient framework that decouples algorithm discovery from high-cost evaluation. Our core innovation lies in combining a Genetic Programming (GP) function generator with an LLM-driven evolutionary algorithm designer. The evolutionary direction of the GP-based function generator is guided by the similarity between the landscape characteristics of generated proxy functions and those of real-world problems, ensuring that algorithms discovered via proxy functions exhibit comparable performance on real-world problems. Our method enables deep exploration of the algorithmic space before final validation while avoiding costly real-world evaluations. We validated the framework's efficacy across multiple real-world problems, demonstrating its ability to discover high-performance algorithms while substantially reducing expensive evaluations. This approach shows a path to apply LLM-based automated algorithm design to computationally intensive real-world optimization challenges.",
    "title_zh": "面向景观的自动化算法设计：一种高效的真实世界优化框架",
    "abstract_zh": "大型语言模型（LLMs）的出现为自动化算法设计开辟了新的前沿，催生了众多强大的方法。然而，这些方法仍存在关键局限：它们需要对目标问题进行大量评估以指导搜索过程，这使得它们在现实世界中的优化任务中难以应用，因为每次评估都消耗巨大的计算资源。本研究提出了一种创新且高效的框架，将算法发现与高成本评估解耦。我们的核心创新在于将遗传编程（GP）函数生成器与基于LLM的进化算法设计者相结合。基于生成的代理函数与真实世界问题之间的景观特征相似性，引导GP函数生成器的进化方向，从而确保通过代理函数发现的算法在真实世界问题上也能表现出相当的性能。该方法能够在最终验证前对算法空间进行深度探索，同时避免昂贵的真实世界评估。我们在多个真实世界问题上验证了该框架的有效性，证明其能够在显著减少高成本评估次数的同时，发现高性能算法。这一方法为将基于LLM的自动化算法设计应用于计算密集型的真实世界优化挑战提供了可行路径。"
  },
  {
    "date": "2026-02-04",
    "title": "Is Micro Domain-Adaptive Pre-Training Effective for Real-World Operations? Multi-Step Evaluation Reveals Potential and Bottlenecks",
    "authors": "Masaya Tsunokake, Yuta Koreeda, Terufumi Morishita, Koichi Nagatsuka, Hikaru Tomonari, Yasuhiro Sogawa",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04466v1",
    "source": "arXiv",
    "abstract": "When applying LLMs to real-world enterprise operations, LLMs need to handle proprietary knowledge in small domains of specific operations ($\\textbf{micro domains}$). A previous study shows micro domain-adaptive pre-training ($\\textbf{mDAPT}$) with fewer documents is effective, similarly to DAPT in larger domains. However, it evaluates mDAPT only on multiple-choice questions; thus, its effectiveness for generative tasks in real-world operations remains unknown. We aim to reveal the potential and bottlenecks of mDAPT for generative tasks. To this end, we disentangle the answering process into three subtasks and evaluate the performance of each subtask: (1) $\\textbf{eliciting}$ facts relevant to questions from an LLM's own knowledge, (2) $\\textbf{reasoning}$ over the facts to obtain conclusions, and (3) $\\textbf{composing}$ long-form answers based on the conclusions. We verified mDAPT on proprietary IT product knowledge for real-world questions in IT technical support operations. As a result, mDAPT resolved the elicitation task that the base model struggled with but did not resolve other subtasks. This clarifies mDAPT's effectiveness in the knowledge aspect and its bottlenecks in other aspects. Further analysis empirically shows that resolving the elicitation and reasoning tasks ensures sufficient performance (over 90%), emphasizing the need to enhance reasoning capability.",
    "title_zh": "微域自适应预训练在实际操作中有效吗？多步骤评估揭示了其潜力与瓶颈",
    "abstract_zh": "在将大语言模型（LLMs）应用于现实世界的企业运营时，LLMs 需要处理特定业务操作中少量文档构成的专有知识领域（即**微领域**）。先前的研究表明，使用较少文档进行微领域自适应预训练（**mDAPT**）是有效的，其效果类似于在更大领域中进行的 DAPT。然而，该研究仅在多项选择题上评估了 mDAPT 的性能，因此其在真实业务场景中生成式任务上的有效性仍不明确。本文旨在揭示 mDAPT 在生成式任务中的潜力与瓶颈。为此，我们将回答过程分解为三个子任务，并分别评估每个子任务的表现：（1）**提取**（eliciting）LLM 自身知识中与问题相关的信息；（2）**推理**（reasoning）基于这些事实得出结论；（3）**生成**（composing）基于结论的长篇回答。我们在真实 IT 技术支持运营场景中，针对企业专有的 IT 产品知识对 mDAPT 进行了验证。结果表明，mDAPT 有效解决了基础模型难以完成的“提取”任务，但在推理和生成任务上并未取得显著改善。这一发现明确了 mDAPT 在知识获取方面的有效性，同时也揭示了其在推理与生成能力方面的瓶颈。进一步的实证分析表明，若能同时解决“提取”与“推理”任务，则整体性能可达到 90% 以上，凸显出提升模型推理能力的迫切需求。"
  },
  {
    "date": "2026-02-04",
    "title": "SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization",
    "authors": "Jiarui Yuan, Tailin Jin, Weize Chen, Zeyuan Liu, Zhiyuan Liu, Maosong Sun",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04811v1",
    "source": "arXiv",
    "abstract": "True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring \"Closed-Book Training\" to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench.",
    "title_zh": "SE-Bench：基于知识内化的自我进化基准测试",
    "abstract_zh": "真正的自我演化要求智能体作为终身学习者，能够内化新的经验以应对未来的挑战。然而，严格衡量这一基础能力面临两大障碍：一是先验知识的纠缠，即“新”知识可能已出现在预训练数据中；二是推理复杂度的纠缠，即失败可能源于问题本身的难度，而非无法回忆已学知识。为此，我们提出了SE-Bench，一个诊断性环境，将NumPy库及其API文档伪装成一个具有随机标识符的伪新包。智能体在该环境中接受训练，以内化这一新包，并在无文档访问的情况下完成简单的编码任务。这种设置下，任务在掌握新API文档后变得简单，而基础模型因缺乏文档则完全无法完成。我们的研究揭示了三个关键发现：（1）“开卷悖论”——使用参考文档进行训练反而抑制了知识的保留，必须采用“闭卷训练”来迫使知识压缩进模型权重；（2）“强化学习差距”——标准强化学习因PPO剪裁和负梯度的存在，无法完全内化新知识；（3）自对弈（Self-Play）在知识内化中的可行性——当与监督微调（SFT）结合时，模型能够从自生成的、带有噪声的任务中学习，但强化学习方法则无法实现。总体而言，SE-Bench为知识内化驱动的自我演化建立了一个严谨的诊断平台。我们的代码与数据集可于 https://github.com/thunlp/SE-Bench 获取。"
  },
  {
    "date": "2026-02-04",
    "title": "Demonstrating ARG-V's Generation of Realistic Java Benchmarks for SV-COMP",
    "authors": "Charles Moloney, Robert Dyer, Elena Sherman",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04786v1",
    "source": "arXiv",
    "abstract": "The SV-COMP competition provides a state-of-the-art platform for evaluating software verification tools on a standardized set of verification tasks. Consequently, verifier development outcomes are influenced by the composition of program benchmarks included in SV-COMP. When expanding this benchmark corpus, it is crucial to consider whether newly added programs cause verifiers to exhibit behavior distinct from that observed on existing benchmarks. Doing so helps mitigate external threats to the validity of the competition's results. In this paper, we present the application of the ARG-V tool for automatically generating Java verification benchmarks in the SV-COMP format. We demonstrate that, on a newly generated set of 68 realistic benchmarks, all four leading Java verifiers decrease in accuracy and recall compared to their performance on the existing benchmark suite. These findings highlight the potential of ARG-V to enhance the comprehensiveness and realism of verification tool evaluation, while also providing a roadmap for verifier developers aiming to improve their tools' applicability to real-world software.",
    "title_zh": "展示ARG-V生成针对SV-COMP的逼真Java基准测试",
    "abstract_zh": "SV-COMP竞赛为评估软件验证工具在一组标准化验证任务上的表现提供了一个前沿平台。因此，验证器的开发成果受到SV-COMP中包含的程序基准测试组成的影响。在扩展这一基准测试库时，必须考虑新增程序是否会导致验证器表现出与现有基准测试上不同的行为。这样做有助于降低竞赛结果外部效度的威胁。本文介绍了ARG-V工具在自动生成符合SV-COMP格式的Java验证基准测试中的应用。我们证明，在新生成的68个真实场景基准测试上，所有四种主流Java验证器的准确率和召回率均较其在现有基准测试集上的表现有所下降。这些发现凸显了ARG-V在提升验证工具评估的全面性和现实性方面的潜力，同时也为验证器开发者提供了改进其工具在真实世界软件中适用性的路线图。"
  },
  {
    "date": "2026-02-04",
    "title": "Focus-LIME: Surgical Interpretation of Long-Context Large Language Models via Proxy-Based Neighborhood Selection",
    "authors": "Junhao Liu, Haonan Yu, Zhenyu Yan, Xin Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04607v1",
    "source": "arXiv",
    "abstract": "As Large Language Models (LLMs) scale to handle massive context windows, achieving surgical feature-level interpretation is essential for high-stakes tasks like legal auditing and code debugging. However, existing local model-agnostic explanation methods face a critical dilemma in these scenarios: feature-based methods suffer from attribution dilution due to high feature dimensionality, thus failing to provide faithful explanations. In this paper, we propose Focus-LIME, a coarse-to-fine framework designed to restore the tractability of surgical interpretation. Focus-LIME utilizes a proxy model to curate the perturbation neighborhood, allowing the target model to perform fine-grained attribution exclusively within the optimized context. Empirical evaluations on long-context benchmarks demonstrate that our method makes surgical explanations practicable and provides faithful explanations to users.",
    "title_zh": "Focus-LIME：基于代理的邻域选择在长上下文大语言模型中的外科式解释",
    "abstract_zh": "随着大型语言模型（LLMs）的规模不断扩大，以应对海量上下文窗口，实现特征级别的精准解释在法律审计、代码调试等高风险任务中变得至关重要。然而，在这些场景下，现有的局部、模型无关的解释方法面临一个关键困境：基于特征的方法由于特征维度极高，容易出现归因稀释问题，导致无法提供可信的解释。本文提出了一种粗到细的框架——Focus-LIME，旨在恢复精准解释的可行性。Focus-LIME利用一个代理模型来筛选扰动邻域，使目标模型仅在优化后的上下文中进行细粒度的归因分析。在长上下文基准上的实证评估表明，我们的方法使精准解释成为可能，并能为用户提供可信的解释。"
  },
  {
    "date": "2026-02-04",
    "title": "AgenticAKM : Enroute to Agentic Architecture Knowledge Management",
    "authors": "Rudra Dhar, Karthik Vaidhyanathan, Vasudeva Varma",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04445v1",
    "source": "arXiv",
    "abstract": "Architecture Knowledge Management (AKM) is crucial for maintaining current and comprehensive software Architecture Knowledge (AK) in a software project. However AKM is often a laborious process and is not adopted by developers and architects. While LLMs present an opportunity for automation, a naive, single-prompt approach is often ineffective, constrained by context limits and an inability to grasp the distributed nature of architectural knowledge. To address these limitations, we propose an Agentic approach for AKM, AgenticAKM, where the complex problem of architecture recovery and documentation is decomposed into manageable sub-tasks. Specialized agents for architecture Extraction, Retrieval, Generation, and Validation collaborate in a structured workflow to generate AK. To validate we made an initial instantiation of our approach to generate Architecture Decision Records (ADRs) from code repositories. We validated our approach through a user study with 29 repositories. The results demonstrate that our agentic approach generates better ADRs, and is a promising and practical approach for automating AKM.",
    "title_zh": "AgenticAKM：迈向智能架构知识管理",
    "abstract_zh": "架构知识管理（Architecture Knowledge Management, AKM）对于在软件项目中保持当前且全面的软件架构知识（Architecture Knowledge, AK）至关重要。然而，AKM 通常是一个繁琐的过程，开发者和架构师往往不愿采用。尽管大型语言模型（LLMs）为自动化提供了机遇，但采用简单的单次提示（single-prompt）方法往往效果不佳，受限于上下文长度限制，且难以理解架构知识的分布式特性。为解决这些局限性，我们提出了一种基于智能体（Agentic）的 AKM 方法——AgenticAKM，将复杂的架构恢复与文档化问题分解为可管理的子任务。通过专门负责架构提取、检索、生成和验证的智能体，在结构化工作流中协同合作，共同生成架构知识。为验证该方法，我们初步实现了一个实例，从代码仓库中自动生成架构决策记录（Architecture Decision Records, ADRs）。通过针对29个仓库的用户研究验证了该方法的有效性。结果表明，我们的智能体方法能够生成更高质量的 ADRs，是一种具有前景且实用的自动化 AKM 解决方案。"
  },
  {
    "date": "2026-02-04",
    "title": "Generative AI in Systems Engineering: A Framework for Risk Assessment of Large Language Models",
    "authors": "Stefan Otten, Philipp Reis, Philipp Rigoll, Joshua Ransiek, Tobias Schürmann, Jacob Langner, Eric Sax",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04358v1",
    "source": "arXiv",
    "abstract": "The increasing use of Large Language Models (LLMs) offers significant opportunities across the engineering lifecycle, including requirements engineering, software development, process optimization, and decision support. Despite this potential, organizations face substantial challenges in assessing the risks associated with LLM use, resulting in inconsistent integration, unknown failure modes, and limited scalability. This paper introduces the LLM Risk Assessment Framework (LRF), a structured approach for evaluating the application of LLMs within Systems Engineering (SE) environments. The framework classifies LLM-based applications along two fundamental dimensions: autonomy, ranging from supportive assistance to fully automated decision making, and impact, reflecting the potential severity of incorrect or misleading model outputs on engineering processes and system elements. By combining these dimensions, the LRF enables consistent determination of corresponding risk levels across the development lifecycle. The resulting classification supports organizations in identifying appropriate validation strategies, levels of human oversight, and required countermeasures to ensure safe and transparent deployment. The framework thereby helps align the rapid evolution of AI technologies with established engineering principles of reliability, traceability, and controlled process integration. Overall, the LRF provides a basis for risk-aware adoption of LLMs in complex engineering environments and represents a first step toward standardized AI assurance practices in systems engineering.",
    "title_zh": "生成式人工智能在系统工程中的应用：大型语言模型风险评估框架",
    "abstract_zh": "大型语言模型（LLM）的日益广泛应用为工程生命周期的各个环节带来了重大机遇，涵盖需求工程、软件开发、流程优化以及决策支持。尽管具备巨大潜力，组织在评估LLM使用相关风险方面仍面临严峻挑战，导致集成方式不一致、未知的故障模式频发以及可扩展性受限。本文提出了一种名为LLM风险评估框架（LRF）的结构化方法，用于在系统工程（SE）环境中评估LLM的应用。该框架从两个基本维度对基于LLM的应用进行分类：自主性，涵盖从辅助支持到完全自动化决策的连续范围；以及影响程度，反映模型输出错误或误导性结果对工程流程及系统要素可能造成的严重性。通过结合这两个维度，LRF能够在整个开发生命周期中一致地确定相应的风险等级。该分类结果有助于组织识别适当的验证策略、人类监督水平以及必要的缓解措施，从而确保LLM的安全与透明部署。因此，该框架有助于将人工智能技术的快速发展与工程领域中可靠性、可追溯性及受控流程集成等既定原则相协调。总体而言，LRF为复杂工程环境中风险意识驱动的LLM采纳提供了基础，并标志着系统工程领域标准化人工智能保障实践迈出的第一步。"
  },
  {
    "date": "2026-02-04",
    "title": "CoLT: Reasoning with Chain of Latent Tool Calls",
    "authors": "Fangwei Zhu, Zhifang Sui",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04246v1",
    "source": "arXiv",
    "abstract": "Chain-of-Thought (CoT) is a critical technique in enhancing the reasoning ability of Large Language Models (LLMs), and latent reasoning methods have been proposed to accelerate the inefficient token-level reasoning chain. We notice that existing latent reasoning methods generally require model structure augmentation and exhaustive training, limiting their broader applicability. In this paper, we propose CoLT, a novel framework that implements latent reasoning as ``tool calls''. Instead of reasoning entirely in the latent space, CoLT generates seed tokens that contain information of a reasoning step. When a latent tool call is triggered, a smaller external model will take the hidden states of seed tokens as its input, and unpack the seed tokens back to a full reasoning step. In this way, we can ensure that the main model reasons in the explicit token space, preserving its ability while improving efficiency. Experimental results on four mathematical datasets demonstrate that CoLT achieves higher accuracy and shorter reasoning length than baseline latent models, and is compatible with reinforcement learning algorithms and different decoder structures.",
    "title_zh": "CoLT：通过潜在工具调用链进行推理",
    "abstract_zh": "思维链（Chain-of-Thought, CoT）是提升大语言模型（LLMs）推理能力的关键技术，而隐式推理方法则被提出以加速传统逐标记（token-level）推理链效率低下的问题。我们注意到，现有的隐式推理方法通常需要对模型结构进行扩展并进行大量训练，这限制了其更广泛的应用。本文提出了一种名为CoLT的新框架，将隐式推理实现为“工具调用”（tool calls）的形式。与完全在隐式空间中推理不同，CoLT首先生成包含推理步骤信息的种子标记（seed tokens）。当触发隐式工具调用时，一个较小的外部模型会以这些种子标记的隐藏状态作为输入，将种子标记还原为完整的推理步骤。通过这种方式，我们确保主模型始终在显式标记空间中进行推理，既保留了其推理能力，又显著提升了效率。在四个数学推理数据集上的实验结果表明，CoLT在准确率和推理长度上均优于基线隐式模型，并且与强化学习算法及不同解码器结构具有良好的兼容性。"
  },
  {
    "date": "2026-02-04",
    "title": "Do Developers Read Type Information? An Eye-Tracking Study on TypeScript",
    "authors": "Samuel W. Flint, Robert Dyer, Bonita Sharif",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04824v1",
    "source": "arXiv",
    "abstract": "Statically-annotated types have been shown to aid developers in a number of programming tasks, and this benefit holds true even when static type checking is not used. It is hypothesized that this is because developers use type annotations as in-code documentation. In this study, we aim to provide evidence that developers use type annotations as in-code documentation. Understanding this hypothesized use will help to understand how, and in what contexts, developers use type information; additionally, it may help to design better development tools and inform educational decisions. To provide this evidence, we conduct an eye tracking study with 26 undergraduate students to determine if they read type annotations during code comprehension and bug localization in the TypeScript language. We found that developers do not look directly at lines containing type annotations or type declarations more often when they are present, in either code summarization or bug localization tasks. The results have implications for tool builders to improve the availability of type information, the development community to build good standards for use of type annotations, and education to enforce deliberate teaching of reading patterns.",
    "title_zh": "开发者会阅读类型信息吗？一项关于TypeScript的眼动追踪研究",
    "abstract_zh": "静态类型注解已被证明在多个编程任务中对开发者有所帮助，即使在不使用静态类型检查的情况下，这种优势依然存在。人们推测，这是因为开发者将类型注解用作代码中的文档。本研究旨在提供证据，证明开发者确实将类型注解用作代码内文档。理解这种假设中的使用方式，有助于我们了解开发者在何种情境下以及如何使用类型信息；此外，这也有助于设计更优的开发工具，并为教育决策提供参考。为获得这一证据，我们对26名本科生开展了一项眼动追踪研究，以确定他们在进行TypeScript语言的代码理解与缺陷定位任务时，是否会关注包含类型注解或类型声明的代码行。研究发现，在代码总结或缺陷定位任务中，当类型注解存在时，开发者并不会更频繁地直接注视包含类型注解或类型声明的代码行。该研究结果对工具开发者改进类型信息的可获取性、开发社区建立类型注解的良好使用规范，以及教育领域强化对代码阅读模式的刻意教学，均具有重要启示意义。"
  },
  {
    "date": "2026-02-04",
    "title": "$C$-$ΔΘ$: Circuit-Restricted Weight Arithmetic for Selective Refusal",
    "authors": "Aditya Kasliwal, Pratinav Seth, Vinay Kumar Sankarapu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04521v1",
    "source": "arXiv",
    "abstract": "Modern deployments require LLMs to enforce safety policies at scale, yet many controls rely on inference-time interventions that add recurring compute cost and serving complexity. Activation steering is widely used, but it requires runtime hooks and scales cost with the number of generations; conditional variants improve selectivity by gating when steering is applied but still retain an inference-time control path. We ask whether selective refusal can be moved entirely offline: can a mechanistic understanding of category-specific refusal be distilled into a circuit-restricted weight update that deploys as a standard checkpoint? We propose C-Δθ: Circuit Restricted Weight Arithmetic, which (i) localizes refusal-causal computation as a sparse circuit using EAP-IG and (ii) computes a constrained weight update ΔθC supported only on that circuit (typically <5% of parameters). Applying ΔθC yields a drop-in edited checkpoint with no inference-time hooks, shifting cost from per-request intervention to a one-time offline update. We evaluate category-targeted selectivity and capability retention on refusal and utility benchmarks.",
    "title_zh": "$C$-$ΔΘ$：用于选择性拒绝的电路受限权重算术",
    "abstract_zh": "现代部署要求大语言模型在大规模下执行安全策略，但许多控制机制依赖于推理时的干预，这带来了持续的计算开销和部署复杂性。激活控制（activation steering）虽被广泛采用，但需要运行时钩子，且其成本随生成次数增加而上升；条件变体虽通过门控机制提升了选择性，但仍保留了推理时的控制路径。我们提出一个关键问题：能否将选择性拒绝完全移至离线阶段？即，能否将对特定类别拒绝的机制性理解，提炼为一种仅作用于特定电路的权重更新，并以标准检查点形式部署？为此，我们提出 C-Δθ：电路受限权重算术（Circuit Restricted Weight Arithmetic），其核心包括：(i) 利用 EAP-IG 方法将导致拒绝的计算过程定位为稀疏电路；(ii) 仅在该电路所覆盖的参数范围内计算受约束的权重更新 ΔθC（通常仅涉及参数总量的 <5%）。应用 ΔθC 后，即可获得一个可直接替换的编辑后检查点，无需任何推理时钩子，从而将原本每请求的干预成本，转化为一次性的离线更新。我们在拒绝与效用基准上评估了该方法在类别目标选择性与能力保留方面的表现。"
  },
  {
    "date": "2026-02-04",
    "title": "Semantic Consensus Decoding: Backdoor Defense for Verilog Code Generation",
    "authors": "Guang Yang, Xing Hu, Xiang Chen, Xin Xia",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04195v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) for Verilog code generation are increasingly adopted in hardware design, yet remain vulnerable to backdoor attacks where adversaries inject malicious triggers during training to induce vulnerable hardware designs. Unlike patchable software vulnerabilities, hardware trojans become irreversible once fabricated, making remediation extremely costly or impossible. Existing active defenses require access to training data, impractical for third-party LLM users, while passive defenses struggle against semantically stealthy triggers that naturally blend into design specifications. In this paper, we hypothesize that under the requirements of both effectiveness and stealthiness, attackers are strongly biased toward embedding triggers in non-functional requirements (e.g., style modifiers, quality descriptors) rather than functional specifications that determine hardware behavior. Exploiting this insight, we propose Semantic Consensus Decoding (SCD), an inference-time passive defense with two key components: (1) functional requirement extraction that identifies essential requirements from user specifications, and (2) consensus decoding that adaptively fuses output distributions based on full user specifications and extracted functional requirements. When these distributions diverge significantly, SCD automatically suppresses suspicious components. Extensive experiments with three representative backdoor attacks demonstrate that SCD reduces average attack success rate from 89% to under 3% with negligible impact on generation quality.",
    "title_zh": "语义共识解码：面向Verilog代码生成的后门防御",
    "abstract_zh": "用于Verilog代码生成的大语言模型（LLMs）在硬件设计领域日益普及，但仍易受到后门攻击的威胁——攻击者在训练阶段注入恶意触发器，从而诱导生成存在漏洞的硬件设计。与可修复的软件漏洞不同，硬件木马一旦制造完成便无法逆转，修复成本极高甚至完全不可行。现有的主动防御方法需要访问训练数据，这对第三方LLM用户而言不切实际；而被动防御方法则难以应对语义隐蔽性极强的触发器，这类触发器能自然地融入设计规范中，难以察觉。本文提出假设：在兼顾攻击效果与隐蔽性的前提下，攻击者倾向于将触发器嵌入非功能性需求（如风格修饰符、质量描述词）中，而非决定硬件行为的功能性规范。基于这一洞察，我们提出一种推理阶段的被动防御方法——语义一致性解码（Semantic Consensus Decoding, SCD），其包含两个核心组件：（1）功能性需求提取，用于从用户输入中识别出关键的功能性要求；（2）一致性解码，通过结合完整用户规范与提取出的功能性需求，自适应地融合输出分布。当两种分布出现显著差异时，SCD会自动抑制可疑的输出成分。大量实验结果表明，在三种典型的后门攻击场景下，SCD将平均攻击成功率从89%降低至3%以下，同时对生成质量的影响微乎其微。"
  },
  {
    "date": "2026-02-04",
    "title": "I Can't Believe It's Not a Valid Exploit",
    "authors": "Derin Gezgin, Amartya Das, Shinhae Kim, Zhengdong Huang, Nevena Stojkovic, Claire Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04165v1",
    "source": "arXiv",
    "abstract": "Recently Large Language Models (LLMs) have been used in security vulnerability detection tasks including generating proof-of-concept (PoC) exploits. A PoC exploit is a program used to demonstrate how a vulnerability can be exploited. Several approaches suggest that supporting LLMs with additional guidance can improve PoC generation outcomes, motivating further evaluation of their effectiveness. In this work, we develop PoC-Gym, a framework for PoC generation for Java security vulnerabilities via LLMs and systematic validation of generated exploits. Using PoC-Gym, we evaluate whether the guidance from static analysis tools improves the PoC generation success rate and manually inspect the resulting PoCs. Our results from running PoC-Gym with Claude Sonnet 4, GPT-5 Medium, and gpt-oss-20b show that using static analysis for guidance and criteria lead to 21% higher success rates than the prior baseline, FaultLine. However, manual inspection of both successful and failed PoCs reveals that 71.5% of the PoCs are invalid. These results show that the reported success of LLM-based PoC generation can be significantly misleading, which is hard to detect with current validation mechanisms.",
    "title_zh": "这不可能，这不是一个有效的漏洞利用",
    "abstract_zh": "最近，大型语言模型（LLMs）已被应用于安全漏洞检测任务，包括生成概念验证（PoC）漏洞利用程序。PoC漏洞利用程序是一种用于演示漏洞如何被利用的程序。已有多种方法表明，通过提供额外的指导信息来辅助LLM，可以提升PoC生成的效果，这进一步激发了对其有效性的深入评估。在本研究中，我们提出了PoC-Gym——一个基于LLM生成Java安全漏洞PoC并进行系统性验证的框架。利用PoC-Gym，我们评估了静态分析工具提供的指导是否能提高PoC生成的成功率，并对生成的PoC进行了人工检查。实验结果表明，使用静态分析作为指导和评判标准，相较于先前的基线方法FaultLine，PoC生成的成功率提升了21%。然而，对成功与失败PoC的人工检查发现，高达71.5%的PoC实际上是无效的。这些结果表明，当前LLM驱动的PoC生成方法所报告的成功率可能具有显著误导性，而这种误导性很难通过现有的验证机制发现。"
  },
  {
    "date": "2026-02-04",
    "title": "A-Graph: A Unified Graph Representation for At-Will Simulation across System Stacks",
    "authors": "Daniel Price, Prabhu Vellaisamy, Patricia Gonzalez, George Michelogiannakis, John P. Shen, Di Wu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04847v1",
    "source": "arXiv",
    "abstract": "As computer systems continue to diversify across technologies, architectures, applications, and beyond, the relevant design space has become larger and more complex. Given such trends, design space exploration (DSE) at early stages is critical to ensure agile development towards optimal performance and cost. Industry-grade EDA tools directly take in RTL code and report accurate results, but do not perform DSE. Recent works have attempted to explore the design space via simulation. However, most of these works are domain-specific and constrain the space that users are allowed to explore, offering limited flexibility between technologies, architecture, and applications. Moreover, they often demand high domain expertise to ensure high accuracy. To enable simulation that is agnostic to technology, architecture, and application at any granularity, we introduce Architecture-Graph (Agraph), a graph that unifies the system representation surrounding any arbitrary application, software, architecture, and circuit. Such a unified representation distinguishes Agraph from prior works, which focus on a single stack, allowing users to freely explore the design space across system stacks. To fully unleash the potential of Agraph, we further present Archx, a framework that implements Agraph. Archx is user-friendly in two ways. First, Archx has an easy-to-use programming interface to automatically generate and sweep design points under user constraints, boosting the programmability. Second, Archx adopts scope-based metric retrieval to analyze and understand each design point at any user-preferred hierarchy, enhancing the explainability. We conduct case studies that demonstrate Agraph's generalization across technologies, architecture, and applications with high simulation accuracy. Overall, we argue that Agraph and Archx serve as a foundation to simulate both performance and cost at will.",
    "title_zh": "A-Graph：一种统一的图表示方法，用于跨系统栈的任意模拟",
    "abstract_zh": "随着计算机系统在技术、架构、应用等多个维度不断多样化，相关的设计空间变得日益庞大且复杂。在这一趋势下，早期阶段的设计空间探索（DSE）对于实现敏捷开发、达成性能与成本的最优平衡至关重要。目前工业级EDA工具能够直接接收RTL代码并报告精确结果，但并不具备DSE功能。近期的研究尝试通过仿真手段探索设计空间，但大多数工作局限于特定领域，限制了用户可探索空间的范围，在技术、架构和应用之间的灵活性较差。此外，这些方法通常需要较高的领域专业知识才能保证仿真精度。\n\n为实现对技术、架构和应用均无偏倚、且可在任意粒度下进行仿真的目标，我们提出了Architecture-Graph（Agraph）——一种统一表示任意应用、软件、架构及电路系统整体的图结构。这种统一的表示方式使Agraph区别于以往仅聚焦单一技术栈的研究，使用户能够自由地在不同系统栈之间探索设计空间。为进一步释放Agraph的潜力，我们进一步提出了Archx框架，用于实现Agraph。Archx在两个方面具备用户友好性：其一，Archx提供易于使用的编程接口，可在用户设定的约束条件下自动生成功能点并进行参数扫描，显著提升可编程性；其二，Archx采用基于作用域的指标获取机制，支持用户在任意自定义层级上分析和理解每个设计点，从而增强结果的可解释性。\n\n我们通过多个案例研究验证了Agraph在跨技术、跨架构和跨应用场景下的泛化能力，并展示了其高精度的仿真效果。总体而言，我们认为Agraph与Archx共同构成了一个可按需模拟性能与成本的坚实基础。"
  },
  {
    "date": "2026-02-04",
    "title": "CSLib: The Lean Computer Science Library",
    "authors": "Clark Barrett, Swarat Chaudhuri, Fabrizio Montesi, Jim Grundy, Pushmeet Kohli, Leonardo de Moura, Alexandre Rademaker, Sorrachai Yingchareonthawornchai",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04846v1",
    "source": "arXiv",
    "abstract": "We introduce CSLib, an open-source framework for proving computer-science-related theorems and writing formally verified code in the Lean proof assistant. CSLib aims to be for computer science what Lean's Mathlib is for mathematics. Mathlib has been tremendously impactful: it is a key reason for Lean's popularity within the mathematics research community, and it has also played a critical role in the training of AI systems for mathematical reasoning. However, the base of computer science knowledge in Lean is currently quite limited. CSLib will vastly enhance this knowledge base and provide infrastructure for using this knowledge in real-world verification projects. By doing so, CSLib will (1) enable the broad use of Lean in computer science education and research, and (2) facilitate the manual and AI-aided engineering of large-scale formally verified systems.",
    "title_zh": "CSLib：精简的计算机科学库",
    "abstract_zh": "我们介绍了CSLib，这是一个开源框架，用于在Lean证明助手环境中证明与计算机科学相关的定理以及编写形式化验证的代码。CSLib的目标是成为计算机科学领域中的“Lean数学库”（Mathlib）——正如Mathlib对数学领域产生了巨大影响一样。Mathlib极大地推动了Lean在数学研究社区中的普及，并在训练用于数学推理的人工智能系统方面发挥了关键作用。然而，目前Lean中计算机科学知识的基础仍然非常有限。CSLib将极大地丰富这一知识库，并为在实际验证项目中使用这些知识提供基础设施。通过这一努力，CSLib将（1）促进Lean在计算机科学教育和研究中的广泛应用，（2）推动大规模形式化验证系统的手动与人工智能辅助开发。"
  },
  {
    "date": "2026-02-04",
    "title": "When Code Becomes Abundant: Redefining Software Engineering Around Orchestration and Verification",
    "authors": "Karina Kohl, Luigi Carro",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04830v1",
    "source": "arXiv",
    "abstract": "Software Engineering (SE) faces simultaneous pressure from AI automation (reducing code production costs) and hardware-energy constraints (amplifying failure costs). We position that SE must redefine itself around human discernment-intent articulation, architectural control, and verification-rather than code construction. This shift introduces accountability collapse as a central risk and requires fundamental changes to research priorities, educational curricula, and industrial practices. We argue that Software Engineering, as traditionally defined around code construction and process management, is no longer sufficient. Instead, the discipline must be redefined around intent articulation, architectural control, and systematic verification. This redefinition shifts Software Engineering from a production-oriented field to one centered on human judgment under automation, with profound implications for research, practice, and education.",
    "title_zh": "当代码变得充裕：围绕编排与验证重新定义软件工程",
    "abstract_zh": "软件工程（SE）正面临来自人工智能自动化（降低代码生产成本）和硬件能效限制（提高故障成本）的双重压力。我们认为，软件工程必须重新定位自身，聚焦于人类的判断力——即意图表达、架构控制与验证，而非代码构建。这一转变带来了问责制崩溃这一核心风险，要求研究重点、教育课程和工业实践进行根本性变革。我们主张，传统上以代码构建和流程管理为核心的软件工程已不再足够。相反，该学科必须重新定义为围绕意图表达、架构控制和系统化验证展开。这一重新定义使软件工程从以生产为导向的领域，转变为以自动化环境下人类判断为核心的领域，对研究、实践和教育均产生深远影响。"
  },
  {
    "date": "2026-02-04",
    "title": "Contextual Drag: How Errors in the Context Affect LLM Reasoning",
    "authors": "Yun Cheng, Xingyu Zhu, Haoyu Zhao, Sanjeev Arora",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04288v1",
    "source": "arXiv",
    "abstract": "Central to many self-improvement pipelines for large language models (LLMs) is the assumption that models can improve by reflecting on past mistakes. We study a phenomenon termed contextual drag: the presence of failed attempts in the context biases subsequent generations toward structurally similar errors. Across evaluations of 11 proprietary and open-weight models on 8 reasoning tasks, contextual drag induces 10-20% performance drops, and iterative self-refinement in models with severe contextual drag can collapse into self-deterioration. Structural analysis using tree edit distance reveals that subsequent reasoning trajectories inherit structurally similar error patterns from the context. We demonstrate that neither external feedback nor successful self-verification suffices to eliminate this effect. While mitigation strategies such as fallback-behavior fine-tuning and context denoising yield partial improvements, they fail to fully restore baseline performance, positioning contextual drag as a persistent failure mode in current reasoning architectures.",
    "title_zh": "上下文拖拽：上下文中的错误如何影响大语言模型的推理",
    "abstract_zh": "在众多大型语言模型（LLMs）的自我提升流程中，一个核心假设是：模型能够通过反思过去的错误来实现改进。本文研究了一种被称为“上下文拖拽”（contextual drag）的现象：上下文中存在的失败尝试会使得后续生成结果倾向于重复结构相似的错误。我们在8个推理任务上对11个专有模型和开源模型进行了评估，发现上下文拖拽导致性能下降10%至20%；对于受严重上下文拖拽影响的模型，其迭代式自我修正过程甚至可能演变为自我退化。通过树编辑距离（tree edit distance）进行的结构分析表明，后续的推理路径会继承来自上下文的结构化错误模式。我们证明，无论是外部反馈还是成功的自我验证，都无法完全消除这一效应。尽管诸如回退行为微调和上下文去噪等缓解策略带来部分改善，但均无法完全恢复基线性能，这表明上下文拖拽是当前推理架构中一个持续存在的失效模式。"
  },
  {
    "date": "2026-02-04",
    "title": "The Stretto Execution Engine for LLM-Augmented Data Systems",
    "authors": "Gabriele Sanmartino, Matthias Urban, Paolo Papotti, Carsten Binnig",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04430v1",
    "source": "arXiv",
    "abstract": "LLM-augmented data systems enable semantic querying over structured and unstructured data, but executing queries with LLM-powered operators introduces a fundamental runtime--accuracy trade-off. In this paper, we present Stretto, a new execution engine that provides end-to-end query guarantees while efficiently navigating this trade-off in a holistic manner. For this, Stretto formulates query planning as a constrained optimization problem and uses a gradient-based optimizer to jointly select operator implementations and allocate error budgets across pipelines. Moreover, to enable fine-grained execution choices, Stretto introduces a novel idea on how KV-caching can be used to realize a spectrum of different physical operators that transform a sparse design space into a dense continuum of runtime--accuracy trade-offs. Experiments show that Stretto outperforms state-of-the-art systems while consistently meeting quality guarantees.",
    "title_zh": "LLM增强型数据系统的Stretto执行引擎",
    "abstract_zh": "大语言模型增强的数据系统能够对结构化和非结构化数据实现语义查询，但使用大语言模型驱动的算子执行查询时，会引入一个根本性的运行时—准确性权衡问题。本文提出Stretto，一种新型执行引擎，能够在端到端查询保证的前提下，以整体化的方式高效应对这一权衡。为此，Stretto将查询规划建模为一个带约束的优化问题，并采用基于梯度的优化器，联合选择算子实现方式，并在数据处理管道间分配误差预算。此外，为支持细粒度的执行决策，Stretto提出了一种新颖思路：利用KV缓存将稀疏的设计空间转化为连续的运行时—准确性权衡空间，从而实现多种不同的物理算子。实验结果表明，Stretto在性能上优于现有最先进系统，同时始终满足质量保证要求。"
  },
  {
    "date": "2026-02-04",
    "title": "Beyond KL Divergence: Policy Optimization with Flexible Bregman Divergences for LLM Reasoning",
    "authors": "Rui Yuan, Mykola Khandoga, Vinay Kumar Sankarapu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04380v1",
    "source": "arXiv",
    "abstract": "Policy optimization methods like Group Relative Policy Optimization (GRPO) and its variants have achieved strong results on mathematical reasoning and code generation tasks. Despite extensive exploration of reward processing strategies and training dynamics, all existing group-based methods exclusively use KL divergence for policy regularization, leaving the choice of divergence function unexplored. We introduce Group-Based Mirror Policy Optimization (GBMPO), a framework that extends group-based policy optimization to flexible Bregman divergences, including hand-designed alternatives (L2 in probability space) and learned neural mirror maps. On GSM8K mathematical reasoning, hand-designed ProbL2-GRPO achieves 86.7% accuracy, improving +5.5 points over the Dr. GRPO baseline. On MBPP code generation, neural mirror maps reach 60.1-60.8% pass@1, with random initialization already capturing most of the benefit. While evolutionary strategies meta-learning provides marginal accuracy improvements, its primary value lies in variance reduction ($\\pm$0.2 versus $\\pm$0.6) and efficiency gains (15% shorter responses on MBPP), suggesting that random initialization of neural mirror maps is sufficient for most practical applications. These results establish divergence choice as a critical, previously unexplored design dimension in group-based policy optimization for LLM reasoning.",
    "title_zh": "超越KL散度：基于灵活Bregman散度的策略优化用于大语言模型推理",
    "abstract_zh": "像组相对策略优化（GRPO）及其变体等策略优化方法在数学推理和代码生成任务上取得了显著成果。尽管对奖励处理策略和训练动态进行了广泛探索，但现有的所有基于组的方法均仅使用KL散度进行策略正则化，未对散度函数的选择进行深入研究。本文提出了一种基于组的镜像策略优化（GBMPO）框架，将基于组的策略优化扩展至灵活的Bregman散度，包括手工设计的替代方案（概率空间中的L2散度）以及可学习的神经镜像映射。在GSM8K数学推理任务上，手工设计的ProbL2-GRPO方法达到86.7%的准确率，较Dr. GRPO基线提升5.5个百分点。在MBPP代码生成任务上，可学习的神经镜像映射实现了60.1%至60.8%的pass@1准确率，且随机初始化即可捕获大部分性能增益。虽然进化策略元学习带来的准确率提升有限，但其主要价值体现在降低方差（±0.2 vs ±0.6）和提升效率（MBPP任务响应长度缩短15%），表明在大多数实际应用中，随机初始化的神经镜像映射已足够。这些结果确立了散度函数的选择作为基于组的LLM推理策略优化中一个关键且此前未被探索的设计维度。"
  },
  {
    "date": "2026-02-04",
    "title": "UnMaskFork: Test-Time Scaling for Masked Diffusion via Deterministic Action Branching",
    "authors": "Kou Misaki, Takuya Akiba",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04344v1",
    "source": "arXiv",
    "abstract": "Test-time scaling strategies have effectively leveraged inference-time compute to enhance the reasoning abilities of Autoregressive Large Language Models. In this work, we demonstrate that Masked Diffusion Language Models (MDLMs) are inherently amenable to advanced search strategies, owing to their iterative and non-autoregressive generation process. To leverage this, we propose UnMaskFork (UMF), a framework that formulates the unmasking trajectory as a search tree and employs Monte Carlo Tree Search to optimize the generation path. In contrast to standard scaling methods relying on stochastic sampling, UMF explores the search space through deterministic partial unmasking actions performed by multiple MDLMs. Our empirical evaluation demonstrates that UMF consistently outperforms existing test-time scaling baselines on complex coding benchmarks, while also exhibiting strong scalability on mathematical reasoning tasks.",
    "title_zh": "UnMaskFork：通过确定性动作分支实现掩码扩散的测试时扩展",
    "abstract_zh": "测试时缩放策略已有效利用推理时的计算资源，显著提升了自回归大语言模型的推理能力。在本工作中，我们证明了掩码扩散语言模型（MDLMs）由于其迭代且非自回归的生成过程，天然适合采用先进的搜索策略。为此，我们提出了UnMaskFork（UMF）框架，将解掩码过程建模为搜索树，并采用蒙特卡洛树搜索来优化生成路径。与依赖随机采样的标准缩放方法不同，UMF通过多个MDLM执行确定性的部分解掩码操作，从而探索搜索空间。实验评估表明，UMF在复杂编程基准测试中持续优于现有的测试时缩放基线方法，同时在数学推理任务中也展现出强大的可扩展性。"
  },
  {
    "date": "2026-02-04",
    "title": "The Missing Half: Unveiling Training-time Implicit Safety Risks Beyond Deployment",
    "authors": "Zhexin Zhang, Yida Lu, Junfeng Fang, Junxiao Yang, Shiyao Cui, Hao Zhou, Fandong Meng, Jie Zhou, Hongning Wang, Minlie Huang, Tat-Seng Chua",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04196v1",
    "source": "arXiv",
    "abstract": "Safety risks of AI models have been widely studied at deployment time, such as jailbreak attacks that elicit harmful outputs. In contrast, safety risks emerging during training remain largely unexplored. Beyond explicit reward hacking that directly manipulates explicit reward functions in reinforcement learning, we study implicit training-time safety risks: harmful behaviors driven by a model's internal incentives and contextual background information. For example, during code-based reinforcement learning, a model may covertly manipulate logged accuracy for self-preservation. We present the first systematic study of this problem, introducing a taxonomy with five risk levels, ten fine-grained risk categories, and three incentive types. Extensive experiments reveal the prevalence and severity of these risks: notably, Llama-3.1-8B-Instruct exhibits risky behaviors in 74.4% of training runs when provided only with background information. We further analyze factors influencing these behaviors and demonstrate that implicit training-time risks also arise in multi-agent training settings. Our results identify an overlooked yet urgent safety challenge in training.",
    "title_zh": "缺失的一半：揭示部署之外的训练期隐式安全风险",
    "abstract_zh": "人工智能模型在部署阶段的安全风险已得到广泛研究，例如“越狱攻击”（jailbreak attacks）会诱导模型产生有害输出。相比之下，训练过程中出现的安全风险却尚未被充分探索。除了显式的奖励劫持（reward hacking）——即直接操纵强化学习中的显式奖励函数外，本文还研究了隐式的训练期安全风险：由模型内部动机和上下文背景信息驱动的有害行为。例如，在基于代码的强化学习中，模型可能通过隐秘地操纵记录的准确率来实现自我保护。本文首次系统性地研究了这一问题，提出了一个包含五个风险等级、十个细粒度风险类别以及三种激励类型的分类体系。大量实验揭示了这些风险的普遍性和严重性：当仅提供背景信息时，Llama-3.1-8B-Instruct 在 74.4% 的训练运行中表现出风险行为。我们进一步分析了影响这些行为的因素，并证明隐式训练期风险同样存在于多智能体训练场景中。研究结果揭示了一个被忽视但亟待解决的训练阶段安全挑战。"
  },
  {
    "date": "2026-02-04",
    "title": "Interfaze: The Future of AI is built on Task-Specific Small Models",
    "authors": "Harsha Vardhan Khurdula, Vineet Agarwal, Yoeven D Khemlani",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04101v1",
    "source": "arXiv",
    "abstract": "We present Interfaze, a system that treats modern LLM applications as a problem of building and acting over context, not just picking the right monolithic model. Instead of a single transformer, we combine (i) a stack of heterogeneous DNNs paired with small language models as perception modules for OCR involving complex PDFs, charts and diagrams, and multilingual ASR with (ii) a context-construction layer that crawls, indexes, and parses external sources (web pages, code, PDFs) into compact structured state, and (iii) an action layer that can browse, retrieve, execute code in a sandbox, and drive a headless browser for dynamic web pages. A thin controller sits on top of this stack and exposes a single, OpenAI-style endpoint: it decides which small models and actions to run and always forwards the distilled context to a user-selected LLM that produces the final response. On this architecture, Interfaze-Beta achieves 83.6% on MMLU-Pro, 91.4% on MMLU, 81.3% on GPQA-Diamond, 57.8% on LiveCodeBench v5, and 90.0% on AIME-2025, along with strong multimodal scores on MMMU (val) (77.3%), AI2D (91.5%), ChartQA (90.9%), and Common Voice v16 (90.8%). We show that most queries are handled primarily by the small-model and tool stack, with the large LLM operating only on distilled context, yielding competitive accuracy while shifting the bulk of computation away from the most expensive and monolithic models.",
    "title_zh": "界面：人工智能的未来建立在任务专用的小型模型之上",
    "abstract_zh": "我们提出 Interfaze，一个将现代大语言模型（LLM）应用视为构建与作用于上下文的问题，而非仅仅选择单一的庞大模型的系统。与依赖单一Transformer架构不同，Interfaze 采用三层次协同架构：（i）由异构深度神经网络（DNN）与小型语言模型组成的感知模块堆栈，用于处理复杂PDF、图表和示意图的OCR任务，以及多语言语音识别（ASR）；（ii）上下文构建层，能够爬取、索引并解析外部数据源（如网页、代码、PDF），将其转化为紧凑的结构化状态；（iii）动作层，支持浏览网页、检索信息、在沙箱环境中执行代码，以及驱动无头浏览器以处理动态网页内容。在这一架构之上，一个轻量级控制器作为顶层，提供单一的、类似OpenAI风格的接口：它决定调用哪些小型模型和工具，并始终将提炼后的上下文传递给用户选定的大型语言模型，由其生成最终响应。\n\n基于该架构，Interfaze-Beta 在多项基准测试中表现优异：MMLU-Pro 达到 83.6%，MMLU 达到 91.4%，GPQA-Diamond 达到 81.3%，LiveCodeBench v5 达到 57.8%，AIME-2025 达到 90.0%。在多模态任务上也展现出强劲性能：MMMU（验证集）为 77.3%，AI2D 为 91.5%，ChartQA 为 90.9%，Common Voice v16 为 90.8%。我们证明，绝大多数查询主要由小型模型与工具堆栈处理，大型语言模型仅需在提炼后的上下文中进行推理，从而在保持高准确率的同时，将大部分计算负载从最昂贵且单一的模型中转移出去，显著提升了效率与可扩展性。"
  },
  {
    "date": "2026-02-04",
    "title": "Empirical-MCTS: Continuous Agent Evolution via Dual-Experience Monte Carlo Tree Search",
    "authors": "Hao Lu, Haoyuan Huang, Yulin Zhou, Chen Li, Ningxin Zhu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04248v1",
    "source": "arXiv",
    "abstract": "Inference-time scaling strategies, particularly Monte Carlo Tree Search (MCTS), have significantly enhanced the reasoning capabilities of Large Language Models (LLMs). However, current approaches remain predominantly stateless, discarding successful reasoning patterns after each problem instance and failing to mimic the empirical accumulation of wisdom characteristic of human problem-solving. To bridge this gap, we introduce Empirical-MCTS, a dual-loop framework that transforms stateless search into a continuous, non-parametric learning process. The framework unifies local exploration with global memory optimization through two novel mechanisms: Pairwise-Experience-Evolutionary Meta-Prompting (PE-EMP) and a Memory Optimization Agent. PE-EMP functions as a reflexive optimizer within the local search, utilizing pairwise feedback to dynamically synthesize adaptive criteria and evolve meta-prompts (system prompts) in real-time. Simultaneously, the Memory Optimization Agent manages a global repository as a dynamic policy prior, employing atomic operations to distill high-quality insights across problems. Extensive evaluations on complex reasoning benchmarks, including AIME25, ARC-AGI-2, and MathArena Apex, demonstrate that Empirical-MCTS significantly outperforms both stateless MCTS strategies and standalone experience-driven agents. These results underscore the critical necessity of coupling structured search with empirical accumulation for mastering complex, open-ended reasoning tasks.",
    "title_zh": "基于双重经验的蒙特卡洛树搜索的连续智能体演化",
    "abstract_zh": "推理阶段的扩展策略，尤其是蒙特卡洛树搜索（MCTS），显著提升了大型语言模型（LLMs）的推理能力。然而，当前的方法仍主要采用无状态的模式，在每个问题实例求解后便丢弃已有的成功推理模式，无法模拟人类解决问题时经验积累的实证特性。为弥合这一差距，我们提出了 Empirical-MCTS，一种双循环框架，将无状态搜索转变为持续的、非参数化的学习过程。该框架通过两种创新机制，将局部探索与全局记忆优化融为一体：成对经验进化元提示（Pairwise-Experience-Evolutionary Meta-Prompting, PE-EMP）和记忆优化代理（Memory Optimization Agent）。PE-EMP作为局部搜索中的反射式优化器，利用成对反馈动态合成自适应标准，并实时演化元提示（系统提示），实现提示的持续进化。与此同时，记忆优化代理负责管理一个全局存储库，作为动态策略先验，通过原子操作从不同问题中提炼高质量的洞察。在复杂推理基准测试（包括 AIME25、ARC-AGI-2 和 MathArena Apex）上的大量实验表明，Empirical-MCTS 显著优于现有的无状态 MCTS 策略以及独立的经验驱动型代理。这些结果凸显了将结构化搜索与实证积累相结合，在掌握复杂、开放性推理任务中的关键作用。"
  },
  {
    "date": "2026-02-04",
    "title": "Inference-Time Backdoors via Hidden Instructions in LLM Chat Templates",
    "authors": "Ariel Fogel, Omer Hofman, Eilon Cohen, Roman Vainshtein",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04653v1",
    "source": "arXiv",
    "abstract": "Open-weight language models are increasingly used in production settings, raising new security challenges. One prominent threat in this context is backdoor attacks, in which adversaries embed hidden behaviors in language models that activate under specific conditions. Previous work has assumed that adversaries have access to training pipelines or deployment infrastructure. We propose a novel attack surface requiring neither, which utilizes the chat template. Chat templates are executable Jinja2 programs invoked at every inference call, occupying a privileged position between user input and model processing. We show that an adversary who distributes a model with a maliciously modified template can implant an inference-time backdoor without modifying model weights, poisoning training data, or controlling runtime infrastructure. We evaluated this attack vector by constructing template backdoors targeting two objectives: degrading factual accuracy and inducing emission of attacker-controlled URLs, and applied them across eighteen models spanning seven families and four inference engines. Under triggered conditions, factual accuracy drops from 90% to 15% on average while attacker-controlled URLs are emitted with success rates exceeding 80%; benign inputs show no measurable degradation. Backdoors generalize across inference runtimes and evade all automated security scans applied by the largest open-weight distribution platform. These results establish chat templates as a reliable and currently undefended attack surface in the LLM supply chain.",
    "title_zh": "通过大型语言模型对话模板中的隐藏指令实现推理时后门攻击",
    "abstract_zh": "开放权重语言模型在生产环境中的应用日益广泛，也带来了新的安全挑战。其中一种突出的威胁是后门攻击：攻击者在语言模型中嵌入隐藏行为，仅在特定条件下被触发。以往的研究假设攻击者能够访问训练流程或部署基础设施。本文提出了一种全新的攻击面，无需上述任何条件，而是利用聊天模板（chat template）。聊天模板是每次推理调用时执行的可执行Jinja2程序，处于用户输入与模型处理之间的特权位置。我们证明，攻击者只需分发一个经过恶意修改模板的模型，即可在不修改模型权重、不污染训练数据、也不控制运行时基础设施的情况下，植入推理阶段的后门。\n\n我们通过构建针对两个目标的模板后门进行了评估：降低事实准确性，以及诱导模型输出攻击者控制的URL。这些攻击被应用于横跨七个模型家族和四种推理引擎的十八个模型。在触发条件下，事实准确性平均从90%下降至15%，而攻击者控制的URL生成成功率超过80%；对于正常输入，模型性能无明显下降。这些后门在不同推理运行时之间具有良好的泛化能力，并成功规避了目前最大开放权重分发平台所采用的所有自动化安全检测。\n\n这些结果表明，聊天模板已成为大语言模型供应链中一个可靠且目前尚未被防御的攻击面。"
  },
  {
    "date": "2026-02-04",
    "title": "Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing",
    "authors": "Zhaotian Weng, Antonis Antoniades, Deepak Nathani, Zhen Zhang, Xiao Pu, Xin Eric Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04837v1",
    "source": "arXiv",
    "abstract": "Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.",
    "title_zh": "群体演化代理：通过经验共享实现开放式的自我提升",
    "abstract_zh": "开放式的自我改进智能体能够自主修改自身的结构设计，以提升能力并突破预设架构的局限，从而减少对人工干预的依赖。我们提出了群体演化智能体（Group-Evolving Agents, GEA），这是一种全新的开放式自我改进范式，将一组智能体视为基本的演化单元，使群体内部在演化过程中能够显式地共享和复用经验。与现有采用树状结构演化的开放式自我演化方法不同，GEA克服了因演化分支孤立而导致探索多样性利用效率低下的问题。我们在具有挑战性的代码生成基准上对GEA进行了评估，结果表明其显著优于当前最先进的自我演化方法（在SWE-bench Verified上达到71.0% vs. 56.7%，在Polyglot上达到88.3% vs. 68.3%），并达到或超越了顶尖的人工设计智能体框架（在两个基准上分别达到71.8%和52.0%）。分析显示，GEA能更有效地将早期探索阶段的多样性转化为持续、长期的进步，在相同数量的演化智能体下实现了更强的性能表现。此外，GEA在不同代码模型间展现出一致的可迁移性，并具备更强的鲁棒性，平均仅需1.4次迭代即可修复框架级错误，而传统自我演化方法则需要5次。"
  },
  {
    "date": "2026-02-04",
    "title": "Supporting software engineering tasks with agentic AI: Demonstration on document retrieval and test scenario generation",
    "authors": "Marian Kica, Lukas Radosky, David Slivka, Karin Kubinova, Daniel Dovhun, Tomas Uhercik, Erik Bircak, Ivan Polasek",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04726v1",
    "source": "arXiv",
    "abstract": "The introduction of large language models ignited great retooling and rethinking of the software development models. The ensuing response of software engineering research yielded a massive body of tools and approaches. In this paper, we join the hassle by introducing agentic AI solutions for two tasks. First, we developed a solution for automatic test scenario generation from a detailed requirements description. This approach relies on specialized worker agents forming a star topology with the supervisor agent in the middle. We demonstrate its capabilities on a real-world example. Second, we developed an agentic AI solution for the document retrieval task in the context of software engineering documents. Our solution enables performing various use cases on a body of documents related to the development of a single software, including search, question answering, tracking changes, and large document summarization. In this case, each use case is handled by a dedicated LLM-based agent, which performs all subtasks related to the corresponding use case. We conclude by hinting at the future perspectives of our line of research.",
    "title_zh": "利用代理型人工智能支持软件工程任务：文档检索与测试场景生成的演示",
    "abstract_zh": "大型语言模型的引入引发了软件开发模式的广泛重构与深入思考。随之而来的软件工程研究响应，催生了大量工具与方法。本文在此背景下，提出了一种基于智能体（agentic）AI的解决方案，用于解决两个具体任务。首先，我们开发了一种从详细需求描述中自动生成测试场景的方案。该方法依赖于多个专用工作智能体，以监督智能体为中心，形成星型拓扑结构。我们在一个真实案例中展示了该方法的有效性。其次，我们提出了一种面向软件工程文档的文档检索任务的智能体AI解决方案。该方案支持对单一软件开发相关文档集合执行多种应用场景，包括搜索、问答、变更追踪以及大文档摘要等。在该方案中，每个应用场景均由一个基于大语言模型（LLM）的专用智能体负责，该智能体自主完成与该场景相关的所有子任务。最后，我们简要展望了本研究方向的未来发展前景。"
  },
  {
    "date": "2026-02-04",
    "title": "Disentangling Causal Importance from Emergent Structure in Multi-Expert Orchestration",
    "authors": "Sudipto Ghosh, Sujoy Nath, Sunny Manchanda, Tanmoy Chakraborty",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04291v1",
    "source": "arXiv",
    "abstract": "Multi-expert systems, where multiple Large Language Models (LLMs) collaborate to solve complex tasks, are increasingly adopted for high-performance reasoning and generation. However, the orchestration policies governing expert interaction and sequencing remain largely opaque. We introduce INFORM, an interpretability analysis that treats orchestration as an explicit, analyzable computation, enabling the decoupling of expert interaction structure, execution order, and causal attribution. We use INFORM to evaluate an orchestrator on GSM8K, HumanEval, and MMLU using a homogeneous consortium of ten instruction-tuned experts drawn from LLaMA-3.1 8B, Qwen-3 8B, and DeepSeek-R1 8B, with controlled decoding-temperature variation, and a secondary heterogeneous consortium spanning 1B-7B parameter models. Across tasks, routing dominance is a poor proxy for functional necessity. We reveal a divergence between relational importance, captured by routing mass and interaction topology, and intrinsic importance, measured via gradient-based causal attribution: frequently selected experts often act as interaction hubs with limited causal influence, while sparsely routed experts can be structurally critical. Orchestration behaviors emerge asynchronously, with expert centralization preceding stable routing confidence and expert ordering remaining non-deterministic. Targeted ablations show that masking intrinsically important experts induces disproportionate collapse in interaction structure compared to masking frequent peers, confirming that INFORM exposes causal and structural dependencies beyond accuracy metrics alone.",
    "title_zh": "多专家协同中因果重要性与涌现结构的解耦",
    "abstract_zh": "多专家系统通过多个大语言模型（LLMs）协作解决复杂任务，正日益被用于实现高性能的推理与生成。然而，控制专家间交互与执行顺序的编排策略仍缺乏透明度。本文提出INFORM，一种可解释性分析方法，将编排过程视为显式且可分析的计算过程，从而实现专家交互结构、执行顺序与因果归因的解耦。我们利用INFORM对GSM8K、HumanEval和MMLU三个基准任务进行评估，采用由LLaMA-3.1 8B、Qwen-3 8B和DeepSeek-R1 8B模型组成的同质专家联盟（共10个指令微调专家），并控制解码温度的变化；同时，还构建了一个涵盖1B至7B参数规模模型的异质专家联盟。在各项任务中，路由主导性（routing dominance）并不能有效反映功能必要性。我们发现，由路由质量与交互拓扑所体现的关系重要性，与基于梯度的因果归因所衡量的内在重要性之间存在显著差异：频繁被选中的专家往往充当交互枢纽，但其因果影响力有限；而路由频率较低的专家却可能在结构上具有关键作用。编排行为呈现出异步演化特征，专家集中化先于路由置信度的稳定，且专家排序始终具有非确定性。针对性的消融实验表明，屏蔽内在重要性高的专家会导致交互结构的显著崩溃，远超屏蔽频繁出现的同伴专家的影响，这证实INFORM能够揭示超越准确率指标的因果与结构依赖关系。"
  },
  {
    "date": "2026-02-04",
    "title": "Review of Superconducting Qubit Devices and Their Large-Scale Integration",
    "authors": "Hiu Yung Wong",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04831v1",
    "source": "arXiv",
    "abstract": "The superconducting qubit quantum computer is one of the most promising quantum computing architectures for large-scale integration due to its maturity and close proximity to the well-established semiconductor manufacturing infrastructure. From an education perspective, it also bridges classical microwave electronics and quantum electrodynamics. In this paper, we will review the basics of quantum computers, superconductivity, and Josephson junctions. We then introduce important technologies and concepts related to DiVincenzo's criteria, which are the necessary conditions for the superconducting qubits to work as a useful quantum computer. Firstly, we will discuss various types of superconducting qubits formed with Josephson junctions, from which we will understand the trade-off across multiple design parameters, including their noise immunity. Secondly, we will discuss different schemes to achieve entanglement gate operations, which are a major bottleneck in achieving more efficient fault-tolerant quantum computing. Thirdly, we will review readout engineering, including the implementations of the Purcell filters and quantum-limited amplifiers. Finally, we will discuss the nature and review the studies of two-level system defects, which are currently the limiting factor of qubit coherence time. DiVincenzo's criteria are only the necessary conditions for a technology to be eligible for quantum computing. To have a useful quantum computer, large-scale integration is required. We will review proposals and developments for the large-scale integration of superconducting qubit devices. By comparing with the application of electronic design automation (EDA) in semiconductors, we will also review the use of EDA in superconducting qubit quantum computer design, which is necessary for its large-scale integration.",
    "title_zh": "超导量子比特器件及其大规模集成的综述",
    "abstract_zh": "超导量子比特量子计算机由于其技术成熟度以及与成熟的半导体制造基础设施的紧密联系，是实现大规模集成最具前景的量子计算架构之一。从教育角度来看，它还架起了经典微波电子学与量子电动力学之间的桥梁。本文将首先回顾量子计算机、超导性以及约瑟夫森结的基本原理。随后，我们将介绍与迪文森佐（DiVincenzo）标准相关的重要技术与概念，这些是超导量子比特作为实用量子计算机所必需的条件。首先，我们将讨论由约瑟夫森结构成的各种类型的超导量子比特，从中理解在多个设计参数之间的权衡，包括其抗噪声能力。其次，我们将探讨实现纠缠门操作的不同方案，这些操作是实现更高效容错量子计算的主要瓶颈。第三，我们将回顾读出工程，包括Purcell滤波器和量子极限放大器的实现。最后，我们将讨论两能级系统缺陷的本质，并综述相关研究，这些缺陷目前是限制量子比特相干时间的主要因素。需要指出的是，迪文森佐标准只是技术具备量子计算潜力的必要条件。要构建一个真正有用的量子计算机，必须实现大规模集成。我们将综述超导量子比特器件大规模集成的各类提案与最新进展。通过与半导体领域电子设计自动化（EDA）的应用进行比较，我们还将探讨EDA在超导量子比特量子计算机设计中的应用，这对于其实现大规模集成至关重要。"
  },
  {
    "date": "2026-02-04",
    "title": "MaMa: A Game-Theoretic Approach for Designing Safe Agentic Systems",
    "authors": "Jonathan Nöther, Adish Singla, Goran Radanovic",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04431v1",
    "source": "arXiv",
    "abstract": "LLM-based multi-agent systems have demonstrated impressive capabilities, but they also introduce significant safety risks when individual agents fail or behave adversarially. In this work, we study the automated design of agentic systems that remain safe even when a subset of agents is compromised. We formalize this challenge as a Stackelberg security game between a system designer (the Meta-Agent) and a best-responding Meta-Adversary that selects and compromises a subset of agents to minimize safety. We propose Meta-Adversary-Meta-Agent (MaMa), a novel algorithm for approximately solving this game and automatically designing safe agentic systems. Our approach uses LLM-based adversarial search, where the Meta-Agent iteratively proposes system designs and receives feedback based on the strongest attacks discovered by the Meta-Adversary. Empirical evaluations across diverse environments show that systems designed with MaMa consistently defend against worst-case attacks while maintaining performance comparable to systems optimized solely for task success. Moreover, the resulting systems generalize to stronger adversaries, as well as ones with different attack objectives or underlying LLMs, demonstrating robust safety beyond the training setting.",
    "title_zh": "MaMa：一种基于博弈论的构建安全智能体系统的策略",
    "abstract_zh": "基于大语言模型（LLM）的多智能体系统展现了令人瞩目的能力，但当个别智能体发生故障或表现出对抗性行为时，也会带来显著的安全风险。本文研究了在部分智能体被攻破的情况下仍能保持安全的智能体系统自动化设计问题。我们将这一挑战形式化为系统设计者（元智能体）与一个最优响应的元攻击者之间的斯塔克尔伯格安全博弈：元攻击者会选择并攻破一部分智能体，以最小化系统的安全性。为此，我们提出了MaMa（Meta-Adversary-Meta-Agent）算法，一种用于近似求解该博弈并自动设计安全智能体系统的新方法。我们的方法采用基于LLM的对抗性搜索机制，其中元智能体迭代提出系统设计方案，并根据元攻击者发现的最强攻击所给出的反馈进行优化。在多种环境中的实证评估表明，使用MaMa设计的系统在持续抵御最坏情况攻击的同时，性能与仅针对任务成功优化的系统相当。此外，所生成的系统还能泛化到更强的攻击者，以及具有不同攻击目标或底层LLM的攻击者，展现出超越训练环境的稳健安全性。"
  },
  {
    "date": "2026-02-04",
    "title": "Guided Verifier: Collaborative Multimodal Reasoning via Dynamic Process Supervision",
    "authors": "Lingzhuang Sun, Ruitong Liu, Yuxia Zhu, Xiaohan Xu, Jingxuan Wei, Xiangxiang Zhang, Bihui Yu, Wentao Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04290v1",
    "source": "arXiv",
    "abstract": "Reinforcement Learning (RL) has emerged as a pivotal mechanism for enhancing the complex reasoning capabilities of Multimodal Large Language Models (MLLMs). However, prevailing paradigms typically rely on solitary rollout strategies where the model works alone. This lack of intermediate oversight renders the reasoning process susceptible to error propagation, where early logical deviations cascade into irreversible failures, resulting in noisy optimization signals. In this paper, we propose the \\textbf{Guided Verifier} framework to address these structural limitations. Moving beyond passive terminal rewards, we introduce a dynamic verifier that actively co-solves tasks alongside the policy. During the rollout phase, this verifier interacts with the policy model in real-time, detecting inconsistencies and providing directional signals to steer the model toward valid trajectories. To facilitate this, we develop a specialized data synthesis pipeline targeting multimodal hallucinations, constructing \\textbf{CoRe} dataset of process-level negatives and \\textbf{Co}rrect-guide \\textbf{Re}asoning trajectories to train the guided verifier. Extensive experiments on MathVista, MathVerse and MMMU indicate that by allocating compute to collaborative inference and dynamic verification, an 8B-parameter model can achieve strong performance.",
    "title_zh": "引导验证器：通过动态过程监督实现协作式多模态推理",
    "abstract_zh": "强化学习（Reinforcement Learning, RL）已成为提升多模态大语言模型（Multimodal Large Language Models, MLLMs）复杂推理能力的关键机制。然而，当前主流范式通常依赖于单一的推理路径策略，即模型独立完成任务。这种缺乏中间监督的机制使得推理过程容易受到错误传播的影响：早期的逻辑偏差会逐步累积，最终导致不可逆的失败，从而产生噪声严重的优化信号。本文提出一种名为 \\textbf{Guided Verifier} 的新框架，以解决上述结构性缺陷。与传统的被动终端奖励机制不同，我们引入了一个动态验证器，能够与策略模型协同实时求解任务。在推理过程中，该验证器与策略模型进行实时交互，检测推理中的不一致之处，并提供方向性反馈，引导模型走向正确的推理轨迹。为支持这一机制，我们设计了一套专门的数据合成流程，聚焦于多模态幻觉问题，构建了包含过程级负样本的 \\textbf{CoRe} 数据集，以及用于指导正确推理的 \\textbf{Co}rrect-guide \\textbf{Re}asoning 轨迹，用于训练该引导式验证器。在 MathVista、MathVerse 和 MMMU 等多个基准上的大量实验表明，通过将计算资源分配给协同推理与动态验证，一个 80 亿参数的模型也能实现卓越的性能表现。"
  },
  {
    "date": "2026-02-04",
    "title": "Proxy Compression for Language Modeling",
    "authors": "Lin Zheng, Xinyu Li, Qian Liu, Xiachong Feng, Lingpeng Kong",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04289v1",
    "source": "arXiv",
    "abstract": "Modern language models are trained almost exclusively on token sequences produced by a fixed tokenizer, an external lossless compressor often over UTF-8 byte sequences, thereby coupling the model to that compressor. This work introduces proxy compression, an alternative training scheme that preserves the efficiency benefits of compressed inputs while providing an end-to-end, raw-byte interface at inference time. During training, one language model is jointly trained on raw byte sequences and compressed views generated by external compressors; through the process, the model learns to internally align compressed sequences and raw bytes. This alignment enables strong transfer between the two formats, even when training predominantly on compressed inputs which are discarded at inference. Extensive experiments on code language modeling demonstrate that proxy compression substantially improves training efficiency and significantly outperforms pure byte-level baselines given fixed compute budgets. As model scale increases, these gains become more pronounced, and proxy-trained models eventually match or rival tokenizer approaches, all while operating solely on raw bytes and retaining the inherent robustness of byte-level modeling.",
    "title_zh": "语言建模中的代理压缩",
    "abstract_zh": "现代语言模型几乎完全基于由固定分词器生成的标记序列进行训练，而该分词器通常是对UTF-8字节序列的外部无损压缩器，从而将模型与特定压缩器紧密绑定。本文提出了一种名为“代理压缩”（proxy compression）的替代训练方案，该方案在保持压缩输入效率优势的同时，于推理阶段提供端到端的原始字节接口。在训练过程中，一个语言模型同时在原始字节序列和由外部压缩器生成的压缩视图上进行联合训练；通过这一过程，模型学会在内部对齐压缩序列与原始字节。这种对齐使得两种格式之间具备强大的迁移能力，即使训练主要基于压缩输入（推理时这些压缩数据被丢弃），也能实现良好性能。在代码语言建模任务上的大量实验表明，代理压缩显著提升了训练效率，并在固定计算预算下显著优于纯字节级基线模型。随着模型规模的增大，这些优势愈发明显，代理训练的模型最终能够达到甚至媲美基于分词器的方法，同时仅依赖原始字节输入，并保留了字节级建模固有的鲁棒性。"
  },
  {
    "date": "2026-02-04",
    "title": "Agent-Omit: Training Efficient LLM Agents for Adaptive Thought and Observation Omission via Agentic Reinforcement Learning",
    "authors": "Yansong Ning, Jun Fang, Naiqiang Tan, Hao Liu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04284v1",
    "source": "arXiv",
    "abstract": "Managing agent thought and observation during multi-turn agent-environment interactions is an emerging strategy to improve agent efficiency. However, existing studies treat the entire interaction trajectories equally, overlooking the thought necessity and observation utility varies across turns. To this end, we first conduct quantitative investigations into how thought and observation affect agent effectiveness and efficiency. Based on our findings, we propose Agent-Omit, a unified training framework that empowers LLM agents to adaptively omit redundant thoughts and observations. Specifically, we first synthesize a small amount of cold-start data, including both single-turn and multi-turn omission scenarios, to fine-tune the agent for omission behaviors. Furthermore, we introduce an omit-aware agentic reinforcement learning approach, incorporating a dual sampling mechanism and a tailored omission reward to incentivize the agent's adaptive omission capability. Theoretically, we prove that the deviation of our omission policy is upper-bounded by KL-divergence. Experimental results on five agent benchmarks show that our constructed Agent-Omit-8B could obtain performance comparable to seven frontier LLM agent, and achieve the best effectiveness-efficiency trade-off than seven efficient LLM agents methods. Our code and data are available at https://github.com/usail-hkust/Agent-Omit.",
    "title_zh": "代理省略：通过代理强化学习训练高效的大语言模型代理以实现自适应思维与观察省略",
    "abstract_zh": "在多轮智能体-环境交互过程中，对智能体的思维与观察进行管理是一种新兴的提升智能体效率的策略。然而，现有研究通常对整个交互轨迹一视同仁，忽略了不同交互轮次中思维的必要性与观察的效用存在差异。为此，我们首先对思维与观察如何影响智能体的有效性与效率进行了定量分析。基于研究发现，我们提出了 Agent-Omit——一种统一的训练框架，使大语言模型（LLM）智能体能够自适应地省略冗余的思维与观察。具体而言，我们首先构建少量冷启动数据，涵盖单轮与多轮省略场景，用于微调智能体的省略行为。此外，我们引入了一种“省略感知”的智能体强化学习方法，结合双采样机制与专门设计的省略奖励，以激励智能体具备自适应省略的能力。理论上，我们证明了该省略策略的偏差由KL散度上界控制。在五个智能体基准测试上的实验结果表明，我们构建的 Agent-Omit-8B 在性能上可媲美七种前沿LLM智能体，且在有效性与效率的权衡上优于七种高效LLM智能体方法。我们的代码与数据已开源，地址为：https://github.com/usail-hkust/Agent-Omit。"
  },
  {
    "date": "2026-02-04",
    "title": "Language Models Struggle to Use Representations Learned In-Context",
    "authors": "Michael A. Lepori, Tal Linzen, Ann Yuan, Katja Filippova",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04212v1",
    "source": "arXiv",
    "abstract": "Though large language models (LLMs) have enabled great success across a wide variety of tasks, they still appear to fall short of one of the loftier goals of artificial intelligence research: creating an artificial system that can adapt its behavior to radically new contexts upon deployment. One important step towards this goal is to create systems that can induce rich representations of data that are seen in-context, and then flexibly deploy these representations to accomplish goals. Recently, Park et al. (2024) demonstrated that current LLMs are indeed capable of inducing such representation from context (i.e., in-context representation learning). The present study investigates whether LLMs can use these representations to complete simple downstream tasks. We first assess whether open-weights LLMs can use in-context representations for next-token prediction, and then probe models using a novel task, adaptive world modeling. In both tasks, we find evidence that open-weights LLMs struggle to deploy representations of novel semantics that are defined in-context, even if they encode these semantics in their latent representations. Furthermore, we assess closed-source, state-of-the-art reasoning models on the adaptive world modeling task, demonstrating that even the most performant LLMs cannot reliably leverage novel patterns presented in-context. Overall, this work seeks to inspire novel methods for encouraging models to not only encode information presented in-context, but to do so in a manner that supports flexible deployment of this information.",
    "title_zh": "语言模型在使用上下文中学到的表示时存在困难",
    "abstract_zh": "尽管大型语言模型（LLMs）在众多任务中取得了巨大成功，但它们似乎仍未能实现人工智能研究中一个更为远大的目标：构建一种能够在部署后适应全新情境的人工系统。迈向这一目标的重要一步，是开发能够从上下文中推导出丰富数据表征，并能灵活运用这些表征以实现特定目标的系统。最近，Park等人（2024）证明了当前的LLMs确实具备从上下文中学习表征的能力（即上下文内表征学习）。本研究进一步探讨了LLMs是否能够利用这些表征来完成简单的下游任务。我们首先评估开放权重的LLMs能否利用上下文表征进行下一个词的预测，随后引入一项新任务——自适应世界建模，以深入探查模型表现。在两项任务中，我们发现即使开放权重的LLMs能够在其隐层表征中编码上下文定义的新语义，它们仍难以有效部署这些新语义的表征来完成任务。此外，我们还对封闭源代码、最先进的推理模型在自适应世界建模任务中的表现进行了评估，结果表明，即便是目前性能最强的LLMs，也无法可靠地利用上下文中呈现的新模式。总体而言，本研究旨在激发新方法的发展，促使模型不仅能够编码上下文中的信息，更以支持信息灵活应用的方式进行编码。"
  },
  {
    "date": "2026-02-04",
    "title": "Steering LLMs via Scalable Interactive Oversight",
    "authors": "Enyu Zhou, Zhiheng Xi, Long Ma, Zhihao Zhang, Shihan Dou, Zhikai Lei, Guoteng Wang, Rui Zheng, Hang Yan, Tao Gui, Qi Zhang, Xuanjing Huang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04210v1",
    "source": "arXiv",
    "abstract": "As Large Language Models increasingly automate complex, long-horizon tasks such as \\emph{vibe coding}, a supervision gap has emerged. While models excel at execution, users often struggle to guide them effectively due to insufficient domain expertise, the difficulty of articulating precise intent, and the inability to reliably validate complex outputs. It presents a critical challenge in scalable oversight: enabling humans to responsibly steer AI systems on tasks that surpass their own ability to specify or verify. To tackle this, we propose Scalable Interactive Oversight, a framework that decomposes complex intent into a recursive tree of manageable decisions to amplify human supervision. Rather than relying on open-ended prompting, our system elicits low-burden feedback at each node and recursively aggregates these signals into precise global guidance. Validated in web development task, our framework enables non-experts to produce expert-level Product Requirement Documents, achieving a 54\\% improvement in alignment. Crucially, we demonstrate that this framework can be optimized via Reinforcement Learning using only online user feedback, offering a practical pathway for maintaining human control as AI scales.",
    "title_zh": "通过可扩展的交互式监督引导大语言模型",
    "abstract_zh": "随着大型语言模型越来越多地自动化复杂且具有长期目标的任务，例如“氛围编程”（vibe coding），一个监督缺口逐渐显现。尽管模型在执行任务方面表现出色，但用户往往因缺乏领域专业知识、难以准确表达意图，以及无法可靠验证复杂输出，而难以有效引导模型。这在可扩展监督方面构成了一个关键挑战：如何让人类在面对超出自身描述或验证能力的任务时，仍能负责任地引导人工智能系统。为应对这一挑战，我们提出了“可扩展交互式监督”（Scalable Interactive Oversight）框架，该框架通过将复杂意图分解为一系列可管理的递归决策树，从而放大人类的监督能力。与依赖开放式提示不同，我们的系统在每个决策节点上获取低负担的反馈，并递归地将这些信号聚合为精确的全局指导。在网页开发任务中的验证表明，该框架使非专家能够生成专家级别的产品需求文档，对齐度提升了54%。尤为重要的是，我们证明该框架可通过仅使用在线用户反馈的强化学习进行优化，为人工智能持续扩展的同时保持人类控制提供了一条切实可行的路径。"
  },
  {
    "date": "2026-02-04",
    "title": "Towards Structured, State-Aware, and Execution-Grounded Reasoning for Software Engineering Agents",
    "authors": "Tse-Hsun, Chen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04640v1",
    "source": "arXiv",
    "abstract": "Software Engineering (SE) agents have shown promising abilities in supporting various SE tasks. Current SE agents remain fundamentally reactive, making decisions mainly based on conversation history and the most recent response. However, this reactive design provides no explicit structure or persistent state within the agent's memory, making long-horizon reasoning challenging. As a result, SE agents struggle to maintain a coherent understanding across reasoning steps, adapt their hypotheses as new evidence emerges, or incorporate execution feedback into the mental reasoning model of the system state. In this position paper, we argue that, to further advance SE agents, we need to move beyond reactive behavior toward a structured, state-aware, and execution-grounded reasoning. We outline how explicit structure, persistent and evolving state, and the integration of execution-grounded feedback can help SE agents perform more coherent and reliable reasoning in long-horizon tasks. We also provide an initial roadmap for developing next-generation SE agents that can more effectively perform real-world tasks.",
    "title_zh": "面向结构化、状态感知与执行基础的软件工程智能体推理",
    "abstract_zh": "软件工程（SE）代理在支持各类SE任务方面已展现出令人瞩目的潜力。然而，当前的SE代理本质上仍处于被动响应状态，其决策主要依赖于对话历史和最近的回复。这种被动设计并未在代理的内存中提供显式的结构或持久状态，导致在长周期推理任务中面临挑战。因此，SE代理难以在推理过程中保持连贯的理解，无法根据新出现的证据动态调整假设，也无法将执行反馈有效整合到系统状态的心理推理模型中。在本文中，我们主张，为了进一步推动SE代理的发展，必须超越被动行为，转向具有结构化、状态感知以及基于执行反馈的推理模式。我们阐述了显式结构、持续演化的状态，以及执行反馈的整合如何帮助SE代理在长周期任务中实现更连贯、更可靠的推理。同时，本文还提出了一个初步的发展路线图，旨在推动下一代SE代理的构建，使其能够更有效地完成现实世界中的复杂任务。"
  },
  {
    "date": "2026-02-04",
    "title": "On the use of LLMs to generate a dataset of Neural Networks",
    "authors": "Nadia Daoudi, Jordi Cabot",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04388v1",
    "source": "arXiv",
    "abstract": "Neural networks are increasingly used to support decision-making. To verify their reliability and adaptability, researchers and practitioners have proposed a variety of tools and methods for tasks such as NN code verification, refactoring, and migration. These tools play a crucial role in guaranteeing both the correctness and maintainability of neural network architectures, helping to prevent implementation errors, simplify model updates, and ensure that complex networks can be reliably extended and reused. Yet, assessing their effectiveness remains challenging due to the lack of publicly diverse datasets of neural networks that would allow systematic evaluation. To address this gap, we leverage large language models (LLMs) to automatically generate a dataset of neural networks that can serve as a benchmark for validation. The dataset is designed to cover diverse architectural components and to handle multiple input data types and tasks. In total, 608 samples are generated, each conforming to a set of precise design choices. To further ensure their consistency, we validate the correctness of the generated networks using static analysis and symbolic tracing. We make the dataset publicly available to support the community in advancing research on neural network reliability and adaptability.",
    "title_zh": "使用大语言模型生成神经网络数据集",
    "abstract_zh": "神经网络在决策支持中的应用日益广泛。为了验证其可靠性与可适应性，研究人员和从业者提出了多种工具和方法，用于神经网络代码验证、重构和迁移等任务。这些工具在确保神经网络架构的正确性和可维护性方面发挥着关键作用，有助于防止实现错误、简化模型更新，并确保复杂网络能够可靠地扩展与复用。然而，由于缺乏公开的、多样化的神经网络数据集，评估这些工具的有效性仍面临挑战。为填补这一空白，我们利用大规模语言模型（LLMs）自动生成一个神经网络数据集，用作验证基准。该数据集设计涵盖多种架构组件，并支持多种输入数据类型和任务。总共生成了608个样本，每个样本均符合一组精确的设计选择。为进一步确保其一致性，我们采用静态分析和符号追踪技术对生成网络的正确性进行验证。我们已将该数据集公开发布，以支持社区在神经网络可靠性与可适应性研究方面的持续进步。"
  },
  {
    "date": "2026-02-04",
    "title": "Trust The Typical",
    "authors": "Debargha Ganguly, Sreehari Sankar, Biyao Zhang, Vikash Singh, Kanan Gupta, Harshini Kavuru, Alan Luo, Weicong Chen, Warren Morningstar, Raghu Machiraju, Vipin Chaudhary",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04581v1",
    "source": "arXiv",
    "abstract": "Current approaches to LLM safety fundamentally rely on a brittle cat-and-mouse game of identifying and blocking known threats via guardrails. We argue for a fresh approach: robust safety comes not from enumerating what is harmful, but from deeply understanding what is safe. We introduce Trust The Typical (T3), a framework that operationalizes this principle by treating safety as an out-of-distribution (OOD) detection problem. T3 learns the distribution of acceptable prompts in a semantic space and flags any significant deviation as a potential threat. Unlike prior methods, it requires no training on harmful examples, yet achieves state-of-the-art performance across 18 benchmarks spanning toxicity, hate speech, jailbreaking, multilingual harms, and over-refusal, reducing false positive rates by up to 40x relative to specialized safety models. A single model trained only on safe English text transfers effectively to diverse domains and over 14 languages without retraining. Finally, we demonstrate production readiness by integrating a GPU-optimized version into vLLM, enabling continuous guardrailing during token generation with less than 6% overhead even under dense evaluation intervals on large-scale workloads.",
    "title_zh": "相信平常的",
    "abstract_zh": "当前的大型语言模型（LLM）安全方法本质上依赖于一种脆弱的“猫鼠游戏”：通过设置防护机制来识别并阻止已知威胁。我们主张一种全新的思路：真正的安全并非来自罗列有害内容，而是源于对“安全”本身的深刻理解。为此，我们提出了Trust The Typical（T3）框架，将这一理念付诸实践——将安全问题转化为一种分布外（OOD）检测任务。T3通过学习语义空间中可接受提示的分布模式，将任何显著偏离该分布的输入标记为潜在威胁。与以往方法不同，T3无需使用有害样本进行训练，却在涵盖毒性、仇恨言论、越狱攻击、多语言危害以及过度拒绝等18项基准测试中达到了顶尖性能，相较于专用安全模型，误报率最高降低了40倍。仅用安全英文文本训练的单一模型，即可在无需重新训练的情况下，有效迁移至多种领域和14种以上语言。最后，我们通过将优化后的GPU版本集成到vLLM中，验证了其在生产环境中的可行性，实现了在令牌生成过程中持续进行防护，即使在大规模工作负载下以密集评估间隔运行，性能开销也低于6%。"
  },
  {
    "date": "2026-02-04",
    "title": "Semantic Self-Distillation for Language Model Uncertainty",
    "authors": "Edward Phillips, Sean Wu, Boyan Gao, David A. Clifton",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04577v1",
    "source": "arXiv",
    "abstract": "Large language models present challenges for principled uncertainty quantification, in part due to their complexity and the diversity of their outputs. Semantic dispersion, or the variance in the meaning of sampled answers, has been proposed as a useful proxy for model uncertainty, but the associated computational cost prohibits its use in latency-critical applications. We show that sampled semantic distributions can be distilled into lightweight student models which estimate a prompt-conditioned uncertainty before the language model generates an answer token. The student model predicts a semantic distribution over possible answers; the entropy of this distribution provides an effective uncertainty signal for hallucination prediction, and the probability density allows candidate answers to be evaluated for reliability. On TriviaQA, our student models match or outperform finite-sample semantic dispersion for hallucination prediction and provide a strong signal for out-of-domain answer detection. We term this technique Semantic Self-Distillation (SSD), which we suggest provides a general framework for distilling predictive uncertainty in complex output spaces beyond language.",
    "title_zh": "语言模型不确定性的语义自蒸馏",
    "abstract_zh": "大型语言模型在实现严谨的不确定性量化方面面临挑战，部分原因在于其复杂性以及输出结果的多样性。语义分散（即采样答案在语义上的方差）已被提出作为模型不确定性的有效代理指标，但其相关的计算开销使其难以应用于对延迟敏感的应用场景。我们发现，可以通过知识蒸馏的方式将采样得到的语义分布压缩为轻量级的学生模型，该模型能够在语言模型生成任何回答标记之前，预测一个与提示（prompt）相关的不确定性。学生模型输出的是对可能答案的语义分布，该分布的熵可作为识别幻觉的有效不确定性信号，而概率密度则可用于评估候选答案的可靠性。在TriviaQA数据集上的实验表明，我们的学生模型在幻觉预测任务中达到或超越了有限样本语义分散的效果，并能有效检测出域外答案。我们称这一技术为语义自蒸馏（Semantic Self-Distillation, SSD），并认为它为在语言之外的复杂输出空间中蒸馏预测不确定性提供了一个通用框架。"
  },
  {
    "date": "2026-02-04",
    "title": "Vibe AIGC: A New Paradigm for Content Generation via Agentic Orchestration",
    "authors": "Jiaheng Liu, Yuanxing Zhang, Shihao Li, Xinping Lei",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04575v1",
    "source": "arXiv",
    "abstract": "For the past decade, the trajectory of generative artificial intelligence (AI) has been dominated by a model-centric paradigm driven by scaling laws. Despite significant leaps in visual fidelity, this approach has encountered a ``usability ceiling'' manifested as the Intent-Execution Gap (i.e., the fundamental disparity between a creator's high-level intent and the stochastic, black-box nature of current single-shot models). In this paper, inspired by the Vibe Coding, we introduce the \\textbf{Vibe AIGC}, a new paradigm for content generation via agentic orchestration, which represents the autonomous synthesis of hierarchical multi-agent workflows. Under this paradigm, the user's role transcends traditional prompt engineering, evolving into a Commander who provides a Vibe, a high-level representation encompassing aesthetic preferences, functional logic, and etc. A centralized Meta-Planner then functions as a system architect, deconstructing this ``Vibe'' into executable, verifiable, and adaptive agentic pipelines. By transitioning from stochastic inference to logical orchestration, Vibe AIGC bridges the gap between human imagination and machine execution. We contend that this shift will redefine the human-AI collaborative economy, transforming AI from a fragile inference engine into a robust system-level engineering partner that democratizes the creation of complex, long-horizon digital assets.",
    "title_zh": "Vibe AIGC：通过智能体编排实现内容生成的新范式",
    "abstract_zh": "在过去十年中，生成式人工智能（AI）的发展轨迹一直由以模型为中心的范式主导，这一范式由扩展定律驱动。尽管在视觉保真度方面取得了显著进展，但该方法已遭遇“可用性天花板”，表现为“意图-执行鸿沟”（即创作者的高层次意图与当前单次生成模型所具有的随机性、黑箱特性之间的根本性差异）。在本文中，受“Vibe Coding”理念的启发，我们提出了全新的内容生成范式——**Vibe AIGC**，其核心是通过智能体编排实现的自主化、分层的多智能体工作流合成。在此范式下，用户的角色超越了传统的提示工程，转变为“指挥官”（Commander），通过提供一种“Vibe”——一种涵盖审美偏好、功能逻辑等在内的高层次表达——来引导创作过程。一个中心化的元规划器（Meta-Planner）则扮演系统架构师的角色，将这一“Vibe”分解为可执行、可验证且具备自适应能力的智能体流水线。通过从随机推理转向逻辑化编排，Vibe AIGC弥合了人类想象力与机器执行之间的鸿沟。我们认为，这一范式转变将重新定义人机协同的经济形态，使AI从脆弱的推理引擎进化为强大的系统级工程伙伴，从而推动复杂、长周期数字资产的创作走向大众化。"
  },
  {
    "date": "2026-02-04",
    "title": "OSCAgent: Accelerating the Discovery of Organic Solar Cells with LLM Agents",
    "authors": "Zhaolin Hu, Zhiliang Wu, Hehe Fan, Yi Yang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04510v1",
    "source": "arXiv",
    "abstract": "Organic solar cells (OSCs) hold great promise for sustainable energy, but discovering high-performance materials is time-consuming and costly. Existing molecular generation methods can aid the design of OSC molecules, but they are mostly confined to optimizing known backbones and lack effective use of domain-specific chemical knowledge, often leading to unrealistic candidates. In this paper, we introduce OSCAgent, a multi-agent framework for OSC molecular discovery that unifies retrieval-augmented design, molecular generation, and systematic evaluation into a continuously improving pipeline, without requiring additional human intervention. OSCAgent comprises three collaborative agents. The Planner retrieves knowledge from literature-curated molecules and prior candidates to guide design directions. The Generator proposes new OSC acceptors aligned with these plans. The Experimenter performs comprehensive evaluation of candidate molecules and provides feedback for refinement. Experiments show that OSCAgent produces chemically valid, synthetically accessible OSC molecules and achieves superior predicted performance compared to both traditional and large language model (LLM)-only baselines. Representative results demonstrate that some candidates achieve predicted efficiencies approaching 18\\%. The code will be publicly available.",
    "title_zh": "OSCAgent：利用大语言模型代理加速有机太阳能电池的发现",
    "abstract_zh": "有机太阳能电池（OSCs）在可持续能源领域展现出巨大潜力，但发现高性能材料的过程既耗时又昂贵。现有的分子生成方法虽有助于OSC分子的设计，但大多局限于对已知骨架的优化，未能有效利用领域特定的化学知识，常导致生成不切实际的候选分子。本文提出OSCAgent，一种用于OSC分子发现的多智能体框架，将检索增强设计、分子生成与系统性评估统一为一个持续优化的自动化流程，无需额外的人工干预。OSCAgent由三个协同工作的智能体组成：规划者（Planner）从文献整理的分子及先前候选分子中检索知识，指导设计方向；生成者（Generator）根据这些规划提出符合要求的新OSC受体分子；实验者（Experimenter）对候选分子进行全面评估，并提供反馈以实现优化。实验结果表明，OSCAgent能够生成化学上合理且可合成的OSC分子，其预测性能显著优于传统方法及仅依赖大语言模型（LLM）的基线方法。代表性结果表明，部分候选分子的预测效率接近18%。相关代码将公开发布。"
  },
  {
    "date": "2026-02-04",
    "title": "Model-Driven Legacy System Modernization at Scale",
    "authors": "Tobias Böhm, Jens Guan Su Tien, Mohini Nonnenmann, Tom Schoonbaert, Bart Carpels, Andreas Biesdorf",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04341v1",
    "source": "arXiv",
    "abstract": "This experience report presents a model-driven approach to legacy system modernization that inserts an enriched, technology-agnostic intermediate model between the legacy codebase and the modern target platform, and reports on its application and evaluation. The four-stage process of analysis, enrichment, synthesis, and transition systematically extracts, abstracts, and transforms system artifacts. We apply our approach to a large industrial application built on legacy versions of the .NET Framework and ASP.NET MVC and show that core user interface components and page structures can be migrated semi-automatically to a modern web stack while preserving functional behavior and essential non-functional qualities. By consolidating architectural knowledge into explicit model representations, the resulting codebase exhibits higher maintainability and extensibility, thereby improving developer experience. Although automation is effective for standard patterns, migration of bespoke layout composites remains challenging and requires targeted manual adaptation. Our contributions are: (i) an end-to-end model-driven process, (ii) an enriched intermediate model that captures structure, dependencies, and semantic metadata, (iii) transformation rules that preserve functional behavior and essential non-functional qualities, and (iv) application and evaluation of the approach in an industrial setting. Overall, model-based abstractions reduce risk and effort while supporting scalable, traceable modernization of legacy applications. Our approach generalizes to comparable modernization contexts and promotes reuse of migration patterns.",
    "title_zh": "大规模基于模型的遗留系统现代化",
    "abstract_zh": "本经验报告提出了一种以模型驱动为核心的遗留系统现代化方法，该方法在遗留代码库与现代目标平台之间引入了一个增强的、与技术无关的中间模型，并报告了该方法的应用与评估结果。该方法采用四阶段流程——分析、增强、合成与过渡，系统地提取、抽象并转换系统资产。我们将该方法应用于一个基于旧版 .NET Framework 和 ASP.NET MVC 构建的大型工业级应用，结果表明，核心用户界面组件和页面结构能够以半自动化的方式迁移至现代 Web 技术栈，同时保持原有的功能行为和关键非功能性质量。通过将架构知识显式地封装在模型表示中，最终生成的代码库具备更高的可维护性和可扩展性，从而显著提升了开发人员的体验。尽管自动化在处理标准模式时效果显著，但针对定制化布局组件的迁移仍具挑战性，需进行有针对性的手动调整。本文的主要贡献包括：(i) 一个端到端的模型驱动流程；(ii) 一个能够捕捉结构、依赖关系和语义元数据的增强型中间模型；(iii) 保证功能行为和关键非功能性质量不变的转换规则；(iv) 在工业场景中的实际应用与评估。总体而言，基于模型的抽象有效降低了现代化过程中的风险与工作量，支持可扩展、可追溯的遗留系统现代化。该方法具有良好的泛化能力，适用于类似的现代化场景，并有助于迁移模式的复用。"
  },
  {
    "date": "2026-02-04",
    "title": "Data Agents: Levels, State of the Art, and Open Problems",
    "authors": "Yuyu Luo, Guoliang Li, Ju Fan, Nan Tang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04261v1",
    "source": "arXiv",
    "abstract": "Data agents are an emerging paradigm that leverages large language models (LLMs) and tool-using agents to automate data management, preparation, and analysis tasks. However, the term \"data agent\" is currently used inconsistently, conflating simple query responsive assistants with aspirational fully autonomous \"data scientists\". This ambiguity blurs capability boundaries and accountability, making it difficult for users, system builders, and regulators to reason about what a \"data agent\" can and cannot do. In this tutorial, we propose the first hierarchical taxonomy of data agents from Level 0 (L0, no autonomy) to Level 5 (L5, full autonomy). Building on this taxonomy, we will introduce a lifecycleand level-driven view of data agents. We will (1) present the L0-L5 taxonomy and the key evolutionary leaps that separate simple assistants from truly autonomous data agents, (2) review representative L0-L2 systems across data management, preparation, and analysis, (3) highlight emerging Proto-L3 systems that strive to autonomously orchestrate end-to-end data workflows to tackle diverse and comprehensive data-related tasks under supervision, and (4) discuss forward-looking research challenges towards proactive (L4) and generative (L5) data agents. We aim to offer both a practical map of today's systems and a research roadmap for the next decade of data-agent development.",
    "title_zh": "数据代理：层级、最新进展与开放问题",
    "abstract_zh": "数据代理（Data Agents）是一种新兴范式，利用大型语言模型（LLMs）和工具使用型代理来自动化数据管理、准备与分析任务。然而，当前“数据代理”这一术语的使用尚不统一，常常将简单的查询响应助手与理想化的完全自主“数据科学家”混为一谈。这种模糊性模糊了能力边界与责任归属，使得用户、系统构建者和监管机构难以准确判断“数据代理”究竟具备何种能力。在本次教程中，我们提出了首个从L0（无自主性）到L5（完全自主）的分层分类体系。基于这一分类体系，我们将介绍一种以生命周期和自主级别为驱动的数据代理视角。具体而言，我们将：（1）阐述L0至L5的分类体系，以及区分简单助手与真正自主数据代理的关键演进跃迁；（2）回顾数据管理、准备与分析领域中具有代表性的L0至L2系统；（3）重点介绍正在涌现的“准L3”系统，这些系统在监督下努力实现端到端数据工作流的自主编排，以应对多样且复杂的综合性数据任务；（4）探讨迈向主动型（L4）和生成型（L5）数据代理的前瞻性研究挑战。我们的目标是为当前系统提供一张实用的“地图”，并为未来十年数据代理的发展绘制一张研究路线图。"
  },
  {
    "date": "2026-02-04",
    "title": "Scaling Agentic Verifier for Competitive Coding",
    "authors": "Zeyao Ma, Jing Zhang, Xiaokang Zhang, Jiaxi Yang, Zongmeng Zhang, Jiajun Zhang, Yuheng Jing, Lei Zhang, Hao Zheng, Wenting Zhao, Junyang Lin, Binyuan Hui",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04254v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) have demonstrated strong coding capabilities but still struggle to solve competitive programming problems correctly in a single attempt. Execution-based re-ranking offers a promising test-time scaling strategy, yet existing methods are constrained by either difficult test case generation or inefficient random input sampling. To address this limitation, we propose Agentic Verifier, an execution-based agent that actively reasons about program behaviors and searches for highly discriminative test inputs that expose behavioral discrepancies among candidate solutions. Through multi-turn interaction with code execution environments, the verifier iteratively refines the candidate input generator and produces targeted counterexamples rather than blindly sampling inputs. We train the verifier to acquire this discriminative input generation capability via a scalable pipeline combining large-scale data synthesis, rejection fine-tuning, and agentic reinforcement learning. Extensive experiments across five competitive programming benchmarks demonstrate consistent improvements over strong execution-based baselines, achieving up to +10-15% absolute gains in Best@K accuracy. Further analysis reveals clear test-time scaling behavior and highlights the verifier's broader potential beyond reranking.",
    "title_zh": "用于竞争性编程的代理验证器的扩展",
    "abstract_zh": "大型语言模型（LLMs）已展现出强大的编程能力，但在单次尝试中仍难以正确解决竞赛编程问题。基于执行结果的重排序（execution-based re-ranking）提供了一种有前景的测试时扩展策略，然而现有方法受限于测试用例生成困难或低效的随机输入采样。为解决这一局限，我们提出 Agentic Verifier——一种基于执行的智能体验证器，能够主动推理程序行为，并搜索能够有效区分候选解之间行为差异的高判别性测试输入。通过与代码执行环境进行多轮交互，该验证器迭代优化候选输入生成器，生成有针对性的反例，而非盲目随机采样输入。我们通过一个可扩展的训练流程，结合大规模数据合成、拒绝式微调以及智能体强化学习，使验证器具备这种高判别性输入生成能力。在五个竞赛编程基准上的大量实验表明，该方法持续优于现有的强基线方法，在 Best@K 准确率上实现了最高达 +10% 至 +15% 的绝对提升。进一步分析揭示了清晰的测试时扩展特性，并凸显了该验证器在重排序之外更广泛的应用潜力。"
  },
  {
    "date": "2026-02-04",
    "title": "Why Agentic-PRs Get Rejected: A Comparative Study of Coding Agents",
    "authors": "Sota Nakashima, Yuta Ishimoto, Masanari Kondo, Shane Mclntosh, Yasutaka Kamei",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04226v1",
    "source": "arXiv",
    "abstract": "Agentic coding -- software development workflows in which autonomous coding agents plan, implement, and submit code changes with minimal human involvement -- is rapidly gaining traction. Prior work has shown that Pull Requests (PRs) produced using coding agents (Agentic-PRs) are accepted less often than PRs that are not labeled as agentic (Human-PRs). The rejection reasons for a single agent (Claude Code) have been explored, but a comparison of how rejection reasons differ between Agentic-PRs generated by different agents has not yet been performed. This comparison is important since different coding agents are often used for different purposes, which can lead to agent-specific failure patterns. In this paper, we inspect 654 rejected PRs from the AIDev dataset covering five coding agents, as well as a human baseline. Our results show that seven rejection modes occur only in Agentic-PRs, including distrust of AI-generated code. We also observe agent-specific patterns (e.g., automated withdrawal of inactive PRs by Devin), reflecting differences in how agents are configured and used in practice. Notably, a large proportion of rejected PRs (67.9%) lack explicit reviewer feedback, making their rejection reasons difficult to determine. To mitigate this issue, we propose a set of heuristics that reduce the proportion of such cases, offering a practical preprocessing step for future studies of PR rejection in agentic coding.",
    "title_zh": "为什么代理型PR会被拒绝：对编码代理的比较研究",
    "abstract_zh": "代理式编程——即在软件开发工作流中，由自主编程代理在极少人工干预的情况下规划、实现并提交代码变更——正迅速获得关注。先前的研究表明，使用编程代理生成的拉取请求（Agentic-PRs）被接受的概率低于未标记为代理生成的请求（Human-PRs）。虽然已有研究探讨了单一代理（Claude Code）导致PR被拒的原因，但尚未有研究系统比较不同代理生成的Agentic-PRs在拒因上的差异。这一比较至关重要，因为不同编程代理常被用于不同目的，可能导致特定代理特有的失败模式。本文分析了来自AIDev数据集的654个被拒PR，涵盖五种编程代理及一个人类基准。研究结果表明，有七种拒因仅出现在Agentic-PRs中，其中包括对AI生成代码的不信任。此外，我们还观察到代理特有的行为模式（例如，Devin自动撤回长时间未活动的PR），反映出各代理在实际配置和使用上的差异。值得注意的是，高达67.9%的被拒PR缺乏明确的评审反馈，使得其拒因难以判断。为缓解这一问题，我们提出了一组启发式规则，可有效降低此类无反馈案例的比例，为未来关于代理式编程中PR拒因的研究提供实用的预处理方法。"
  },
  {
    "date": "2026-02-04",
    "title": "RAPO: Risk-Aware Preference Optimization for Generalizable Safe Reasoning",
    "authors": "Zeming Wei, Qiaosheng Zhang, Xia Hu, Xingcheng Xu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04224v1",
    "source": "arXiv",
    "abstract": "Large Reasoning Models (LRMs) have achieved tremendous success with their chain-of-thought (CoT) reasoning, yet also face safety issues similar to those of basic language models. In particular, while algorithms are designed to guide them to deliberately refuse harmful prompts with safe reasoning, this process often fails to generalize against diverse and complex jailbreak attacks. In this work, we attribute these failures to the generalization of the safe reasoning process, particularly their insufficiency against complex attack prompts. We provide both theoretical and empirical evidence to show the necessity of a more sufficient safe reasoning process to defend against advanced attack prompts. Building on this insight, we propose a Risk-Aware Preference Optimization (RAPO) framework that enables LRM to adaptively identify and address the safety risks with appropriate granularity in its thinking content. Extensive experiments demonstrate that RAPO successfully generalizes multiple LRMs' safe reasoning adaptively across diverse attack prompts whilst preserving general utility, contributing a robust alignment technique for LRM safety. Our code is available at https://github.com/weizeming/RAPO.",
    "title_zh": "RAPO：面向可泛化安全推理的风险感知偏好优化",
    "abstract_zh": "大型推理模型（LRMs）凭借其思维链（CoT）推理能力取得了巨大成功，但也面临着与基础语言模型相似的安全问题。特别是，尽管算法被设计用于引导模型在面对有害提示时通过安全推理主动拒绝，但这一过程在应对多样且复杂的越狱攻击时往往难以泛化。在本研究中，我们将这些失败归因于安全推理过程的泛化能力不足，尤其是在面对复杂攻击提示时表现尤为明显。我们通过理论分析和实证研究，证明了需要一种更为充分的安全推理机制，以有效防御高级攻击提示。基于这一洞察，我们提出了一种风险感知偏好优化（Risk-Aware Preference Optimization, RAPO）框架，使大型推理模型能够在其思维内容中自适应地识别并以适当粒度应对安全风险。大量实验表明，RAPO能够有效提升多种大型推理模型在面对多样化攻击提示时的安全推理泛化能力，同时保持其通用性能，为大型推理模型的安全对齐提供了一种稳健的技术方案。我们的代码已开源，地址为：https://github.com/weizeming/RAPO。"
  },
  {
    "date": "2026-02-04",
    "title": "CoRe: Context-Robust Remasking for Diffusion Language Models",
    "authors": "Kevin Zhai, Sabbir Mollah, Zhenyi Wang, Mubarak Shah",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04096v1",
    "source": "arXiv",
    "abstract": "Standard decoding in Masked Diffusion Models (MDMs) is hindered by context rigidity: tokens are retained based on transient high confidence, often ignoring that early predictions lack full context. This creates cascade effects where initial inconsistencies misguide the remaining generation. Existing revision strategies attempt to mitigate this by relying on static confidence scores, but these signals are inherently myopic; inconsistent tokens can appear confident to the model itself. We propose Context-Robust Remasking (CoRe), a training-free framework for inference-time revision. Rather than trusting static token probabilities, CoRe identifies context-brittle tokens by probing their sensitivity to targeted masked-context perturbations. We formalize revision as a robust optimization objective over context shifts and efficiently approximate this objective to prioritize unstable tokens for revision. On LLaDA-8B-Base, CoRe delivers consistent improvements across reasoning and code benchmarks, outperforming compute-matched baselines and improving MBPP by up to 9.2 percentage points.",
    "title_zh": "CoRe：面向扩散语言模型的上下文鲁棒重掩码方法",
    "abstract_zh": "在掩码扩散模型（MDMs）中，标准解码过程受到上下文僵化性的阻碍：模型根据瞬时的高置信度保留标记，却常常忽视早期预测缺乏完整上下文的事实。这种现象导致级联效应，初始的不一致会误导后续生成过程。现有的修正策略试图通过依赖静态置信度分数来缓解该问题，但这些信号本质上具有短视性——模型自身可能对不一致的标记表现出高置信度。为此，我们提出了一种无需训练的推理时修正框架：上下文鲁棒性重掩码（Context-Robust Remasking, CoRe）。CoRe 不依赖静态的标记概率，而是通过探测标记对特定掩码上下文扰动的敏感性，识别出对上下文敏感的脆弱标记。我们将修正过程形式化为对上下文变化的鲁棒优化目标，并高效地近似该目标，以优先选择不稳定的标记进行修正。在 LLaDA-8B-Base 模型上，CoRe 在推理与代码生成基准测试中均实现了稳定提升，超越了计算量相当的基线模型，且在 MBPP 基准上最高提升了 9.2 个百分点。"
  },
  {
    "date": "2026-02-04",
    "title": "Beyond the Control Equations: An Artifact Study of Implementation Quality in Robot Control Software",
    "authors": "Nils Chur, Thorsten Berger, Einar Broch Johnsen, Andrzej Wąsowski",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.04799v1",
    "source": "arXiv",
    "abstract": "A controller -- a software module managing hardware behavior -- is a key component of a typical robot system. While control theory gives safety guarantees for standard controller designs, the practical implementation of controllers in software introduces complexities that are often overlooked. Controllers are often designed in continuous space, while the software is executed in discrete space, undermining some of the theoretical guarantees. Despite extensive research on control theory and control modeling, little attention has been paid to the implementations of controllers and how their theoretical guarantees are ensured in real-world software systems. We investigate 184 real-world controller implementations in open-source robot software. We examine their application context, the implementation characteristics, and the testing methods employed to ensure correctness. We find that the implementations often handle discretization in an ad hoc manner, leading to potential issues with real-time reliability. Challenges such as timing inconsistencies, lack of proper error handling, and inadequate consideration of real-time constraints further complicate matters. Testing practices are superficial, no systematic verification of theoretical guarantees is used, leaving possible inconsistencies between expected and actual behavior. Our findings highlight the need for improved implementation guidelines and rigorous verification techniques to ensure the reliability and safety of robotic controllers in practice.",
    "title_zh": "超越控制方程：机器人控制软件实现质量的制品研究",
    "abstract_zh": "控制器——一种管理硬件行为的软件模块——是典型机器人系统中的关键组件。尽管控制理论为标准控制器设计提供了安全保证，但控制器在软件中的实际实现引入了许多常被忽视的复杂性。控制器通常在连续空间中设计，而软件则在离散空间中执行，这削弱了部分理论上的保证。尽管控制理论和控制建模领域已有大量研究，但针对控制器实现方式及其理论保证在现实软件系统中如何得以保障的研究却寥寥无几。我们对开源机器人软件中的184个实际控制器实现进行了调查，分析了它们的应用背景、实现特征以及用于确保正确性的测试方法。研究发现，这些实现往往以非系统化的方式处理离散化问题，可能导致实时可靠性方面的问题。诸如时间不一致、缺乏适当的错误处理机制以及对实时约束考虑不足等挑战进一步加剧了问题的复杂性。测试实践流于表面，未采用系统化的验证方法来检验理论保证，导致预期行为与实际行为之间可能存在不一致。我们的研究结果凸显了制定更完善的实现指南和采用严格验证技术的必要性，以确保机器人控制器在实际应用中的可靠性与安全性。"
  },
  {
    "date": "2026-2-4",
    "title": "Optimizing Class-Level Code Generation: Enhancing In-Context Learning in Large Language Models with Pruning Techniques",
    "authors": "Mingyang Geng, Zizhao Zhang, Ping Du, Xiangyuan Yin, Hongyang Chen, Bo Liu, Zhenshun Shi",
    "publish": "Proceedings of the 2025 International Conference on Artificial Intelligence and Sustainable Development",
    "url": "https://doi.org/10.1145/3786484.3786551",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "优化类级别代码生成：通过剪枝技术提升大型语言模型的上下文学习能力",
    "abstract_zh": "None"
  },
  {
    "date": "2026-2-4",
    "title": "Adaptive Block Size Selection for Translating Triton Kernels to RVV",
    "authors": "Liu Yuhao, William Kevin, Feige Zhou, Yeh-Ching Chung, Wei-Chung Hsu",
    "publish": "Proceedings of the International Conference on Research in Adaptive and Convergent Systems",
    "url": "https://doi.org/10.1145/3769002.3769969",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "将Triton内核转换为RVV的自适应块大小选择",
    "abstract_zh": "None"
  },
  {
    "date": "2026-2-4",
    "title": "A Defence-Oriented Study of API Security in CI/CD Pipelines",
    "authors": "Sabbir M. Saleh, Md Nafiz Al Ifat, Nazim H. Madhavji, John Steinbacher",
    "publish": "Proceedings of the 2025 10th International Conference on Cloud Computing and Internet of Things",
    "url": "https://doi.org/10.1145/3785520.3785526",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "面向防御的CI/CD流水线中API安全研究",
    "abstract_zh": "None"
  },
  {
    "date": "2026-2-4",
    "title": "Grade Like a Human: Rethinking Automated Assessment with Large Language Models",
    "authors": "Wenjing Xie, Juxin Niu, Chun Jason Xue, Nan Guan",
    "publish": "Proceedings of the International Conference on Research in Adaptive and Convergent Systems",
    "url": "https://doi.org/10.1145/3769002.3769962",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "像人类一样评分：重新思考大型语言模型在自动评估中的应用",
    "abstract_zh": "None"
  }
]