[
  {
    "date": "2026-02-18",
    "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents",
    "authors": "Wenxuan Ding, Nicholas Tomlin, Greg Durrett",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.16699v1",
    "source": "arXiv",
    "abstract": "LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.",
    "title_zh": "校准-再行动：大语言模型智能体中的成本感知探索",
    "abstract_zh": "大型语言模型（LLMs）正越来越多地被用于解决复杂问题，而这些问题往往无法通过单次响应完成，而是需要与环境交互以获取信息。在这些场景中，LLMs 必须权衡内在的成本-不确定性权衡，决定何时停止探索并最终做出回答。例如，在编程任务中，如果 LLM 对生成的代码片段的正确性存疑，就应编写测试代码进行验证；虽然编写测试本身存在成本，但通常远低于犯错所带来的代价。在本研究中，我们展示了可以通过引导 LLM 显式地思考这种成本-不确定性权衡，从而实现更优的环境探索行为。我们将多个任务（包括信息检索和编程）形式化为在不确定性下的序列决策问题，每个问题都包含一个可通过先验知识进行推理的潜在环境状态，该先验知识被传递给 LLM 代理。我们提出了一种名为“校准-执行”（Calibrate-Then-Act, CTA）的框架，通过向 LLM 提供这一额外上下文，使其能够更优地采取行动。即使在对基线模型和 CTA 模型均进行强化学习训练的情况下，该改进依然有效。我们在信息获取型问答任务以及一个简化的编程任务上的实验结果表明，通过 CTA 显式地表达成本-收益权衡，能够帮助智能体发现更优的决策策略。"
  },
  {
    "date": "2026-02-18",
    "title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments",
    "authors": "Yangjie Xu, Lujun Li, Lama Sleem, Niccolo Gentile, Yewei Song, Yiqun Wang, Siming Ji, Wenbo Wu, Radu State",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.16653v1",
    "source": "arXiv",
    "abstract": "Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.",
    "title_zh": "代理技能框架：小型语言模型在工业环境中的潜力探讨",
    "abstract_zh": "代理技能框架（Agent Skill framework）目前已获得GitHub Copilot、LangChain和OpenAI等主要技术平台的广泛且官方支持，在使用私有化模型时表现尤为出色，能够有效提升上下文工程能力、减少幻觉现象，并显著提高任务执行的准确性。基于上述观察，本文开展了一项研究，旨在探究该代理技能范式是否同样能为小型语言模型（SLMs）带来类似优势。这一问题在工业应用场景中具有重要意义：由于数据安全和预算限制，持续依赖公共API往往不可行，而小型语言模型在高度定制化场景中又常表现出泛化能力有限的问题。\n\n本研究首先提出了代理技能过程的正式数学定义，随后对不同规模的语言模型在多个应用场景中进行了系统性评估。评估涵盖两个开源任务以及一个真实的保险理赔数据集。实验结果表明，极小规模模型在可靠地选择技能方面存在困难，而中等规模的SLMs（参数量约为12B至30B）则能从代理技能方法中获得显著收益。此外，参数量约为80B的代码专项优化型模型，其性能可与闭源基线模型相媲美，同时显著提升了GPU使用效率。\n\n综上所述，本研究全面而深入地刻画了该框架的能力边界与局限性，为在以小型语言模型为核心的环境中有效部署代理技能提供了切实可行的实践指导。"
  },
  {
    "date": "2026-02-18",
    "title": "Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs",
    "authors": "Felix Fricke, Simon Malberg, Georg Groh",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.16512v1",
    "source": "arXiv",
    "abstract": "Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or unseen problem types. Additionally, these schemes are often under-optimized in terms of hyperparameters, prompts, runtime, and prompting cost. To address these limitations, we introduce Framework of Thoughts (FoT)--a general-purpose foundation framework for building and optimizing dynamic reasoning schemes. FoT comes with built-in features for hyperparameter tuning, prompt optimization, parallel execution, and intelligent caching, unlocking the latent performance potential of reasoning schemes. We demonstrate FoT's capabilities by implementing three popular schemes--Tree of Thoughts, Graph of Thoughts, and ProbTree--within FoT. We empirically show that FoT enables significantly faster execution, reduces costs, and achieves better task scores through optimization. We release our codebase to facilitate the development of future dynamic and efficient reasoning schemes.",
    "title_zh": "思维框架：基于链、树与图的动态优化推理基础框架",
    "abstract_zh": "像思维链（Chain of Thought）、思维树（Tree of Thoughts）和思维图（Graph of Thoughts）这类提示策略，能够显著提升大型语言模型的推理能力。然而，现有的大多数策略需要用户预先定义静态的、针对特定问题的推理结构，难以适应动态或未见过的问题类型。此外，这些策略在超参数、提示设计、运行时间和提示成本等方面往往缺乏充分优化。为解决上述局限，我们提出了“思维框架”（Framework of Thoughts, FoT）——一种通用的基础框架，用于构建和优化动态推理策略。FoT 内置了超参数调优、提示优化、并行执行和智能缓存等功能，充分释放了推理策略的潜在性能。我们通过在 FoT 中实现三种流行的推理策略——思维树、思维图和 ProbTree，展示了其强大能力。实验证明，FoT 能显著加快执行速度，降低运行成本，并通过优化获得更优的任务表现分数。我们已开源代码库，以促进未来动态、高效推理策略的开发。"
  },
  {
    "date": "2026-02-18",
    "title": "Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling",
    "authors": "Jeffrey T. H. Wong, Zixi Zhang, Junyi Liu, Yiren Zhao",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.16485v1",
    "source": "arXiv",
    "abstract": "Existing Multi-Agent Systems (MAS) typically rely on static, homogeneous model configurations, limiting their ability to exploit the distinct strengths of differently post-trained models. To address this, we introduce Team-of-Thoughts, a novel MAS architecture that leverages the complementary capabilities of heterogeneous agents via an orchestrator-tool paradigm. Our framework introduces two key mechanisms to optimize performance: (1) an orchestrator calibration scheme that identifies models with superior coordination capabilities, and (2) a self-assessment protocol where tool agents profile their own domain expertise to account for variations in post-training skills. During inference, the orchestrator dynamically activates the most suitable tool agents based on these proficiency profiles. Experiments on five reasoning and code generation benchmarks show that Team-of-Thoughts delivers consistently superior task performance. Notably, on AIME24 and LiveCodeBench, our approach achieves accuracies of 96.67% and 72.53%, respectively, substantially outperforming homogeneous role-play baselines, which score 80% and 65.93%.",
    "title_zh": "思维团队：通过协调工具调用实现智能体系统的高效测试时扩展",
    "abstract_zh": "现有的多智能体系统（MAS）通常依赖于静态且同质的模型配置，这限制了其利用不同后训练模型各自优势的能力。为解决这一问题，我们提出了“思维团队”（Team-of-Thoughts）——一种新颖的多智能体架构，通过“协调器-工具”范式，充分发挥异构智能体之间的互补能力。我们的框架引入了两项关键机制以优化性能：（1）一种协调器校准方案，用于识别具备更优协作能力的模型；（2）一种自我评估协议，使工具型智能体能够自主评估自身在特定领域的专业能力，从而应对后训练技能差异带来的影响。在推理阶段，协调器会根据这些能力评估结果，动态激活最合适的工具型智能体。在五个推理与代码生成基准测试中的实验表明，“思维团队”在各项任务中均表现出显著更优的性能。特别地，在AIME24和LiveCodeBench基准上，我们的方法分别取得了96.67%和72.53%的准确率，显著优于同质角色扮演基线方法（分别为80%和65.93%）。"
  },
  {
    "date": "2026-02-18",
    "title": "Synthesis and Verification of Transformer Programs",
    "authors": "Hongjian Jiang, Matthew Hague, Philipp Rümmer, Anthony Widjaja Lin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.16473v1",
    "source": "arXiv",
    "abstract": "C-RASP is a simple programming language that was recently shown to capture concepts expressible by transformers. In this paper, we develop new algorithmic techniques for automatically verifying C-RASPs. To this end, we establish a connection to the verification of synchronous dataflow programs in Lustre, which enables us to exploit state-of-the-art model checkers utilizing highly optimized SMT-solvers. Our second contribution addresses learning a C-RASP program in the first place. To this end, we provide a new algorithm for learning a C-RASP from examples using local search. We demonstrate efficacy of our implementation for benchmarks of C-RASPs in the literature, in particular in connection to the following applications: (1) transformer program optimization, and (2) constrained learning of transformer programs (based on a partial specification).",
    "title_zh": "变压器程序的合成与验证",
    "abstract_zh": "C-RASP是一种最近被证明能够表达Transformer所具备概念的简单编程语言。在本文中，我们开发了新的算法技术，用于自动验证C-RASP程序。为此，我们建立了一种与Lustre中同步数据流程序验证之间的联系，从而能够利用最先进的模型检测器，这些检测器采用高度优化的SMT求解器。我们的第二个贡献是解决如何首次学习C-RASP程序的问题。为此，我们提出了一种基于局部搜索的新算法，用于从示例中学习C-RASP程序。我们通过文献中已有的C-RASP基准测试验证了所实现方法的有效性，特别是在以下两个应用中的表现：（1）Transformer程序优化，以及（2）基于部分规范的Transformer程序约束学习。"
  },
  {
    "date": "2026-02-18",
    "title": "Bibby AI -- AI Latex Editor writing assistant for researchers vs Overleaf Alternative vs OpenAI Prism. (Bibby AI Latex Editor)",
    "authors": "Nilesh jain, Rohit Yadav, Andrej Karpathy",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.16432v1",
    "source": "arXiv",
    "abstract": "Large language models are increasingly integrated into academic writing workflows; however, the most widely used \\LaTeX\\ editors remain AI-peripheral -- offering compilation and collaboration, but no native intelligence. This separation forces researchers to leave their editing environment for AI assistance, fragmenting document context and interrupting writing flow. We present Bibby AI (trybibby.com), a native, AI-first \\LaTeX\\ editor that unifies the complete research writing lifecycle within a single interface. Bibby embeds an AI writing assistant, smart citation search, AI table and equation generation, an AI paper reviewer, abstract generator, literature review drafting, a deep research assistant, and real-time \\LaTeX\\ error detection and auto-fix -- all natively, without plugins or copy-paste workflows. We introduce LaTeXBench-500, a benchmark of 500 real-world compilation errors across six categories. Bibby achieves 91.4\\% detection accuracy and 83.7\\% one-click fix accuracy, outperforming Overleaf's native diagnostics (61.2\\%) and OpenAI Prism (78.3 / 64.1\\%) by large margins. Bibby demonstrates that a privacy-preserving, research-first AI editor can meaningfully accelerate every stage of academic manuscript preparation. We found that Bibby AI is a far superior alternative to overleaf latex and better than OpenAI Prism functionalities and AI.",
    "title_zh": "Bibby AI — 面向研究人员的AI LaTeX编辑器写作助手，与Overleaf替代方案及OpenAI Prism对比（Bibby AI LaTeX编辑器）",
    "abstract_zh": "大型语言模型正越来越多地融入学术写作工作流程，然而目前使用最广泛的 \\LaTeX\\ 编辑器仍处于人工智能的“外围”——仅提供编译和协作功能，缺乏原生智能支持。这种分离迫使研究人员不得不离开其编辑环境去寻求AI辅助，导致文档上下文被割裂，写作流程受到中断。我们推出了 Bibby AI（trybibby.com），这是一款原生的、以AI为核心的 \\LaTeX\\ 编辑器，将完整的科研写作生命周期统一于单一界面之中。Bibby 内置了AI写作助手、智能引用搜索、AI生成表格与公式、AI论文评审、摘要生成、文献综述草拟、深度研究助手，以及实时 \\LaTeX\\ 错误检测与一键自动修复功能——所有功能均原生集成，无需插件或复制粘贴操作。我们提出了 LaTeXBench-500，一个包含500个真实世界编译错误的基准测试集，涵盖六大类别。Bibby 在错误检测上达到91.4%的准确率，在一键修复上达到83.7%的准确率，显著优于 Overleaf 原生诊断（61.2%）和 OpenAI Prism（78.3 / 64.1%）。实验表明，一款注重隐私保护、以科研为核心目标的AI编辑器，能够切实加速学术稿件准备的每一个阶段。我们发现，Bibby AI 远优于 Overleaf 的 \\LaTeX\\ 功能，也优于 OpenAI Prism 的各项功能与AI能力。"
  },
  {
    "date": "2026-02-18",
    "title": "HiPER: Hierarchical Reinforcement Learning with Explicit Credit Assignment for Large Language Model Agents",
    "authors": "Jiangweizhi Peng, Yuanxin Liu, Ruida Zhou, Charles Fleming, Zhaoran Wang, Alfredo Garcia, Mingyi Hong",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.16165v1",
    "source": "arXiv",
    "abstract": "Training LLMs as interactive agents for multi-turn decision-making remains challenging, particularly in long-horizon tasks with sparse and delayed rewards, where agents must execute extended sequences of actions before receiving meaningful feedback. Most existing reinforcement learning (RL) approaches model LLM agents as flat policies operating at a single time scale, selecting one action at each turn. In sparse-reward settings, such flat policies must propagate credit across the entire trajectory without explicit temporal abstraction, which often leads to unstable optimization and inefficient credit assignment. We propose HiPER, a novel Hierarchical Plan-Execute RL framework that explicitly separates high-level planning from low-level execution. HiPER factorizes the policy into a high-level planner that proposes subgoals and a low-level executor that carries them out over multiple action steps. To align optimization with this structure, we introduce a key technique called hierarchical advantage estimation (HAE), which carefully assigns credit at both the planning and execution levels. By aggregating returns over the execution of each subgoal and coordinating updates across the two levels, HAE provides an unbiased gradient estimator and provably reduces variance compared to flat generalized advantage estimation. Empirically, HiPER achieves state-of-the-art performance on challenging interactive benchmarks, reaching 97.4\\% success on ALFWorld and 83.3\\% on WebShop with Qwen2.5-7B-Instruct (+6.6\\% and +8.3\\% over the best prior method), with especially large gains on long-horizon tasks requiring multiple dependent subtasks. These results highlight the importance of explicit hierarchical decomposition for scalable RL training of multi-turn LLM agents.",
    "title_zh": "HiPER：面向大语言模型智能体的显式信用分配层次强化学习",
    "abstract_zh": "将大型语言模型（LLM）训练为用于多轮决策的交互式智能体仍然具有挑战性，尤其是在长时程任务中，这些任务通常具有稀疏且延迟的奖励信号，智能体必须执行一系列连续动作后才能获得有意义的反馈。现有的大多数强化学习（RL）方法将LLM智能体建模为单一时间尺度上的扁平策略，在每一轮仅选择一个动作。在稀疏奖励环境下，这种扁平策略必须在整个轨迹上进行信用传播，而缺乏显式的时序抽象，这常常导致优化过程不稳定以及信用分配效率低下。\n\n我们提出了一种名为HiPER的新框架——一种分层规划-执行强化学习架构，该框架明确地将高层规划与底层执行分离。HiPER将策略分解为两个部分：高层规划器负责提出子目标，而底层执行器则在多个动作步骤中完成这些子目标。为了使优化过程与这一结构相匹配，我们引入了一项关键技术——分层优势估计（Hierarchical Advantage Estimation, HAE），该方法在规划和执行两个层面精确地分配信用。通过聚合每个子目标执行过程中的回报，并协调两个层级之间的更新，HAE提供了一个无偏的梯度估计器，并在理论上证明其方差低于传统的扁平化广义优势估计（GAE）。\n\n实验结果表明，HiPER在多个具有挑战性的交互式基准测试中达到了当前最优性能：在ALFWorld上取得了97.4%的成功率，在WebShop上达到83.3%的成功率（使用Qwen2.5-7B-Instruct模型，分别比之前最佳方法提升6.6%和8.3%），尤其在需要完成多个相互依赖子任务的长时程任务中表现尤为突出。这些结果凸显了显式分层分解对于实现可扩展的多轮LLM智能体强化学习训练的重要性。"
  },
  {
    "date": "2026-02-18",
    "title": "E-Graphs as a Persistent Compiler Abstraction",
    "authors": "Jules Merckx, Alexandre Lopoukhine, Samuel Coward, Jianyi Cheng, Bjorn De Sutter, Tobias Grosser",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.16707v1",
    "source": "arXiv",
    "abstract": "Recent algorithmic advances have made equality saturation an appealing approach to program optimization because it avoids the phase-ordering problem. Existing work uses external equality saturation libraries, or custom implementations that are deeply tied to the specific application. However, these works only apply equality saturation at a single level of abstraction, or discard the discovered equalities when code is transformed by other compiler passes. We propose an alternative approach that represents an e-graph natively in the compiler's intermediate representation, facilitating the application of constructive compiler passes that maintain the e-graph state throughout the compilation flow. We build on a Python-based MLIR framework, xDSL, and introduce a new MLIR dialect, eqsat, that represents e-graphs in MLIR code. We show that this representation expands the scope of equality saturation in the compiler, allowing us to interleave pattern rewriting with other compiler transformations. The eqsat dialect provides a unified abstraction for compilers to utilize equality saturation across various levels of intermediate representations concurrently within the same MLIR flow.",
    "title_zh": "E-图作为持久化编译器抽象",
    "abstract_zh": "近期的算法进展使得等式饱和（equality saturation）成为程序优化中一个极具吸引力的方法，因为它能够避免阶段顺序问题。现有的研究通常依赖外部的等式饱和库，或使用与特定应用深度绑定的定制实现。然而，这些方法仅在单一抽象层次上应用等式饱和，或在其他编译阶段对代码进行变换时丢弃已发现的等式。我们提出了一种替代方案：在编译器的中间表示（IR）中原生地表示 e-图（e-graph），从而支持构造性编译阶段的执行，使 e-图状态在整个编译流程中得以保持。我们基于 Python 驱动的 MLIR 框架 xDSL，引入了一种新的 MLIR 语言（dialect）——eqsat，用于在 MLIR 代码中表示 e-图。我们证明，这种表示方式扩展了等式饱和在编译器中的应用范围，使我们能够将模式重写与其他编译变换交错进行。eqsat 语言为编译器提供了一个统一的抽象，使得等式饱和可以在同一 MLIR 流程中，同时应用于不同层次的中间表示。"
  },
  {
    "date": "2026-02-18",
    "title": "Mind the Gap: Evaluating LLMs for High-Level Malicious Package Detection vs. Fine-Grained Indicator Identification",
    "authors": "Ahmed Ryan, Ibrahim Khalil, Abdullah Al Jahid, Md Erfan, Akond Ashfaque Ur Rahman, Md Rayhanur Rahman",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.16304v1",
    "source": "arXiv",
    "abstract": "The prevalence of malicious packages in open-source repositories, such as PyPI, poses a critical threat to the software supply chain. While Large Language Models (LLMs) have emerged as a promising tool for automated security tasks, their effectiveness in detecting malicious packages and indicators remains underexplored. This paper presents a systematic evaluation of 13 LLMs for detecting malicious software packages. Using a curated dataset of 4,070 packages (3,700 benign and 370 malicious), we evaluate model performance across two tasks: binary classification (package detection) and multi-label classification (identification of specific malicious indicators). We further investigate the impact of prompting strategies, temperature settings, and model specifications on detection accuracy. We find a significant \"granularity gap\" in LLMs' capabilities. While GPT-4.1 achieves near-perfect performance in binary detection (F1 $\\approx$ 0.99), performance degrades by approximately 41\\% when the task shifts to identifying specific malicious indicators. We observe that general models are best for filtering out the majority of threats, while specialized coder models are better at detecting attacks that follow a strict, predictable code structure. Our correlation analysis indicates that parameter size and context width have negligible explanatory power regarding detection accuracy. We conclude that while LLMs are powerful detectors at the package level, they lack the semantic depth required for precise identification at the granular indicator level.",
    "title_zh": "注意差距：评估大模型在高级恶意软件包检测与细粒度指标识别中的表现",
    "abstract_zh": "开源代码仓库（如 PyPI）中恶意软件包的泛滥对软件供应链构成了严重威胁。尽管大型语言模型（LLMs）作为自动化安全任务的新兴工具展现出巨大潜力，但其在检测恶意软件包及其具体恶意指标方面的有效性仍缺乏深入研究。本文系统评估了13种LLM在检测恶意软件包方面的表现。基于一个精心筛选的4,070个软件包数据集（其中3,700个为良性，370个为恶意），我们从两个任务维度评估模型性能：二分类任务（软件包检测）和多标签分类任务（识别特定恶意指标）。此外，我们还研究了提示策略、温度设置及模型规格对检测准确率的影响。研究发现，LLM在能力上存在显著的“粒度差距”：尽管GPT-4.1在二分类任务中表现近乎完美（F1 ≈ 0.99），但当任务转向识别具体恶意指标时，性能下降约41%。我们观察到，通用模型在过滤绝大多数威胁方面表现最佳，而专为代码编写设计的模型则更擅长识别具有严格、可预测代码结构的攻击。相关性分析表明，参数量和上下文宽度对检测准确率的解释力微乎其微。综上所述，尽管LLM在软件包层面具备强大的检测能力，但在细粒度指标识别方面仍缺乏必要的语义深度。"
  },
  {
    "date": "2026-02-18",
    "title": "Discrete Stochastic Localization for Non-autoregressive Generation",
    "authors": "Yunshu Wu, Jiayi Cheng, Partha Thakuria, Rob Brekelmans, Evangelos E. Papalexakis, Greg Ver Steeg",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.16169v1",
    "source": "arXiv",
    "abstract": "Non-autoregressive (NAR) generation reduces decoding latency by predicting many tokens in parallel, but iterative refinement often suffers from error accumulation and distribution shift under self-generated drafts. Masked diffusion language models (MDLMs) and their remasking samplers (e.g., ReMDM) can be viewed as modern NAR iterative refinement, where generation repeatedly revises a partially observed draft. In this work we show that \\emph{training alone} can substantially improve the step-efficiency of MDLM/ReMDM sampling. We propose \\textsc{DSL} (Discrete Stochastic Localization), which trains a single SNR-invariant denoiser across a continuum of corruption levels, bridging intermediate draft noise and mask-style endpoint corruption within one Diffusion Transformer. On OpenWebText, \\textsc{DSL} fine-tuning yields large MAUVE gains at low step budgets, surpassing the MDLM+ReMDM baseline with \\(\\sim\\)4$\\times$ fewer denoiser evaluations, and matches autoregressive quality at high budgets. Analyses show improved self-correction and uncertainty calibration, making remasking markedly more compute-efficient.",
    "title_zh": "非自回归生成的离散随机定位",
    "abstract_zh": "非自回归（NAR）生成通过并行预测多个标记来降低解码延迟，但迭代精炼过程常因自生成草稿导致错误累积和分布偏移。掩码扩散语言模型（MDLMs）及其重掩码采样器（如 ReMDM）可被视为现代的 NAR 迭代精炼方法，其生成过程反复修正一个部分观测到的草稿。在本工作中，我们表明仅通过训练即可显著提升 MDLM/ReMDM 采样的步骤效率。我们提出了 \\textsc{DSL}（离散随机定位），该方法在一个扩散 Transformer 中训练一个对信噪比（SNR）不变的去噪器，覆盖从中间草稿噪声到掩码风格终点扰动的连续污染水平。在 OpenWebText 数据集上，\\textsc{DSL} 的微调在低步骤预算下实现了显著的 MAUVE 分数提升，相比 MDLM+ReMDM 基线，所需去噪器评估次数减少约 4 倍，而在高预算下则达到自回归模型的生成质量。分析表明，该方法提升了自我修正能力与不确定性校准效果，使重掩码过程显著更高效地利用计算资源。"
  },
  {
    "date": "2026-02-18",
    "title": "Algorithm-Based Pipeline for Reliable and Intent-Preserving Code Translation with LLMs",
    "authors": "Shahriar Rumi Dipto, Saikat Mondal, Chanchal K. Roy",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.16106v1",
    "source": "arXiv",
    "abstract": "Code translation, the automatic conversion of programs between languages, is a growing use case for Large Language Models (LLMs). However, direct one-shot translation often fails to preserve program intent, leading to errors in control flow, type handling, and I/O behavior. We propose an algorithm-based pipeline that introduces a language-neutral intermediate specification to capture these details before code generation. This study empirically evaluates the extent to which structured planning can improve translation accuracy and reliability relative to direct translation. We conduct an automated paired experiment - direct and algorithm-based to translate between Python and Java using five widely used LLMs on the Avatar and CodeNet datasets. For each combination (model, dataset, approach, and direction), we compile and execute the translated program and run the tests provided. We record compilation results, runtime behavior, timeouts (e.g., infinite loop), and test outcomes. We compute accuracy from these tests, counting a translation as correct only if it compiles, runs without exceptions or timeouts, and passes all tests. We then map every failed compile-time and runtime case to a unified, language-aware taxonomy and compare subtype frequencies between the direct and algorithm-based approaches. Overall, the Algorithm-based approach increases micro-average accuracy from 67.7% to 78.5% (10.8% increase). It eliminates lexical and token errors by 100%, reduces incomplete constructs by 72.7%, and structural and declaration issues by 61.1%. It also substantially lowers runtime dependency and entry-point failures by 78.4%. These results demonstrate that algorithm-based pipelines enable more reliable, intent-preserving code translation, providing a foundation for robust multilingual programming assistants.",
    "title_zh": "基于算法的管道实现可靠且意图保持的代码翻译（使用大语言模型）",
    "abstract_zh": "代码翻译，即程序在不同编程语言间的自动转换，正成为大型语言模型（LLMs）的一个日益重要的应用场景。然而，直接的一次性翻译往往无法保留程序的原始意图，导致控制流、类型处理和输入/输出行为等方面出现错误。为此，我们提出了一种基于算法的流水线方法，通过引入一种与语言无关的中间规范来捕捉这些关键细节，再进行代码生成。本研究通过实证评估，探讨了结构化规划在提升翻译准确性和可靠性方面相较于直接翻译的改进程度。我们设计并执行了一项自动化配对实验，使用五个广泛使用的LLM，在Avatar和CodeNet数据集上，对Python与Java之间的双向代码翻译进行对比测试。针对每一种组合（模型、数据集、方法和方向），我们对翻译后的程序进行编译与执行，并运行所提供的测试用例。我们记录了编译结果、运行时行为、超时情况（如无限循环）以及测试结果。基于这些测试，我们计算准确率：只有当翻译结果能够成功编译、运行时无异常或超时，并且通过所有测试时，才视为正确。随后，我们将所有编译期和运行期的失败案例映射到一个统一的、具备语言感知能力的分类体系中，并对比直接翻译与算法基方法在各类子类型失败频率上的差异。总体而言，算法基方法将微平均准确率从67.7%提升至78.5%（提升10.8%）。该方法实现了词法和标记错误的完全消除（100%减少），不完整结构的减少达72.7%，结构与声明问题减少61.1%。此外，运行时依赖和入口点失败也大幅降低78.4%。这些结果表明，基于算法的流水线能够实现更可靠、更忠实于原始意图的代码翻译，为构建稳健的多语言编程助手奠定了坚实基础。"
  },
  {
    "date": "2026-02-18",
    "title": "CADEvolve: Creating Realistic CAD via Program Evolution",
    "authors": "Maksim Elistratov, Marina Barannikov, Gregory Ivanov, Valentin Khrulkov, Anton Konushin, Andrey Kuznetsov, Dmitrii Zhemchuzhnikov",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.16317v1",
    "source": "arXiv",
    "abstract": "Computer-Aided Design (CAD) delivers rapid, editable modeling for engineering and manufacturing. Recent AI progress now makes full automation feasible for various CAD tasks. However, progress is bottlenecked by data: public corpora mostly contain sketch-extrude sequences, lack complex operations, multi-operation composition and design intent, and thus hinder effective fine-tuning. Attempts to bypass this with frozen VLMs often yield simple or invalid programs due to limited 3D grounding in current foundation models. We present CADEvolve, an evolution-based pipeline and dataset that starts from simple primitives and, via VLM-guided edits and validations, incrementally grows CAD programs toward industrial-grade complexity. The result is 8k complex parts expressed as executable CadQuery parametric generators. After multi-stage post-processing and augmentation, we obtain a unified dataset of 1.3m scripts paired with rendered geometry and exercising the full CadQuery operation set. A VLM fine-tuned on CADEvolve achieves state-of-the-art results on the Image2CAD task across the DeepCAD, Fusion 360, and MCB benchmarks.",
    "title_zh": "CADEvolve：通过程序演化创建真实的CAD",
    "abstract_zh": "计算机辅助设计（CAD）为工程与制造领域提供了快速、可编辑的建模能力。近年来，人工智能的进展使得多种CAD任务的全面自动化成为可能。然而，当前的发展受到数据瓶颈的制约：现有的公开数据集大多仅包含草图-拉伸序列，缺乏复杂操作、多步骤组合以及设计意图信息，因而难以有效支持模型的微调。一些研究尝试通过冻结视觉语言模型（VLM）来绕过这一问题，但往往因现有基础模型在三维空间中的语义理解能力有限，导致生成的程序过于简单或无效。为此，我们提出了 CADEvolve——一个基于进化思想的流程与数据集，从简单的几何体出发，通过VLM引导的编辑与验证，逐步迭代生成具备工业级复杂度的CAD程序。最终，我们得到了8,000个复杂的零件，均以可执行的CadQuery参数化生成器形式表达。经过多阶段后处理与增强，我们构建了一个统一的数据集，包含130万条脚本，每条脚本均配有渲染后的几何形态，并完整覆盖了CadQuery的所有操作功能。基于CADEvolve微调的VLM在Image2CAD任务上，在DeepCAD、Fusion 360和MCB等多个基准测试中均取得了当前最优表现。"
  },
  {
    "date": "2026-02-18",
    "title": "Beyond Learning: A Training-Free Alternative to Model Adaptation",
    "authors": "Namkyung Yoon, Kyeonghyun Yoo, Wooyong Jung, Sanghong Kim, Hwangnam Kim",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.16189v1",
    "source": "arXiv",
    "abstract": "Despite the continuous research and evolution of language models, they sometimes underperform previous versions. Existing approaches to overcome these challenges are resource-intensive, highlighting the need for alternatives that enable immediate action. We assume that each language model has a local module inside that is suitable for a specific function. First, this work identifies a set of modules showing consistent and local activation changes under an inference workload through activation-based analysis. Subsequently, we transplant an internal module that is properly activated for a specific task into the target model, leading to immediate and measurable functional changes without additional training or fine-tuning. To experimentally demonstrate the effectiveness of the transplant technique, we quantify the relationship between transplant strength and performance improvement under different conditions for two language models. In the cross-generation setting, we find that transplanting activation-selected modules can substantially improve the underperforming model, reaching up to twice the target baseline and achieving gap-based recovery above 100%. Moreover, in transplant experiments between a base model and its instruction-tuned counterpart, transplantation improves the underperforming model toward the stronger baseline, yielding up to about 2.33 times the target baseline with gap-based recovery reaching up to 100% in the best case. These results show that meaningful capacity transfer can be realized through the implantation of highly localized modules implied by language models. Overall, this work provides empirical evidence for task-localized modularity in language models and presents a new research area: model transplantation.",
    "title_zh": "超越学习：一种无需训练的模型适配替代方案",
    "abstract_zh": "尽管语言模型持续研究与演进，但它们有时仍会表现不如之前的版本。现有克服此类挑战的方法通常资源消耗巨大，凸显出对能够立即生效的替代方案的迫切需求。我们假设每个语言模型内部都包含一个适合特定功能的局部模块。本文首先通过基于激活的分析，识别出在推理负载下表现出一致且局部激活变化的一组模块。随后，我们将一个在特定任务中被恰当激活的内部模块移植到目标模型中，从而在无需额外训练或微调的情况下，实现立即且可测量的功能改进。为实验验证该移植技术的有效性，我们量化了两种语言模型在不同条件下，移植强度与性能提升之间的关系。在跨代际设置中，我们发现，将激活选择的模块进行移植，可显著提升表现欠佳的模型，性能最高可达目标基线的两倍，且基于差距的恢复率超过100%。此外，在基础模型与其指令微调版本之间的移植实验中，移植同样使表现较弱的模型向更强的基线靠拢，性能最高可达目标基线的约2.33倍，最佳情况下差距恢复率也达到100%。这些结果表明，通过植入语言模型中隐含的高度局部化模块，可以实现有意义的能力迁移。总体而言，本研究为语言模型中任务局部化的模块性提供了实证支持，并开启了一个全新的研究方向：模型移植。"
  },
  {
    "date": "2026-02-18",
    "title": "\"What I'm Interested in is Something that Violates the Law\": Regulatory Practitioner Views on Automated Detection of Deceptive Design Patterns",
    "authors": "Arianna Rossi, Simon Parkin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.16302v1",
    "source": "arXiv",
    "abstract": "Although deceptive design patterns are subject to growing regulatory oversight, enforcement races to keep up with the scale of the problem. One promising solution is automated detection tools, many of which are developed within academia. We interviewed nine experienced practitioners working within or alongside regulatory bodies to understand their work against deceptive design patterns, including the use of supporting tools and the prospect of automation. Computing technologies have their place in regulatory practice, but not as envisioned in research. For example, investigations require utmost transparency and accountability in all the activities we identify as accompanying dark pattern detection, which many existing tools cannot provide. Moreover, tools need to map interfaces to legal violations to be of use. We thus recommend conducting user requirement research to maximize research impact, supporting ancillary activities beyond detection, and establishing practical tech adoption pathways that account for the needs of both scientific and regulatory activities.",
    "title_zh": "“我感兴趣的是那些违反法律的东西”：监管实践者对自动化检测欺骗性设计模式的看法",
    "abstract_zh": "尽管欺骗性设计模式正面临日益严格的监管审查，但执法力度仍难以跟上问题的规模。一种有前景的解决方案是自动化检测工具，其中许多工具由学术界开发。我们采访了九位在监管机构内部或与其合作的资深从业者，以了解他们应对欺骗性设计模式的工作，包括所使用的辅助工具以及自动化前景。计算技术在监管实践中确实有其位置，但并非如研究中所设想的那样。例如，调查工作要求在所有与暗模式检测相关的活动中保持最高程度的透明度和问责性，而许多现有工具无法提供这种保障。此外，工具必须能够将用户界面与法律违规行为对应起来，才能真正发挥作用。因此，我们建议开展用户需求研究，以最大化研究成果的影响，支持检测之外的辅助活动，并建立切实可行的技术采纳路径，充分考虑科研与监管活动的双重需求。"
  },
  {
    "date": "2026-02-18",
    "title": "Policy Compiler for Secure Agentic Systems",
    "authors": "Nils Palumbo, Sarthak Choudhary, Jihye Choi, Prasad Chalasani, Mihai Christodorescu, Somesh Jha",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.16708v1",
    "source": "arXiv",
    "abstract": "LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement. Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning. PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.",
    "title_zh": "安全代理系统策略编译器",
    "abstract_zh": "基于大语言模型（LLM）的智能体正越来越多地应用于需要复杂授权策略的场景，如客户服务流程、审批工作流、数据访问限制以及合规监管。将这些策略直接嵌入提示词（prompts）中无法提供任何强制执行的保障。为此，我们提出了PCAS——一种面向智能体系统的策略编译器（Policy Compiler for Agentic Systems），能够实现确定性的策略强制执行。\n\n强制执行此类策略需要追踪智能体之间信息的流动，而传统的线性消息历史记录无法有效捕捉这种动态关系。为此，PCAS将智能体系统的状态建模为一个依赖图（dependency graph），以显式刻画事件之间的因果关系，例如工具调用、工具返回结果以及消息传递等。策略以一种源自Datalog的语言表达，采用声明式规则的形式，能够处理传递性信息流和跨智能体的溯源关系。\n\nPCAS引入了一个参考监视器（reference monitor），拦截所有操作行为，并在执行前阻止违规行为，从而实现与模型推理过程无关的确定性策略强制。PCAS接收一个已有的智能体实现和策略规范，将其编译为一个经过增强的系统，该系统从构造之初就符合策略要求，无需进行任何专门的安全架构重构。\n\n我们在三个案例研究中评估了PCAS：用于防御提示注入攻击的信息流策略、多智能体药物警戒系统中的审批工作流，以及客户服务场景下的组织级策略。在客户服务任务中，PCAS将前沿模型的策略合规率从48%提升至93%，且在所有经过增强的运行中实现了零策略违规。"
  },
  {
    "date": "2026-02-18",
    "title": "MerLean: An Agentic Framework for Autoformalization in Quantum Computation",
    "authors": "Yuanjie Ren, Jinzheng Li, Yidi Qi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.16554v1",
    "source": "arXiv",
    "abstract": "We introduce MerLean, a fully automated agentic framework for autoformalization in quantum computation. MerLean extracts mathematical statements from \\LaTeX{} source files, formalizes them into verified Lean~4 code built on Mathlib, and translates the result back into human-readable \\LaTeX{} for semantic review. We evaluate MerLean on three theoretical quantum computing papers producing 2,050 Lean declarations from 114 statements in total. MerLean achieves end-to-end formalization on all three papers, reducing the verification burden to only the newly introduced definitions and axioms. Our results demonstrate that agentic autoformalization can scale to frontier research, offering both a practical tool for machine-verified peer review and a scalable engine for mining high-quality synthetic data to train future reasoning models. Our approach can also be generalized to any other rigorous research in mathematics and theoretical physics.",
    "title_zh": "MerLean：量子计算中自动形式化的智能体框架",
    "abstract_zh": "我们提出了 MerLean，一个用于量子计算领域自动形式化的全自动化智能体框架。MerLean 能够从 \\LaTeX{} 源文件中提取数学命题，将其形式化为基于 Mathlib 构建的可验证 Lean~4 代码，并将结果转换回人类可读的 \\LaTeX{} 格式，以供语义审查。我们在三篇理论量子计算论文上评估了 MerLean，共从 114 个命题中生成了 2,050 条 Lean 声明。MerLean 在所有三篇论文上均实现了端到端的形式化，将验证工作量仅限于新引入的定义和公理。我们的结果表明，智能体式自动形式化能够扩展至前沿研究领域，既可作为机器验证同行评审的实用工具，也可作为生成高质量合成数据的可扩展引擎，用于训练未来的推理模型。此外，该方法还可推广至数学和理论物理等其他严谨研究领域。"
  },
  {
    "date": "2026-02-18",
    "title": "MMA: Multimodal Memory Agent",
    "authors": "Yihao Lu, Wanru Cheng, Zeyu Zhang, Hao Tang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.16493v1",
    "source": "arXiv",
    "abstract": "Long-horizon multimodal agents depend on external memory; however, similarity-based retrieval often surfaces stale, low-credibility, or conflicting items, which can trigger overconfident errors. We propose Multimodal Memory Agent (MMA), which assigns each retrieved memory item a dynamic reliability score by combining source credibility, temporal decay, and conflict-aware network consensus, and uses this signal to reweight evidence and abstain when support is insufficient. We also introduce MMA-Bench, a programmatically generated benchmark for belief dynamics with controlled speaker reliability and structured text-vision contradictions. Using this framework, we uncover the \"Visual Placebo Effect\", revealing how RAG-based agents inherit latent visual biases from foundation models. On FEVER, MMA matches baseline accuracy while reducing variance by 35.2% and improving selective utility; on LoCoMo, a safety-oriented configuration improves actionable accuracy and reduces wrong answers; on MMA-Bench, MMA reaches 41.18% Type-B accuracy in Vision mode, while the baseline collapses to 0.0% under the same protocol. Code: https://github.com/AIGeeksGroup/MMA.",
    "title_zh": "多模态记忆代理",
    "abstract_zh": "长时程多模态智能体依赖外部记忆；然而，基于相似性的检索常会返回过时、低可信度或相互矛盾的记忆项，从而引发过度自信的错误。我们提出多模态记忆智能体（Multimodal Memory Agent, MMA），通过结合源可信度、时间衰减以及冲突感知的网络共识机制，为每条检索到的记忆项动态分配可靠性评分，并利用该信号重新加权证据，在支持不足时选择不回答。我们还引入了MMA-Bench，一个通过程序生成的基准测试框架，用于研究信念动态，具备可控的说话人可信度和结构化的文本-视觉矛盾。基于此框架，我们揭示了“视觉安慰剂效应”，揭示了基于RAG的智能体如何继承基础模型中的潜在视觉偏见。在FEVER数据集上，MMA在保持基线准确率的同时，将方差降低35.2%，并提升了选择性实用性；在LoCoMo数据集上，面向安全性的配置显著提升了可操作准确率并减少了错误回答；在MMA-Bench上，MMA在视觉模式下达到41.18%的Type-B准确率，而基线模型在同一协议下骤降至0.0%。代码地址：https://github.com/AIGeeksGroup/MMA。"
  },
  {
    "date": "2026-02-18",
    "title": "SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation",
    "authors": "Jaid Monwar Chowdhury, Chi-An Fu, Reyhaneh Jabbarvand",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.16671v1",
    "source": "arXiv",
    "abstract": "Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.",
    "title_zh": "SPARC：面向自动化C单元测试生成的情景规划与推理",
    "abstract_zh": "C语言的自动化单元测试生成仍然是一个严峻的挑战，其根源在于高层次程序意图与指针运算及手动内存管理所固有的严格语法约束之间的语义鸿沟。尽管大型语言模型（LLMs）展现出强大的代码生成能力，但直接从意图到代码的合成过程常常陷入“过早生成代码”的失败模式——即模型在缺乏对程序结构、约束和语义充分理解的情况下就提前输出代码。这会导致生成的测试无法编译、虚构函数签名、分支覆盖率低，以及语义无关的断言，从而无法有效捕捉缺陷。为此，我们提出了SPARC，一种基于场景的神经符号化框架，通过四个阶段弥合这一鸿沟：(1) 控制流图（CFG）分析，(2) 操作映射（Operation Map），将LLM的推理锚定在经过验证的实用辅助函数上，(3) 面向路径的目标测试生成，以及(4) 利用编译器和运行时反馈进行迭代式自我修正的验证循环。我们在59个真实世界和算法性项目上对SPARC进行了评估，结果表明，相比原始提示生成基线，其在行覆盖率上提升31.36%，分支覆盖率提升26.01%，突变得分提升20.78%，在复杂任务上达到甚至超越符号执行工具KLEE的性能。SPARC通过迭代修复保留了94.3%的测试用例，并生成了显著更受开发者认可、可读性和可维护性更高的代码。通过将LLM的推理与程序结构对齐，SPARC为工业级遗留C代码库的测试提供了一条可扩展的路径。"
  },
  {
    "date": "2026-02-18",
    "title": "Learning to Learn from Language Feedback with Social Meta-Learning",
    "authors": "Jonathan Cook, Diego Antognini, Martin Klissarov, Claudiu Musat, Edward Grefenstette",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.16488v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) often struggle to learn from corrective feedback within a conversational context. They are rarely proactive in soliciting this feedback, even when faced with ambiguity, which can make their dialogues feel static, one-sided, and lacking the adaptive qualities of human conversation. To address these limitations, we draw inspiration from social meta-learning (SML) in humans - the process of learning how to learn from others. We formulate SML as a finetuning methodology, training LLMs to solicit and learn from language feedback in simulated pedagogical dialogues, where static tasks are converted into interactive social learning problems. SML effectively teaches models to use conversation to solve problems they are unable to solve in a single turn. This capability generalises across domains; SML on math problems produces models that better use feedback to solve coding problems and vice versa. Furthermore, despite being trained only on fully-specified problems, these models are better able to solve underspecified tasks where critical information is revealed over multiple turns. When faced with this ambiguity, SML-trained models make fewer premature answer attempts and are more likely to ask for the information they need. This work presents a scalable approach to developing AI systems that effectively learn from language feedback.",
    "title_zh": "通过社交元学习从语言反馈中学会学习",
    "abstract_zh": "大型语言模型（LLMs）在对话情境中往往难以从纠正性反馈中有效学习。它们很少主动寻求此类反馈，即使面对模糊信息时也是如此，这使得对话显得呆板、单向，缺乏人类交流所具有的动态适应特性。为解决这些局限，我们受到人类社会元学习（SML）的启发——即学会如何从他人那里学习的过程。我们将SML构想为一种微调方法，通过在模拟教学对话中训练大模型主动寻求并从语言反馈中学习，将原本静态的任务转化为互动性的社会学习问题。SML有效地教会模型利用对话来解决单次交互无法完成的问题。这一能力具有跨领域的泛化性：在数学问题上进行SML训练的模型，在处理编程问题时能更有效地利用反馈，反之亦然。此外，尽管这些模型仅在完整明确的问题上进行训练，但在面对信息逐步揭示的不完整任务时，表现也更为出色。当面临不确定性时，经过SML训练的模型更少做出过早的回答，也更倾向于主动询问所需信息。本研究提出了一种可扩展的方法，用于开发能够高效从语言反馈中学习的人工智能系统。"
  },
  {
    "date": "2026-02-18",
    "title": "Evolutionary Context Search for Automated Skill Acquisition",
    "authors": "Qi Sun, Stefan Nielsen, Rio Yokota, Yujin Tang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.16113v1",
    "source": "arXiv",
    "abstract": "Large Language Models cannot reliably acquire new knowledge post-deployment -- even when relevant text resources exist, models fail to transform them into actionable knowledge without retraining. Retrieval-Augmented Generation attempts to bridge this gap by surfacing relevant documents at inference time, yet similarity-based retrieval often fails to identify context that actually improves task performance. We introduce Evolutionary Context Search (ECS), an evolutionary method that searches context combinations using accuracy on a small development set, requiring only inference calls without weight updates. ECS moves beyond semantic similarity to discover non-obvious context pairings that significantly boost performance. Our empirical results show that ECS improves BackendBench by 27\\% and $τ$-bench airline by 7\\%. The evolved contexts are model-agnostic, as those evolved with Gemini-3-Flash transfer effectively to Claude Sonnet and DeepSeek. This suggests that ECS opens a path toward automated context discovery for skill acquisition -- an efficient alternative to manual prompt engineering or costly fine-tuning.",
    "title_zh": "基于进化背景的自动化技能获取搜索",
    "abstract_zh": "大型语言模型在部署后无法可靠地获取新知识——即使存在相关文本资源，模型也难以在不重新训练的情况下将这些资源转化为可操作的知识。检索增强生成（Retrieval-Augmented Generation）试图通过在推理时提取相关文档来弥补这一差距，但基于相似性的检索常常无法识别真正能提升任务性能的上下文。我们提出了进化式上下文搜索（Evolutionary Context Search, ECS），这是一种基于进化的上下文组合搜索方法，利用小规模开发集上的准确率作为评估标准，仅需推理调用而无需权重更新。ECS超越了语义相似性，能够发现非显而易见的上下文组合，显著提升模型性能。实验结果表明，ECS使BackendBench的性能提升了27%，使$τ$-bench航空任务的性能提升了7%。所生成的上下文具有模型无关性：在Gemini-3-Flash上进化出的上下文可有效迁移至Claude Sonnet和DeepSeek模型。这表明，ECS为自动化上下文发现开辟了新路径，为技能获取提供了一种高效替代方案，既无需繁琐的手动提示工程，也避免了昂贵的微调成本。"
  },
  {
    "date": "2026-02-18",
    "title": "Who can we trust? LLM-as-a-jury for Comparative Assessment",
    "authors": "Mengjie Qian, Guangzhi Sun, Mark J. F. Gales, Kate M. Knill",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.16610v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) are increasingly applied as automatic evaluators for natural language generation assessment often using pairwise comparative judgements. Existing approaches typically rely on single judges or aggregate multiple judges assuming equal reliability. In practice, LLM judges vary substantially in performance across tasks and aspects, and their judgment probabilities may be biased and inconsistent. Furthermore, human-labelled supervision for judge calibration may be unavailable. We first empirically demonstrate that inconsistencies in LLM comparison probabilities exist and show that it limits the effectiveness of direct probability-based ranking. To address this, we study the LLM-as-a-jury setting and propose BT-sigma, a judge-aware extension of the Bradley-Terry model that introduces a discriminator parameter for each judge to jointly infer item rankings and judge reliability from pairwise comparisons alone. Experiments on benchmark NLG evaluation datasets show that BT-sigma consistently outperforms averaging-based aggregation methods, and that the learned discriminator strongly correlates with independent measures of the cycle consistency of LLM judgments. Further analysis reveals that BT-sigma can be interpreted as an unsupervised calibration mechanism that improves aggregation by modelling judge reliability.",
    "title_zh": "我们该信任谁？基于大语言模型的“陪审团”进行比较评估",
    "abstract_zh": "大型语言模型（LLMs）在自然语言生成评估中正越来越多地被用作自动评价工具，通常采用成对比较判断的方式。现有方法通常依赖单一评价员或对多个评价员的判断进行聚合，且假设所有评价员的可靠性相同。然而在实际应用中，LLM评价员在不同任务和不同维度上的表现差异显著，其判断概率可能存在偏差且不一致。此外，用于校准评价员性能的人工标注监督数据可能难以获取。我们首先通过实证研究证明了LLM比较概率中确实存在不一致性，并指出这种不一致性会限制基于概率直接排序的有效性。为解决该问题，我们研究了“LLM作为陪审团”（LLM-as-a-jury）的设定，提出BT-sigma——一种面向评价员的布拉德利-特里（Bradley-Terry）模型扩展方法，该方法为每位评价员引入一个判别参数，仅通过成对比较数据即可联合推断项目排序与评价员可靠性。在基准自然语言生成评估数据集上的实验表明，BT-sigma始终优于基于平均的聚合方法，且所学习到的判别参数与独立测量的LLM判断循环一致性高度相关。进一步分析表明，BT-sigma可被解释为一种无监督校准机制，通过建模评价员可靠性来提升整体聚合效果。"
  },
  {
    "date": "2026-02-18",
    "title": "Optimizing Soft Prompt Tuning via Structural Evolution",
    "authors": "Zhenzhen Huang, Chaoning Zhang, Haoyu Bian, Songbo Zhang, Chi-lok Andy Tai, Jiaquan Zhang, Caiyan Qin, Jingjing Qu, Yalan Ye, Yang Yang, Heng Tao Shen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.16500v1",
    "source": "arXiv",
    "abstract": "Soft prompt tuning leverages continuous embeddings to capture task-specific information in large pre-trained language models (LLMs), achieving competitive performance in few-shot settings. However, soft prompts rely on high-dimensional, implicit representations and lack explicit semantics and traceable training behaviors, which limits their interpretability. To address this limitation, we propose a soft prompt tuning optimization method based on topological morphological evolution. Specifically, we employ persistent homology from topological data analysis (TDA) to quantify the structural representations of soft prompts in continuous parameter space and their training process evolution. Quantitative analysis shows that topologically stable and compact soft prompts achieve better downstream performance. Based on this empirical observation, we construct a loss function for optimizing soft prompt tuning, termed Topological Soft Prompt Loss (TSLoss). TSLoss guides the model to learn structurally stable adaptations by quantifying inter-parameter connectivity and redundancy. Extensive experiments show that training with TSLoss accelerates convergence and improves tuning performance, providing an interpretable method to understand and optimize soft prompt tuning from structural and topological perspectives.",
    "title_zh": "通过结构演化优化软提示调优",
    "abstract_zh": "软提示调优利用连续嵌入来捕捉大型预训练语言模型（LLMs）中的任务特定信息，在少样本设置下取得了具有竞争力的性能。然而，软提示依赖于高维且隐式的表示，缺乏明确的语义和可追溯的训练行为，限制了其可解释性。为解决这一局限，我们提出一种基于拓扑形态演化优化的软提示调优方法。具体而言，我们采用拓扑数据分析（TDA）中的持久同调技术，量化软提示在连续参数空间中的结构表征及其训练过程中的演化特征。定量分析表明，拓扑上稳定且紧凑的软提示能够获得更优的下游性能。基于这一实证观察，我们构建了一种用于优化软提示调优的损失函数，称为拓扑软提示损失（TSLoss）。TSLoss通过量化参数间的连通性与冗余度，引导模型学习结构稳定的适应性调整。大量实验表明，使用TSLoss进行训练能够加速收敛并提升调优性能，为从结构与拓扑视角理解与优化软提示调优提供了一种可解释的方法。"
  },
  {
    "date": "2026-2-18",
    "title": "Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing",
    "authors": "Leszek Sliwko, Jolanta Mizeria-Pietraszko",
    "publish": "IEEE Access",
    "url": "https://doi.org/10.1109/access.2026.3665989",
    "source": "IEEE",
    "abstract": "Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration.",
    "title_zh": "集群工作负载分配：基于自然语言处理的语义软亲和性",
    "abstract_zh": "集群工作负载分配通常需要复杂的配置，导致用户体验上的差距。本文提出了一种基于自然语言处理（NLP）的语义化、意图驱动的集群调度范式。该系统通过集成大型语言模型（LLM）的Kubernetes调度扩展器，解析自然语言形式的工作负载分配提示注解，以实现对软亲和性偏好（soft affinity preferences）的理解。研究团队开发了一个原型系统，包含集群状态缓存和使用AWS Bedrock实现的意图分析模块。实证评估显示，顶级模型如Amazon Nova Pro/Premier和Mistral Pixtral Large在评估基准数据集上的LLM解析准确率超过95%（子集准确率），显著优于基线引擎。在六个不同场景下的调度质量测试表明，该原型系统在工作负载部署效果上达到或优于标准Kubernetes配置，尤其在复杂和定量场景中表现突出，并能有效处理软偏好之间的冲突。结果验证了利用LLM实现更易用调度的可行性，但也揭示了同步LLM延迟等局限性，建议采用异步处理以提升生产环境的可用性。本研究证实了语义化软亲和性在简化工作负载编排方面的可行性与潜力。"
  },
  {
    "date": "2026-2-18",
    "title": "Green AI or Overlooked Software Costs? Quantifying the Real Impact of Software Stacks on Scientific ML Emissions",
    "authors": "Amar Banerjee",
    "publish": "Computing in Science &amp; Engineering",
    "url": "https://doi.org/10.1109/mcse.2026.3665365",
    "source": "IEEE",
    "abstract": "Recent claims of “Green AI” position efficiency as a core design principle for modern Machine Learning (ML), yet an unresolved question remains: are current workflows genuinely sustainable, or are they inadvertently overlooking significant software-induced environmental costs? This issue is especially relevant for scientific and engineering domains, where reproducibility, robustness, and quantifiable error bounds are critical. While prior work focuses largely on pruning, compression, and hardware acceleration, the environmental impact of software-layer choices, programming language, compiler strategy, runtime overhead, and implementation design, remains underexplored. This paper quantifies the contribution of software decisions to the energy and carbon dioxide (<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">CO</i> <sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sub>) equivalent footprint of scientific ML pipelines. Using four workloads, we compare Python and equivalent C++ implementations across desktop CPUs and edge devices. Results show 4×–24× energy savings with negligible accuracy loss. We propose C3-ECO, a design-oriented sustainability framework highlighting Compact modeling, Conscious coding, Energy-aware execution, and Conscious Optimization.",
    "title_zh": "绿色人工智能还是被忽视的软件成本？量化软件栈对科学机器学习碳排放的真实影响",
    "abstract_zh": "近期关于“绿色人工智能”（Green AI）的主张将效率定位为现代机器学习（ML）的核心设计原则，然而一个尚未解决的问题依然存在：当前的工作流程是否真正可持续，还是在无意中忽略了显著的软件引发的环境成本？这一问题在科学与工程领域尤为关键，因为可复现性、鲁棒性以及可量化的误差边界至关重要。尽管以往研究主要聚焦于剪枝、压缩和硬件加速，但软件层的选择、编程语言、编译策略、运行时开销以及实现设计对环境的影响仍鲜有探讨。本文量化了软件决策在科学机器学习工作流中的能源消耗与二氧化碳（CO₂）排放等效足迹的贡献。通过四个典型工作负载，我们在桌面CPU和边缘设备上对比了Python与等效的C++实现。结果表明，C++实现可带来4倍至24倍的能效提升，同时几乎不损失精度。为此，我们提出了C3-ECO——一种面向设计的可持续性框架，强调紧凑建模（Compact modeling）、有意识编码（Conscious coding）、能源感知执行（Energy-aware execution）和有意识优化（Conscious Optimization）四大原则。"
  },
  {
    "date": "2026-2-18",
    "title": "Data-Driven Approach for Dynamic Comparator in Analog-to-Digital Conversion",
    "authors": "Buddhi Prakash Dharma, Anu Gupta, Chandra Shekhar",
    "publish": "TENCON 2025 - 2025 IEEE Region 10 Conference (TENCON)",
    "url": "https://doi.org/10.1109/tencon66050.2025.11375215",
    "source": "IEEE",
    "abstract": "The performance of dynamic comparators significantly influences the efficiency, speed, and accuracy of Analog-to-Digital Converters (ADCs), especially in modern low-power and high-speed applications. Traditional design methodologies relying on exhaustive simulations and manual tuning are inefficient and fail to adapt to process, voltage, and temperature (PVT) variations. This paper proposes a comprehensive data-driven optimization framework that leverages machine learning (ML) and artificial intelligence (AI) to enhance dynamic comparator performance across key metrics. The framework introduces a four-stage pipeline comprising feature extraction from simulation and measurement data, supervised learning for offset voltage prediction, reinforcement learning (RL) for adaptive calibration, and Bayesian optimization for automated sizing of circuit components. These methods collectively reduce design complexity, improve accuracy and power efficiency, and enable real-time adaptability. While integration with Electronic Design Automation (EDA) tools remains a future direction, the proposed framework lays the foundation for scalable, intelligent comparator design targeting applications such as IoT, biomedical systems, and high-speed communications.",
    "title_zh": "基于数据驱动的模拟-数字转换中动态比较器方法",
    "abstract_zh": "动态比较器的性能显著影响模数转换器（ADC）的效率、速度和精度，尤其是在现代低功耗和高速应用中。传统的设计方法依赖于全面的仿真和人工调参，效率低下，且难以应对工艺、电压和温度（PVT）变化。本文提出了一种全面的数据驱动优化框架，利用机器学习（ML）和人工智能（AI）技术，以提升动态比较器在关键性能指标上的表现。该框架采用四阶段流程：从仿真和实测数据中提取特征；利用监督学习预测失调电压；通过强化学习（RL）实现自适应校准；借助贝叶斯优化实现电路元件的自动尺寸设计。这些方法协同作用，有效降低设计复杂度，提升精度与功耗效率，并实现实时自适应能力。尽管与电子设计自动化（EDA）工具的集成仍是未来发展方向，但所提出的框架为可扩展、智能化的比较器设计奠定了基础，适用于物联网、生物医学系统以及高速通信等应用场景。"
  },
  {
    "date": "2026-2-18",
    "title": "Low-Power VLSI Accelerator Architecture for AI-Enhanced Real-Time Audio and Video Processing at the Network Edge",
    "authors": "D Shaiksha Vali, Ala'a Al-Shaikh, Venkata Ramaiah Kavuri, SkGouse John, Sravanthi, R Suresh Kumar",
    "publish": "2025 Tenth International Conference on Science Technology Engineering and Mathematics (ICONSTEM)",
    "url": "https://doi.org/10.1109/iconstem65670.2025.11374546",
    "source": "IEEE",
    "abstract": "Multiple multimedia analytics functions that utilize edge-based Artificial Intelligence (AI) require efficient and low-power machines that can support real-time audio and video streams. The current accelerator architectures (GPUs and FPGAs) are powerful but have high energy consumption, high data movement latency, and a high level of scalability, which does not allow simultaneous processing of many modalities. In order to overcome these shortcomings, this study adopts a Low-Power VLSI Accelerator Architecture that has been tuned to run AI-based real-time audio and video processing in the network edge. The proposed model integrates the hierarchical dataflow, adaptive clock gating, and multi-core neural processing unit (NPU) that is tuned to systolic array-based computing with the dynamic voltage-frequency scaling (DVFS) that ensures the efficiency of the workload balancing with lesser energy leakage. Moreover, the architecture uses on-chip memory compression and shared buffer reuse techniques that reduce redundancy in accessing memory; this improves the throughput besides bandwidth use. The experimental analysis shown that the proposed model performs well with the improvement in reduction of power consumption, reduction of latency and improved energy consumption of a GPU. The results of this study should play a significant role in the implementation of intelligent, scalable, and sustainable edge computing systems to next-generation IoT and multimedia applications that would allow real-time decision-making with a low latency and power consumption.",
    "title_zh": "面向网络边缘的低功耗VLSI加速器架构：用于AI增强的实时音视频处理",
    "abstract_zh": "采用基于边缘的人工智能（AI）的多种多媒体分析功能，需要高效且低功耗的设备，以支持实时音视频流处理。当前的加速器架构（如GPU和FPGA）虽然性能强大，但存在能耗高、数据传输延迟大以及可扩展性差等问题，难以实现多种模态的并行处理。为克服这些不足，本研究提出了一种低功耗VLSI加速器架构，专为在网络边缘运行基于AI的实时音视频处理而优化。所提出的模型集成了分层数据流、自适应时钟门控以及针对阵列计算优化的多核神经处理单元（NPU），并结合动态电压频率调节（DVFS）技术，有效实现工作负载均衡，降低能量泄漏。此外，该架构采用片上内存压缩和共享缓冲区复用技术，减少了内存访问的冗余，从而在提升吞吐量的同时优化了带宽利用率。实验分析表明，该模型在降低功耗、减少延迟以及提升能效方面均优于传统GPU。本研究的结果将对下一代物联网和多媒体应用中智能、可扩展且可持续的边缘计算系统的实现起到重要作用，支持低延迟、低功耗的实时决策。"
  },
  {
    "date": "2026-2-18",
    "title": "Late Binding ROM: Flexible, In-Field Updateable Post Quantum Cryptography in Hardware",
    "authors": "John Barry, Marek Zmuda, Nikola Radovanovic, Yarden Hareven, Paul Stanley Sundar, Christine Severns-Williams",
    "publish": "2025 Cyber Research Conference - Ireland (Cyber-RCI)",
    "url": "https://doi.org/10.1109/cyber-rci68134.2025.11385265",
    "source": "IEEE",
    "abstract": "The threat which quantum computers pose to the security of the asymmetric cryptographic algorithms underpinning digital infrastructure has been widely recognized and is actively being mitigated across the industry. Post quantum cryptographic algorithms have been developed and standardized which are resistant to attacks using quantum computing techniques. However, new algorithms are under development and the implementations of the new standards are, relative to established algorithms such as RSA or ECDSA, immature and untested in the field. For this reason, hardware manufacturers must be prepared for the fact that the definition of these algorithms may change in the near future. This paper proposes a field-upgradeable mechanism which enables post quantum algorithm implementations within a hardware Root of Trust for Detection. The proposed mechanism ensures a secure boot flow while allowing for new algorithms to be deployed or vulnerabilities which emerge in existing algorithms or their implementations to be rectified in the field.",
    "title_zh": "晚期绑定ROM：硬件中灵活且可现场更新的后量子密码学",
    "abstract_zh": "量子计算机对支撑数字基础设施的非对称加密算法安全性的威胁已广为人知，业界正在积极采取措施加以应对。目前已开发并标准化了一批抗量子计算攻击的后量子加密算法。然而，新的算法仍在持续研发中，相较于RSA或ECDSA等成熟算法，新标准的实现仍处于初期阶段，尚未在实际应用中经过充分验证。因此，硬件制造商必须做好准备，应对这些算法定义在未来短期内可能发生变更的情况。本文提出一种可现场升级的机制，能够在硬件可信根（Root of Trust）中实现后量子加密算法。该机制在确保安全启动流程的同时，支持在实际部署环境中更新新算法，或修复现有算法及其实现中暴露出的安全漏洞。"
  },
  {
    "date": "2026-2-18",
    "title": "Reinforcing CBDC Integrity: A Novel Anti Money Laundering Solution by Integrating Blockchain, Machine Learning, and Taint Analysis",
    "authors": "Shamama Tul Amber, Huma Ghafoor, Pham Duy Thanh, Chih-Hsien Hsia",
    "publish": "IEEE Open Journal of the Computer Society",
    "url": "https://doi.org/10.1109/ojcs.2026.3666199",
    "source": "IEEE",
    "abstract": "As the central bank digital currency (CBDC) is rapidly becoming operational, there are both opportunities for financial inclusion and efficiency, as well as problems of security threats and anti money laundering (AML), particularly due to concerns about user anonymity. In order to address the risks associated with anonymous transactions, this research proposes a multi-layered security framework for CBDC that makes use of smart contracts built on the Ethereum platform, taint analysis, and machine learning (ML) models. The solution makes use of programmable smart contracts to automate policy enforcement and transaction validation within the CBDC ecosystem. Taint analysis techniques are incorporated to track the movement of illicit funds and identify questionable transaction patterns across the blockchain network. This is further enhanced by ML models that are optimized to learn from transaction data in order to reliably identify anomalous or illicit actions before they occur. We have generated two synthetic datasets that include two case scenarios and trained six ML models to evaluate them comparatively. Of them, random forest had the highest level of accuracy, 91.11%, in the cross-border case, whereas the support vector machine had a accuracy of 93.12% in the case of real estate transactions. In addition, we conducted a performance comparison of five environments; traditional banking, CBDC with baseline blockchain, CBDC with blockchain, CBDC with ML, and CBDC with blockchain, taint analysis and ML. Weused different metrics to test the performance of our proposed scheme and found that our AML tracking algorithm took an average of 0.55 s inference time, which is faster than the underlying reference method. According to our results, the proposed combined framework ensures high-level protection that improved risk detection in digital currencies, with a significantly reduced risk of money laundering and related hazards when using CBDC systems.",
    "title_zh": "强化央行数字货币的完整性：融合区块链、机器学习与污染分析的新型反洗钱解决方案",
    "abstract_zh": "随着中央银行数字货币（CBDC）迅速进入实际运行阶段，其在促进金融包容性和提升效率方面展现出巨大潜力，同时也带来了安全威胁和反洗钱（AML）方面的挑战，尤其是用户匿名性引发的担忧。为应对匿名交易带来的风险，本研究提出了一种基于以太坊平台的智能合约、污点分析技术以及机器学习（ML）模型的多层安全框架。该方案利用可编程智能合约，在CBDC生态系统中实现政策执行与交易验证的自动化。通过引入污点分析技术，可追踪非法资金的流动，并识别区块链网络中可疑的交易模式。该机制进一步通过经过优化的机器学习模型得到增强，这些模型能够从交易数据中学习，从而在异常或非法行为发生前可靠地识别出潜在风险。我们生成了两个包含不同场景的合成数据集，并训练了六种机器学习模型进行对比评估。结果显示，在跨境交易场景中，随机森林模型的准确率最高，达到91.11%；而在房地产交易场景中，支持向量机模型的准确率最高，达到93.12%。此外，我们对五种不同环境进行了性能对比：传统银行系统、基于基础区块链的CBDC、基于区块链的CBDC、基于机器学习的CBDC，以及结合区块链、污点分析与机器学习的CBDC。通过多种指标测试所提出方案的性能，结果表明，我们的反洗钱追踪算法平均推理时间仅为0.55秒，优于底层参考方法。根据研究结果，所提出的综合框架能够提供高水平的安全保障，显著提升数字货币中的风险识别能力，有效降低CBDC系统使用过程中洗钱及相关风险的发生概率。"
  },
  {
    "date": "2026-2-18",
    "title": "Hardware Software Co-Design of 2D Modulation Schemes Otfs and Otsm on System-on-Chip",
    "authors": "Sai Kumar Dora, Rakesh Kumar Yadav, Himanshu B. Mishra, Amitav Panda",
    "publish": "TENCON 2025 - 2025 IEEE Region 10 Conference (TENCON)",
    "url": "https://doi.org/10.1109/tencon66050.2025.11375128",
    "source": "IEEE",
    "abstract": "In this paper, we design low-complexity hardware architectures for the basic modules of the two-dimensional (2D) modulation scheme, orthogonal time sequency multiplexing (OTSM). OTSM scheme works in the delay-sequence domain by using the basic modules inverse Walsh-Hadamard transform (IWHT) and Walsh-Hadamard transform (WHT) at the transmitter and receiver, respectively. We next compare the performance of the proposed architectures of the abovementioned basic modules with that of its counterpart modules of the another 2D modulation technique Zak based orthogonal time frequency space (OTFS). Note that Zak-OTFS operates in the delay-Doppler domain, requiring the primary modules as 2D inverse Zak (IZak) and Zak transforms at the transmitter and receiver, respectively. This comprehensive comparative analysis is conducted on the computational complexity, timing performance, and power consumption of both schemes, evaluated on the ZCU706 Zynq SoC board. The results indicate that OTSM outperforms the Zak-OTFS in terms of area, power consumption, and latency. Zak-OTFS requires more programmable logic (PL) resources, utilizing <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathbf{2 8, 8 7 9}$</tex> LUTs and 26,124 FFs, while OTSM uses significantly fewer resources, with 5,440 LUTs and 6,216 FFs.",
    "title_zh": "片上系统中二维调制方案OTFS与OTSM的软硬件协同设计",
    "abstract_zh": "本文设计了二维（2D）调制方案——正交时序复用（OTSM）基本模块的低复杂度硬件架构。OTSM方案在延迟-序列域中工作，发射端和接收端分别使用基本模块逆沃尔什-哈达玛变换（IWHT）和沃尔什-哈达玛变换（WHT）。随后，我们将所提出的上述基本模块的硬件架构与另一种二维调制技术——基于Zak变换的正交时频空（Zak-OTFS）的对应模块进行性能对比。需要注意的是，Zak-OTFS在延迟-多普勒域中运行，其发射端和接收端主要依赖于二维逆Zak变换（IZak）和Zak变换。本研究在ZCU706 Zynq SoC开发板上对两种方案的计算复杂度、时序性能及功耗进行了全面的比较分析。结果表明，OTSM在面积占用、功耗和延迟方面均优于Zak-OTFS。Zak-OTFS需要更多的可编程逻辑（PL）资源，共使用28,879个LUTs和26,124个FFs，而OTSM则显著减少了资源消耗，仅需5,440个LUTs和6,216个FFs。"
  },
  {
    "date": "2026-2-18",
    "title": "Powerlist-Based High-Level Programming Model for Parallelism",
    "authors": "Jeya Ganesh M R, Anshu S Anand",
    "publish": "TENCON 2025 - 2025 IEEE Region 10 Conference (TENCON)",
    "url": "https://doi.org/10.1109/tencon66050.2025.11375320",
    "source": "IEEE",
    "abstract": "Designing efficient parallel programs is a nontrivial task that requires one to have an in-depth knowledge of the underlying parallel architecture, memory hierarchy and programming model besides other aspects. This may require considerable efforts, time, and cost. To improve the programmers' productivity, there is a need for parallel abstractions that can simplify the programmers job of first specifying the computation and then optimizing it for performance. Powerlist [1] is one such potential parallel abstraction that is able to exploit both parallelism and recursion at the same time, resulting in succinct representations of computations. In this work, we propose the use of Powerlist as a Domain Specific Language (DSL) and implement it as a Source-to-Source compiler that translates the high-level powerlist specification into sequential and parallel C++ programs. Several new powerlist specifications were also devised for computations and in this process, the powerlist notation was also enriched. The proposed framework considerably improves the programmers' productivity by providing a high-level abstraction resulting in succinct and intuitive specification of computations, absolving them of the task of exploiting parallelism and tuning for performance, which is taken care of by the implementation.",
    "title_zh": "基于Powerlist的并行性高级编程模型",
    "abstract_zh": "设计高效的并行程序是一项非平凡的任务，需要程序员深入掌握底层并行架构、内存层次结构以及编程模型等多方面知识，这往往需要投入大量精力、时间和成本。为了提高程序员的生产效率，亟需引入能够简化编程工作的并行抽象机制，使程序员能够首先专注于计算逻辑的描述，然后由系统自动完成性能优化。Powerlist [1] 就是一种具有潜力的并行抽象，它能够同时利用并行性和递归性，从而以简洁的方式表达计算过程。本文提出将 Powerlist 作为一种领域特定语言（DSL），并实现为一个源到源的编译器，将高层的 Powerlist 规范自动转换为顺序和并行的 C++ 程序。此外，我们还为多种计算任务设计了若干新的 Powerlist 规范，并在此过程中进一步丰富了 Powerlist 的表达能力。所提出的框架通过提供高层抽象，显著提升了程序员的生产效率，使得计算过程的描述更加简洁直观，同时将并行性挖掘和性能调优等复杂任务交由实现系统自动完成，从而大大减轻了程序员的负担。"
  },
  {
    "date": "2026-2-18",
    "title": "Effective Emulation of PSS Systems via Retrieval Augmented Generation",
    "authors": "Xiangwei Zhou, Guan Wang, Ningning Zhang, Aziguli Wulamu, Ao He",
    "publish": "2025 IEEE 6th International Conference on Computer, Big Data, Artificial Intelligence (ICCBD+AI)",
    "url": "https://doi.org/10.1109/iccbdai66607.2025.11388283",
    "source": "IEEE",
    "abstract": "We present a training-free retrieval-augmented generation (RAG) framework that emulates the behavioral interface of legacy Unisys mainframes used in airline Passenger Service Systems (PSS). By dynamically injecting domain-specific command syntax, transaction workflows and error codes into a frozen 32 B-code-specialised LLM, our system produces syntactically valid and semantically accurate responses to mainframe commands without gradient updates or live system access. Experiments on four core PNR segment types show that RAG raises exact-match accuracy from 0% to 68–87 % on a held-out expert-labelled test set, while providing isolated, on-demand environments that eliminate MIPS-licence costs and shared-hardware contention. To our knowledge, this is the first demonstration that parametric-free RAG can serve as a drop-in substitute for physical mainframes in safety-critical, data-scarce settings.",
    "title_zh": "通过检索增强生成实现PSS系统的有效模拟",
    "abstract_zh": "我们提出了一种无需训练的检索增强生成（RAG）框架，该框架模拟了航空旅客服务系统（PSS）中传统Unisys大型机的行为接口。通过动态注入领域特定的命令语法、交易流程和错误代码到一个冻结的320亿参数专用大语言模型中，我们的系统能够在不进行梯度更新或访问实时系统的情况下，生成语法正确且语义准确的大型机命令响应。在四个核心PNR段类型上的实验表明，RAG将精确匹配准确率从0%提升至68%–87%（在独立专家标注的测试集上），同时提供隔离的、按需启用的运行环境，从而消除了MIPS许可证成本和共享硬件资源争用问题。据我们所知，这是首个证明无参数RAG可在安全关键、数据稀缺场景中作为物理大型机即插即用替代方案的实例。"
  },
  {
    "date": "2026-2-18",
    "title": "A Critical Review and Evaluation of LLMs for RTL Generation",
    "authors": "Arun Ravindran, Aditya Patra, Vahid Babaey, Suresh Purini",
    "publish": "IEEE Access",
    "url": "https://doi.org/10.1109/access.2026.3665894",
    "source": "IEEE",
    "abstract": "Large language models (LLMs) are emerging as powerful tools for hardware design, with recent work exploring their ability to generate register-transfer level (RTL) code directly from natural-language specifications. This paper presents a critical review and empirical evaluation of LLM-based RTL generation. We examine thirty-one published efforts, classifying their use of techniques such as fine-tuning, reinforcement learning, retrieval-augmented prompting, and multi-agent orchestration across eight methodological dimensions including debugging support, post-RTL metrics, and benchmark development. Building on this synthesis, we experimentally evaluate frontier commercial and open-weight models—GPT-4.1, GPT-4.1-mini, Claude Sonnet 4, and Llama 4 Maverick —on the VerilogEvalV2 and RTLLMv2.0 benchmarks under both single-shot generation and a lightweight ReAct-style reflection loop, with compilation and simulation performed through Icarus Verilog interfaced via the Model Context Protocol (MCP). Results show that these models achieve up to 89.74% on VerilogEval and 96.08% on RTLLM, matching or exceeding prior domain-specific pipelines without specialized fine-tuning. Detailed failure analysis reveals systematic error modes, including FSM mis-sequencing, handshake drift, blocking vs. non-blocking misuse, and state-space oversimplification. Finally, we outline a forward-looking research roadmap toward natural-language-to-System-on-Chip (SoC) design, emphasizing realistic benchmarks and open flows, richer specification formalisms, AI–human collaborative design environments, and system-level feedback that spans physical design, firmware, and design space exploration. Together, this work provides both a synthesis of recent advances and a baseline evaluation of frontier LLMs, highlighting opportunities and challenges in moving toward AI-native electronic design automation.",
    "title_zh": "对用于RTL生成的大型语言模型的批判性回顾与评估",
    "abstract_zh": "大型语言模型（LLMs）正逐渐成为硬件设计领域的强大工具，近期研究探索了其直接从自然语言规范生成寄存器传输级（RTL）代码的能力。本文对基于LLM的RTL生成技术进行了批判性综述与实证评估。我们系统分析了31项已发表的研究工作，从微调、强化学习、检索增强提示、多智能体协同等技术手段出发，围绕调试支持、RTL后评估指标、基准测试开发等八个方法论维度进行分类与比较。在此基础上，我们对前沿的商用与开源模型——GPT-4.1、GPT-4.1-mini、Claude Sonnet 4以及Llama 4 Maverick——在VerilogEvalV2和RTLLMv2.0基准测试上的表现进行了实验评估，分别采用单次生成与轻量级ReAct风格的反思循环（reflection loop）两种模式，并通过Icarus Verilog结合模型上下文协议（MCP）实现编译与仿真。结果表明，这些模型在VerilogEval上最高达到89.74%的得分，在RTLLM上达到96.08%，在无需领域专用微调的情况下，性能已达到甚至超越以往专门设计的流水线系统。详细的失败分析揭示了系统性的错误模式，包括有限状态机（FSM）时序错乱、握手协议漂移、阻塞与非阻塞赋值误用，以及状态空间过度简化等问题。最后，本文提出了一条面向自然语言到系统级芯片（SoC）设计的前瞻性研究路线图，强调构建更真实的基准测试、开放的工具流程、更丰富的规格形式化表达、AI与人类协同的设计环境，以及涵盖物理设计、固件开发与设计空间探索的系统级反馈机制。综上，本工作不仅系统总结了近期的技术进展，还为前沿LLM提供了基准评估，揭示了迈向AI原生电子设计自动化（EDA）过程中所蕴含的机遇与挑战。"
  },
  {
    "date": "2026-2-18",
    "title": "Structural Chunking: A Semantic-Structural Integrated Method for Retrieval-Augmented Generation",
    "authors": "Sangyong Lee, NaHun Kim, Junseok Lee",
    "publish": "2026 International Conference on Electronics, Information, and Communication (ICEIC)",
    "url": "https://doi.org/10.1109/iceic69189.2026.11386163",
    "source": "IEEE",
    "abstract": "Retrieval-Augmented Generation (RAG) systems rely heavily on the quality of document chunking, which determines the granularity and contextual continuity of retrievable units. Existing chunking methods inevitably trade off between semantic coherence and computational efficiency. To overcome this limitation, we propose Structural Chunking, a novel paradigm that integrates semantic cohesion with structural consistency by leveraging both surface- and physical-level document features. Unlike purely semantic methods, Structural Chunking quantifies structural patterns and fuses them with semantic embeddings to compute a semantic-structural cohesion score. Chunk boundaries are then detected where this composite distance sharply increases. Experiments on five BEIR benchmark datasets demonstrate that Structural Chunking achieves the most consistent recall and average precision across domains, outperforming semantic and statistical baselines while maintaining stable chunk-size distributions. The results indicate that incorporating structural hierarchy into the RAG preprocessing pipeline substantially enhances retrieval efficiency and contextual fidelity, suggesting a new direction for structure-aware information retrieval.",
    "title_zh": "结构化分块：一种用于检索增强生成的语义-结构集成方法",
    "abstract_zh": "检索增强生成（RAG）系统在很大程度上依赖于文档分块的质量，而分块的粒度与可检索单元的上下文连续性直接相关。现有的分块方法不可避免地在语义连贯性与计算效率之间存在权衡。为克服这一局限，我们提出了一种名为“结构分块”（Structural Chunking）的新范式，该方法通过利用文档的表面特征与物理层面特征，将语义一致性与结构一致性相结合。与纯粹基于语义的方法不同，结构分块量化了文档的结构模式，并将其与语义嵌入融合，以计算出一个语义-结构一致性得分。随后，通过检测该综合距离显著上升的位置来确定分块边界。在五个BEIR基准数据集上的实验表明，结构分块在不同领域中均实现了最一致的召回率和平均精度，优于基于语义和统计的基线方法，同时保持了稳定的分块大小分布。结果表明，将结构层次信息融入RAG预处理流程，能够显著提升检索效率与上下文保真度，为面向结构感知的信息检索开辟了新的方向。"
  },
  {
    "date": "2026-2-18",
    "title": "A Systematic Literature Review of Visualization in Code Clone Detection Models",
    "authors": "Nur Fatin Syahidah Mohd Zamri, Al-Fahim Mubarak Ali, Aziman Abdullah, Siti Salwani Yaacob",
    "publish": "2025 7th International Conference on Interdisciplinary Computer Science and Engineering (ICICSE)",
    "url": "https://doi.org/10.1109/icicse67247.2025.11390858",
    "source": "IEEE",
    "abstract": "Code clones, which are instances of the same code segments copied in a software system, are created by developers for a variety of purposes. Code duplication is frequently brought on by programmers’ efficiency-seeking tendencies, which lead them to use copy-paste methods to speed up coding work. Numerous studies have proposed code clone visualization approaches to reduce code snippet replication and manage clone occurrences effectively. Visualization helps developers understand and avoid having duplicate code segments in software systems by giving them visual representations. Thus, in order to improve developers’ comprehension and stop the replication of code fragments, the present study provides a comprehensive evaluation of visualization approaches addressing code clone occurrences. The purpose of this study is to provide a systematic literature review of current code clone visualization approaches. The review and results of the study are presented, along with recommendations for further research projects that will enhance the current code clone visualization approaches.",
    "title_zh": "代码克隆检测模型中可视化技术的系统性文献综述",
    "abstract_zh": "代码克隆是指在软件系统中复制相同代码片段的实例，开发者出于多种目的创建这些克隆。代码重复通常源于程序员追求效率的倾向，他们通过复制粘贴的方式加快编码速度。已有大量研究提出了代码克隆可视化方法，以减少代码片段的重复并有效管理克隆现象。可视化通过提供直观的视觉表示，帮助开发者理解并避免在软件系统中出现重复代码。因此，为提升开发人员的理解能力并防止代码片段的重复，本研究对现有代码克隆可视化方法进行了全面评估。本研究旨在系统性地回顾当前的代码克隆可视化方法，呈现综述结果，并提出未来研究的建议，以进一步改进现有的代码克隆可视化技术。"
  },
  {
    "date": "2026-2-18",
    "title": "Scalable VLSI Design for Edge AI Acceleration in Real-Time Audio-Visual Processing Systems",
    "authors": "K Bhasakara Rao, Safeyah Tawil, R Suresh Kumar, G Sridevi, Sreekanth Sura, Popuri Ramesh Babu",
    "publish": "2025 Tenth International Conference on Science Technology Engineering and Mathematics (ICONSTEM)",
    "url": "https://doi.org/10.1109/iconstem65670.2025.11374722",
    "source": "IEEE",
    "abstract": "The high rate of Edge AI application development requires scalable streamlined energy-effective hardware solutions, which can handle real-time audio-visual processing at the edge with little latency and consumption. Current accelerators such as GPUs and FPGAs frequently do not support multimodal data streams which is utilizing high energy, with an efficient synchronization. In a bid to avoid these constraints, the proposed Scalable VLSI Design Framework to Edge AI Acceleration in Real-Time Audio-Visual Processing Systems is a synthesis of hierarchical dataflow-based VLSI architecture, systolic array-based parallel computing, and dynamic voltage and frequency scaling (DVFS) as means of optimizing performance. Swift compression-conscious interconnects and dynamic hardware division are also built in to the design in order to create the efficient synchronization of audio and visual channels. The proposed model is analysed with 7 nm CMOS technology that demonstrates the notable enhancements in throughput processing and efficiency of power along with interference latency estimation which is less than 10 ms per frame and accuracy enhancement of 13% which is better than the other existing frameworks. The obtained results are clearly depicting that the proposed framework is well suitable for real time applications such as smart surveillance, UAVs and in HMIs.",
    "title_zh": "面向实时音视频处理系统的边缘AI加速的可扩展VLSI设计",
    "abstract_zh": "边缘人工智能应用开发的高速率要求具备可扩展、高效能、低功耗的硬件解决方案，以在边缘端实现低延迟、低功耗的实时音视频处理。当前的加速器（如GPU和FPGA）通常无法有效支持多模态数据流，且在实现高效同步时能耗较高。为克服这些限制，本文提出了一种面向实时音视频处理系统的可扩展VLSI设计框架，用于边缘人工智能加速。该框架融合了基于分层数据流的VLSI架构、基于阵列的并行计算结构以及动态电压频率调节（DVFS）技术，以优化系统性能。此外，设计中还集成了快速压缩感知的互连结构和动态硬件划分机制，以实现音视频通道间的高效同步。基于7 nm CMOS工艺的仿真分析表明，该模型在处理吞吐量、能效方面均有显著提升，同时干扰延迟低于每帧10毫秒，准确率提升达13%，优于现有其他框架。实验结果明确表明，该框架非常适合应用于智能监控、无人机（UAV）以及人机交互界面（HMI）等实时场景。"
  },
  {
    "date": "2026-2-18",
    "title": "Building Students' Industry Workplace Skills Using Artificial Intelligence",
    "authors": "Chien Ching Lee, Ryan Fraser Kirwan",
    "publish": "TENCON 2025 - 2025 IEEE Region 10 Conference (TENCON)",
    "url": "https://doi.org/10.1109/tencon66050.2025.11375048",
    "source": "IEEE",
    "abstract": "Effective written communication is a crucial skill for students transitioning to industry, where accurate documentation of project activities and decisions are paramount. However, students' logbook entries are sometimes descriptive, unclear and incoherent. This might be due to them not having a critical reader in mind when writing. This paper evaluates the impact of a structured series of embedded workshops designed to enhance logbook writing skills among Applied Computing students to prepare them for their eightmonth internship under an Integrated Work Study Programme (IWSP). Generative AI (GenAI) took on the role of an interactive and critical reader: as a group of clients in a Requirements Engineering (RE) exercise (lesson 1), as a writing coach (lesson 2) and as an editor (lesson 3). Data was collected via a comparison of students' logbook entries in lesson three compared to lesson one and an end-of-workshop survey. The primary improvements observed between the students' logbook submissions were in critical analysis, content relevance and depth, and the integration of supporting ideas. The survey findings revealed that GenAI, when deployed as a user, helped generate ideas during brainstorming sessions and provided a basic and structured skeleton to kickstart the writing process. GenAI, as a coach, helped students be more objective by considering multiple perspectives. As an editor, it offered clarity and formatting suggestions, which helped students write more clearly and coherently. The findings underscore the effectiveness of AI-assisted workshops in preparing students for industry by developing essential written communication skills, thereby fostering better project documentation and reporting practices.",
    "title_zh": "利用人工智能培养学生的职业岗位技能",
    "abstract_zh": "有效的书面沟通能力是学生从校园过渡到职场时至关重要的技能，因为在工业界，项目活动与决策的准确记录至关重要。然而，学生的日志记录有时过于描述性、模糊且缺乏条理，这可能是因为他们在写作时未能设想一个批判性读者的角色。本文评估了一套嵌入式工作坊对应用计算专业学生日志写作能力的提升效果，旨在帮助他们为为期八个月的综合工作学习计划（IWSP）实习做好准备。生成式人工智能（GenAI）在其中扮演了互动且具有批判性的读者角色：在第一课中作为需求工程（RE）练习中的一组客户，在第二课中作为写作教练，在第三课中作为编辑。数据通过对比学生在第三课与第一课的日志内容，以及工作坊结束后的调查问卷收集。研究发现，学生日志提交的质量在批判性分析、内容的相关性与深度，以及支持性观点的整合方面均有显著提升。调查结果表明，当GenAI作为“用户”参与时，有助于在头脑风暴环节激发创意，并提供一个基础而结构化的框架，以启动写作过程；作为写作教练，它帮助学生更客观地思考问题，考虑多种视角；作为编辑，它提供了清晰度和格式方面的建议，使学生能够写出更清晰、连贯的内容。研究结果强调了AI辅助工作坊在培养学生行业所需关键书面沟通技能方面的有效性，从而促进更高质量的项目文档与报告实践。"
  },
  {
    "date": "2026-2-18",
    "title": "Distributed Inference Software for Large Language Models",
    "authors": "Harshavardhan M G, P Praveen Raj, Rishith P, Vasugi I",
    "publish": "2025 International Conference on Advances in Next-Gen Computer Science (ICANCS)",
    "url": "https://doi.org/10.1109/icancs65819.2025.11377729",
    "source": "IEEE",
    "abstract": "Large Language Models (LLMs) such as LLaMA and BLOOM have revolutionized natural language processing by enabling advanced applications like dialogue systems, summarization, and reasoning. However, their extensive computational demands in terms of GPU memory, processing power, and bandwidth make them unsuitable for execution on consumer-grade or low-resource devices. Current solutions often rely on centralized cloud APIs, which introduce privacy risks, high costs, and dependency on external infrastructure. To overcome these limitations, this research proposes a distributed inference framework that partitions LLMs layer-wise across multiple devices connected via a trusted local network. Each device, acting as either a client or server, contributes according to its computational capacity. Servers host subsets of model layers, while clients orchestrate inference by routing inputs across available servers. The system integrates Hivemind's decentralized hash table (DHT) built on libp2p for expert discovery, pipeline parallelism for distributed execution, 8-bit quantization for memory efficiency, and KV catching for fault tolerance. Evaluation results across LAN and internet environments validate feasibility, demonstrating that models like GPT-2 can be executed on consumer laptops with 4GB VRAM. This work highlights the potential for democratizing access to powerful AI capabilities through collaborative inference.",
    "title_zh": "大型语言模型的分布式推理软件",
    "abstract_zh": "大型语言模型（LLMs）如LLaMA和BLOOM通过支持对话系统、摘要生成和推理等高级应用，彻底改变了自然语言处理领域。然而，这些模型在GPU内存、计算能力和带宽方面巨大的计算需求，使其难以在消费级或低资源设备上运行。当前的解决方案通常依赖于集中式的云API，这带来了隐私风险、高昂成本以及对外部基础设施的依赖。为克服这些局限，本研究提出了一种分布式推理框架，将LLM按层划分，并分布到通过可信本地网络连接的多个设备上。每个设备根据其计算能力充当客户端或服务器角色：服务器托管模型的部分层，而客户端则负责协调推理过程，将输入在可用服务器间路由。该系统集成了基于libp2p的Hivemind去中心化哈希表（DHT）用于专家发现，采用流水线并行实现分布式执行，利用8位量化提升内存效率，并通过KV缓存机制实现容错。在局域网和互联网环境下的评估结果验证了该方案的可行性，证明像GPT-2这样的模型可以在仅配备4GB显存的消费级笔记本电脑上成功运行。本研究展示了通过协作式推理实现强大人工智能能力普惠化的巨大潜力。"
  },
  {
    "date": "2026-2-18",
    "title": "Graph Neural Networks-Guided Code Optimization System for Software Performance Enhancement",
    "authors": "Ramachandiran R, B. Harichandana, Vikram Kaushik, Vijaylaxmi Inamdar, S. Prathi, Vikas Raheja",
    "publish": "2025 Tenth International Conference on Science Technology Engineering and Mathematics (ICONSTEM)",
    "url": "https://doi.org/10.1109/iconstem65670.2025.11374680",
    "source": "IEEE",
    "abstract": "The rapid nature of software system development necessitates new approaches to guarantee that there is optimization of code to achieve improved performance. The paper presents a Graph Neural Networks (GNN)-based Code Optimization System, which takes the form of graphs and represents the programs to identify performance bottlenecks and offer performance optimization guidelines. The system is founded on the applications of Graph Convolutional Network (GCNs) as its core method and realized with the assistance of the PyTorch Geometric system that can be discussed as effective in the capture of syntactic and semantic dependencies within the code. Experimental results of running benchmark software projects have revealed that there have been tremendous gains in the execution time, use of memory and computation efficiency compared to the conventional code optimization tools. The developed solution offers the software developers with a scaled and automated solution, reducing human intervention and reducing the high-performance software development cycles.",
    "title_zh": "图神经网络引导的代码优化系统用于软件性能提升",
    "abstract_zh": "软件系统开发的快速节奏要求采用新的方法，以确保代码优化，从而提升性能。本文提出了一种基于图神经网络（GNN）的代码优化系统，该系统将程序表示为图结构，以识别性能瓶颈并提供性能优化建议。该系统以图卷积网络（GCN）为核心方法，并借助PyTorch Geometric框架实现，能够有效捕捉代码中的语法和语义依赖关系。对基准软件项目进行的实验结果表明，与传统代码优化工具相比，该系统在执行时间、内存使用和计算效率方面均取得了显著提升。所开发的解决方案为软件开发者提供了一种可扩展且自动化的优化手段，减少了人工干预，缩短了高性能软件的开发周期。"
  },
  {
    "date": "2026-2-18",
    "title": "Automated Firmware Extraction and Vulnerability Analysis in Embedded IoT Devices: A Replicable Cybersecurity Infrastructure Framework",
    "authors": "Lucas O'Mahony, Toyosi Oyinloye, Jakir Hossain, Lee Speakman",
    "publish": "2025 Cyber Research Conference - Ireland (Cyber-RCI)",
    "url": "https://doi.org/10.1109/cyber-rci68134.2025.11385254",
    "source": "IEEE",
    "abstract": "Embedded IoT devices continue to exhibit firmware-level weaknesses caused by unsafe memory operations, legacy libraries and inconsistent validation. These issues are difficult to study due to proprietary firmware, diverse hardware interfaces and the lack of reproducible extraction workflows. This work presents a lightweight, repeatable framework for firmware acquisition and analysis using low-cost hardware and open-source tools. The framework integrates UART, SPI and chip-off extraction with an automated pipeline using magnify.py for ELF triage and flawedfunctions.db, a database of 224 unsafe <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$C$</tex> functions. Experiments on four devices show that chip-off extraction provides complete and reliable firmware acquisition, while UART and SPI offer partial access depending on hardware design. Automated scanning revealed recurring high-severity memorysafety issues, with Ghidra confirming insecure calls such as strcpy and sprintf. These early results demonstrate the feasibility of accessible, reproducible firmware extraction and static analysis. Future work focuses on scripted Ghidra analysis, broader device testing and enhanced risk scoring.",
    "title_zh": "嵌入式物联网设备的自动化固件提取与漏洞分析：一种可复现的网络安全基础设施框架",
    "abstract_zh": "嵌入式物联网设备持续暴露出由不安全的内存操作、遗留库以及不一致的验证机制引起的固件级漏洞。由于专有固件、多样的硬件接口以及缺乏可复现的提取流程，这些漏洞难以研究。本文提出了一种轻量级、可重复的固件获取与分析框架，采用低成本硬件和开源工具实现。该框架整合了UART、SPI以及芯片取下（chip-off）提取技术，并通过自动化流程结合 magnify.py 进行ELF文件的初步筛查，以及 flawedfunctions.db 数据库（包含224个不安全的C函数）进行漏洞识别。在四款设备上的实验表明，芯片取下提取方法能够实现完整且可靠的固件获取，而UART和SPI提取则因硬件设计差异仅能提供部分访问。自动化扫描揭示了反复出现的高严重性内存安全问题，Ghidra工具确认了诸如 strcpy 和 sprintf 等不安全函数的调用。这些初步结果证明了可访问、可复现的固件提取与静态分析的可行性。未来工作将聚焦于脚本化Ghidra分析、更广泛的设备测试以及风险评分机制的增强。"
  },
  {
    "date": "2026-2-18",
    "title": "A Multilingual Intelligent Document Processing System",
    "authors": "Ravi Kishore Kodali, Varsha Sanga, Sai Veerendra Prasad Kuruguti, Lakshmi Boppana",
    "publish": "TENCON 2025 - 2025 IEEE Region 10 Conference (TENCON)",
    "url": "https://doi.org/10.1109/tencon66050.2025.11375106",
    "source": "IEEE",
    "abstract": "In today's digital world, processing multilingual documents is critical for business, legal tasks, and information retrieval. This study describes a Multilingual Document Processing System that uses Optical Character Recognition (OCR) and Retrieval-Augmented Generation (RAG) to extract, query and summarize text in multiple languages. The system employs advanced OCR models to correctly recognize text from scanned documents, images, and handwriting in various scripts. By incorporating RAG, it improves comprehension and response generation, allowing users to retrieve and summarize information in English even when the original language is different. This approach takes advantage of recent advances in natural language processing, large language models (LLM), and multimodal AI to address challenges in multilingual data accessibility, knowledge synthesis, and real-time communication. The system provides a scalable AI-driven solution to improve document processing, eliminate language barriers, and increase global user engagement. AWS services support scalable document processing but cold starts in AWS Lambda hinder real time tasks.",
    "title_zh": "多语言智能文档处理系统",
    "abstract_zh": "在当今的数字世界中，处理多语言文档对于商业、法律事务以及信息检索至关重要。本研究介绍了一种多语言文档处理系统，该系统结合光学字符识别（OCR）与检索增强生成（RAG）技术，实现多语言文本的提取、查询与摘要。系统采用先进的OCR模型，能够准确识别扫描文档、图像及手写文本中各种文字脚本的文本内容。通过引入RAG技术，系统显著提升了对文本的理解与响应生成能力，使用户即使面对非英语原文，也能以英语检索并生成摘要信息。该方法充分利用了自然语言处理、大型语言模型（LLM）以及多模态人工智能的最新进展，有效应对多语言数据可访问性、知识融合与实时沟通等方面的挑战。该系统提供了一种可扩展的AI驱动解决方案，显著提升文档处理效率，打破语言障碍，增强全球用户的参与度。尽管AWS服务支持可扩展的文档处理，但AWS Lambda的冷启动问题仍会影响实时任务的执行。"
  }
]