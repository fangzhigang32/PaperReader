[
  {
    "date": "2026-01-26",
    "title": "POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration",
    "authors": "Yuxiao Qu, Amrith Setlur, Virginia Smith, Ruslan Salakhutdinov, Aviral Kumar",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18779v1",
    "source": "arXiv",
    "abstract": "Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks.",
    "title_zh": "POPE：通过特权在线探索学习解决难题的推理能力",
    "abstract_zh": "强化学习（RL）已显著提升了大型语言模型（LLMs）的推理能力，但当前最先进的方法在许多训练问题上仍无法有效学习。在面对难题时，基于策略的强化学习（on-policy RL）几乎从不探索出任何正确的执行路径，导致奖励为零，从而缺乏推动改进的学习信号。我们发现，经典强化学习中用于缓解探索困境的自然解决方案——如熵奖励、更宽松的重要性比例裁剪，或直接优化 pass@k 目标——均无法解决这一问题，且往往在未提升可解性的情况下加剧优化过程的不稳定性。一个自然的替代方案是利用简单问题上的知识迁移。然而，我们证明，在强化学习训练过程中混合简单与复杂问题会适得其反，因为存在“射线干扰”（ray interference）现象：优化过程过度聚焦于已可解决的简单问题，反而主动抑制了对更难问题的进展。\n\n为应对这一挑战，我们提出了**特权在线探索**（Privileged On-Policy Exploration, POPE）方法。该方法利用人类或其他专家（oracle）提供的解决方案作为特权信息，引导模型在困难问题上的探索，这与那些将专家解用作训练目标的方法（如离策略强化学习或从监督微调SFT中进行预热）有本质区别。POPE通过在困难问题前添加专家解的前缀，使强化学习能够在引导式推演中获得非零奖励。关键在于，这种由指导带来的行为能够通过指令遵循与推理能力之间的协同作用，有效回传至原始的无引导问题上。\n\n实证结果表明，POPE显著扩展了可解决的问题范围，并在多个具有挑战性的推理基准测试中大幅提升了性能。"
  },
  {
    "date": "2026-01-26",
    "title": "CASSANDRA: Programmatic and Probabilistic Learning and Inference for Stochastic World Modeling",
    "authors": "Panagiotis Lymperopoulos, Abhiramon Rajasekharan, Ian Berlot-Attwell, Stéphane Aroca-Ouellette, Kaheer Suleman",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18620v1",
    "source": "arXiv",
    "abstract": "Building world models is essential for planning in real-world domains such as businesses. Since such domains have rich semantics, we can leverage world knowledge to effectively model complex action effects and causal relationships from limited data. In this work, we propose CASSANDRA, a neurosymbolic world modeling approach that leverages an LLM as a knowledge prior to construct lightweight transition models for planning. CASSANDRA integrates two components: (1) LLM-synthesized code to model deterministic features, and (2) LLM-guided structure learning of a probabilistic graphical model to capture causal relationships among stochastic variables. We evaluate CASSANDRA in (i) a small-scale coffee-shop simulator and (ii) a complex theme park business simulator, where we demonstrate significant improvements in transition prediction and planning over baselines.",
    "title_zh": "卡桑德拉：用于随机世界建模的程序化与概率学习及推理",
    "abstract_zh": "构建世界模型对于现实世界领域（如商业）中的规划至关重要。由于这些领域具有丰富的语义信息，我们可以利用世界知识，从有限的数据中有效建模复杂动作效应和因果关系。在本研究中，我们提出了CASSANDRA——一种神经符号化世界建模方法，该方法利用大语言模型（LLM）作为先验知识，构建轻量级的转移模型以支持规划。CASSANDRA整合了两个核心组件：(1) 由LLM生成的代码，用于建模确定性特征；(2) LLM引导的概率图模型结构学习，以捕捉随机变量之间的因果关系。我们在两个场景中评估了CASSANDRA的表现：(i) 小规模的咖啡店模拟器，以及(ii) 复杂的主题公园商业模拟器。实验结果表明，与基线方法相比，CASSANDRA在转移预测和规划方面均取得了显著提升。"
  },
  {
    "date": "2026-01-26",
    "title": "Deconstructing Instruction-Following: A New Benchmark for Granular Evaluation of Large Language Model Instruction Compliance Abilities",
    "authors": "Alberto Purpura, Li Wang, Sahil Badyal, Eugenio Beaufrand, Adam Faulkner",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18554v1",
    "source": "arXiv",
    "abstract": "Reliably ensuring Large Language Models (LLMs) follow complex instructions is a critical challenge, as existing benchmarks often fail to reflect real-world use or isolate compliance from task success. We introduce MOSAIC (MOdular Synthetic Assessment of Instruction Compliance), a modular framework that uses a dynamically generated dataset with up to 20 application-oriented generation constraints to enable a granular and independent analysis of this capability. Our evaluation of five LLMs from different families based on this new benchmark demonstrates that compliance is not a monolithic capability but varies significantly with constraint type, quantity, and position. The analysis reveals model-specific weaknesses, uncovers synergistic and conflicting interactions between instructions, and identifies distinct positional biases such as primacy and recency effects. These granular insights are critical for diagnosing model failures and developing more reliable LLMs for systems that demand strict adherence to complex instructions.",
    "title_zh": "指令遵循能力的解构：一项用于大语言模型指令遵从能力细粒度评估的新基准",
    "abstract_zh": "可靠地确保大型语言模型（LLMs）遵循复杂指令是一项关键挑战，因为现有的评估基准往往无法反映真实应用场景，也难以将指令遵从性与任务成功率区分开来。为此，我们提出了MOSAIC（MOdular Synthetic Assessment of Instruction Compliance），一个模块化框架，通过使用动态生成的数据集，最多包含20个面向实际应用的生成约束，从而实现对这一能力的细粒度且独立的分析。基于该新基准对来自不同家族的五种LLM进行评估后发现，指令遵从性并非单一能力，而是显著受约束类型、数量及位置的影响。分析揭示了各模型特有的弱点，发现了指令之间的协同与冲突关系，并识别出明显的顺序偏差，如首因效应和近因效应。这些细粒度的洞察对于诊断模型失败原因、开发在复杂指令下仍能严格遵守要求的更可靠的LLM至关重要。"
  },
  {
    "date": "2026-01-26",
    "title": "Beyond Retention: Orchestrating Structural Safety and Plasticity in Continual Learning for LLMs",
    "authors": "Fei Meng",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18255v1",
    "source": "arXiv",
    "abstract": "Continual learning in Large Language Models (LLMs) faces the critical challenge of balancing stability (retaining old knowledge) and plasticity (learning new tasks). While Experience Replay (ER) is a standard countermeasure against catastrophic forgetting, its impact across diverse capabilities remains underexplored. In this work, we uncover a critical dichotomy in ER's behavior: while it induces positive backward transfer on robust, unstructured tasks (e.g., boosting performance on previous NLP classification tasks through repeated rehearsal), it causes severe negative transfer on fragile, structured domains like code generation (e.g., a significant relative drop in coding accuracy). This reveals that ER trades structural integrity for broad consolidation. To address this dilemma, we propose \\textbf{Orthogonal Subspace Wake-up (OSW)}. OSW identifies essential parameter subspaces of previous tasks via a brief \"wake-up\" phase and enforces orthogonal updates for new tasks, providing a mathematically grounded \"safety guarantee\" for established knowledge structures. Empirical results across a diverse four-task sequence demonstrate that OSW uniquely succeeds in preserving fragile coding abilities where Replay fails, while simultaneously maintaining high plasticity for novel tasks. Our findings emphasize the necessity of evaluating structural safety alongside average retention in LLM continual learning.",
    "title_zh": "超越记忆：在持续学习中协调大语言模型的结构安全与可塑性",
    "abstract_zh": "大规模语言模型（LLMs）中的持续学习面临一个核心挑战：如何在稳定性（保留旧知识）与可塑性（学习新任务）之间取得平衡。尽管经验回放（Experience Replay, ER）是应对灾难性遗忘的标准方法，但其在不同能力维度上的影响仍缺乏深入探索。本文揭示了ER行为中一个关键的二元对立现象：虽然它在稳健、非结构化的任务上能带来正向的反向迁移效果（例如，通过重复演练提升对先前自然语言处理分类任务的表现），但在脆弱且高度结构化的领域（如代码生成）中却导致严重的负向迁移（例如，编码准确率出现显著的相对下降）。这一发现表明，ER在实现广泛知识巩固的同时，牺牲了结构完整性。为解决这一困境，我们提出**正交子空间唤醒（Orthogonal Subspace Wake-up, OSW）**。OSW通过短暂的“唤醒”阶段识别出先前任务的关键参数子空间，并对新任务施加正交更新，从而为已建立的知识结构提供数学上严谨的“安全保证”。在涵盖四个多样化任务序列的实证研究中，OSW展现出独特优势：在ER失效的脆弱代码生成能力上实现了有效保护，同时保持了对新任务的高度可塑性。我们的研究强调，在评估LLM持续学习性能时，必须将结构安全性与平均保留率并重。"
  },
  {
    "date": "2026-01-26",
    "title": "EvolVE: Evolutionary Search for LLM-based Verilog Generation and Optimization",
    "authors": "Wei-Po Hsin, Ren-Hao Deng, Yao-Ting Hsieh, En-Ming Huang, Shih-Hao Hung",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18067v1",
    "source": "arXiv",
    "abstract": "Verilog's design cycle is inherently labor-intensive and necessitates extensive domain expertise. Although Large Language Models (LLMs) offer a promising pathway toward automation, their limited training data and intrinsic sequential reasoning fail to capture the strict formal logic and concurrency inherent in hardware systems. To overcome these barriers, we present EvolVE, the first framework to analyze multiple evolution strategies on chip design tasks, revealing that Monte Carlo Tree Search (MCTS) excels at maximizing functional correctness, while Idea-Guided Refinement (IGR) proves superior for optimization. We further leverage Structured Testbench Generation (STG) to accelerate the evolutionary process. To address the lack of complex optimization benchmarks, we introduce IC-RTL, targeting industry-scale problems derived from the National Integrated Circuit Contest. Evaluations establish EvolVE as the new state-of-the-art, achieving 98.1% on VerilogEval v2 and 92% on RTLLM v2. Furthermore, on the industry-scale IC-RTL suite, our framework surpasses reference implementations authored by contest participants, reducing the Power, Performance, Area (PPA) product by up to 66% in Huffman Coding and 17% in the geometric mean across all problems. The source code of the IC-RTL benchmark is available at https://github.com/weiber2002/ICRTL.",
    "title_zh": "EvolVE：基于大语言模型的Verilog生成与优化的进化搜索",
    "abstract_zh": "Verilog设计流程本质上劳动密集，且需要深厚的领域专业知识。尽管大型语言模型（LLMs）为自动化提供了有前景的路径，但其训练数据有限以及固有的顺序推理能力难以捕捉硬件系统中严格的逻辑形式性与并发特性。为克服这些障碍，我们提出了EvolVE——首个针对芯片设计任务分析多种进化策略的框架。研究发现，蒙特卡洛树搜索（MCTS）在最大化功能正确性方面表现卓越，而基于思想引导的精炼方法（IGR）则在优化方面更具优势。此外，我们引入结构化测试平台生成（STG）以加速进化过程。为解决复杂优化基准缺失的问题，我们提出了IC-RTL基准，该基准面向源自“全国集成电路设计大赛”的工业级规模问题。评估结果表明，EvolVE已成为新的行业标杆，在VerilogEval v2上达到98.1%的得分，在RTLLM v2上达到92%。此外，在工业级IC-RTL测试套件中，我们的框架超越了参赛者编写的参考实现，在霍夫曼编码任务中将功耗、性能、面积（PPA）乘积降低高达66%，在所有问题上的几何平均值也降低了17%。IC-RTL基准的源代码已公开，地址为：https://github.com/weiber2002/ICRTL。"
  },
  {
    "date": "2026-01-26",
    "title": "AI Agent for Reverse-Engineering Legacy Finite-Difference Code and Translating to Devito",
    "authors": "Yinghan Hou, Zongyou Yang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18381v1",
    "source": "arXiv",
    "abstract": "To facilitate the transformation of legacy finite difference implementations into the Devito environment, this study develops an integrated AI agent framework. Retrieval-Augmented Generation (RAG) and open-source Large Language Models are combined through multi-stage iterative workflows in the system's hybrid LangGraph architecture. The agent constructs an extensive Devito knowledge graph through document parsing, structure-aware segmentation, extraction of entity relationships, and Leiden-based community detection. GraphRAG optimisation enhances query performance across semantic communities that include seismic wave simulation, computational fluid dynamics, and performance tuning libraries. A reverse engineering component derives three-level query strategies for RAG retrieval through static analysis of Fortran source code. To deliver precise contextual information for language model guidance, the multi-stage retrieval pipeline performs parallel searching, concept expansion, community-scale retrieval, and semantic similarity analysis. Code synthesis is governed by Pydantic-based constraints to guarantee structured outputs and reliability. A comprehensive validation framework integrates conventional static analysis with the G-Eval approach, covering execution correctness, structural soundness, mathematical consistency, and API compliance. The overall agent workflow is implemented on the LangGraph framework and adopts concurrent processing to support quality-based iterative refinement and state-aware dynamic routing. The principal contribution lies in the incorporation of feedback mechanisms motivated by reinforcement learning, enabling a transition from static code translation toward dynamic and adaptive analytical behavior.",
    "title_zh": "用于逆向工程遗留有限差分代码并将其转换为 Devito 的 AI Agent",
    "abstract_zh": "为促进传统有限差分实现向Devito环境的迁移，本研究开发了一个集成式AI代理框架。该系统采用混合LangGraph架构，通过多阶段迭代工作流将检索增强生成（RAG）与开源大语言模型相结合。代理通过文档解析、结构感知分段、实体关系提取以及基于Leiden算法的社区检测，构建了一个全面的Devito知识图谱。GraphRAG优化显著提升了在包含地震波模拟、计算流体动力学及性能调优库等语义社区中的查询性能。反向工程模块通过对Fortran源代码进行静态分析，推导出三级查询策略以支持RAG检索。为向语言模型提供精确的上下文信息以指导其行为，多阶段检索管道执行并行搜索、概念扩展、社区级检索及语义相似性分析。代码生成受Pydantic约束机制控制，确保输出结构化且可靠。一个综合验证框架融合了传统的静态分析方法与G-Eval评估方法，涵盖执行正确性、结构合理性、数学一致性及API合规性等方面。整个代理工作流基于LangGraph框架实现，并采用并发处理机制，支持基于质量的迭代优化与状态感知的动态路由。本研究的主要贡献在于引入了受强化学习启发的反馈机制，推动代码转换从静态翻译迈向动态、自适应的分析行为。"
  },
  {
    "date": "2026-01-26",
    "title": "Code over Words: Overcoming Semantic Inertia via Code-Grounded Reasoning",
    "authors": "Manjie Xu, Isabella Yin, Xinyi Tu, Chi Zhang, Yixin Zhu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18352v1",
    "source": "arXiv",
    "abstract": "LLMs struggle with Semantic Inertia: the inability to inhibit pre-trained priors (e.g., \"Lava is Dangerous\") when dynamic, in-context rules contradict them. We probe this phenomenon using Baba Is You, where physical laws are mutable text rules, enabling precise evaluation of models' ability to override learned priors when rules change. We quantatively observe that larger models can exhibit inverse scaling: they perform worse than smaller models when natural language reasoning requires suppressing pre-trained associations (e.g., accepting \"Lava is Safe\"). Our analysis attributes this to natural language encoding, which entangles descriptive semantics and logical rules, leading to persistent hallucinations of familiar physics despite explicit contradictory rules. Here we show that representing dynamics as executable code, rather than descriptive text, reverses this trend and enables effective prior inhibition. We introduce Code-Grounded Vistas (LCV), which fine-tunes models on counterfactual pairs and identifies states with contradictory rules, thereby forcing attention to logical constraints rather than visual semantics. This training-time approach outperforms expensive inference-time search methods in both efficiency and accuracy. Our results demonstrate that representation fundamentally determines whether scaling improves or impairs contextual reasoning. This challenges the assumption that larger models are universally better, with implications for domains that require dynamic overriding of learned priors.",
    "title_zh": "代码胜于文字：通过代码基础推理克服语义惯性",
    "abstract_zh": "大型语言模型（LLMs）在处理“语义惯性”问题时表现不佳：当动态的上下文规则与预训练阶段形成的先验知识相矛盾时，模型难以抑制这些先验（例如，“熔岩是危险的”）。我们通过《Baba Is You》这一游戏对这一现象进行了探究——该游戏中的物理法则以可变的文本规则形式呈现，能够精确评估模型在规则变更时克服已有先验的能力。定量分析发现，更大的模型反而可能出现反向缩放现象：在需要通过自然语言推理抑制预训练关联（如接受“熔岩是安全的”）的情况下，其表现反而不如较小的模型。我们的分析表明，这主要源于自然语言编码方式将描述性语义与逻辑规则纠缠在一起，导致即使面对明确的相反规则，模型仍会持续产生符合熟悉物理规律的幻觉。\n\n本文提出，将动态规则表示为可执行代码而非描述性文本，可以逆转这一趋势，并有效实现先验抑制。为此，我们引入了**代码基础视图**（Code-Grounded Vistas, LCV），该方法通过在反事实规则对上微调模型，并识别出存在规则冲突的状态，从而迫使模型关注逻辑约束而非视觉语义。这种训练阶段的策略在效率和准确性上均优于昂贵的推理阶段搜索方法。\n\n我们的研究结果表明，表示方式从根本上决定了模型规模扩展是否能提升上下文推理能力。这一发现挑战了“模型越大越优”的普遍假设，尤其对那些需要动态覆盖已学习先验的知识领域具有重要启示。"
  },
  {
    "date": "2026-01-26",
    "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering",
    "authors": "Ji Zeng, Dayuan Fu, Tiantian Mi, Yumin Zhuang, Yaxing Huang, Xuefeng Li, Lyumanshan Ye, Muhang Xie, Qishuo Hua, Zhen Huang, Mohan Jiang, Hanning Wang, Jifan Lin, Yang Xiao, Jie Sun, Yunze Wu, Pengfei Liu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18418v1",
    "source": "arXiv",
    "abstract": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...",
    "title_zh": "daVinci-Dev：面向软件工程的原生代理中训练",
    "abstract_zh": "最近，大型语言模型（LLM）能力的前沿已从单轮代码生成转向代理式软件工程——一种模型能够自主导航、编辑和测试复杂代码库的范式。尽管后训练方法已成为代码代理的主流方案，但**代理式中训练**（agentic mid-training）——即在大规模、反映真实代理工作流的数据上进行中训练（MT）——由于资源需求巨大，仍面临严重探索不足的问题。然而，与依赖昂贵强化学习相比，代理式中训练为培养基础代理行为提供了更具可扩展性的路径。实现有效代理中训练的核心挑战在于：静态训练数据与真实开发环境中动态、反馈丰富的环境之间存在分布差异。\n\n为解决这一问题，我们开展了一项系统性研究，提出了代理中训练的有效数据合成原则与训练方法论，以实现大规模代理能力的高效发展。我们方法的核心是**代理原生数据**（agent-native data），其监督信号包含两种互补的轨迹类型：  \n- **上下文原生轨迹**（contextually-native trajectories）：完整保留代理在实际操作中经历的信息流，具有广泛的覆盖范围和多样性；  \n- **环境原生轨迹**（environmentally-native trajectories）：从可执行代码仓库中收集，观测值来源于真实的工具调用和测试执行，具备深度与交互真实性。\n\n我们在 `SWE-Bench Verified` 上验证了模型的代理能力。在使用对齐的基础模型和代理架构的两种后训练设置下，我们的方法显著优于此前开源的软件工程中训练方案 `Kimi-Dev`，且所用中训练 token 数量不足其一半（仅 731B）。除相对优势外，我们表现最佳的 32B 和 72B 模型分别达到了 **56.1%** 和 **58.5%** 的问题解决率，这……"
  },
  {
    "date": "2026-01-26",
    "title": "TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance",
    "authors": "Elena Bruches, Vadim Alperovich, Dari Baturova, Roman Derunets, Daniil Grebenkin, Georgy Mkrtchyan, Oleg Sedukhin, Mikhail Klementev, Ivan Bondarenko, Nikolay Bushkov, Stanislav Moiseev",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18241v1",
    "source": "arXiv",
    "abstract": "While Large Language Models (LLMs) have shown promise in software engineering, their application to unit testing remains largely confined to isolated test generation or oracle prediction, neglecting the broader challenge of test suite maintenance. We introduce TAM-Eval (Test Automated Maintenance Evaluation), a framework and benchmark designed to evaluate model performance across three core test maintenance scenarios: creation, repair, and updating of test suites. Unlike prior work limited to function-level tasks, TAM-Eval operates at the test file level, while maintaining access to full repository context during isolated evaluation, better reflecting real-world maintenance workflows. Our benchmark comprises 1,539 automatically extracted and validated scenarios from Python, Java, and Go projects. TAM-Eval supports system-agnostic evaluation of both raw LLMs and agentic workflows, using a reference-free protocol based on test suite pass rate, code coverage, and mutation testing. Empirical results indicate that state-of-the-art LLMs have limited capabilities in realistic test maintenance processes and yield only marginal improvements in test effectiveness. We release TAM-Eval as an open-source framework to support future research in automated software testing. Our data and code are publicly available at https://github.com/trndcenter/TAM-Eval.",
    "title_zh": "TAM-Eval：评估大语言模型在自动化单元测试维护中的应用",
    "abstract_zh": "尽管大型语言模型（LLMs）在软件工程领域展现出巨大潜力，但其在单元测试中的应用仍主要局限于孤立的测试用例生成或断言预测，未能充分应对测试套件维护这一更广泛而复杂的挑战。为此，我们提出了TAM-Eval（Test Automated Maintenance Evaluation），一个用于评估模型在三种核心测试维护场景——测试套件的创建、修复与更新——中表现的框架与基准。与以往仅限于函数级任务的研究不同，TAM-Eval 在测试文件级别进行评估，同时在独立评估过程中仍能访问完整的代码库上下文，从而更真实地反映实际的维护工作流程。该基准从Python、Java和Go项目中自动提取并验证了1,539个测试维护场景。TAM-Eval支持对原始LLM及智能体工作流的系统无关评估，采用基于测试通过率、代码覆盖率和变异测试的无参考评估协议。实证结果表明，当前最先进的LLMs在真实的测试维护流程中能力有限，对测试有效性的提升微乎其微。我们已将TAM-Eval作为开源框架发布，以推动自动化软件测试领域的后续研究。相关数据与代码已公开，可访问 https://github.com/trndcenter/TAM-Eval 获取。"
  },
  {
    "date": "2026-01-26",
    "title": "Handling Scope Checks (Extended Version)",
    "authors": "Michael Lee, Ningning Xie, Oleg Kiselyov, Jeremy Yallop",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18793v1",
    "source": "arXiv",
    "abstract": "Metaprogramming and effect handlers interact in unexpected, and sometimes undesirable, ways. One example is scope extrusion: the generation of ill-scoped code. Scope extrusion can either be preemptively prevented, via static type systems, or retroactively detected, via dynamic checks. Static type systems exist in theory, but struggle with a range of implementation and usability problems in practice. In contrast, dynamic checks exist in practice (e.g. in MetaOCaml), but are understudied in theory. Designers of metalanguages are thus given little guidance regarding the design and implementation of checks. We present the first formal study of dynamic scope extrusion checks, introducing a calculus ($λ_{\\langle\\langle\\text{op}\\rangle\\rangle}$) for describing and evaluating checks. Further, we introduce a novel dynamic check $\\unicode{x2014}$ the \"Cause-for-Concern\" check $\\unicode{x2014}$ which we prove correct, characterise without reference to its implementation, and argue combines the advantages of existing dynamic checks. Finally, we extend our framework with refined environment classifiers, which statically prevent scope extrusion, and compare their expressivity with the dynamic checks.",
    "title_zh": "处理作用域检查（扩展版）",
    "abstract_zh": "元编程与效应处理器以出人意料且有时令人不悦的方式相互作用。其中一个典型例子是作用域外溢：生成作用域不正确的代码。作用域外溢要么通过静态类型系统预先防止，要么通过动态检查事后检测。虽然理论上存在静态类型系统，但在实际应用中却面临一系列实现和可用性问题。相比之下，动态检查在实践中已存在（例如在MetaOCaml中），但其理论研究仍显不足。因此，元语言设计者在设计和实现检查机制方面缺乏充分的指导。本文首次对动态作用域外溢检查进行了形式化研究，提出一个用于描述和评估检查的演算系统（$λ_{\\langle\\langle\\text{op}\\rangle\\rangle}$）。此外，我们引入了一种新颖的动态检查——“令人担忧”检查（Cause-for-Concern check），并证明了其正确性，以不依赖具体实现的方式对其性质进行了刻画，同时论证该检查结合了现有动态检查的优势。最后，我们通过引入更精细的环境分类器扩展了该框架，这些分类器可静态地防止作用域外溢，并将其表达能力与动态检查进行了比较。"
  },
  {
    "date": "2026-01-26",
    "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
    "authors": "Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu, Yuefeng Huang, Xinyi Wang, Jiannan Cao, Jianwei Yin, Xuhong Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18771v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs' ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales.",
    "title_zh": "Dep-Search：通过持久化记忆学习依赖感知的推理轨迹",
    "abstract_zh": "大型语言模型（LLMs）在复杂推理任务中展现了卓越的能力，尤其是在结合搜索机制以系统性地探索外部知识库的情况下。该领域已从传统的检索增强生成（RAG）框架发展为更先进的基于搜索的框架，这些框架通过明确的搜索策略实现多步推理的协调。然而，现有的搜索框架仍严重依赖隐式的自然语言推理来决定搜索策略以及如何在不同推理步骤中利用检索到的信息。这种对隐式推理的依赖带来了根本性的挑战：难以有效管理子问题之间的依赖关系、难以高效复用先前检索到的知识，也难以通过强化学习来学习最优的搜索策略。\n\n为解决上述局限，我们提出了Dep-Search——一种具备依赖感知能力的搜索框架。该框架超越了现有方法，通过GRPO（Generalized Reinforcement Policy Optimization）整合结构化推理、信息检索与持久记忆机制。Dep-Search引入了显式的控制机制，使模型能够显式分解具有依赖关系的问题，按需检索信息，访问存储于记忆中的过往知识，并将长时推理上下文总结为可复用的记忆条目。\n\n在七个多样化的问答数据集上进行的大量实验表明，Dep-Search显著提升了LLMs处理复杂多跳推理任务的能力，在不同模型规模下均大幅优于多个强基线方法。"
  },
  {
    "date": "2026-01-26",
    "title": "$α^3$-SecBench: A Large-Scale Evaluation Suite of Security, Resilience, and Trust for LLM-based UAV Agents over 6G Networks",
    "authors": "Mohamed Amine Ferrag, Abderrahmane Lakas, Merouane Debbah",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18754v1",
    "source": "arXiv",
    "abstract": "Autonomous unmanned aerial vehicle (UAV) systems are increasingly deployed in safety-critical, networked environments where they must operate reliably in the presence of malicious adversaries. While recent benchmarks have evaluated large language model (LLM)-based UAV agents in reasoning, navigation, and efficiency, systematic assessment of security, resilience, and trust under adversarial conditions remains largely unexplored, particularly in emerging 6G-enabled settings. We introduce $α^{3}$-SecBench, the first large-scale evaluation suite for assessing the security-aware autonomy of LLM-based UAV agents under realistic adversarial interference. Building on multi-turn conversational UAV missions from $α^{3}$-Bench, the framework augments benign episodes with 20,000 validated security overlay attack scenarios targeting seven autonomy layers, including sensing, perception, planning, control, communication, edge/cloud infrastructure, and LLM reasoning. $α^{3}$-SecBench evaluates agents across three orthogonal dimensions: security (attack detection and vulnerability attribution), resilience (safe degradation behavior), and trust (policy-compliant tool usage). We evaluate 23 state-of-the-art LLMs from major industrial providers and leading AI labs using thousands of adversarially augmented UAV episodes sampled from a corpus of 113,475 missions spanning 175 threat types. While many models reliably detect anomalous behavior, effective mitigation, vulnerability attribution, and trustworthy control actions remain inconsistent. Normalized overall scores range from 12.9% to 57.1%, highlighting a significant gap between anomaly detection and security-aware autonomous decision-making. We release $α^{3}$-SecBench on GitHub: https://github.com/maferrag/AlphaSecBench",
    "title_zh": "α³-SecBench：面向6G网络中基于大语言模型的无人机智能体的安全性、韧性与可信性大规模评估套件",
    "abstract_zh": "自主无人飞行器（UAV）系统在日益复杂的网络化安全关键环境中部署越来越多，必须能够在恶意对手存在的情况下可靠运行。尽管近期基准测试已评估了基于大语言模型（LLM）的UAV智能体在推理、导航和效率方面的表现，但在对抗性条件下对安全性、鲁棒性和可信度的系统性评估仍基本处于空白状态，尤其是在新兴的6G赋能场景中尤为突出。我们提出了 $α^{3}$-SecBench，这是首个用于评估基于LLM的UAV智能体在真实对抗干扰下具备安全意识自主能力的大规模评测框架。该框架基于 $α^{3}$-Bench 中的多轮对话式UAV任务，通过引入20,000个经过验证的安全叠加攻击场景，对正常任务流程进行增强，覆盖感知、感知、规划、控制、通信、边缘/云基础设施以及LLM推理在内的七个自主层级。$α^{3}$-SecBench 从三个正交维度评估智能体：安全性（攻击检测与漏洞归因）、鲁棒性（安全降级行为）以及可信度（符合策略的工具使用）。我们利用来自包含175种威胁类型的113,475个任务语料库中采样的数千个对抗增强型UAV任务，对来自主要工业厂商及领先AI实验室的23个前沿LLM进行了评估。尽管多数模型能够可靠地检测异常行为，但有效的缓解措施、漏洞归因以及可信的控制行为仍表现出不一致。综合得分范围为12.9%至57.1%，凸显出异常检测与具备安全意识的自主决策之间存在显著差距。我们已在GitHub上开源 $α^{3}$-SecBench：https://github.com/maferrag/AlphaSecBench"
  },
  {
    "date": "2026-01-26",
    "title": "Agentic Much? Adoption of Coding Agents on GitHub",
    "authors": "Romain Robbes, Théo Matricon, Thomas Degueule, Andre Hora, Stefano Zacchiroli",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18341v1",
    "source": "arXiv",
    "abstract": "In the first half of 2025, coding agents have emerged as a category of development tools that have very quickly transitioned to the practice. Unlike ''traditional'' code completion LLMs such as Copilot, agents like Cursor, Claude Code, or Codex operate with high degrees of autonomy, up to generating complete pull requests starting from a developer-provided task description. This new mode of operation is poised to change the landscape in an even larger way than code completion LLMs did, making the need to study their impact critical. Also, unlike traditional LLMs, coding agents tend to leave more explicit traces in software engineering artifacts, such as co-authoring commits or pull requests. We leverage these traces to present the first large-scale study (129,134 projects) of the adoption of coding agents on GitHub, finding an estimated adoption rate of 15.85%--22.60%, which is very high for a technology only a few months old--and increasing. We carry out an in-depth study of the adopters we identified, finding that adoption is broad: it spans the entire spectrum of project maturity; it includes established organizations; and it concerns diverse programming languages or project topics. At the commit level, we find that commits assisted by coding agents are larger than commits only authored by human developers, and have a large proportion of features and bug fixes. These findings highlight the need for further investigation into the practical use of coding agents.",
    "title_zh": "代理化程度有多高？GitHub 上编码代理的采用情况",
    "abstract_zh": "2025年上半年，编码代理作为一种开发工具类别迅速从概念走向实践。与传统的代码补全大语言模型（如Copilot）不同，Cursor、Claude Code或Codex等编码代理具备高度自主性，能够根据开发者提供的任务描述自动生成完整的拉取请求（pull request）。这种新型操作模式预计将对软件开发领域产生比代码补全大语言模型更为深远的影响，因此研究其影响变得尤为关键。此外，与传统大语言模型相比，编码代理在软件工程产物中往往留下更明显的痕迹，例如在提交记录或拉取请求中体现为共同作者身份。我们利用这些可追踪的痕迹，开展了首个大规模研究（涵盖129,134个GitHub项目），发现编码代理的采用率估计在15.85%至22.60%之间——对于一个仅问世数月的技术而言，这一比例已相当高且呈持续上升趋势。我们对所识别出的采用者进行了深入分析，发现其采用范围极为广泛：覆盖了从初创到成熟项目的整个生命周期；涉及各类成熟企业组织；并涵盖多种编程语言和项目主题。在提交层面，我们发现由编码代理辅助生成的提交内容比纯人类编写的提交更大，且其中包含大量功能新增和缺陷修复。这些发现凸显了进一步探究编码代理实际应用价值的重要性。"
  },
  {
    "date": "2026-01-26",
    "title": "CovertComBench: The First Domain-Specific Testbed for LLMs in Wireless Covert Communication",
    "authors": "Zhaozhi Liu, Jiaxin Chen, Yuanai Xie, Yuna Jiang, Minrui Xu, Xiao Zhang, Pan Lai, Zan Zhou",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18315v1",
    "source": "arXiv",
    "abstract": "The integration of Large Language Models (LLMs) into wireless networks presents significant potential for automating system design. However, unlike conventional throughput maximization, Covert Communication (CC) requires optimizing transmission utility under strict detection-theoretic constraints, such as Kullback-Leibler divergence limits. Existing benchmarks primarily focus on general reasoning or standard communication tasks and do not adequately evaluate the ability of LLMs to satisfy these rigorous security constraints. To address this limitation, we introduce CovertComBench, a unified benchmark designed to assess LLM capabilities across the CC pipeline, encompassing conceptual understanding (MCQs), optimization derivation (ODQs), and code generation (CGQs). Furthermore, we analyze the reliability of automated scoring within a detection-theoretic ``LLM-as-Judge'' framework. Extensive evaluations across state-of-the-art models reveal a significant performance discrepancy. While LLMs achieve high accuracy in conceptual identification (81%) and code implementation (83%), their performance in the higher-order mathematical derivations necessary for security guarantees ranges between 18% and 55%. This limitation indicates that current LLMs serve better as implementation assistants rather than autonomous solvers for security-constrained optimization. These findings suggest that future research should focus on external tool augmentation to build trustworthy wireless AI systems.",
    "title_zh": "CovertComBench：无线隐蔽通信中大语言模型的首个领域专用测试平台",
    "abstract_zh": "将大型语言模型（LLMs）融入无线网络具有显著潜力，可实现系统设计的自动化。然而，与传统的吞吐量最大化不同，隐蔽通信（CC）需要在严格的检测理论约束下优化传输效用，例如Kullback-Leibler散度的限制。现有的基准测试主要关注通用推理或标准通信任务，未能充分评估LLMs满足这些严格安全约束的能力。为解决这一局限性，我们提出了CovertComBench——一个统一的基准，用于评估LLM在隐蔽通信全流程中的能力，涵盖概念理解（多选题MCQs）、优化推导（推导题ODQs）和代码生成（代码题CGQs）。此外，我们还分析了在检测理论框架下的“LLM作为裁判”机制中自动评分的可靠性。对前沿模型的广泛评估显示，性能存在显著差异：尽管LLMs在概念识别（81%准确率）和代码实现（83%准确率）方面表现良好，但在保障安全所需的高阶数学推导任务中，其表现仅为18%至55%。这一局限表明，当前的LLMs更适合作为实现辅助工具，而非自主求解安全约束优化问题的智能体。这些发现提示未来研究应聚焦于外部工具的增强，以构建可信的无线AI系统。"
  },
  {
    "date": "2026-01-26",
    "title": "Think-Augmented Function Calling: Improving LLM Parameter Accuracy Through Embedded Reasoning",
    "authors": "Lei Wei, Jinpeng Ou, Xiao Peng, Bin Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18282v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in function calling for autonomous agents, yet current mechanisms lack explicit reasoning transparency during parameter generation, particularly for complex functions with interdependent parameters. While existing approaches like chain-of-thought prompting operate at the agent level, they fail to provide fine-grained reasoning guidance for individual function parameters. To address these limitations, we propose Think-Augmented Function Calling (TAFC), a novel framework that enhances function calling accuracy through explicit reasoning at both function and parameter levels. Our method introduces a universal \"think\" parameter augmentation that enables models to articulate their decision-making process, with dynamic optimization for parameter descriptions to improve reasoning quality. For complex parameters, TAFC automatically triggers granular reasoning based on complexity scoring, ensuring appropriate justification for critical decisions. Additionally, we propose reasoning-guided optimization to align generated reasoning with human expectations. TAFC requires no architectural modifications to existing LLMs while maintaining full API compatibility. Evaluation on ToolBench across proprietary and open-source models demonstrates significant improvements in parameter generation accuracy and reasoning coherence for multi-parameter functions, while providing enhanced interpretability for debugging AI agent behaviors.",
    "title_zh": "思维增强型函数调用：通过嵌入式推理提升大语言模型参数的准确性",
    "abstract_zh": "大型语言模型（LLMs）在自主代理的功能调用方面展现了卓越的能力，但现有的机制在参数生成过程中缺乏显式的推理透明性，尤其是在涉及相互依赖参数的复杂函数中表现尤为明显。尽管现有方法如思维链提示（chain-of-thought prompting）在代理层面有所应用，却无法为单个函数参数提供细粒度的推理指导。为解决这些局限性，我们提出了一种名为“思维增强型函数调用”（Think-Augmented Function Calling, TAFC）的新框架，通过在函数和参数两个层面引入显式推理，显著提升函数调用的准确性。我们的方法引入了一种通用的“思考”参数增强机制，使模型能够清晰阐述其决策过程，并通过动态优化参数描述来提升推理质量。对于复杂参数，TAFC会基于复杂度评分自动触发细粒度推理，确保关键决策获得充分的解释。此外，我们还提出了基于推理引导的优化策略，以使生成的推理内容更符合人类预期。TAFC无需对现有大型语言模型进行架构修改，同时保持了完整的API兼容性。在ToolBench基准测试中，针对专有及开源模型的评估表明，TAFC在多参数函数的参数生成准确率和推理连贯性方面均取得显著提升，同时增强了对AI代理行为的可解释性，有助于调试与分析。"
  },
  {
    "date": "2026-01-26",
    "title": "FGGM: Fisher-Guided Gradient Masking for Continual Learning",
    "authors": "Chao-Hong Tan, Qian Chen, Wen Wang, Yukun Ma, Chong Zhang, Chong Deng, Qinglin Zhang, Xiangang Li, Jieping Ye",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18261v1",
    "source": "arXiv",
    "abstract": "Catastrophic forgetting impairs the continuous learning of large language models. We propose Fisher-Guided Gradient Masking (FGGM), a framework that mitigates this by strategically selecting parameters for updates using diagonal Fisher Information. FGGM dynamically generates binary masks with adaptive thresholds, preserving critical parameters to balance stability and plasticity without requiring historical data. Unlike magnitude-based methods such as MIGU, our approach offers a mathematically principled parameter importance estimation. On the TRACE benchmark, FGGM shows a 9.6% relative improvement in retaining general capabilities over supervised fine-tuning (SFT) and a 4.4% improvement over MIGU on TRACE tasks. Additional analysis on code generation tasks confirms FGGM's superior performance and reduced forgetting, establishing it as an effective solution.",
    "title_zh": "FGGM：用于持续学习的费舍尔引导梯度掩码",
    "abstract_zh": "灾难性遗忘严重影响大型语言模型的持续学习能力。我们提出了一种基于Fisher信息引导的梯度掩码方法（Fisher-Guided Gradient Masking, FGGM），通过利用对角线Fisher信息矩阵，智能地选择需要更新的参数，从而缓解该问题。FGGM动态生成具有自适应阈值的二进制掩码，有效保留关键参数，在不依赖历史数据的前提下，实现了模型稳定性和可塑性的良好平衡。与基于权重大小的方法（如MIGU）不同，我们的方法提供了数学上更严谨的参数重要性估计。在TRACE基准测试中，FGGM相较于监督微调（SFT）在保持通用能力方面取得了9.6%的相对提升，相比MIGU在TRACE任务上提升了4.4%。对代码生成任务的进一步分析也证实了FGGM在性能优越性和遗忘减少方面的显著优势，表明其是一种高效可靠的解决方案。"
  },
  {
    "date": "2026-01-26",
    "title": "MalURLBench: A Benchmark Evaluating Agents' Vulnerabilities When Processing Web URLs",
    "authors": "Dezhang Kong, Zhuxi Wu, Shiqi Liu, Zhicheng Tan, Kuichen Lu, Minghao Li, Qichen Liu, Shengyu Chu, Zhenhua Xu, Xuan Liu, Meng Han",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18113v1",
    "source": "arXiv",
    "abstract": "LLM-based web agents have become increasingly popular for their utility in daily life and work. However, they exhibit critical vulnerabilities when processing malicious URLs: accepting a disguised malicious URL enables subsequent access to unsafe webpages, which can cause severe damage to service providers and users. Despite this risk, no benchmark currently targets this emerging threat. To address this gap, we propose MalURLBench, the first benchmark for evaluating LLMs' vulnerabilities to malicious URLs. MalURLBench contains 61,845 attack instances spanning 10 real-world scenarios and 7 categories of real malicious websites. Experiments with 12 popular LLMs reveal that existing models struggle to detect elaborately disguised malicious URLs. We further identify and analyze key factors that impact attack success rates and propose URLGuard, a lightweight defense module. We believe this work will provide a foundational resource for advancing the security of web agents. Our code is available at https://github.com/JiangYingEr/MalURLBench.",
    "title_zh": "MalURLBench：一个评估代理在处理网络URL时漏洞的基准测试",
    "abstract_zh": "基于大语言模型（LLM）的网络代理在日常生活和工作中日益普及，展现出巨大实用价值。然而，在处理恶意URL时，这些系统暴露出严重的安全漏洞：一旦接受伪装后的恶意URL，便可能导致后续访问不安全网页，从而对服务提供商和用户造成严重损害。尽管存在这一重大风险，目前尚无专门针对此类新兴威胁的评估基准。为填补这一空白，我们提出了MalURLBench——首个用于评估LLM对恶意URL脆弱性的基准测试。MalURLBench包含61,845个攻击实例，覆盖10种真实世界场景以及7类真实的恶意网站。通过对12个主流LLM进行实验，我们发现现有模型难以识别精心伪装的恶意URL。此外，我们进一步识别并分析了影响攻击成功率的关键因素，并提出了一种轻量级防御模块——URLGuard。我们认为，本工作将为提升网络代理的安全性提供重要的基础资源。相关代码已开源，地址为：https://github.com/JiangYingEr/MalURLBench。"
  },
  {
    "date": "2026-01-26",
    "title": "Mitigating the OWASP Top 10 For Large Language Models Applications using Intelligent Agents",
    "authors": "Mohammad Fasha, Faisal Abul Rub, Nasim Matar, Bilal Sowan, Mohammad Al Khaldy",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18105v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) have emerged as a transformative and disruptive technology, enabling a wide range of applications in natural language processing, machine translation, and beyond. However, this widespread integration of LLMs also raised several security concerns highlighted by the Open Web Application Security Project (OWASP), which has identified the top 10 security vulnerabilities inherent in LLM applications. Addressing these vulnerabilities is crucial, given the increasing reliance on LLMs and the potential threats to data integrity, confidentiality, and service availability. This paper presents a framework designed to mitigate the security risks outlined in the OWASP Top 10. Our proposed model leverages LLM-enabled intelligent agents, offering a new approach to proactively identify, assess, and counteract security threats in real-time. The proposed framework serves as an initial blueprint for future research and development, aiming to enhance the security measures of LLMs and protect against emerging threats in this rapidly evolving landscape.",
    "title_zh": "使用智能代理缓解大型语言模型应用中的OWASP十大安全风险",
    "abstract_zh": "大型语言模型（LLMs）作为一种变革性且具有颠覆性的技术，已在自然语言处理、机器翻译等多个领域实现了广泛应用。然而，LLMs的广泛集成也引发了一系列安全问题，这些问题已被开放网络应用安全项目（OWASP）所关注，并列出了LLM应用中固有的十大安全漏洞。鉴于人们对LLMs日益依赖，以及数据完整性、机密性和服务可用性面临潜在威胁，解决这些漏洞至关重要。本文提出了一种旨在缓解OWASP十大安全漏洞风险的框架。我们提出的模型利用基于LLM的智能代理，为实时主动识别、评估和应对安全威胁提供了一种创新方法。该框架可作为未来研究与开发的初步蓝图，致力于提升LLM的安全防护能力，以应对这一快速演进领域中的新兴威胁。"
  },
  {
    "date": "2026-01-26",
    "title": "Let's Make Every Pull Request Meaningful: An Empirical Analysis of Developer and Agentic Pull Requests",
    "authors": "Haruhiko Yoshioka, Takahiro Monno, Haruka Tokumasu, Taiki Wakamatsu, Yuki Ota, Nimmi Weeraddana, Kenichi Matsumoto",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18749v1",
    "source": "arXiv",
    "abstract": "The automatic generation of pull requests (PRs) using AI agents has become increasingly common. Although AI-generated PRs are fast and easy to create, their merge rates have been reported to be lower than those created by humans. In this study, we conduct a large-scale empirical analysis of 40,214 PRs collected from the AIDev dataset. We extract 64 features across six families and fit statistical regression models to compare PR merge outcomes for human and agentic PRs, as well as across three AI agents. Our results show that submitter attributes dominate merge outcomes for both groups, while review-related features exhibit contrasting effects between human and agentic PRs. The findings of this study provide insights into improving PR quality through human-AI collaboration.",
    "title_zh": "让每一次拉取请求都富有意义：对开发者与代理型拉取请求的实证分析",
    "abstract_zh": "使用AI代理自动生成拉取请求（PR）的做法日益普遍。尽管AI生成的PR创建速度快、操作简便，但其合并率却低于人类编写的PR。在本研究中，我们对来自AIDev数据集的40,214个PR进行了大规模实证分析。我们提取了六大类共64个特征，并构建统计回归模型，以比较人类与AI代理生成的PR在合并结果上的差异，同时对比了三种不同AI代理的表现。研究结果表明，提交者属性对两类PR的合并结果均具有主导作用，而与评审相关的特征对人类PR和AI代理PR的影响则呈现相反趋势。本研究的发现为通过人机协作提升PR质量提供了重要启示。"
  },
  {
    "date": "2026-01-26",
    "title": "Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models",
    "authors": "Siyan Zhao, Zhihui Xie, Mengchen Liu, Jing Huang, Guan Pang, Feiyu Chen, Aditya Grover",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18734v1",
    "source": "arXiv",
    "abstract": "Knowledge distillation improves large language model (LLM) reasoning by compressing the knowledge of a teacher LLM to train smaller LLMs. On-policy distillation advances this approach by having the student sample its own trajectories while a teacher LLM provides dense token-level supervision, addressing the distribution mismatch between training and inference in off-policy distillation methods. However, on-policy distillation typically requires a separate, often larger, teacher LLM and does not explicitly leverage ground-truth solutions available in reasoning datasets. Inspired by the intuition that a sufficiently capable LLM can rationalize external privileged reasoning traces and teach its weaker self (i.e., the version without access to privileged information), we introduce On-Policy Self-Distillation (OPSD), a framework where a single model acts as both teacher and student by conditioning on different contexts. The teacher policy conditions on privileged information (e.g., verified reasoning traces) while the student policy sees only the question; training minimizes the per-token divergence between these distributions over the student's own rollouts. We demonstrate the efficacy of our method on multiple mathematical reasoning benchmarks, achieving 4-8x token efficiency compared to reinforcement learning methods such as GRPO and superior performance over off-policy distillation methods.",
    "title_zh": "自蒸馏推理者：面向大型语言模型的策略内自蒸馏",
    "abstract_zh": "知识蒸馏通过将教师大语言模型（LLM）的知识压缩，用于训练更小的LLM，从而提升大语言模型的推理能力。在策略内蒸馏（on-policy distillation）中，学生模型会自主采样自己的轨迹，而教师LLM则提供密集的逐标记级监督，有效缓解了传统离策略蒸馏方法在训练与推理阶段存在的分布不匹配问题。然而，现有的策略内蒸馏通常需要一个独立的、往往更大的教师模型，并且未能充分利用推理数据集中已有的真实答案（ground-truth solutions）。受如下直觉启发：一个足够强大的LLM能够解释外部提供的优势推理轨迹，并指导其自身较弱版本（即无法访问这些优势信息的版本），我们提出了**策略内自蒸馏**（On-Policy Self-Distillation, OPSD）框架。该框架仅使用单一模型，通过不同的上下文条件，使其同时扮演教师和学生角色：教师策略基于特权信息（如经验证的推理轨迹）进行条件化，而学生策略仅能看到问题本身；训练过程通过最小化学生自身生成轨迹中各标记上的分布差异来实现。我们在多个数学推理基准测试中验证了该方法的有效性，结果表明，相较于GRPO等强化学习方法，OPSD实现了4至8倍的token效率提升，并在性能上优于传统的离策略蒸馏方法。"
  },
  {
    "date": "2026-01-26",
    "title": "From Fuzzy to Exact: The Halo Architecture for Infinite-Depth Reasoning via Rational Arithmetic",
    "authors": "Hansheng Ren",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18702v1",
    "source": "arXiv",
    "abstract": "Current paradigms in Deep Learning prioritize computational throughput over numerical precision, relying on the assumption that intelligence emerges from statistical correlation at scale. In this paper, we challenge this orthodoxy. We propose the Exactness Hypothesis: that General Intelligence (AGI), specifically high-order causal inference, requires a computational substrate capable of Arbitrary Precision Arithmetic. We argue that the \"hallucinations\" and logical incoherence seen in current Large Language Models (LLMs) are artifacts of IEEE 754 floating-point approximation errors accumulating over deep compositional functions. To mitigate this, we introduce the Halo Architecture, a paradigm shift to Rational Arithmetic ($\\mathbb{Q}$) supported by a novel Exact Inference Unit (EIU). Empirical validation on the Huginn-0125 prototype demonstrates that while 600B-parameter scale BF16 baselines collapse in chaotic systems, Halo maintains zero numerical divergence indefinitely. This work establishes exact arithmetic as a prerequisite for reducing logical uncertainty in System 2 AGI.",
    "title_zh": "从模糊到精确：通过有理数算术实现无限深度推理的光环架构",
    "abstract_zh": "当前深度学习的范式倾向于追求计算吞吐量而非数值精度，其基础假设是：智能源于大规模统计相关性。在本文中，我们挑战这一主流观点。我们提出“精确性假说”：通用智能（AGI），特别是高阶因果推理，需要一个能够实现任意精度算术的计算基础架构。我们认为，当前大型语言模型（LLMs）所表现出的“幻觉”与逻辑不连贯现象，本质上是IEEE 754浮点数近似误差在深层组合函数中累积所致。为缓解此问题，我们提出了“光环架构”（Halo Architecture），这是一种范式革新——采用有理数算术（$\\mathbb{Q}$），并配备一种新型精确推理单元（Exact Inference Unit, EIU）。在Huginn-0125原型系统的实证验证中，尽管参数规模达6000亿的BF16基线模型在混沌系统中迅速崩溃，Halo架构却能无限期保持零数值发散。本研究确立了精确算术作为降低系统2型AGI中逻辑不确定性的必要前提。"
  },
  {
    "date": "2026-01-26",
    "title": "From Verifiable Dot to Reward Chain: Harnessing Verifiable Reference-based Rewards for Reinforcement Learning of Open-ended Generation",
    "authors": "Yuxin Jiang, Yufei Wang, Qiyuan Zhang, Xingshan Zeng, Liangyou Li, Jierun Chen, Chaofan Tao, Haoli Bai, Lifeng Shang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18533v1",
    "source": "arXiv",
    "abstract": "Reinforcement learning with verifiable rewards (RLVR) succeeds in reasoning tasks (e.g., math and code) by checking the final verifiable answer (i.e., a verifiable dot signal). However, extending this paradigm to open-ended generation is challenging because there is no unambiguous ground truth. Relying on single-dot supervision often leads to inefficiency and reward hacking. To address these issues, we propose reinforcement learning with verifiable reference-based rewards (RLVRR). Instead of checking the final answer, RLVRR extracts an ordered linguistic signal from high-quality references (i.e, reward chain). Specifically, RLVRR decomposes rewards into two dimensions: content, which preserves deterministic core concepts (e.g., keywords), and style, which evaluates adherence to stylistic properties through LLM-based verification. In this way, RLVRR combines the exploratory strength of RL with the efficiency and reliability of supervised fine-tuning (SFT). Extensive experiments on more than 10 benchmarks with Qwen and Llama models confirm the advantages of our approach. RLVRR (1) substantially outperforms SFT trained with ten times more data and advanced reward models, (2) unifies the training of structured reasoning and open-ended generation, and (3) generalizes more effectively while preserving output diversity. These results establish RLVRR as a principled and efficient path toward verifiable reinforcement learning for general-purpose LLM alignment. We release our code and data at https://github.com/YJiangcm/RLVRR.",
    "title_zh": "从可验证的点到奖励链：基于可验证引用的奖励在开放式生成强化学习中的应用",
    "abstract_zh": "基于可验证奖励的强化学习（RLVR）通过检查最终可验证的答案（即“可验证点”信号）在推理任务（如数学和代码生成）中取得了成功。然而，将这一范式扩展到开放式生成任务面临挑战，因为此类任务缺乏明确的真值标准。依赖单一“点”监督往往导致效率低下和奖励欺骗问题。为解决这些问题，我们提出了基于可验证参考的强化学习（RLVRR）。与仅检查最终答案不同，RLVRR从高质量参考文本中提取有序的语言信号（即“奖励链”），从而实现更精细的奖励设计。具体而言，RLVRR将奖励分解为两个维度：内容维度，用于保留确定性的核心概念（如关键词）；风格维度，通过大语言模型（LLM）进行验证以评估对写作风格属性的遵循程度。通过这种方式，RLVRR结合了强化学习的探索能力与监督微调（SFT）的高效性和可靠性。\n\n在Qwen和Llama系列模型上超过10个基准测试的大量实验表明，本方法具有显著优势：（1）在性能上远超使用十倍数据量并结合先进奖励模型训练的SFT；（2）统一了结构化推理与开放式生成的训练流程；（3）在保持输出多样性的同时展现出更强的泛化能力。这些结果确立了RLVRR作为通用大模型对齐中一种原理清晰且高效的可验证强化学习路径。相关代码与数据已开源，地址为：https://github.com/YJiangcm/RLVRR。"
  },
  {
    "date": "2026-01-26",
    "title": "KeyMemRT Compiler and Runtime: Unlocking Memory-Scalable FHE",
    "authors": "Eymen Ünay, Björn Franke, Jackson Woodruff",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18445v1",
    "source": "arXiv",
    "abstract": "Fully Homomorphic Encryption (FHE) enables privacy preserving computation but it suffers from high latency and memory consumption. The computations are secured with special keys called rotation keys which often take up the majority of memory. In complex FHE applications, these rotation keys can cause a large memory bottleneck limiting program throughput. Existing compilers make little effort to solve this problem, instead relying on systems with massive memory availability. This resource requirement is a barrier to FHE uptake because optimizing FHE programs by hand is challenging due to their scale, complexity and expertise required. In this work, we present KeyMemRT; an MLIR based compiler and runtime framework that individually manages rotation key lifetimes to lower memory utilization and to allow arbitrary number of rotation indices to be supported without memory bloating. KeyMemRT relies on dataflow analysis to determine key lifetimes and is the first FHE compiler to provide automatic key management, handle fine-grained key-mangement and manage boostrap keys. We implement frontends for Orion and HEIR and show improvements over state-of-the-art FHE compilers. KeyMemRT achieves memory reduction of 1.74x and a speedup of 1.20x over ANT-ACE, and memory reduction of 1.16x and a speedup of 1.73x over memory-optimized compiler Fhelipe. We provide KeyMemRT as a post-optimizing compiler that can be targeted by any FHE compiler.",
    "title_zh": "KeyMemRT 编译器与运行时：解锁可扩展内存的全同态加密",
    "abstract_zh": "全同态加密（FHE）能够实现隐私保护计算，但其面临高延迟和高内存消耗的问题。计算过程通过一种特殊的密钥——旋转密钥来保障安全，而这些旋转密钥通常占据绝大部分内存。在复杂的FHE应用中，旋转密钥可能成为严重的内存瓶颈，限制程序的吞吐量。现有的编译器对此问题关注甚少，往往依赖于具备海量内存的系统。这种资源需求已成为阻碍FHE广泛应用的障碍，因为手动优化FHE程序极为困难，原因在于其规模庞大、结构复杂且需要高度专业知识。\n\n本文提出KeyMemRT：一个基于MLIR的编译器与运行时框架，能够独立管理旋转密钥的生命周期，从而降低内存占用，并支持任意数量的旋转索引而不会导致内存膨胀。KeyMemRT利用数据流分析来确定密钥的生命周期，是首个提供自动密钥管理、支持细粒度密钥管理以及可管理bootstrapping密钥的FHE编译器。我们为Orion和HEIR实现了前端，并展示了相较于现有最先进FHE编译器的显著改进。KeyMemRT相比ANT-ACE实现了1.74倍的内存减少和1.20倍的性能提升；相比内存优化型编译器Fhelipe，实现了1.16倍的内存减少和1.73倍的性能提升。我们还将KeyMemRT作为后端优化编译器提供，可被任何FHE编译器调用以进行优化。"
  },
  {
    "date": "2026-01-26",
    "title": "RTeAAL Sim: Using Tensor Algebra to Represent and Accelerate RTL Simulation (Extended Version)",
    "authors": "Yan Zhu, Boru Chen, Christopher W. Fletcher, Nandeeka Nayak",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18140v1",
    "source": "arXiv",
    "abstract": "RTL simulation on CPUs remains a persistent bottleneck in hardware design. State-of-the-art simulators embed the circuit directly into the simulation binary, resulting in long compilation times and execution that is fundamentally CPU frontend-bound, with severe instruction-cache pressure. This work proposes RTeAAL Sim, which reformulates RTL simulation as a sparse tensor algebra problem. By representing RTL circuits as tensors and simulation as a sparse tensor algebra kernel, RTeAAL Sim decouples simulation behavior from binary size and makes RTL simulation amenable to well-studied tensor algebra optimizations. We demonstrate that a prototype of our tensor-based simulator, even with a subset of these optimizations, already mitigates the compilation overhead and frontend pressure and achieves performance competitive with the highly optimized Verilator simulator across multiple CPUs and ISAs.",
    "title_zh": "RTeAAL Sim：使用张量代数表示与加速RTL仿真（扩展版）",
    "abstract_zh": "在硬件设计中，基于CPU的RTL仿真始终是一个持续存在的性能瓶颈。当前最先进的仿真器将电路直接嵌入仿真二进制文件中，导致编译时间过长，并且执行过程本质上受CPU前端限制，产生严重的指令缓存压力。本文提出了一种名为RTeAAL Sim的新方法，将RTL仿真重新建模为稀疏张量代数问题。通过将RTL电路表示为张量，并将仿真过程视为稀疏张量代数核运算，RTeAAL Sim实现了仿真行为与二进制大小的解耦，使RTL仿真能够应用已广泛研究的张量代数优化技术。我们证明，即使仅采用部分优化技术，该基于张量的仿真器原型也已显著缓解了编译开销和前端压力，在多个CPU架构和指令集体系结构（ISAs）上，其性能已可与高度优化的Verilator仿真器相媲美。"
  },
  {
    "date": "2026-01-26",
    "title": "AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security",
    "authors": "Dongrui Liu, Qihan Ren, Chen Qian, Shuai Shao, Yuejin Xie, Yu Li, Zhonghao Yang, Haoyu Luo, Peng Wang, Qingyu Liu, Binxin Hu, Ling Tang, Jilin Mei, Dadi Guo, Leitao Yuan, Junyao Yang, Guanxu Chen, Qihao Lin, Yi Yu, Bo Zhang, Jiaxuan Guo, Jie Zhang, Wenqi Shao, Huiqi Deng, Zhiheng Xi, Wenjie Wang, Wenxuan Wang, Wen Shen, Zhikai Chen, Haoyu Xie, Jialing Tao, Juntao Dai, Jiaming Ji, Zhongjie Ba, Linfeng Zhang, Yong Liu, Quanshi Zhang, Lei Zhu, Zhihua Wei, Hui Xue, Chaochao Lu, Jing Shao, Xia Hu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18491v1",
    "source": "arXiv",
    "abstract": "The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.",
    "title_zh": "AgentDoG：一种用于AI代理安全与防护的诊断护栏框架",
    "abstract_zh": "人工智能代理的兴起带来了复杂的安全与保障挑战，这些挑战源于代理的自主工具使用及其与环境的交互。当前的防护模型缺乏对代理风险的认知以及在风险诊断中的透明性。为应对复杂且多样的风险行为，我们首先提出一个统一的三维分类体系，从“来源”（何处）、“故障模式”（如何）和“后果”（什么）三个维度对代理风险进行正交分类。基于这一结构化、分层化的分类体系，我们构建了一个细粒度的代理安全基准测试（ATBench），并提出了AgentDoG（Agent Safety and Security Diagnostic Guardrail）框架，用于实现代理的安全与安全保障。AgentDoG能够在代理的行为轨迹中提供细粒度且上下文相关的监控。更重要的是，AgentDoG能够诊断不安全行为及看似安全但不合理行为的根本原因，不仅提供可追溯性与透明性，还超越了简单的二元标签判断，从而有效促进代理的对齐。AgentDoG已推出三种不同规模的版本（4B、7B 和 8B 参数），涵盖 Qwen 与 Llama 模型系列。大量实验结果表明，AgentDoG 在多样且复杂的交互场景中，实现了当前最先进的代理安全调控性能。所有模型与数据集均已公开发布。"
  },
  {
    "date": "2026-01-26",
    "title": "Promises, Perils, and (Timely) Heuristics for Mining Coding Agent Activity",
    "authors": "Romain Robes Théo Matricon, Thomas Degueule, Andre Hora, Stefano Zacchiroli",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18345v1",
    "source": "arXiv",
    "abstract": "In 2025, coding agents have seen a very rapid adoption. Coding agents leverage Large Language Models (LLMs) in ways that are markedly different from LLM-based code completion, making their study critical. Moreover, unlike LLM-based completion, coding agents leave visible traces in software repositories, enabling the use of MSR techniques to study their impact on SE practices. This paper documents the promises, perils, and heuristics that we have gathered from studying coding agent activity on GitHub.",
    "title_zh": "承诺、风险与（及时的）启发式方法：挖掘编码代理活动",
    "abstract_zh": "到2025年，编码代理已得到极为迅速的普及。与基于大语言模型（LLM）的代码补全方式显著不同，编码代理以全新的方式利用大语言模型，因此对其研究显得尤为重要。此外，与基于LLM的代码补全不同，编码代理在软件仓库中留下了可被追踪的痕迹，这使得我们能够运用软件工程中的挖掘与分析（MSR）技术来研究它们对软件工程实践的影响。本文总结了我们在研究GitHub上编码代理活动过程中所发现的机遇、风险以及实用经验法则。"
  },
  {
    "date": "2026-01-26",
    "title": "Probing the Future of Meta-Analysis: Eliciting Design Principles via an Agentic Research IDE",
    "authors": "Sizhe Cheng, Feng Liang, Yuhan Wen, Xipei Yu, Yong Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18239v1",
    "source": "arXiv",
    "abstract": "Meta-analyses and systematic reviews demand rigorous abductive reasoning to build, test, and refine hypotheses across vast, heterogeneous literature. While NLP advancements have automated parts of this pipeline, existing tools often detach researchers from the cognitive loop or function merely as retrieval engines, leading to loss of intellectual ownership and frequent context switching. We present Research IDE, a prototype reimagining authoring environments through the \"Research as Code\" metaphor. Research IDE embeds a multi-agent backend into the writing flow, enabling in-situ verification via \"hypothesis breakpoints.\" A one-week field deployment with 8 domain experts, followed by a reflective workshop, as a Research through Design (RtD) probe, reveals that users strongly preferred this verification workflow, actively leveraged prior knowledge for confirmation, and reported that breakpoints sparked insights. Drawing from participant feedback and suggestions, we derive design implications for future AI-assisted research tools that fully preserve researcher autonomy and intellectual ownership while harnessing computational scale.",
    "title_zh": "探索元分析的未来：通过智能研究IDE提炼设计原则",
    "abstract_zh": "元分析与系统综述需要严谨的溯因推理，以在庞大且异质的文献中构建、检验并不断优化假设。尽管自然语言处理（NLP）技术已部分自动化了这一流程，但现有工具往往使研究者脱离认知循环，或仅充当简单的检索引擎，导致研究者失去对工作的智力主导权，并频繁在不同上下文间切换。我们提出了“Research IDE”——一个基于“研究即代码”（Research as Code）理念重新构想的创作环境原型。Research IDE 将多智能体后端嵌入写作流程，通过“假设断点”实现即时验证。为期一周的真实场景部署，面向八位领域专家，并辅以反思性工作坊，作为设计研究（Research through Design, RtD）的探针，结果显示用户强烈偏好这种验证工作流，积极运用已有知识进行确认，并反馈称断点机制激发了新的洞见。结合参与者反馈与建议，我们提炼出未来AI辅助研究工具的设计启示：在充分利用计算规模的同时，充分保障研究者的自主性与智力所有权。"
  },
  {
    "date": "2026-01-26",
    "title": "Yunjue Agent Tech Report: A Fully Reproducible, Zero-Start In-Situ Self-Evolving Agent System for Open-Ended Tasks",
    "authors": "Haotian Li, Shijun Yang, Weizhen Qi, Silei Zhao, Rui Hua, Mingzhu Song, Xiaojian Yang, Chao Peng",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18226v1",
    "source": "arXiv",
    "abstract": "Conventional agent systems often struggle in open-ended environments where task distributions continuously drift and external supervision is scarce. Their reliance on static toolsets or offline training lags behind these dynamics, leaving the system's capability boundaries rigid and unknown. To address this, we propose the In-Situ Self-Evolving paradigm. This approach treats sequential task interactions as a continuous stream of experience, enabling the system to distill short-term execution feedback into long-term, reusable capabilities without access to ground-truth labels. Within this framework, we identify tool evolution as the critical pathway for capability expansion, which provides verifiable, binary feedback signals. Within this framework, we develop Yunjue Agent, a system that iteratively synthesizes, optimizes, and reuses tools to navigate emerging challenges. To optimize evolutionary efficiency, we further introduce a Parallel Batch Evolution strategy. Empirical evaluations across five diverse benchmarks under a zero-start setting demonstrate significant performance gains over proprietary baselines. Additionally, complementary warm-start evaluations confirm that the accumulated general knowledge can be seamlessly transferred to novel domains. Finally, we propose a novel metric to monitor evolution convergence, serving as a function analogous to training loss in conventional optimization. We open-source our codebase, system traces, and evolved tools to facilitate future research in resilient, self-evolving intelligence.",
    "title_zh": "云觉智能体技术报告：一种可完全复现、从零开始的原位自进化智能体系统，适用于开放性任务",
    "abstract_zh": "传统智能体系统在任务分布持续漂移且外部监督稀缺的开放环境下面临严峻挑战。其对静态工具集或离线训练的依赖，难以跟上环境动态变化，导致系统能力边界僵化且难以界定。为应对这一问题，我们提出“就地自演化”（In-Situ Self-Evolving）范式。该方法将连续的任务交互视为一种持续的经验流，使系统能够在无真实标签的情况下，从短期执行反馈中提炼出长期可复用的能力。在此框架下，我们识别出工具演化是能力扩展的关键路径，它能提供可验证的二元反馈信号。基于此，我们构建了“云觉智能体”（Yunjue Agent），一个能够迭代合成、优化并重用工具以应对新兴挑战的系统。为进一步提升演化效率，我们引入了“并行批量演化”策略。在五种不同基准测试中，于零起点设置下的实证评估表明，该系统显著优于现有专有基线。此外，补充的暖启动评估也证实，积累的通用知识可无缝迁移至新领域。最后，我们提出一种新颖的度量指标，用于监测演化收敛性，其功能类似于传统优化中的训练损失。我们已开源代码库、系统轨迹及演化生成的工具，以推动韧性自演化智能的未来研究。"
  },
  {
    "date": "2026-01-26",
    "title": "Think When Needed: Model-Aware Reasoning Routing for LLM-based Ranking",
    "authors": "Huizhong Guo, Tianjun Wei, Dongxia Wang, Yingpeng Du, Ziyan Wang, Jie Zhang, Zhu Sun",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18146v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) are increasingly applied to ranking tasks in retrieval and recommendation. Although reasoning prompting can enhance ranking utility, our preliminary exploration reveals that its benefits are inconsistent and come at a substantial computational cost, suggesting that when to reason is as crucial as how to reason. To address this issue, we propose a reasoning routing framework that employs a lightweight, plug-and-play router head to decide whether to use direct inference (Non-Think) or reasoning (Think) for each instance before generation. The router head relies solely on pre-generation signals: i) compact ranking-aware features (e.g., candidate dispersion) and ii) model-aware difficulty signals derived from a diagnostic checklist reflecting the model's estimated need for reasoning. By leveraging these features before generation, the router outputs a controllable token that determines whether to apply the Think mode. Furthermore, the router can adaptively select its operating policy along the validation Pareto frontier during deployment, enabling dynamic allocation of computational resources toward instances most likely to benefit from Think under varying system constraints. Experiments on three public ranking datasets with different scales of open-source LLMs show consistent improvements in ranking utility with reduced token consumption (e.g., +6.3\\% NDCG@10 with -49.5\\% tokens on MovieLens with Qwen3-4B), demonstrating reasoning routing as a practical solution to the accuracy-efficiency trade-off.",
    "title_zh": "需要时才思考：面向基于大语言模型的排序的模型感知推理路由",
    "abstract_zh": "大型语言模型（LLMs）在信息检索与推荐系统的排序任务中正得到越来越广泛的应用。尽管推理提示（reasoning prompting）能够提升排序效果，但我们的初步探索发现，其收益并不稳定，且伴随着巨大的计算开销，这表明“何时推理”与“如何推理”同样关键。为解决这一问题，我们提出一种推理路由框架，该框架采用轻量级、即插即用的路由头（router head），在生成前动态判断每个实例应采用直接推理（Non-Think）还是推理模式（Think）。该路由头仅依赖生成前的信号：i）紧凑的排序感知特征（如候选项分布度）；ii）基于诊断检查清单生成的模型感知难度信号，反映模型对推理需求的预估。通过利用这些生成前特征，路由头输出一个可控的标记 token，决定是否启用 Think 模式。此外，在部署阶段，路由头可自适应地沿验证集帕累托前沿选择最优运行策略，实现计算资源在最可能从 Think 模式中获益的实例上的动态分配，以应对不同系统约束。在三个具有不同规模开源 LLM 的公开排序数据集上的实验表明，该方法在显著降低 token 消耗的同时持续提升了排序性能（例如，在 MovieLens 数据集上使用 Qwen3-4B 时，NDCG@10 提升 6.3%，token 消耗减少 49.5%），验证了推理路由作为解决精度与效率权衡问题的实用方案的有效性。"
  },
  {
    "date": "2026-01-26",
    "title": "Beyond Text-to-SQL: Can LLMs Really Debug Enterprise ETL SQL?",
    "authors": "Jing Ye, Yiwen Duan, Yonghong Yu, Victor Ma, Yang Gao, Xing Chen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18119v1",
    "source": "arXiv",
    "abstract": "SQL is central to enterprise data engineering, yet generating fully correct SQL code in a single attempt remains difficult, even for experienced developers and advanced text-to-SQL LLMs, often requiring multiple debugging iterations. We introduce OurBench, the first benchmark for enterprise-level SQL reasoning and debugging. Our benchmark is built on two key innovations: (1) an automated construction workflow that uses reverse engineering to systematically inject realistic bugs into large-scale SQL code, enabling scalable and diverse benchmark generation; and (2) an execution-free evaluation framework tailored to enterprise settings, providing fast, accurate, and resource-efficient assessment. OurBench comprises 469 OurBenchSyn queries featuring syntax errors with explicit error messages, and 516 OurBenchSem queries targeting semantic errors in which the code fails to meet user intent. The queries are highly complex, averaging over 140 lines and featuring deep and wide abstract syntax trees. Evaluation of nearly 30 LLMs reveals a substantial performance gap: the best-performing model, Claude-4-Sonnet, achieves only 36.46 percent accuracy on OurBenchSyn and 32.17 percent on OurBenchSem, while most models score below 20 percent. We further explore four solution strategies, identify key challenges, and outline promising directions for enterprise SQL debugging with LLMs.",
    "title_zh": "超越文本到SQL：大语言模型真的能调试企业级ETL SQL吗？",
    "abstract_zh": "SQL在企业级数据工程中占据核心地位，然而即便是经验丰富的开发人员和先进的文本转SQL大语言模型（LLM），也难以一次性生成完全正确的SQL代码，通常需要多次调试才能修复问题。为此，我们提出了OurBench——首个面向企业级SQL推理与调试的基准测试集。OurBench基于两项关键创新：（1）一种自动化的构建流程，通过逆向工程系统性地向大规模SQL代码中注入真实存在的错误，从而实现可扩展且多样化的基准生成；（2）一种无需执行的评估框架，专为企业的实际应用场景设计，能够提供快速、准确且资源高效的评估能力。\n\nOurBench包含469个OurBenchSyn查询，主要针对语法错误，并附带明确的错误提示信息；以及516个OurBenchSem查询，聚焦于语义错误——即代码未能满足用户的真实意图。这些查询具有极高的复杂度，平均超过140行代码，抽象语法树（AST）结构既深又广。\n\n对近30个大语言模型的评估结果显示了显著的性能差距：表现最佳的模型Claude-4-Sonnet在OurBenchSyn上的准确率仅为36.46%，在OurBenchSem上为32.17%，而大多数模型的得分低于20%。我们进一步探索了四种解决方案策略，识别出关键挑战，并为未来基于大语言模型的企业级SQL调试指明了有前景的发展方向。"
  },
  {
    "date": "2026-01-26",
    "title": "From LLMs to LRMs: Rethinking Pruning for Reasoning-Centric Models",
    "authors": "Longwei Ding, Anhao Zhao, Fanghua Ye, Ziyang Chen, Xiaoyu Shen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18091v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) are increasingly costly to deploy, motivating extensive research on model pruning. However, most existing studies focus on instruction-following LLMs, leaving it unclear whether established pruning strategies transfer to reasoning-augmented models that explicitly generate long intermediate reasoning traces. In this work, we conduct a controlled study of pruning for both instruction-following ($\\textbf{LLM-instruct}$) and reasoning-augmented ($\\textbf{LLM-think}$) models. To isolate the effects of pruning, we align pruning calibration and post-pruning recovery data with each model's original training distribution, which we show yields more stable and reliable pruning behavior. We evaluate static depth pruning, static width pruning, and dynamic pruning across 17 tasks spanning classification, generation, and reasoning. Our results reveal clear paradigm-dependent differences: depth pruning outperforms width pruning on classification tasks, while width pruning is more robust for generation and reasoning. Moreover, static pruning better preserves reasoning performance, whereas dynamic pruning excels on classification and generation but remains challenging for long-chain reasoning. These findings underscore the need for pruning strategies that explicitly account for the distinct characteristics of reasoning-augmented LLMs. Our code is publicly available at https://github.com/EIT-NLP/LRM-Pruning.",
    "title_zh": "从大语言模型到小推理模型：重新思考以推理为中心的模型剪枝",
    "abstract_zh": "大型语言模型（LLMs）的部署成本日益增加，这促使了大量关于模型剪枝的研究。然而，目前大多数研究集中于指令跟随型LLMs，尚不清楚现有的剪枝策略是否适用于那些显式生成长中间推理链的推理增强型模型。在本工作中，我们对指令跟随型（$\\textbf{LLM-instruct}$）和推理增强型（$\\textbf{LLM-think}$）模型进行了受控的剪枝研究。为隔离剪枝的影响，我们将剪枝校准与剪枝后的恢复数据与各模型原始训练分布对齐，结果表明这能带来更稳定、可靠的剪枝表现。我们在涵盖分类、生成和推理共17个任务上评估了静态深度剪枝、静态宽度剪枝以及动态剪枝。实验结果揭示出明显的范式依赖差异：在分类任务中，深度剪枝优于宽度剪枝；而在生成和推理任务中，宽度剪枝更具鲁棒性。此外，静态剪枝更能保持推理性能，而动态剪枝在分类和生成任务中表现更优，但在长链推理任务中仍面临挑战。这些发现强调了需要设计能够明确考虑推理增强型LLMs独特特性的剪枝策略。我们的代码已公开，地址为：https://github.com/EIT-NLP/LRM-Pruning。"
  },
  {
    "date": "2026-01-26",
    "title": "Forecasting the Maintained Score from the OpenSSF Scorecard for GitHub Repositories linked to PyPI libraries",
    "authors": "Alexandros Tsakpinis, Efe Berk Ergülec, Emil Schwenger, Alexander Pretschner",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18344v1",
    "source": "arXiv",
    "abstract": "The OpenSSF Scorecard is widely used to assess the security posture of open-source software repositories, with the Maintained metric indicating recent development activity and helping identify potentially abandoned dependencies. However, this metric is inherently retrospective, reflecting only the past 90 days of activity and providing no insight into future maintenance, which limits its usefulness for proactive risk assessment. In this paper, we study to what extent future maintenance activity, as captured by the OpenSSF Maintained score, can be forecasted. We analyze 3,220 GitHub repositories associated with the top 1% most central PyPI libraries by PageRank and reconstruct historical Maintained scores over a three-year period. We formulate the task as multivariate time series forecasting and consider four target representations: raw scores, bucketed maintenance levels, numerical trend slopes, and categorical trend types. We compare a statistical model (VARMA), a machine learning model (Random Forest), and a deep learning model (LSTM) across training windows of 3-12 months and forecasting horizons of 1-6 months. Our results show that future maintenance activity can be predicted with meaningful accuracy, particularly for aggregated representations such as bucketed scores and trend types, achieving accuracies above 0.95 and 0.80, respectively. Simpler statistical and machine learning models perform on par with deep learning approaches, indicating that complex architectures are not required. These findings suggest that predictive modeling can effectively complement existing Scorecard metrics, enabling more proactive assessment of open-source maintenance risks.",
    "title_zh": "基于OpenSSF Scorecard预测GitHub仓库与PyPI库关联的持续得分",
    "abstract_zh": "OpenSSF Scorecard 广泛用于评估开源软件仓库的安全状况，其中“维护”（Maintained）指标反映了近期的开发活动，有助于识别可能已被放弃的依赖项。然而，该指标本质上是回顾性的，仅反映过去90天内的活动情况，无法提供未来维护情况的任何信息，这限制了其在主动风险评估中的应用价值。本文研究了未来维护活动（以 OpenSSF 维护评分所体现的内容）在多大程度上可以被预测。我们分析了3,220个与 PageRank 排名前1%的 PyPI 库相关的 GitHub 仓库，并重建了过去三年的历史维护评分数据。我们将该任务建模为多元时间序列预测问题，考虑四种目标表示形式：原始分数、分桶后的维护水平、数值趋势斜率以及分类的趋势类型。我们在训练窗口为3至12个月、预测周期为1至6个月的情况下，对比了统计模型（VARMA）、机器学习模型（随机森林）和深度学习模型（LSTM）的表现。结果表明，未来维护活动能够以较高的准确度进行预测，尤其是对聚合表示形式（如分桶分数和趋势类型），准确率分别达到0.95以上和0.80以上。相比之下，更简单的统计模型和机器学习模型的表现与深度学习方法相当，表明复杂的模型架构并非必需。这些发现表明，预测建模可有效补充现有的 Scorecard 指标，从而实现对开源项目维护风险的更主动评估。"
  },
  {
    "date": "2026-01-26",
    "title": "Bridging Instead of Replacing Online Coding Communities with AI through Community-Enriched Chatbot Designs",
    "authors": "Junling Wang, Lahari Goswami, Gustavo Kreia Umbelino, Kiara Garcia Chau, Mrinmaya Sachan, April Yi Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18697v1",
    "source": "arXiv",
    "abstract": "LLM-based chatbots like ChatGPT have become popular tools for assisting with coding tasks. However, they often produce isolated responses and lack mechanisms for social learning or contextual grounding. In contrast, online coding communities like Kaggle offer socially mediated learning environments that foster critical thinking, engagement, and a sense of belonging. Yet, growing reliance on LLMs risks diminishing participation in these communities and weakening their collaborative value. To address this, we propose Community-Enriched AI, a design paradigm that embeds social learning dynamics into LLM-based chatbots by surfacing user-generated content and social design feature from online coding communities. Using this paradigm, we implemented a RAG-based AI chatbot leveraging resources from Kaggle to validate our design. Across two empirical studies involving 28 and 12 data science learners, respectively, we found that Community-Enriched AI significantly enhances user trust, encourages engagement with community, and effectively supports learners in solving data science tasks. We conclude by discussing design implications for AI assistance systems that bridge -- rather than replace -- online coding communities.",
    "title_zh": "通过社区增强型聊天机器人设计，实现在线编程社区的融合而非替代",
    "abstract_zh": "基于大语言模型（LLM）的聊天机器人，如ChatGPT，已成为辅助编程任务的流行工具。然而，这些系统往往产生孤立的回应，缺乏社会学习机制或上下文锚定能力。相比之下，Kaggle等在线编程社区提供了以社会互动为中介的学习环境，能够促进批判性思维、增强参与感，并培养归属感。但对LLM日益依赖的风险在于，可能削弱用户在这些社区中的参与度，从而削弱其协作价值。为应对这一挑战，我们提出“社区增强型AI”这一设计范式，通过在基于LLM的聊天机器人中引入来自在线编程社区的用户生成内容与社交设计特征，将社会学习动态融入其中。基于该范式，我们开发了一个基于检索增强生成（RAG）的AI聊天机器人，利用Kaggle平台的资源进行验证。在两项实证研究中，分别涉及28名和12名数据科学学习者，结果表明，“社区增强型AI”显著提升了用户信任感，鼓励用户积极参与社区互动，并有效支持学习者完成数据科学任务。最后，我们讨论了此类AI辅助系统的设计启示：应致力于连接而非取代在线编程社区，实现人机协同的良性发展。"
  },
  {
    "date": "2026-01-26",
    "title": "Explainability Methods for Hardware Trojan Detection: A Systematic Comparison",
    "authors": "Paul Whitten, Francis Wolff, Chris Papachristou",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18696v1",
    "source": "arXiv",
    "abstract": "Hardware trojan detection requires accurate identification and interpretable explanations for security engineers to validate and act on results. This work compares three explainability categories for gate-level trojan detection on the Trust-Hub benchmark: (1) domain-aware property-based analysis of 31 circuit-specific features from gate fanin patterns, flip-flop distances, and I/O connectivity; (2) case-based reasoning using k-nearest neighbors for precedent-based explanations; and (3) model-agnostic feature attribution (LIME, SHAP, gradient). Results show different advantages per approach. Property-based analysis provides explanations through circuit concepts like \"high fanin complexity near outputs indicates potential triggers.\" Case-based reasoning achieves 97.4% correspondence between predictions and training exemplars, offering justifications grounded in precedent. LIME and SHAP provide feature attributions with strong inter-method correlation (r=0.94, p<0.001) but lack circuit-level context for validation. XGBoost classification achieves 46.15% precision and 52.17% recall on 11,392 test samples, a 9-fold precision improvement over prior work (Hasegawa et al.: 5.13%) while reducing false positive rates from 5.6% to 0.25%. Gradient-based attribution runs 481 times faster than SHAP but provides similar domain-opaque insights. This work demonstrates that property-based and case-based approaches offer domain alignment and precedent-based interpretability compared to generic feature rankings, with implications for XAI deployment where practitioners must validate ML predictions.",
    "title_zh": "硬件木马检测的可解释性方法：系统性比较",
    "abstract_zh": "硬件木马检测需要对安全工程师提供准确的识别结果以及可解释的说明，以便其验证并采取相应措施。本文在Trust-Hub基准上对比了三种面向门级木马检测的可解释性方法：（1）基于领域知识的属性分析，利用31个电路特定特征（包括门扇入模式、触发器距离和I/O连接性）进行分析；（2）基于案例的推理，采用k近邻方法实现基于先例的解释；（3）模型无关的特征重要性归因方法（LIME、SHAP、梯度）。实验结果表明，各类方法各有优势。属性分析通过电路概念提供解释，例如“输出附近存在高扇入复杂度可能指示潜在触发点”。基于案例的推理在预测结果与训练样本之间的对应率达到97.4%，能够基于历史先例提供可信的解释依据。LIME与SHAP方法虽能提供特征重要性排序，且两种方法间具有高度相关性（r=0.94，p<0.001），但缺乏电路层面的上下文信息，不利于工程师验证。XGBoost分类模型在11,392个测试样本上实现了46.15%的精确率和52.17%的召回率，相比先前工作（Hasegawa等：5.13%）提升了9倍的精确率，同时将误报率从5.6%降至0.25%。基于梯度的归因方法运行速度比SHAP快481倍，但提供的解释仍属领域无关型。本研究证明，相较于通用特征排序，基于属性和基于案例的方法在领域契合度和基于先例的可解释性方面更具优势，这对实际人工智能可解释性（XAI）部署具有重要意义——因为从业者必须能够验证机器学习预测结果的合理性。"
  },
  {
    "date": "2026-01-26",
    "title": "AI-Driven Fuzzing for Vulnerability Assessment of 5G Traffic Steering Algorithms",
    "authors": "Seyed Bagher Hashemi Natanzi, Hossein Mohammadi, Bo Tang, Vuk Marojevic",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18690v1",
    "source": "arXiv",
    "abstract": "Traffic Steering (TS) dynamically allocates user traffic across cells to enhance Quality of Experience (QoE), load balance, and spectrum efficiency in 5G networks. However, TS algorithms remain vulnerable to adversarial conditions such as interference spikes, handover storms, and localized outages. To address this, an AI-driven fuzz testing framework based on the Non-Dominated Sorting Genetic Algorithm II (NSGA-II) is proposed to systematically expose hidden vulnerabilities. Using NVIDIA Sionna, five TS algorithms are evaluated across six scenarios. Results show that AI-driven fuzzing detects 34.3% more total vulnerabilities and 5.8% more critical failures than traditional testing, achieving superior diversity and edge-case discovery. The observed variance in critical failure detection underscores the stochastic nature of rare vulnerabilities. These findings demonstrate that AI-driven fuzzing offers an effective and scalable validation approach for improving TS algorithm robustness and ensuring resilient 6G-ready networks.",
    "title_zh": "基于人工智能的模糊测试在5G流量引导算法漏洞评估中的应用",
    "abstract_zh": "流量引导（Traffic Steering, TS）通过动态分配用户流量至不同小区，以提升5G网络中的用户体验质量（QoE）、负载均衡及频谱效率。然而，TS算法仍易受干扰突增、切换风暴和局部故障等对抗性条件的影响。为应对这一挑战，本文提出一种基于非支配排序遗传算法II（NSGA-II）的AI驱动模糊测试框架，系统性地暴露隐藏漏洞。利用NVIDIA Sionna平台，对五种TS算法在六个典型场景下进行了评估。结果表明，与传统测试方法相比，AI驱动的模糊测试可检测到多出34.3%的总体漏洞和5.8%的关键故障，显著提升了测试多样性与边缘场景发现能力。关键故障检测结果的显著方差凸显了罕见漏洞固有的随机特性。研究结果表明，AI驱动的模糊测试为提升TS算法鲁棒性、保障6G就绪网络的韧性提供了高效且可扩展的验证途径。"
  },
  {
    "date": "2026-01-26",
    "title": "Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability",
    "authors": "Shobhita Sundaram, John Quan, Ariel Kwiatkowski, Kartik Ahuja, Yann Ollivier, Julia Kempe",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.18778v1",
    "source": "arXiv",
    "abstract": "Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.",
    "title_zh": "让教学模型学会自我教学：在可学习性的边缘进行推理",
    "abstract_zh": "模型能否学会突破自身的学习瓶颈？在初始成功率较低的数据集上，强化学习方法对大型推理模型进行微调时往往会陷入停滞，因为缺乏有效的训练信号。我们提出一个根本性问题：预训练的大语言模型能否利用其潜在知识，自动生成一套针对自身无法解决的问题的自动化教学课程？为探索这一问题，我们设计了SOAR——一种自提升框架，通过元强化学习（meta-RL）揭示这些教学信号。该框架中，一个“教师”版本的模型会为另一个“学生”版本生成合成问题，并根据学生在少量难题上的进步情况获得奖励。关键在于，SOAR将课程设计建立在可测量的学生进展之上，而非依赖于内在的代理奖励。我们在数学基准测试中最困难的子集（初始成功率为0/128）上开展研究，得出三个核心发现：第一，我们证明了实现双层元强化学习是可行的，它能够通过激发预训练模型生成有效“中间步骤”的潜在能力，在稀疏且二值化的奖励环境下实现持续学习；第二，基于实际进展的奖励机制优于以往大语言模型自对弈中使用的内在奖励方案，能稳定地避免后者常见的不稳定性与多样性崩溃现象；第三，对生成问题的分析表明，问题的结构质量与表述严谨性比解的正确性更关键，更能促进学习进展。我们的结果表明，生成有帮助的中间步骤并不需要模型事先具备解决难题的能力，从而为突破推理瓶颈提供了一条无需额外人工标注数据的系统性路径。"
  },
  {
    "date": "2026-1-26",
    "title": "GAEDM: Genetic Algorithm-Enhanced Static Analysis for Detection of API Hashing Obfuscation in Malware",
    "authors": "Yang Lan, Hui Shu, Zihan Sha, Fei Kang, XiaoBing Xiong, JingJing Li",
    "publish": "ACM Transactions on Privacy and Security",
    "url": "https://doi.org/10.1145/3793198",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "GAEDM：一种基于遗传算法增强的静态分析方法，用于检测恶意软件中的API哈希混淆",
    "abstract_zh": "None"
  }
]