[
  {
    "date": "2026-01-29",
    "title": "Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference",
    "authors": "Ziming Dong, Hardik Sharma, Evan O'Toole, Jaya Prakash Champati, Kui Wu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.22132v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) deliver state-of-the-art performance on complex reasoning tasks, but their inference costs limit deployment at scale. Small Language Models (SLMs) offer dramatic cost savings yet lag substantially in accuracy. Existing approaches - routing and cascading - treat the LLM as an all-or-nothing resource: either the query bypasses the LLM entirely, or the LLM generates a complete response at full cost. We introduce LLM Shepherding, a framework that requests only a short prefix (a hint) from the LLM and provides it to SLM. This simple mechanism is surprisingly effective for math and coding tasks: even hints comprising 10-30% of the full LLM response improve SLM accuracy significantly. Shepherding generalizes both routing and cascading, and it achieves lower cost under oracle decision-making. We develop a two-stage predictor that jointly determines whether a hint is needed and how many tokens to request. On the widely-used mathematical reasoning (GSM8K, CNK12) and code generation (HumanEval, MBPP) benchmarks, Shepherding reduces costs by 42-94% relative to LLM-only inference. Compared to state-of-the-art routing and cascading baselines, shepherding delivers up to 2.8x cost reduction while matching accuracy. To our knowledge, this is the first work to exploit token-level budget control for SLM-LLM collaboration.",
    "title_zh": "按提示付费，而非答案：面向低成本推理的大型语言模型引导方法",
    "abstract_zh": "大型语言模型（LLMs）在复杂推理任务上表现出顶尖的性能，但其推理成本限制了大规模部署。小型语言模型（SLMs）虽然能大幅降低计算成本，但在准确性方面仍显著落后。现有的方法——路由（routing）和级联（cascading）——将LLM视为“全有或全无”的资源：要么查询完全绕过LLM，要么由LLM以完整开销生成整个响应。我们提出了“LLM引导”（LLM Shepherding）框架，该框架仅从LLM请求一个简短的前缀（即提示信息），并将该前缀提供给SLM。这一简单机制在数学和编程任务中出人意料地有效：即使提示仅占LLM完整响应的10%-30%，也能显著提升SLM的准确性。引导机制同时兼容并超越了路由与级联策略，在理想决策条件下实现了更低的总体成本。我们设计了一个两阶段预测器，联合判断是否需要提示以及应请求多少个token。在广泛使用的数学推理（GSM8K、CNK12）和代码生成（HumanEval、MBPP）基准测试中，引导方法相比纯LLM推理将成本降低了42%-94%。相较于当前最先进的路由与级联基线，引导方法在保持相同准确率的前提下，最多可实现2.8倍的成本降低。据我们所知，这是首个利用token级别预算控制来实现SLM与LLM协作的研究工作。"
  },
  {
    "date": "2026-01-29",
    "title": "On the Paradoxical Interference between Instruction-Following and Task Solving",
    "authors": "Yunjia Qi, Hao Peng, Xintong Shi, Amy Xin, Xiaozhi Wang, Bin Xu, Lei Hou, Juanzi Li",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.22047v1",
    "source": "arXiv",
    "abstract": "Instruction following aims to align Large Language Models (LLMs) with human intent by specifying explicit constraints on how tasks should be performed. However, we reveal a counterintuitive phenomenon: instruction following can paradoxically interfere with LLMs' task-solving capability. We propose a metric, SUSTAINSCORE, to quantify the interference of instruction following with task solving. It measures task performance drop after inserting into the instruction a self-evident constraint, which is naturally met by the original successful model output and extracted from it. Experiments on current LLMs in mathematics, multi-hop QA, and code generation show that adding the self-evident constraints leads to substantial performance drops, even for advanced models such as Claude-Sonnet-4.5. We validate the generality of the interference across constraint types and scales. Furthermore, we identify common failure patterns, and by investigating the mechanisms of interference, we observe that failed cases allocate significantly more attention to constraints compared to successful ones. Finally, we use SUSTAINSCORE to conduct an initial investigation into how distinct post-training paradigms affect the interference, presenting empirical observations on current alignment strategies. We will release our code and data to facilitate further research",
    "title_zh": "指令遵循与任务解决之间的悖论性干扰",
    "abstract_zh": "指令遵循旨在通过明确指定任务执行的约束条件，使大型语言模型（LLMs）与人类意图对齐。然而，我们揭示了一个反直觉的现象：指令遵循反而可能悖论性地干扰LLM的任务求解能力。为此，我们提出了一种度量指标——SUSTAINSCORE，用于量化指令遵循对任务求解的干扰程度。该指标通过在指令中插入一个自明的约束条件来衡量任务性能的下降，而这一约束条件原本就自然地被原始成功模型输出所满足，并可从中提取得出。\n\n在数学推理、多跳问答和代码生成等任务上对当前主流LLM的实验表明，即使加入这些自明的约束条件，也会导致显著的性能下降，即便是先进的模型如Claude-Sonnet-4.5也未能幸免。我们验证了这种干扰现象在不同类型的约束以及不同规模下的普遍性。此外，我们识别出常见的失败模式，并通过深入分析干扰机制发现：失败案例相较于成功案例，会显著分配更多注意力于指令中的约束条件。\n\n最后，我们利用SUSTAINSCORE对不同后训练范式如何影响该干扰现象进行了初步探究，得出了关于当前对齐策略的一些实证观察。我们将公开我们的代码与数据，以促进后续研究。"
  },
  {
    "date": "2026-01-29",
    "title": "ToolWeaver: Weaving Collaborative Semantics for Scalable Tool Use in Large Language Models",
    "authors": "Bowen Fang, Wen Ye, Yunyue Su, Jinghao Zhang, Qiang Liu, Yesheng Liu, Xin Sun, Shu Wu, Jiabing Yang, Baole Wei, Liang Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21947v1",
    "source": "arXiv",
    "abstract": "Prevalent retrieval-based tool-use pipelines struggle with a dual semantic challenge: their retrievers often employ encoders that fail to capture complex semantics, while the Large Language Model (LLM) itself lacks intrinsic tool knowledge from its natural language pretraining. Generative methods offer a powerful alternative by unifying selection and execution, tasking the LLM to directly learn and generate tool identifiers. However, the common practice of mapping each tool to a unique new token introduces substantial limitations: it creates a scalability and generalization crisis, as the vocabulary size explodes and each tool is assigned a semantically isolated token. This approach also creates a semantic bottleneck that hinders the learning of collaborative tool relationships, as the model must infer them from sparse co-occurrences of monolithic tool IDs within a vast library. To address these limitations, we propose ToolWeaver, a novel generative tool learning framework that encodes tools into hierarchical sequences. This approach makes vocabulary expansion logarithmic to the number of tools. Crucially, it enables the model to learn collaborative patterns from the dense co-occurrence of shared codes, rather than the sparse co-occurrence of monolithic tool IDs. We generate these structured codes through a novel tokenization process designed to weave together a tool's intrinsic semantics with its extrinsic co-usage patterns. These structured codes are then integrated into the LLM through a generative alignment stage, where the model is fine-tuned to produce the hierarchical code sequences. Evaluation results with nearly 47,000 tools show that ToolWeaver significantly outperforms state-of-the-art methods, establishing a more scalable, generalizable, and semantically-aware foundation for advanced tool-augmented agents.",
    "title_zh": "ToolWeaver：为大规模语言模型中可扩展的工具使用编织协作语义",
    "abstract_zh": "主流的基于检索的工具使用流程面临双重语义挑战：其检索模块通常采用无法捕捉复杂语义的编码器，而大型语言模型（LLM）本身也缺乏在自然语言预训练中获得的内在工具知识。生成式方法提供了一种强有力的替代方案，通过统一工具选择与执行过程，使LLM直接学习并生成工具标识符。然而，将每个工具映射为唯一新标记的常见做法带来了显著局限性：它导致词汇量急剧膨胀，引发可扩展性和泛化能力危机，且每个工具被分配一个语义孤立的标记。这种设计还造成了语义瓶颈，阻碍了模型对协作性工具关系的学习，因为模型必须从庞大工具库中稀疏的、单一工具ID共现模式中推断这些关系。为解决上述问题，我们提出ToolWeaver——一种新颖的生成式工具学习框架，将工具编码为层次化序列。该方法使词汇量的增长与工具数量呈对数关系，从而实现高效扩展。更重要的是，它使模型能够从共享代码的密集共现中学习协作模式，而非依赖于单个工具ID在海量工具中的稀疏共现。我们通过一种创新的分词过程生成这些结构化代码，该过程将工具的内在语义与其外在共用模式有机融合。随后，这些结构化代码通过生成式对齐阶段融入LLM，模型在此阶段经过微调，以生成层次化的代码序列。在近47,000个工具上的评估结果表明，ToolWeaver显著优于现有最先进方法，为高级工具增强型智能体奠定了更可扩展、更具泛化能力且语义感知更强的基础。"
  },
  {
    "date": "2026-01-29",
    "title": "Latent Chain-of-Thought as Planning: Decoupling Reasoning from Verbalization",
    "authors": "Jiecong Wang, Hao Peng, Chunyang Liu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21358v1",
    "source": "arXiv",
    "abstract": "Chain-of-Thought (CoT) empowers Large Language Models (LLMs) to tackle complex problems, but remains constrained by the computational cost and reasoning path collapse when grounded in discrete token spaces. Recent latent reasoning approaches attempt to optimize efficiency by performing reasoning within continuous hidden states. However, these methods typically operate as opaque end-to-end mappings from explicit reasoning steps to latent states, and often require a pre-defined number of latent steps during inference. In this work, we introduce PLaT (Planning with Latent Thoughts), a framework that reformulates latent reasoning as planning by fundamentally decouple reasoning from verbalization. We model reasoning as a deterministic trajectory of latent planning states, while a separate Decoder grounds these thoughts into text when necessary. This decoupling allows the model to dynamically determine when to terminate reasoning rather than relying on fixed hyperparameters. Empirical results on mathematical benchmarks reveal a distinct trade-off: while PLaT achieves lower greedy accuracy than baselines, it demonstrates superior scalability in terms of reasoning diversity. This indicates that PLaT learns a robust, broader solution space, offering a transparent and scalable foundation for inference-time search.",
    "title_zh": "潜在思维链作为规划：将推理与语言表达分离",
    "abstract_zh": "思维链（Chain-of-Thought, CoT）使大型语言模型（LLMs）能够应对复杂问题，但在离散的标记空间中进行推理时，仍受限于计算成本以及推理路径的坍塌问题。近期的一些潜在推理方法尝试通过在连续隐藏状态中执行推理来提升效率。然而，这些方法通常作为从显式推理步骤到潜在状态的黑箱端到端映射，且在推理过程中往往需要预先设定潜在步骤的数量。在本工作中，我们提出了PLaT（Latent Thoughts Planning，潜思规划），一种将潜在推理重新定义为规划的新框架，其核心在于从根本上将推理与语言表达解耦。我们把推理建模为一系列确定性的潜在规划状态轨迹，而一个独立的解码器仅在必要时将这些思想转化为文本。这种解耦机制使得模型能够动态决定何时终止推理，而非依赖固定的超参数。在数学基准测试上的实证结果表明，尽管PLaT在贪婪搜索下的准确率低于基线模型，但其在推理多样性方面展现出更优的可扩展性。这表明PLaT学习到了一个更鲁棒、更广泛的解空间，为推理时的搜索提供了一个透明且可扩展的基础。"
  },
  {
    "date": "2026-01-29",
    "title": "SecIC3: Customizing IC3 for Hardware Security Verification",
    "authors": "Qinhan Tan, Akash Gaonkar, Yu-Wei Fan, Aarti Gupta, Sharad Malik",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21353v1",
    "source": "arXiv",
    "abstract": "Recent years have seen significant advances in using formal verification to check hardware security properties. Of particular practical interest are checking confidentiality and integrity of secrets, by checking that there is no information flow between the secrets and observable outputs. A standard method for checking information flow is to translate the corresponding non-interference hyperproperty into a safety property on a self-composition of the design, which has two copies of the design composed together. Although prior efforts have aimed to reduce the size of the self-composed design, there are no state-of-the-art model checkers that exploit their special structure for hardware security verification. In this paper, we propose SecIC3, a hardware model checking algorithm based on IC3 that is customized to exploit this self-composition structure. SecIC3 utilizes this structure in two complementary techniques: symmetric state exploration and adding equivalence predicates. We implement SecIC3 on top of two open-source IC3 implementations and evaluate it on a non-interference checking benchmark consisting of 10 designs. The experiment results show that SecIC3 significantly reduces the time for finding security proofs, with up to 49.3x proof speedup compared to baseline implementations.",
    "title_zh": "SecIC3：针对硬件安全验证的IC3定制化",
    "abstract_zh": "近年来，形式化验证在检查硬件安全属性方面取得了显著进展。其中特别具有实际意义的是对机密信息和完整性进行验证，即通过检查是否存在从秘密信息到可观察输出的信息流来确保安全性。检查信息流的一种标准方法是将相应的非干扰超性质（non-interference hyperproperty）转化为对设计自组合（self-composition）上的安全性质进行验证，该自组合包含两个并行的同一设计副本。尽管先前的研究已致力于减小自组合设计的规模，但目前尚无先进的模型检测器能够利用其特有的结构来进行硬件安全验证。本文提出了一种基于IC3算法的专用硬件模型检查算法——SecIC3，专门针对这种自组合结构进行了优化。SecIC3通过两种互补的技术充分利用了该结构：对称状态探索和添加等价谓词。我们在两个开源IC3实现的基础上实现了SecIC3，并在包含10个设计的非干扰性检查基准测试集上进行了评估。实验结果表明，与基线实现相比，SecIC3显著缩短了发现安全证明所需的时间，最多可实现49.3倍的证明速度提升。"
  },
  {
    "date": "2026-01-29",
    "title": "Developers in the Age of AI: Adoption, Policy, and Diffusion of AI Software Engineering Tools",
    "authors": "Mark Looi, Julianne Quinn",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21305v1",
    "source": "arXiv",
    "abstract": "The rapid advance of Generative AI into software development prompts this empirical investigation of perceptual effects on practice. We study the usage patterns of 147 professional developers, examining perceived correlates of AI tools use, the resulting productivity and quality outcomes, and developer readiness for emerging AI-enhanced development. We describe a virtuous adoption cycle where frequent and broad AI tools use are the strongest correlates of both Perceived Productivity (PP) and quality, with frequency strongest. The study finds no perceptual support for the Quality Paradox and shows that PP is positively correlated with Perceived Code Quality (PQ) improvement. Developers thus report both productivity and quality gains. High current usage, breadth of application, frequent use of AI tools for testing, and ease of use correlate strongly with future intended adoption, though security concerns remain a moderate and statistically significant barrier to adoption. Moreover, AI testing tools' adoption lags that of coding tools, opening a Testing Gap. We identify three developer archetypes (Enthusiasts, Pragmatists, Cautious) that align with an innovation diffusion process wherein the virtuous adoption cycle serves as the individual engine of progression. Our findings reveal that organizational adoption of AI tools follows such a process: Enthusiasts push ahead with tools, creating organizational success that converts Pragmatists. The Cautious are held in organizational stasis: without early adopter examples, they don't enter the virtuous adoption cycle, never accumulate the usage frequency that drives intent, and never attain high efficacy. Policy itself does not predict individuals' intent to increase usage but functions as a marker of maturity, formalizing the successful diffusion of adoption by Enthusiasts while acting as a gateway that the Cautious group has yet to reach.",
    "title_zh": "人工智能时代开发者的应用、政策与AI软件工程工具的扩散",
    "abstract_zh": "生成式人工智能在软件开发领域的迅速发展，促使我们开展一项关于其感知影响的实证研究。本研究调查了147名专业开发者的使用模式，探讨了AI工具使用与感知相关因素之间的关系、由此带来的生产效率与质量结果，以及开发者对新兴AI增强型开发的准备程度。研究揭示了一个良性采纳循环：频繁且广泛地使用AI工具是感知生产力（PP）和代码质量的最强预测因子，其中使用频率的影响最为显著。研究未发现支持“质量悖论”的感知证据，反而表明感知生产力（PP）与感知代码质量（PQ）的提升呈正相关。因此，开发者普遍报告在生产效率和代码质量方面均有所提升。\n\n当前高使用率、应用范围广、频繁使用AI工具进行测试，以及工具易用性，均与未来预期采纳意愿高度相关；尽管如此，安全顾虑仍是阻碍采纳的中等程度但具有统计显著性的障碍。此外，AI测试工具的采纳进度明显滞后于AI编码工具，从而形成“测试缺口”。我们识别出三类开发者原型——热情者、务实者与谨慎者，它们与创新扩散过程相契合，而良性采纳循环则成为个体进步的内在驱动力。\n\n研究发现，组织层面的AI工具采纳也遵循这一过程：热情者率先采用工具并取得组织成功，进而推动务实者跟进；而谨慎者则处于组织停滞状态：由于缺乏早期采纳者的示范效应，他们无法进入良性采纳循环，无法积累驱动意图所需的使用频率，因而也无法达到高水平的效能。政策本身并不能预测个人增加使用意愿，但它作为成熟度的标志，标志着热情者已成功实现创新扩散，并作为一道门槛，目前仍为谨慎者群体所尚未跨越。"
  },
  {
    "date": "2026-01-29",
    "title": "DUET: Distilled LLM Unlearning from an Efficiently Contextualized Teacher",
    "authors": "Yisheng Zhong, Zhengbang Yang, Zhuangdi Zhu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21283v1",
    "source": "arXiv",
    "abstract": "LLM unlearning is a technique to remove the impacts of undesirable knowledge from the model without retraining from scratch, which is indispensable towards trustworthy AI. Existing unlearning methods face significant limitations: conventional tuning-based unlearning is computationally heavy and prone to catastrophic forgetting. In contrast, in-contextualized unlearning is lightweight for precise unlearning but vulnerable to prompt removal or reverse engineering attacks. In response, we propose Distilled Unlearning from an Efficient Teacher (DUET), a novel distillation-based unlearning method that combines the merits of these two lines of work. It learns a student model to imitate the behavior of a prompt-steered teacher that effectively refuses undesirable knowledge generation while preserving general domain knowledge. Extensive evaluations on existing benchmarks with our enriched evaluation protocols demonstrate that DUET achieves higher performance in both forgetting and utility preservation, while being orders of magnitude more data-efficient than state-of-the-art unlearning methods.",
    "title_zh": "DUET：从高效情境化教师中提炼的LLM遗忘机制",
    "abstract_zh": "大语言模型（LLM）的遗忘技术是一种无需从头重新训练即可消除模型中不良知识影响的方法，对于构建可信赖的人工智能至关重要。现有的遗忘方法存在显著局限：基于微调的传统遗忘方法计算开销大，且容易导致灾难性遗忘；而基于上下文的遗忘方法虽然轻量且能实现精准遗忘，但对提示词移除或逆向工程攻击较为脆弱。针对上述问题，我们提出了一种高效教师引导的蒸馏式遗忘方法——DUET（Distilled Unlearning from an Efficient Teacher）。该方法结合了上述两类技术的优势，通过让学生模型模仿一个由提示词控制的教师模型的行为，使教师模型能够有效拒绝生成不良知识，同时保留通用领域知识。在我们扩充的评估协议下，对现有基准数据集进行的大量实验表明，DUET在遗忘效果和知识保留性能方面均优于现有方法，且在数据效率上比当前最先进的遗忘方法高出数个数量级。"
  },
  {
    "date": "2026-01-29",
    "title": "Uncovering Hidden Correctness in LLM Causal Reasoning via Symbolic Verification",
    "authors": "Paul He, Yinya Huang, Mrinmaya Sachan, Zhijing Jin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21210v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) are increasingly being applied to tasks that involve causal reasoning. However, current benchmarks often rely on string matching or surface-level metrics that do not capture whether the output of a model is formally valid under the semantics of causal reasoning. To address this, we propose DoVerifier, a simple symbolic verifier that checks whether LLM-generated causal expressions are derivable from a given causal graph using rules from do-calculus and probability theory. This allows us to recover correct answers to causal queries that would otherwise be marked incorrect due to superficial differences in their causal semantics. Our evaluations on synthetic data and causal QA benchmarks show that DoVerifier more accurately captures semantic correctness of causal reasoning traces, offering a more rigorous and informative way to evaluate LLMs on causal reasoning.",
    "title_zh": "通过符号验证揭示大语言模型因果推理中的隐含正确性",
    "abstract_zh": "大型语言模型（LLMs）正越来越多地应用于涉及因果推理的任务。然而，当前的评估基准往往依赖于字符串匹配或表面层面的指标，无法准确反映模型输出在因果推理语义上是否形式正确。为解决这一问题，我们提出了DoVerifier——一种简单的符号验证工具，能够通过使用do-演算和概率论中的规则，检验LLM生成的因果表达式是否可从给定的因果图中推导得出。该方法使我们能够恢复那些因表层表达差异而被错误标记为错误的正确因果查询答案。我们在合成数据和因果问答基准上的评估表明，DoVerifier能更准确地捕捉因果推理过程的语义正确性，为评估LLM在因果推理方面的能力提供了一种更为严格且富有信息量的方法。"
  },
  {
    "date": "2026-01-29",
    "title": "Multi-objective Integer Linear Programming approach for Automatic Software Cognitive Complexity Reduction",
    "authors": "Adriana Novoa-Hurtado, Rubén Saborido, Francisco Chicano, Manuel Giménez-Medina",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21565v1",
    "source": "arXiv",
    "abstract": "Clear and concise code is necessary to ensure maintainability, so it is crucial that the software is as simple as possible to understand, to avoid bugs and, above all, vulnerabilities. There are many ways to enhance software without changing its functionality, considering the extract method refactoring the primary process to reduce the effort required for code comprehension. The cognitive complexity measure employed in this work is the one defined by SonarSource, which is a company that develops well-known applications for static code analysis. This extraction problem can be modeled as a combinatorial optimization problem. The main difficulty arises from the existence of different criteria for evaluating the solutions obtained, requiring the formulation of the code extraction problem as a multi-objective optimization problem using alternative methods. We propose a multi-objective integer linear programming model to obtain a set of solutions that reduce the cognitive complexity of a given piece of code, such as balancing the number of lines of code and its cognitive complexity. In addition, several algorithms have been developed to validate the model. These algorithms have been integrated into a tool that enables the parameterised resolution of the problem of reducing software cognitive complexity.",
    "title_zh": "基于多目标整数线性规划的自动软件认知复杂度降低方法",
    "abstract_zh": "简洁明了的代码对于确保可维护性至关重要，因此必须尽可能简化软件的可理解性，以避免错误，尤其是安全漏洞。在不改变功能的前提下，有多种方式可以提升软件质量，其中提取方法重构是主要手段之一，有助于降低代码理解所需的努力。本文采用SonarSource定义的认知复杂度度量标准，该公司开发了知名的静态代码分析工具。该代码提取问题可建模为组合优化问题，其主要难点在于存在多种不同的评估标准，因此需将代码提取问题表述为多目标优化问题，并采用不同方法进行求解。为此，我们提出一种多目标整数线性规划模型，旨在获得一组能够降低给定代码段认知复杂度的解决方案，例如在代码行数与认知复杂度之间实现平衡。此外，我们还开发了多种算法以验证该模型的有效性。这些算法已集成到一个工具中，可参数化地求解降低软件认知复杂度的问题。"
  },
  {
    "date": "2026-01-29",
    "title": "KAPSO: A Knowledge-grounded framework for Autonomous Program Synthesis and Optimization",
    "authors": "Alireza Nadaf, Alireza Mohammadshahi, Majid Yazdani",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21526v1",
    "source": "arXiv",
    "abstract": "We introduce KAPSO, a modular framework for autonomous program synthesis and optimization. Given a natural language goal and an evaluation method, KAPSO iteratively performs ideation, code synthesis and editing, execution, evaluation, and learning to improve a runnable artifact toward measurable objectives. Rather than treating synthesis as the endpoint, KAPSO uses synthesis as an operator within a long-horizon optimization loop, where progress is defined by evaluator outcomes. KAPSO targets long-horizon failures common in coding agents, including lost experimental state, brittle debugging, and weak reuse of domain expertise, by integrating three tightly coupled components. First, a git-native experimentation engine isolates each attempt as a branch, producing reproducible artifacts and preserving provenance across iterations. Second, a knowledge system ingests heterogeneous sources, including repositories, internal playbooks, and curated external resources such as documentation, scientific papers, and web search results, and organizes them into a structured representation that supports retrieval over workflows, implementations, and environment constraints. Third, a cognitive memory layer coordinates retrieval and maintains an episodic store of reusable lessons distilled from experiment traces (run logs, diffs, and evaluator feedback), reducing repeated error modes and accelerating convergence. We evaluated KAPSO on MLE-Bench (Kaggle-style ML competitions) and ALE-Bench (AtCoder heuristic optimization), and report end-to-end performance. Code Available at: https://github.com/Leeroo-AI/kapso",
    "title_zh": "KAPSO：一种基于知识的自主程序合成与优化框架",
    "abstract_zh": "我们介绍了KAPSO，这是一个用于自主程序合成与优化的模块化框架。给定自然语言目标和评估方法，KAPSO通过迭代执行构思、代码生成与编辑、执行、评估以及学习等步骤，持续改进可运行的程序成果，以达成可量化的目标。与将合成视为终点的传统方法不同，KAPSO将合成作为长期优化循环中的一个操作符，其进展由评估结果来定义。为应对编码智能体中常见的长周期失败问题——如实验状态丢失、脆弱的调试机制以及领域知识复用能力弱——KAPSO集成了三个紧密耦合的核心组件。\n\n首先，一个原生支持Git的实验引擎将每次尝试隔离为独立分支，从而生成可复现的产物，并在迭代过程中保持完整的溯源记录。其次，一个知识系统能够整合异构信息源，包括代码仓库、内部操作手册以及经过筛选的外部资源（如文档、科研论文和网络搜索结果），并将这些信息组织成结构化表示，支持对工作流、实现方式及环境约束的高效检索。第三，认知记忆层负责协调信息检索，并维护一个情景式存储库，其中包含从实验轨迹（运行日志、代码差异和评估反馈）中提炼出的可复用经验教训，有效减少重复错误模式，加速收敛过程。\n\n我们在MLE-Bench（类似Kaggle风格的机器学习竞赛）和ALE-Bench（AtCoder启发式优化任务）上对KAPSO进行了评估，并报告了端到端的表现结果。代码已开源：https://github.com/Leeroo-AI/kapso"
  },
  {
    "date": "2026-01-29",
    "title": "NEMO: Execution-Aware Optimization Modeling via Autonomous Coding Agents",
    "authors": "Yang Song, Anoushka Vyas, Zirui Wei, Sina Khoshfetrat Pakazad, Henrik Ohlsson, Graham Neubig",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21372v1",
    "source": "arXiv",
    "abstract": "In this paper, we present NEMO, a system that translates Natural-language descriptions of decision problems into formal Executable Mathematical Optimization implementations, operating collaboratively with users or autonomously. Existing approaches typically rely on specialized large language models (LLMs) or bespoke, task-specific agents. Such methods are often brittle, complex and frequently generating syntactically invalid or non-executable code. NEMO instead centers on remote interaction with autonomous coding agents (ACAs), treated as a first-class abstraction analogous to API-based interaction with LLMs. This design enables the construction of higher-level systems around ACAs that structure, consolidate, and iteratively refine task specifications. Because ACAs execute within sandboxed environments, code produced by NEMO is executable by construction, allowing automated validation and repair. Building on this, we introduce novel coordination patterns with and across ACAs, including asymmetric validation loops between independently generated optimizer and simulator implementations (serving as a high-level validation mechanism), external memory for experience reuse, and robustness enhancements via minimum Bayes risk (MBR) decoding and self-consistency. We evaluate NEMO on nine established optimization benchmarks. As depicted in Figure 1, it achieves state-of-the-art performance on the majority of tasks, with substantial margins on several datasets, demonstrating the power of execution-aware agentic architectures for automated optimization modeling.",
    "title_zh": "NEMO：通过自主编码代理实现执行感知的优化建模",
    "abstract_zh": "本文提出NEMO，一个将自然语言描述的决策问题转化为形式化可执行数学优化实现的系统，能够与用户协同工作或自主运行。现有方法通常依赖于专用的大规模语言模型（LLMs）或特定任务的定制化智能体。这类方法往往脆弱、复杂，并经常生成语法错误或不可执行的代码。相比之下，NEMO的核心设计是通过远程交互方式调用自主编码智能体（ACAs），将其作为一类首等抽象，类似于基于API与LLMs的交互。这一设计使得围绕ACAs构建更高层次的系统成为可能，这些系统能够对任务规范进行结构化处理、整合并迭代优化。由于ACAs在沙盒环境中执行，NEMO生成的代码从构造上就是可执行的，从而支持自动验证与修复。在此基础上，我们引入了全新的ACAs内部及跨ACAs的协调模式，包括独立生成的优化器与模拟器实现之间的非对称验证循环（作为高层级验证机制）、用于经验复用的外部记忆，以及通过最小贝叶斯风险（MBR）解码和自洽性增强来提升系统的鲁棒性。我们在九个已确立的优化基准测试上评估了NEMO的表现。如图1所示，NEMO在多数任务中达到了当前最优水平，且在多个数据集上展现出显著优势，充分证明了具备执行感知能力的代理架构在自动化优化建模中的强大潜力。"
  },
  {
    "date": "2026-01-29",
    "title": "READY: Reward Discovery for Meta-Black-Box Optimization",
    "authors": "Zechuan Huang, Zhiguang Cao, Hongshu Guo, Yue-Jiao Gong, Zeyuan Ma",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21847v1",
    "source": "arXiv",
    "abstract": "Meta-Black-Box Optimization (MetaBBO) is an emerging avenue within Optimization community, where algorithm design policy could be meta-learned by reinforcement learning to enhance optimization performance. So far, the reward functions in existing MetaBBO works are designed by human experts, introducing certain design bias and risks of reward hacking. In this paper, we use Large Language Model~(LLM) as an automated reward discovery tool for MetaBBO. Specifically, we consider both effectiveness and efficiency sides. On effectiveness side, we borrow the idea of evolution of heuristics, introducing tailored evolution paradigm in the iterative LLM-based program search process, which ensures continuous improvement. On efficiency side, we additionally introduce multi-task evolution architecture to support parallel reward discovery for diverse MetaBBO approaches. Such parallel process also benefits from knowledge sharing across tasks to accelerate convergence. Empirical results demonstrate that the reward functions discovered by our approach could be helpful for boosting existing MetaBBO works, underscoring the importance of reward design in MetaBBO. We provide READY's project at https://anonymous.4open.science/r/ICML_READY-747F.",
    "title_zh": "准备就绪：面向元黑箱优化的奖励发现",
    "abstract_zh": "元黑箱优化（Meta-Black-Box Optimization, MetaBBO）是优化领域中一个新兴的研究方向，其核心思想是通过强化学习来元学习算法设计策略，从而提升优化性能。迄今为止，现有MetaBBO研究中的奖励函数均由人类专家手动设计，这不可避免地引入了设计偏见以及奖励欺骗（reward hacking）的风险。本文提出将大型语言模型（Large Language Model, LLM）作为自动化奖励发现工具应用于MetaBBO。具体而言，我们从有效性和效率两个方面进行改进：在有效性方面，借鉴启发式方法的演化思想，在基于LLM的程序迭代搜索过程中引入定制化的演化范式，确保优化过程的持续改进；在效率方面，进一步引入多任务演化架构，支持对多种MetaBBO方法并行开展奖励发现，同时各任务间通过知识共享机制实现加速收敛。实验结果表明，本方法所发现的奖励函数能够有效提升现有MetaBBO方法的性能，凸显了奖励设计在MetaBBO中的关键作用。相关项目代码已公开于 https://anonymous.4open.science/r/ICML_READY-747F。"
  },
  {
    "date": "2026-01-29",
    "title": "Optimizing Agentic Workflows using Meta-tools",
    "authors": "Sami Abuzakuk, Anne-Marie Kermarrec, Rishi Sharma, Rasmus Moorits Veski, Martijn de Vos",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.22037v1",
    "source": "arXiv",
    "abstract": "Agentic AI enables LLM to dynamically reason, plan, and interact with tools to solve complex tasks. However, agentic workflows often require many iterative reasoning steps and tool invocations, leading to significant operational expense, end-to-end latency and failures due to hallucinations. This work introduces Agent Workflow Optimization (AWO), a framework that identifies and optimizes redundant tool execution patterns to improve the efficiency and robustness of agentic workflows. AWO analyzes existing workflow traces to discover recurring sequences of tool calls and transforms them into meta-tools, which are deterministic, composite tools that bundle multiple agent actions into a single invocation. Meta-tools bypass unnecessary intermediate LLM reasoning steps and reduce operational cost while also shortening execution paths, leading to fewer failures. Experiments on two agentic AI benchmarks show that AWO reduces the number of LLM calls up to 11.9% while also increasing the task success rate by up to 4.2 percent points.",
    "title_zh": "使用元工具优化智能体工作流程",
    "abstract_zh": "代理型AI使大型语言模型（LLM）能够动态地进行推理、规划并调用工具以解决复杂任务。然而，代理型工作流通常需要大量迭代的推理步骤和工具调用，导致显著的操作成本增加、端到端延迟升高，并因幻觉问题引发失败。本文提出了一种名为代理工作流优化（Agent Workflow Optimization, AWO）的框架，通过识别并优化冗余的工具执行模式，提升代理型工作流的效率与鲁棒性。AWO分析现有的工作流轨迹，发现重复出现的工具调用序列，并将其转化为“元工具”——一种确定性的复合工具，可将多个代理操作打包为单一调用。元工具跳过了不必要的中间LLM推理步骤，降低了操作成本，同时缩短了执行路径，从而减少失败概率。在两个代理型AI基准测试上的实验表明，AWO最多可将LLM调用次数减少11.9%，同时任务成功率最高提升4.2个百分点。"
  },
  {
    "date": "2026-01-29",
    "title": "Task-Awareness Improves LLM Generations and Uncertainty",
    "authors": "Tim Tomov, Dominik Fuchsgruber, Stephan Günnemann",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21500v1",
    "source": "arXiv",
    "abstract": "In many applications of LLMs, natural language responses often have an underlying structure such as representing discrete labels, numerical values, or graphs. Yet, existing decoding and uncertainty estimation methods operate only in language space and largely disregard structural information. We address this by modeling LLM outputs directly in a task-dependent latent structure. By equipping this structure with a dissimilarity measure, we can compute Bayes-optimal responses. These are not selected from sampled generations but are newly synthesized by combining individual responses in the latent space. Across different tasks, Bayes-optimal responses consistently outperform standard decoding methods like beam search. Moreover, quantifying uncertainty via the induced Bayesian risk captures variations in terms of the latent structure and improves alignment with output quality and correctness. Our decision-theoretic framework is applicable to any problem that admits a latent response structure and enables reliable task-aware LLM predictions.",
    "title_zh": "任务感知提升大语言模型的生成效果与不确定性",
    "abstract_zh": "在许多大语言模型（LLM）的应用中，自然语言响应通常具有潜在的结构，例如表示离散标签、数值或图结构。然而，现有的解码和不确定性估计方法仅在语言空间中操作，很大程度上忽略了结构信息。为解决这一问题，我们直接在任务相关的潜在结构空间中建模LLM的输出。通过为该结构赋予一个差异性度量，我们可以计算出贝叶斯最优响应。这些响应并非从采样生成中选取，而是通过在潜在空间中组合各个响应新合成得到。在不同任务中，贝叶斯最优响应始终优于传统的解码方法（如束搜索）。此外，通过所诱导的贝叶斯风险来量化不确定性，能够捕捉潜在结构中的变化，并提升与输出质量及正确性的对齐程度。我们的决策理论框架适用于任何具有潜在响应结构的问题，能够实现可靠的任务感知型LLM预测。"
  },
  {
    "date": "2026-01-29",
    "title": "SOUP: Token-level Single-sample Mix-policy Reinforcement Learning for Large Language Models",
    "authors": "Lei Yang, Wei Bi, Chenxi Sun, Renren Jin, Deyi Xiong",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21476v1",
    "source": "arXiv",
    "abstract": "On-policy reinforcement learning (RL) methods widely used for language model post-training, like Group Relative Policy Optimization (GRPO), often suffer from limited exploration and early saturation due to low sampling diversity. While off-policy data can help, current approaches that mix entire trajectories cause significant policy mismatch and instability. In this work, we propose the $\\textbf{S}$ingle-sample Mix-p$\\textbf{O}$licy $\\textbf{U}$nified $\\textbf{P}$aradigm (SOUP), a framework that unifies off- and on-policy learning within individual samples at the token level. It confines off-policy influence to the prefix of a generated sequence sampled from historical policies, while the continuation is generated on-policy. Through token-level importance ratios, SOUP effectively leverages off-policy information while preserving training stability. Extensive experiments demonstrate that SOUP consistently outperforms standard on-policy training and existing off-policy extensions. Our further analysis clarifies how our fine-grained, single-sample mix-policy training can improve both exploration and final performance in LLM RL.",
    "title_zh": "SOUP：面向大语言模型的基于标记级别的单样本混合策略强化学习",
    "abstract_zh": "在语言模型后训练中广泛使用的基于策略的强化学习（RL）方法，如组相对策略优化（GRPO），常常因采样多样性不足而面临探索能力有限和早期饱和的问题。虽然使用离策略数据可以缓解这一问题，但现有方法通过混合完整轨迹的方式引入离策略数据，往往导致显著的策略不匹配和训练不稳定。在本工作中，我们提出了$\\textbf{S}$ingle-sample Mix-p$\\textbf{O}$licy $\\textbf{U}$nified $\\textbf{P}$aradigm（SOUP）框架，该框架在token级别上统一了离策略与在策略学习。SOUP将离策略的影响限制在从历史策略采样得到的生成序列前缀部分，而后续内容则由在策略方式生成。通过token级别的重要性权重比率，SOUP能够有效利用离策略信息，同时保持训练的稳定性。大量实验表明，SOUP在性能上持续优于标准的在策略训练以及现有的离策略扩展方法。进一步的分析揭示了我们这种细粒度、单样本混合策略训练如何在大语言模型强化学习中同时提升探索能力与最终表现。"
  },
  {
    "date": "2026-01-29",
    "title": "ChipBench: A Next-Step Benchmark for Evaluating LLM Performance in AI-Aided Chip Design",
    "authors": "Zhongkai Yu, Chenyang Zhou, Yichen Lin, Hejia Zhang, Haotian Ye, Junxia Cui, Zaifeng Pan, Jishen Zhao, Yufei Ding",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21448v1",
    "source": "arXiv",
    "abstract": "While Large Language Models (LLMs) show significant potential in hardware engineering, current benchmarks suffer from saturation and limited task diversity, failing to reflect LLMs' performance in real industrial workflows. To address this gap, we propose a comprehensive benchmark for AI-aided chip design that rigorously evaluates LLMs across three critical tasks: Verilog generation, debugging, and reference model generation. Our benchmark features 44 realistic modules with complex hierarchical structures, 89 systematic debugging cases, and 132 reference model samples across Python, SystemC, and CXXRTL. Evaluation results reveal substantial performance gaps, with state-of-the-art Claude-4.5-opus achieving only 30.74\\% on Verilog generation and 13.33\\% on Python reference model generation, demonstrating significant challenges compared to existing saturated benchmarks where SOTA models achieve over 95\\% pass rates. Additionally, to help enhance LLM reference model generation, we provide an automated toolbox for high-quality training data generation, facilitating future research in this underexplored domain. Our code is available at https://github.com/zhongkaiyu/ChipBench.git.",
    "title_zh": "ChipBench：面向AI辅助芯片设计的大型语言模型性能评估的下一步基准",
    "abstract_zh": "尽管大型语言模型（LLMs）在硬件工程领域展现出巨大潜力，但当前的基准测试存在性能饱和和任务多样性不足的问题，难以真实反映 LLM 在工业实际工作流中的表现。为弥补这一差距，我们提出一个全面的AI辅助芯片设计基准，系统评估 LLM 在三个关键任务上的能力：Verilog 生成、调试以及参考模型生成。该基准包含44个具有复杂层次结构的真实模块、89个系统性调试案例，以及涵盖 Python、SystemC 和 CXXRTL 的132个参考模型样本。评估结果揭示了显著的性能差距：当前最先进的 Claude-4.5-opus 模型在 Verilog 生成任务上仅达到 30.74% 的准确率，在 Python 参考模型生成任务上仅为 13.33%，远低于现有饱和基准中 SOTA 模型超过 95% 的通过率，凸显了该领域面临的严峻挑战。此外，为提升 LLM 在参考模型生成方面的能力，我们还提供了一个自动化工具箱，用于生成高质量的训练数据，以推动这一尚处于探索阶段的研究方向的发展。我们的代码已开源，地址为：https://github.com/zhongkaiyu/ChipBench.git。"
  },
  {
    "date": "2026-01-29",
    "title": "White-Box Op-Amp Design via Human-Mimicking Reasoning",
    "authors": "Zihao Chen, Jiayin Wang, Ziyi Sun, Ji Zhuang, Jinyi Shen, Xiaoyue Ke, Li Shang, Xuan Zeng, Fan Yang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21321v1",
    "source": "arXiv",
    "abstract": "This brief proposes \\emph{White-Op}, an interpretable operational amplifier (op-amp) parameter design framework based on the human-mimicking reasoning of large-language-model agents. We formalize the implicit human reasoning mechanism into explicit steps of \\emph{\\textbf{introducing hypothetical constraints}}, and develop an iterative, human-like \\emph{\\textbf{hypothesis-verification-decision}} workflow. Specifically, the agent is guided to introduce hypothetical constraints to derive and properly regulate positions of symbolically tractable poles and zeros, thus formulating a closed-form mathematical optimization problem, which is then solved programmatically and verified via simulation. Theory-simulation result analysis guides the decision-making for refinement. Experiments on 9 op-amp topologies show that, unlike the uninterpretable black-box baseline which finally fails in 5 topologies, White-Op achieves reliable, interpretable behavioral-level designs with only 8.52\\% theoretical prediction error and the design functionality retains after transistor-level mapping for all topologies. White-Op is open-sourced at \\textcolor{blue}{https://github.com/zhchenfdu/whiteop}.",
    "title_zh": "通过模拟人类思维的白盒运算放大器设计",
    "abstract_zh": "本文提出了一种名为 \\emph{White-Op} 的可解释运算放大器（op-amp）参数设计框架，该框架基于大语言模型智能体对人类思维的模拟推理。我们将隐含的人类推理机制形式化为明确的步骤——\\emph{\\textbf{引入假设约束}}，并构建了一种迭代、类人化的 \\emph{\\textbf{假设-验证-决策}} 工作流程。具体而言，智能体被引导引入假设约束，以推导并合理调控符号可处理的极点与零点位置，从而构建一个闭式数学优化问题，随后通过程序化求解，并借助仿真进行验证。理论与仿真结果的分析共同指导后续决策以实现设计优化。在9种运算放大器拓扑结构上的实验表明，与最终在5种拓扑中失败的不可解释黑箱基线方法相比，White-Op 在所有拓扑上均实现了可靠且可解释的行为级设计，理论预测误差仅为8.52\\%，且在晶体管级映射后仍保持设计功能完整性。White-Op 已开源，地址为 \\textcolor{blue}{https://github.com/zhchenfdu/whiteop}。"
  },
  {
    "date": "2026-01-29",
    "title": "Reinforcement Learning from Meta-Evaluation: Aligning Language Models Without Ground-Truth Labels",
    "authors": "Micah Rentschler, Jesse Roberts",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21268v1",
    "source": "arXiv",
    "abstract": "Most reinforcement learning (RL) methods for training large language models (LLMs) require ground-truth labels or task-specific verifiers, limiting scalability when correctness is ambiguous or expensive to obtain. We introduce Reinforcement Learning from Meta-Evaluation (RLME), which optimizes a generator using reward derived from an evaluator's answers to natural-language meta-questions (e.g., \"Is the answer correct?\" or \"Is the reasoning logically consistent?\"). RLME treats the evaluator's probability of a positive judgment as a reward and updates the generator via group-relative policy optimization, enabling learning without labels. Across a suite of experiments, we show that RLME achieves accuracy and sample efficiency comparable to label-based training, enables controllable trade-offs among multiple objectives, steers models toward reliable reasoning patterns rather than post-hoc rationalization, and generalizes to open-domain settings where ground-truth labels are unavailable, broadening the domains in which LLMs may be trained with RL.",
    "title_zh": "基于元评估的强化学习：无需真实标签即可对齐语言模型",
    "abstract_zh": "大多数用于训练大语言模型（LLMs）的强化学习（RL）方法需要真实标签或任务特定的验证器，这在正确性难以判断或获取成本高昂时限制了其可扩展性。我们提出了基于元评估的强化学习（RLME），该方法通过评估者对自然语言元问题（如“答案是否正确？”或“推理是否逻辑一致？”）的回答来生成奖励信号，从而优化生成器。RLME将评估者给出正面判断的概率作为奖励，并采用组相对策略优化方法更新生成器，实现了无需标签的学习。在一系列实验中，我们证明RLME在准确性和样本效率方面可与基于标签的训练相媲美，能够实现多目标间的可控权衡，引导模型形成可靠的推理模式而非事后合理化，且能推广至缺乏真实标签的开放域场景，从而拓展了大语言模型使用强化学习进行训练的应用领域。"
  },
  {
    "date": "2026-01-29",
    "title": "Temporal Guidance for Large Language Models",
    "authors": "Hong-Kai Zheng, Piji Li",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21744v1",
    "source": "arXiv",
    "abstract": "Contrastive Decoding (CD) enhances the generation quality of large language models (LLMs) but incurs significant additional computational overhead due to the need for an auxiliary model. Existing internal self-contrastive decoding methods, such as Decoding by Contrasting Layers (DoLa), focus on discrepancies across different layers, which are notably unstable on small-scale models. In this work, based on the observation that LLMs exhibit local preferences, we propose a novel contrastive guidance strategy along the temporal dimension, namely Temporal Guidance (TeGu). Our method ingeniously leverages Multi-Token Prediction (MTP) to construct weaker amateur predictions for model self-contrast. To standardize the implementation of this mechanism, we further introduce a lightweight Conditional MTP Projector (cMTPP), which avoids maintaining multiple independent networks as required by other MTP modules. Across various model series and benchmarks, TeGu achieves significant performance improvements while maintaining low additional memory consumption and computational overhead.",
    "title_zh": "大型语言模型的时间引导",
    "abstract_zh": "对比解码（Contrastive Decoding, CD）能够提升大语言模型（LLMs）的生成质量，但由于需要额外的辅助模型，带来了显著的计算开销。现有的内部自对比解码方法，如通过对比不同层的差异实现的“Decoding by Contrasting Layers”（DoLa），在小规模模型上表现不稳定，其层间差异难以有效利用。本文基于大语言模型存在局部偏好这一观察，提出一种沿时间维度进行对比的新型引导策略——时间引导（Temporal Guidance, TeGu）。该方法巧妙地利用多标记预测（Multi-Token Prediction, MTP）构建较弱的“业余”预测结果，以实现模型自身的对比学习。为进一步规范该机制的实现，我们还引入了一种轻量级的条件MTP投影器（cMTPP），避免了其他MTP模块所需的多个独立网络结构。在多种模型系列和基准测试中，TeGu均实现了显著的性能提升，同时保持了极低的额外内存占用和计算开销。"
  },
  {
    "date": "2026-01-29",
    "title": "More Bang for the Buck: Improving the Inference of Large Language Models at a Fixed Budget using Reset and Discard (ReD)",
    "authors": "Sagi Meir, Tommer D. Keidar, Noam Levi, Shlomi Reuveni, Barak Hirshberg",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21522v1",
    "source": "arXiv",
    "abstract": "The performance of large language models (LLMs) on verifiable tasks is usually measured by pass@k, the probability of answering a question correctly at least once in k trials. At a fixed budget, a more suitable metric is coverage@cost, the average number of unique questions answered as a function of the total number of attempts. We connect the two metrics and show that the empirically-observed power-law behavior in pass@k leads to a sublinear growth of the coverage@cost (diminishing returns). To solve this problem, we propose Reset-and-Discard (ReD), a query method of LLMs that increases coverage@cost for any given budget, regardless of the pass@k form. Moreover, given a pass@k, we can quantitatively predict the savings in the total number of attempts using ReD. If pass@k is not available for the model, ReD can infer its power-law exponent. Experiments on three LLMs using HumanEval demonstrate that ReD substantially reduces the required attempts, tokens, and USD cost to reach a desired coverage, while also offering an efficient way to measure inference power-laws.",
    "title_zh": "物超所值：通过“重置与丢弃”（ReD）在固定预算下提升大语言模型的推理性能",
    "abstract_zh": "大型语言模型（LLMs）在可验证任务上的表现通常通过 pass@k 来衡量，即在 k 次尝试中至少有一次正确回答问题的概率。在固定预算条件下，更合适的度量指标是 coverage@cost，即作为总尝试次数函数的唯一问题回答数量的平均值。我们建立了这两个指标之间的联系，并指出 pass@k 中观察到的经验幂律行为会导致 coverage@cost 呈次线性增长（收益递减）。为解决这一问题，我们提出了 Reset-and-Discard（ReD）——一种适用于 LLM 的查询方法，能够在任意给定预算下提升 coverage@cost，且不依赖于 pass@k 的具体形式。此外，若已知 pass@k，我们可定量预测使用 ReD 所能节省的总尝试次数。当模型缺乏 pass@k 数据时，ReD 还能推断其幂律指数。在 HumanEval 数据集上对三种 LLM 的实验表明，ReD 显著减少了达到目标覆盖率所需的尝试次数、token 数量以及美元成本，同时提供了一种高效测量推理幂律的方法。"
  },
  {
    "date": "2026-01-29",
    "title": "Late Breaking Results: Conversion of Neural Networks into Logic Flows for Edge Computing",
    "authors": "Daniel Stein, Shaoyi Huang, Rolf Drechsler, Bing Li, Grace Li Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.22151v1",
    "source": "arXiv",
    "abstract": "Neural networks have been successfully applied in various resource-constrained edge devices, where usually central processing units (CPUs) instead of graphics processing units exist due to limited power availability. State-of-the-art research still focuses on efficiently executing enormous numbers of multiply-accumulate (MAC) operations. However, CPUs themselves are not good at executing such mathematical operations on a large scale, since they are more suited to execute control flow logic, i.e., computer algorithms. To enhance the computation efficiency of neural networks on CPUs, in this paper, we propose to convert them into logic flows for execution. Specifically, neural networks are first converted into equivalent decision trees, from which decision paths with constant leaves are then selected and compressed into logic flows. Such logic flows consist of if and else structures and a reduced number of MAC operations. Experimental results demonstrate that the latency can be reduced by up to 14.9 % on a simulated RISC-V CPU without any accuracy degradation. The code is open source at https://github.com/TUDa-HWAI/NN2Logic",
    "title_zh": "最新进展：将神经网络转换为适用于边缘计算的逻辑流",
    "abstract_zh": "神经网络已成功应用于各种资源受限的边缘设备中，由于功耗有限，这些设备通常配备的是中央处理器（CPU）而非图形处理器（GPU）。当前最先进的研究仍聚焦于高效执行大量乘加（MAC）操作。然而，CPU本身并不擅长大规模执行此类数学运算，因为其更适用于执行控制流逻辑，即计算机算法。为了提升神经网络在CPU上的计算效率，本文提出将神经网络转换为逻辑流程进行执行。具体而言，首先将神经网络转化为等价的决策树，然后从中选取具有常数叶节点的决策路径，并将其压缩为逻辑流程。这种逻辑流程由if和else结构组成，同时大幅减少了MAC操作的数量。实验结果表明，在模拟的RISC-V CPU上，延迟可降低高达14.9%，且未造成任何精度损失。代码已在 https://github.com/TUDa-HWAI/NN2Logic 开源。"
  },
  {
    "date": "2026-01-29",
    "title": "Causal Autoregressive Diffusion Language Model",
    "authors": "Junhao Ruan, Bei Li, Yongjing Yin, Pengcheng Huang, Xin Chen, Jingang Wang, Xunliang Cai, Tong Xiao, JingBo Zhu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.22031v1",
    "source": "arXiv",
    "abstract": "In this work, we propose Causal Autoregressive Diffusion (CARD), a novel framework that unifies the training efficiency of ARMs with the high-throughput inference of diffusion models. CARD reformulates the diffusion process within a strictly causal attention mask, enabling dense, per-token supervision in a single forward pass. To address the optimization instability of causal diffusion, we introduce a soft-tailed masking schema to preserve local context and a context-aware reweighting mechanism derived from signal-to-noise principles. This design enables dynamic parallel decoding, where the model leverages KV-caching to adaptively generate variable-length token sequences based on confidence. Empirically, CARD outperforms existing discrete diffusion baselines while reducing training latency by 3 $\\times$ compared to block diffusion methods. Our results demonstrate that CARD achieves ARM-level data efficiency while unlocking the latency benefits of parallel generation, establishing a robust paradigm for next-generation efficient LLMs.",
    "title_zh": "因果自回归扩散语言模型",
    "abstract_zh": "在本工作中，我们提出了因果自回归扩散模型（Causal Autoregressive Diffusion, CARD），这是一种新颖的框架，将自回归模型（ARM）的训练效率与扩散模型的高吞吐推理能力相结合。CARD通过引入严格因果注意力掩码重新定义了扩散过程，使得在单次前向传播中即可实现密集的逐标记监督。为解决因果扩散带来的优化不稳定性问题，我们提出了一种软尾部掩码机制，以保留局部上下文信息，并设计了一种基于信噪比原理的上下文感知重加权机制。该设计支持动态并行解码，模型利用键值缓存（KV-caching）根据置信度自适应生成可变长度的标记序列。实验结果表明，CARD在性能上超越现有离散扩散基线方法，同时相比块扩散方法将训练延迟降低了3倍。我们的研究证明，CARD在保持自回归模型级别的数据效率的同时，实现了并行生成的延迟优势，为下一代高效大语言模型建立了一个稳健的范式。"
  },
  {
    "date": "2026-01-29",
    "title": "astra-langchain4j: Experiences Combining LLMs and Agent Programming",
    "authors": "Rem Collier, Katharine Beaumont, Andrei Ciortea",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21879v1",
    "source": "arXiv",
    "abstract": "Given the emergence of Generative AI over the last two years and the increasing focus on Agentic AI as a form of Multi-Agent System it is important to explore both how such technologies can impact the use of traditional Agent Toolkits and how the wealth of experience encapsulated in those toolkits can influence the design of the new agentic platforms. This paper presents an overview of our experience developing a prototype large language model (LLM) integration for the ASTRA programming language. It presents a brief overview of the toolkit, followed by three example implementations, concluding with a discussion of the experiences garnered through the examples.",
    "title_zh": "astra-langchain4j：结合大语言模型与代理编程的实践经验",
    "abstract_zh": "在过去两年中，生成式人工智能（Generative AI）的兴起，以及以多智能体系统形式出现的代理型人工智能（Agentic AI）日益受到关注，使得我们有必要探讨这些新技术如何影响传统智能体工具包的使用，同时也应思考传统工具包中所蕴含的丰富经验，如何能够指导新型代理平台的设计。本文概述了我们在开发ASTRA编程语言的大型语言模型（LLM）集成原型过程中的经验。文章首先简要介绍了该工具包，随后展示了三个示例实现，最后总结了通过这些实例所获得的经验与见解。"
  },
  {
    "date": "2026-01-29",
    "title": "RAG-E: Quantifying Retriever-Generator Alignment and Failure Modes",
    "authors": "Korbinian Randl, Guido Rocchietti, Aron Henriksson, Ziawasch Abedjan, Tony Lindgren, John Pavlopoulos",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21803v1",
    "source": "arXiv",
    "abstract": "Retrieval-Augmented Generation (RAG) systems combine dense retrievers and language models to ground LLM outputs in retrieved documents. However, the opacity of how these components interact creates challenges for deployment in high-stakes domains. We present RAG-E, an end-to-end explainability framework that quantifies retriever-generator alignment through mathematically grounded attribution methods. Our approach adapts Integrated Gradients for retriever analysis, introduces PMCSHAP, a Monte Carlo-stabilized Shapley Value approximation, for generator attribution, and introduces the Weighted Attribution-Relevance Gap (WARG) metric to measure how well a generator's document usage aligns with a retriever's ranking. Empirical analysis on TREC CAsT and FoodSafeSum reveals critical misalignments: for 47.4% to 66.7% of queries, generators ignore the retriever's top-ranked documents, while 48.1% to 65.9% rely on documents ranked as less relevant. These failure modes demonstrate that RAG output quality depends not solely on individual component performance but on their interplay, which can be audited via RAG-E.",
    "title_zh": "RAG-E：量化检索器-生成器对齐与失效模式",
    "abstract_zh": "检索增强生成（RAG）系统通过结合密集型检索器与语言模型，将大语言模型的输出基于所检索到的文档进行约束。然而，这些组件之间相互作用的不透明性，给其在高风险领域中的部署带来了挑战。我们提出了 RAG-E——一个端到端的可解释性框架，该框架通过数学基础坚实的归因方法量化检索器与生成器之间的对齐程度。我们的方法将集成梯度（Integrated Gradients）适配用于检索器分析，提出 PMCSHAP——一种基于蒙特卡洛稳定的 Shapley 值近似方法，用于生成器的归因分析，并引入加权归因-相关性差距（WARG）指标，以衡量生成器对文档的使用是否与其检索器的排名相一致。在 TREC CAsT 和 FoodSafeSum 数据集上的实证分析揭示了显著的错位现象：在 47.4% 至 66.7% 的查询中，生成器忽略了检索器排名最高的文档；同时，有 48.1% 到 65.9% 的生成内容依赖于被检索器判定为相关性较低的文档。这些失败模式表明，RAG 输出质量不仅取决于各组件自身的性能，更取决于它们之间的协同关系，而这种协同关系可通过 RAG-E 进行审计。"
  },
  {
    "date": "2026-01-29",
    "title": "Zonkey: A Hierarchical Diffusion Language Model with Differentiable Tokenization and Probabilistic Attention",
    "authors": "Alon Rozental",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21768v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) have revolutionized natural language processing, yet they remain constrained by fixed, non-differentiable tokenizers like Byte Pair Encoding (BPE), which hinder end-to-end optimization and adaptability to noisy or domain-specific data. We introduce Zonkey, a hierarchical diffusion model that addresses these limitations through a fully trainable pipeline from raw characters to document-level representations. At its core is a differentiable tokenizer (Segment Splitter) that learns probabilistic beginning-of-sequence (BOS) decisions, enabling adaptive splits that emerge as linguistically meaningful (e.g., word boundaries at spaces, sentence starts at periods) without explicit supervision. This differentiability is enabled by our novel Probabilistic Attention mechanism, which incorporates position-specific existence probabilities to simulate soft masking over theoretically infinite sequences while preserving gradients. Sequences decay probabilistically rather than relying on end-of-sequence tokens, supporting variable-length outputs. Hierarchical levels compress sequences into higher abstractions (e.g., character n-grams to word-like vectors, then sentence-like), with reconstruction via our Denoising Diffusion Mixed Model (DDMM) for stable and efficient denoising in latent space. A Stitcher ensures overlap invariance across segments. Trained end-to-end on Wikipedia, Zonkey generates coherent, variable-length text from noise, demonstrating emergent hierarchies and promising qualitative alignment to data distributions compared to entropy-based learnable tokenizers. Our approach advances toward fully gradient-based LLMs, with potential for better domain adaptation and scalable generation. We release the source code for training and reproducing our experiments.",
    "title_zh": "Zonkey：一种具有可微分分词和概率注意力机制的层次化扩散语言模型",
    "abstract_zh": "大型语言模型（LLMs）已彻底改变了自然语言处理领域，但其仍受限于固定且不可微分的分词器（如字节对编码，BPE），这阻碍了端到端优化，并限制了在噪声数据或特定领域数据上的适应能力。我们提出了 Zonkey，一种层次化的扩散模型，通过从原始字符到文档级表示的全可训练流程，解决了这些局限性。其核心是一个可微分的分词器（段落分割器），能够学习概率性的序列起始（BOS）决策，从而自适应地生成具有语言学意义的切分结果（例如，在空格处识别词边界，在句号处识别句子起始），而无需显式监督。这种可微性得益于我们提出的新型概率注意力机制，该机制引入位置相关的存在概率，以模拟理论上无限序列的软掩码，同时保持梯度传播。序列通过概率衰减而非依赖结束符（EOS）来终止，支持可变长度输出。多层次结构将序列逐步压缩为更高层次的抽象表示（如从字符n-gram映射到类词向量，再进一步映射到类句向量），并通过我们提出的去噪扩散混合模型（DDMM）实现稳定高效的潜在空间去噪。此外，Stitcher 模块确保了不同片段间重叠不变性。Zonkey 在 Wikipedia 数据上进行端到端训练，能够从噪声中生成连贯、可变长度的文本，展现出涌现的层次结构，并在定性上表现出与数据分布更优的对齐效果，优于基于熵的可学习分词器。我们的方法推动了完全基于梯度的 LLM 发展，具备更强的领域适应性和可扩展生成潜力。我们已开源训练及实验复现的源代码。"
  },
  {
    "date": "2026-01-29",
    "title": "Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic",
    "authors": "Shuo Liu, Tianle Chen, Ryan Amiri, Christopher Amato",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21972v1",
    "source": "arXiv",
    "abstract": "Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \\textbf{CoLLM-CC} with a \\textbf{C}entralized \\textbf{C}ritic and \\textbf{CoLLM-DC} with \\textbf{D}ecentralized \\textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.",
    "title_zh": "基于多智能体演员-评论家的去中心化大模型协作学习",
    "abstract_zh": "近期的研究探索了通过多智能体强化学习（MARL）优化大语言模型（LLM）协作的方法。然而，大多数MARL微调方法依赖于预定义的执行协议，这些协议通常需要集中式执行。在实际应用中，去中心化LLM协作更具吸引力，因为智能体可以并行运行推理，并支持灵活的部署方式。此外，当前的方法普遍采用蒙特卡洛（Monte Carlo）方法进行微调，这类方法存在方差过高的问题，因此需要更多的样本才能有效训练。相比之下，演员-评论家（actor-critic）方法在MARL中更为常见，能够有效缓解上述问题。为此，我们提出了多智能体演员-评论家（MAAC）方法，以优化去中心化LLM协作。本文分析了MAAC方法在何种情况下以及为何具有优势。我们提出了两种MAAC方法：\\textbf{CoLLM-CC}（采用集中式评论家）和 \\textbf{CoLLM-DC}（采用去中心式评论家）。我们在写作、编程和游戏等不同领域进行了实验，结果表明，在短时程且奖励密集的任务场景下，蒙特卡洛方法与CoLLM-DC的表现可与CoLLM-CC相媲美；但在长时程或稀疏奖励任务中，两者均表现逊于CoLLM-CC，其中蒙特卡洛方法需要显著更多的样本，而CoLLM-DC则难以收敛。我们的代码已开源，地址为 https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2。"
  },
  {
    "date": "2026-01-29",
    "title": "Just Ask: Curious Code Agents Reveal System Prompts in Frontier LLMs",
    "authors": "Xiang Zheng, Yutao Wu, Hanxun Huang, Yige Li, Xingjun Ma, Bo Li, Yu-Gang Jiang, Cong Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21233v1",
    "source": "arXiv",
    "abstract": "Autonomous code agents built on large language models are reshaping software and AI development through tool use, long-horizon reasoning, and self-directed interaction. However, this autonomy introduces a previously unrecognized security risk: agentic interaction fundamentally expands the LLM attack surface, enabling systematic probing and recovery of hidden system prompts that guide model behavior. We identify system prompt extraction as an emergent vulnerability intrinsic to code agents and present \\textbf{\\textsc{JustAsk}}, a self-evolving framework that autonomously discovers effective extraction strategies through interaction alone. Unlike prior prompt-engineering or dataset-based attacks, \\textsc{JustAsk} requires no handcrafted prompts, labeled supervision, or privileged access beyond standard user interaction. It formulates extraction as an online exploration problem, using Upper Confidence Bound-based strategy selection and a hierarchical skill space spanning atomic probes and high-level orchestration. These skills exploit imperfect system-instruction generalization and inherent tensions between helpfulness and safety. Evaluated on \\textbf{41} black-box commercial models across multiple providers, \\textsc{JustAsk} consistently achieves full or near-complete system prompt recovery, revealing recurring design- and architecture-level vulnerabilities. Our results expose system prompts as a critical yet largely unprotected attack surface in modern agent systems.",
    "title_zh": "只需提问：好奇的代码代理在前沿大模型中揭示系统提示",
    "abstract_zh": "基于大型语言模型构建的自主代码代理，正通过工具使用、长时程推理以及自我驱动的交互方式，深刻重塑软件与人工智能开发。然而，这种自主性引入了一种此前未被识别的安全风险：代理间的交互从根本上扩大了大语言模型的攻击面，使得系统能够进行系统性的探测，并恢复引导模型行为的隐藏系统提示。我们识别出系统提示提取是代码代理中一种新兴的固有漏洞，并提出 \\textbf{\\textsc{JustAsk}}——一个自我演化的框架，仅通过交互即可自主发现有效的提取策略。与以往依赖手工设计提示或数据集的攻击方法不同，\\textsc{JustAsk} 无需人工构造的提示、标注监督，也无需超出标准用户交互权限的特权访问。它将提示提取建模为一个在线探索问题，采用基于上置信界（Upper Confidence Bound）的策略选择机制，并构建了一个涵盖原子探测到高层编排的分层技能空间。这些技能利用了系统指令泛化不完善的问题，以及“助人”与“安全”之间的内在矛盾。在来自多个供应商的 **41** 个黑盒商业模型上进行评估，\\textsc{JustAsk} 均能持续实现完整或近乎完整的系统提示恢复，揭示出反复出现的设计与架构层面的漏洞。我们的研究结果表明，系统提示已成为现代智能体系统中一个关键却几乎未受保护的攻击面。"
  },
  {
    "date": "2026-01-29",
    "title": "CovAgent: Overcoming the 30% Curse of Mobile Application Coverage with Agentic AI and Dynamic Instrumentation",
    "authors": "Wei Minn, Biniam Fisseha Demissie, Yan Naing Tun, Jiakun Liu, Mariano Ceccato, Lwin Khin Shar, David Lo",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21253v1",
    "source": "arXiv",
    "abstract": "Automated GUI testing is crucial for ensuring the quality and reliability of Android apps. However, the efficacy of existing UI testing techniques is often limited, especially in terms of coverage. Recent studies, including the state-of-the-art, struggle to achieve more than 30% activity coverage in real-world apps. This limited coverage can be attributed to a combination of factors such as failing to generate complex user inputs, unsatisfied activation conditions regarding device configurations and external resources, and hard-to-reach code paths that are not easily accessible through the GUI. To overcome these limitations, we propose CovAgent, a novel agentic AI-powered approach to enhance Android app UI testing. Our fuzzer-agnostic framework comprises an AI agent that inspects the app's decompiled Smali code and component transition graph, and reasons about unsatisfied activation conditions within the app code logic that prevent access to the activities that are unreachable by standard and widely adopted GUI fuzzers. Then, another agent generates dynamic instrumentation scripts that satisfy activation conditions required for successful transitions to those activities. We found that augmenting existing fuzzing approaches with our framework achieves a significant improvement in test coverage over the state-of-the-art, LLMDroid, and other baselines such as Fastbot and APE (e.g., 101.1%, 116.3% and 179.7% higher activity coverage, respectively). CovAgent also outperforms all the baselines in other metrics such as class, method, and line coverage. We also conduct investigations into components within CovAgent to reveal further insights regarding the efficacy of Agentic AI in the field of automated app testing such as the agentic activation condition inference accuracy, and agentic activity-launching success rate.",
    "title_zh": "CovAgent：利用代理AI与动态插桩技术克服移动应用覆盖率的30%困境",
    "abstract_zh": "自动化GUI测试对于确保Android应用的质量和可靠性至关重要。然而，现有UI测试技术的效率往往受到限制，尤其是在覆盖率方面。近期的研究，包括最先进的方法，在真实应用中也难以实现超过30%的活动（activity）覆盖率。这种覆盖率的局限性可归因于多种因素，例如无法生成复杂的用户输入、设备配置和外部资源相关的激活条件无法满足，以及通过GUI难以访问的代码路径。为克服这些局限，我们提出了CovAgent——一种新颖的基于智能体（agentic）AI的Android应用UI测试增强方法。我们的框架具有fuzzer无关性，包含一个AI智能体，该智能体会分析应用的反编译Smali代码及组件转换图，并推理出应用程序代码逻辑中未满足的激活条件，这些条件阻碍了标准且广泛使用的GUI模糊测试工具对某些不可达活动的访问。随后，另一个智能体生成动态插桩脚本，以满足成功跳转至这些活动所需的激活条件。实验表明，将我们的框架与现有模糊测试方法结合，显著提升了测试覆盖率，相较于最先进的LLMDroid及其他基线方法（如Fastbot和APE），活动覆盖率分别提高了101.1%、116.3%和179.7%。此外，CovAgent在类、方法和行覆盖率等其他指标上也优于所有基线方法。我们还对CovAgent中的各个组件进行了深入探究，进一步揭示了智能体式AI在自动化应用测试领域中的有效性，例如智能体在激活条件推断的准确性，以及活动启动的成功率。"
  },
  {
    "date": "2026-01-29",
    "title": "Human-Agent versus Human Pull Requests: A Testing-Focused Characterization and Comparison",
    "authors": "Roberto Milanese, Francesco Salzano, Angelica Spina, Antonio Vitale, Remo Pareschi, Fausto Fasano, Mattia Fazzini",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21194v1",
    "source": "arXiv",
    "abstract": "AI-based coding agents are increasingly integrated into software development workflows, collaborating with developers to create pull requests (PRs). Despite their growing adoption, the role of human-agent collaboration in software testing remains poorly understood. This paper presents an empirical study of 6,582 human-agent PRs (HAPRs) and 3,122 human PRs (HPRs) from the AIDev dataset. We compare HAPRs and HPRs along three dimensions: (i) testing frequency and extent, (ii) types of testing-related changes (code-and-test co-evolution vs. test-focused), and (iii) testing quality, measured by test smells. Our findings reveal that, although the likelihood of including tests is comparable (42.9% for HAPRs vs. 40.0% for HPRs), HAPRs exhibit a larger extent of testing, nearly doubling the test-to-source line ratio found in HPRs. While test-focused task distributions are comparable, HAPRs are more likely to add new tests during co-evolution (OR=1.79), whereas HPRs prioritize modifying existing tests. Finally, although some test smell categories differ statistically, negligible effect sizes suggest no meaningful differences in quality. These insights provide the first characterization of how human-agent collaboration shapes testing practices.",
    "title_zh": "人机协作与人际协作的Pull Request：一种以测试为重点的特征描述与比较",
    "abstract_zh": "基于人工智能的编码代理正日益融入软件开发工作流程，与开发者协作生成拉取请求（PR）。尽管其应用日益广泛，但人类与代理协作在软件测试中的作用仍缺乏深入理解。本文基于AIDev数据集，对6,582个“人机协同拉取请求”（HAPRs）和3,122个“纯人类拉取请求”（HPRs）进行了实证研究。我们从三个维度对比分析了HAPRs与HPRs：（i）测试活动的频率与广度；（ii）与测试相关的变更类型（代码与测试共同演化 vs. 以测试为中心）；（iii）测试质量，通过测试异味（test smells）进行衡量。研究发现，尽管包含测试的概率相近（HAPRs为42.9%，HPRs为40.0%），但HAPRs的测试覆盖范围显著更大，其测试代码与源代码行数之比接近HPRs的两倍。虽然以测试为中心的任务分布相似，但HAPRs更倾向于在代码与测试共同演进过程中新增测试（OR=1.79），而HPRs则更侧重于修改已有测试。此外，尽管部分测试异味类别存在统计差异，但效应量极小，表明两者在测试质量上并无实质性区别。这些发现首次系统刻画了人机协作如何影响软件测试实践。"
  },
  {
    "date": "2026-01-29",
    "title": "Do Reasoning Models Enhance Embedding Models?",
    "authors": "Wun Yu Chan, Shaojin Chen, Huihao Jing, Kwun Hang Lau, Elton Chun-Chai Li, Zihao Wang, Haoran Li, Yangqiu Song",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21192v1",
    "source": "arXiv",
    "abstract": "State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.",
    "title_zh": "推理模型能否提升嵌入模型的表现？",
    "abstract_zh": "前沿的嵌入模型正越来越多地基于仅解码器架构的大语言模型（LLM）主干，并通过对比学习进行微调。随着基于可验证奖励强化学习（RLVR）训练的推理模型的出现，一个自然的问题随之而来：当这些模型作为嵌入模型的初始化时，其增强的推理能力是否能转化为更优的语义表示？出乎意料的是，我们在MTEB和BRIGHT数据集上的评估结果表明，这种提升并不存在——**无显著效果**：使用RLVR微调后的模型主干作为初始化所构建的嵌入模型，在采用相同训练流程的情况下，并未表现出相对于基础模型的一致性能优势。\n\n为揭示这一悖论，我们提出了**层级表示相似性分析**（Hierarchical Representation Similarity Analysis, HRSA），一种从表示、几何与功能三个层面分解相似性的分析框架。HRSA发现，尽管RLVR导致潜在流形的局部几何结构发生不可逆重构，同时引发可逆的坐标基漂移，但其仍保持了流形的整体几何结构以及线性读出能力。因此，在后续的对比学习过程中，基于基础模型与推理模型初始化的模型之间产生了强烈的对齐现象，我们称之为**流形重校准**（Manifold Realignment）。\n\n实证研究表明，与监督微调（SFT）不同，RLVR并非从根本上重构语义空间本身，而是优化了在已有语义景观内的路径。"
  },
  {
    "date": "2026-01-29",
    "title": "LLaMEA-SAGE: Guiding Automated Algorithm Design with Structural Feedback from Explainable AI",
    "authors": "Niki van Stein, Anna V. Kononova, Lars Kotthoff, Thomas Bäck",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21511v1",
    "source": "arXiv",
    "abstract": "Large language models have enabled automated algorithm design (AAD) by generating optimization algorithms directly from natural-language prompts. While evolutionary frameworks such as LLaMEA demonstrate strong exploratory capabilities across the algorithm design space, their search dynamics are entirely driven by fitness feedback, leaving substantial information about the generated code unused. We propose a mechanism for guiding AAD using feedback constructed from graph-theoretic and complexity features extracted from the abstract syntax trees of the generated algorithms, based on a surrogate model learned over an archive of evaluated solutions. Using explainable AI techniques, we identify features that substantially affect performance and translate them into natural-language mutation instructions that steer subsequent LLM-based code generation without restricting expressivity. We propose LLaMEA-SAGE, which integrates this feature-driven guidance into LLaMEA, and evaluate it across several benchmarks. We show that the proposed structured guidance achieves the same performance faster than vanilla LLaMEA in a small controlled experiment. In a larger-scale experiment using the MA-BBOB suite from the GECCO-MA-BBOB competition, our guided approach achieves superior performance compared to state-of-the-art AAD methods. These results demonstrate that signals derived from code can effectively bias LLM-driven algorithm evolution, bridging the gap between code structure and human-understandable performance feedback in automated algorithm design.",
    "title_zh": "LLaMEA-SAGE：利用可解释人工智能的结构反馈指导自动化算法设计",
    "abstract_zh": "大型语言模型（LLM）通过直接从自然语言提示生成优化算法，推动了自动化算法设计（AAD）的发展。尽管像LLaMEA这样的进化框架在算法设计空间中展现出强大的探索能力，但其搜索过程完全依赖于适应度反馈，导致生成代码中蕴含的大量信息未被充分利用。为此，我们提出一种基于图论和复杂性特征的引导机制，这些特征从生成算法的抽象语法树（AST）中提取，并通过一个在已评估解档案上学习的代理模型构建反馈信号，从而指导AAD。利用可解释人工智能技术，我们识别出对性能有显著影响的关键特征，并将其转化为自然语言形式的变异指令，以引导后续基于LLM的代码生成，同时不限制表达能力。我们提出了LLaMEA-SAGE，将这种特征驱动的引导机制集成到LLaMEA中，并在多个基准测试中进行了评估。实验结果表明，在小规模受控实验中，所提出的结构化引导方法能够以更快速度达到与原始LLaMEA相当的性能；在更大规模的实验中，使用GECCO-MA-BBOB竞赛中的MA-BBOB套件进行测试时，我们的引导方法在性能上优于当前最先进的AAD方法。这些结果表明，代码本身所蕴含的信号可以有效引导基于大语言模型的算法演化，弥合了代码结构与人类可理解的性能反馈之间的差距，为自动化算法设计提供了新的方向。"
  },
  {
    "date": "2026-01-29",
    "title": "Predicting Developer Acceptance of AI-Generated Code Suggestions",
    "authors": "Jing Jiang, Liehao Li, Jinyun Hou, Xin Tan, Li Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21379v1",
    "source": "arXiv",
    "abstract": "AI-assisted programming tools are widely adopted, yet their practical utility is often undermined by undesired suggestions that interrupt developer workflows and cause frustration. While existing research has explored developer-AI interactions when programming qualitatively, a significant gap remains in quantitative analysis of developers' acceptance of AI-generated code suggestions, partly because the necessary fine-grained interaction data is often proprietary. To bridge this gap, this paper conducts an empirical study using 66,329 industrial developer-AI interactions from a large technology company. We analyze features that are significantly different between accepted code suggestions and rejected ones. We find that accepted suggestions are characterized by significantly higher historical acceptance counts and ratios for both developers and projects, longer generation intervals, shorter preceding code context in the project, and older IDE versions. Based on these findings, we introduce CSAP (Code Suggestion Acceptance Prediction) to predict whether a developer will accept the code suggestion before it is displayed. Our evaluation of CSAP shows that it achieves the accuracy of 0.973 and 0.922 on imbalanced and balanced dataset respectively. Compared to a large language model baseline and an in-production industrial filter, CSAP relatively improves the accuracy by 12.6\\% and 69.5\\% on imbalanced dataset, and improves the accuracy by 87.0\\% and 140.1\\% on balanced dataset. Our results demonstrate that targeted personalization is a powerful approach for filtering out code suggestions with predicted rejection and reduce developer interruption. To the best of our knowledge, it is the first quantitative study of code suggestion acceptance on large-scale industrial data, and this work also sheds light on an important research direction of AI-assisted programming.",
    "title_zh": "预测开发者对AI生成代码建议的接受程度",
    "abstract_zh": "AI辅助编程工具已被广泛采用，但其实际效用常因不期望的代码建议而受到干扰，这些建议会打断开发者的开发流程并引发挫败感。尽管现有研究已从定性角度探讨了开发者与AI在编程过程中的交互行为，但在量化分析开发者对AI生成代码建议的接受程度方面仍存在显著空白，部分原因在于所需的细粒度交互数据通常属于企业机密。为填补这一空白，本文基于一家大型科技公司提供的66,329条工业级开发者-AI交互数据，开展了一项实证研究。我们分析了被接受与被拒绝的代码建议之间具有显著差异的特征。研究发现，被接受的建议在开发者和项目层面均表现出更高的历史接受次数与接受比例，更长的生成间隔，项目中较短的前序代码上下文，以及使用较旧版本的IDE。基于上述发现，我们提出了CSAP（Code Suggestion Acceptance Prediction）模型，用于在代码建议显示前预测开发者是否会接受该建议。对CSAP的评估表明，其在非平衡数据集上的准确率达到0.973，在平衡数据集上达到0.922。相较于大型语言模型基线和一个已在生产环境中使用的工业过滤器，CSAP在非平衡数据集上分别提升了12.6%和69.5%的准确率，在平衡数据集上则分别提升了87.0%和140.1%。我们的结果表明，针对性的个性化策略是一种有效方法，可提前筛选出预计会被拒绝的代码建议，从而减少对开发者的干扰。据我们所知，这是首个基于大规模工业数据的代码建议接受度定量研究，同时也为AI辅助编程领域指明了一个重要的研究方向。"
  },
  {
    "date": "2026-01-29",
    "title": "Procedural Pretraining: Warming Up Language Models with Abstract Data",
    "authors": "Liangze Jiang, Zachary Shinnick, Anton van den Hengel, Hemanth Saratchandran, Damien Teney",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21725v1",
    "source": "arXiv",
    "abstract": "Pretraining directly on web-scale corpora is the de facto paradigm for building language models. We study an alternative setting where the model is initially exposed to abstract structured data, as a means to ease the subsequent acquisition of rich semantic knowledge, much like humans learn simple logic and mathematics before higher reasoning. We specifically focus on procedural data, generated by formal languages and other simple algorithms, as such abstract data. We first diagnose the algorithmic skills that different forms of procedural data can improve, often significantly. For example, on context recall (Needle-in-a-haystack), the accuracy jumps from 10 to 98% when pretraining on Dyck sequences (balanced brackets). Second, we study how these gains are reflected in pretraining larger models (up to 1.3B). We find that front-loading as little as 0.1% procedural data significantly outperforms standard pretraining on natural language, code, and informal mathematics (C4, CodeParrot, and DeepMind-Math datasets). Notably, this procedural pretraining enables the models to reach the same loss value with only 55, 67, 86% of the original data. Third, we explore the mechanisms behind and find that procedural pretraining instils non-trivial structure in both attention and MLP layers. The former is particularly important for structured domains (e.g. code), and the latter for language. Finally, we lay a path for combining multiple forms of procedural data. Our results show that procedural pretraining is a simple, lightweight means to improving performance and accelerating language model pretraining, ultimately suggesting the promise of disentangling knowledge acquisition from reasoning in LLMs.",
    "title_zh": "过程式预训练：通过抽象数据唤醒语言模型",
    "abstract_zh": "在大规模网络语料上直接进行预训练，已成为构建语言模型的默认范式。我们研究了一种替代方案：模型最初接触的是抽象的结构化数据，以此作为促进后续获取丰富语义知识的手段，类似于人类在学习高级推理之前先掌握简单的逻辑与数学知识。我们特别关注由形式语言和其他简单算法生成的过程性数据，因为这类数据具有高度抽象性。首先，我们诊断了不同形式的过程性数据对模型算法能力的提升效果，往往能带来显著改善。例如，在上下文回忆任务（“针在 haystack 中”）中，当模型在 Dyck 序列（平衡括号）上进行预训练时，准确率从 10% 提升至 98%。其次，我们研究了这些优势在更大规模模型（最大达 13 亿参数）上的体现。结果发现，仅将 0.1% 的过程性数据前置预训练，其表现就显著优于在自然语言、代码和非正式数学（C4、CodeParrot 和 DeepMind-Math 数据集）上的标准预训练方式。值得注意的是，这种过程性预训练使模型仅需原始数据量的 55%、67% 和 86%，即可达到相同的损失值。第三，我们探索了其背后的机制，发现过程性预训练在注意力层和 MLP 层中都引入了非平凡的结构。其中，注意力层的结构对结构化领域（如代码）尤为重要，而 MLP 层的结构则更有利于自然语言建模。最后，我们提出了整合多种过程性数据的方法路径。我们的研究结果表明，过程性预训练是一种简单且轻量级的有效方法，能够显著提升语言模型性能并加速预训练过程，最终暗示了在大语言模型中将知识获取与推理能力解耦的巨大潜力。"
  },
  {
    "date": "2026-01-29",
    "title": "Beyond Imitation: Reinforcement Learning for Active Latent Planning",
    "authors": "Zhi Zheng, Wee Sun Lee",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21598v1",
    "source": "arXiv",
    "abstract": "Aiming at efficient and dense chain-of-thought (CoT) reasoning, latent reasoning methods fine-tune Large Language Models (LLMs) to substitute discrete language tokens with continuous latent tokens. These methods consume fewer tokens compared to the conventional language CoT reasoning and have the potential to plan in a dense latent space. However, current latent tokens are generally supervised based on imitating language labels. Considering that there can be multiple equivalent but diverse CoT labels for a question, passively imitating an arbitrary one may lead to inferior latent token representations and latent reasoning policies, undermining the potential planning ability and resulting in clear gaps between training and testing. In this work, we emphasize the importance of active planning over the representation space of latent tokens in achieving the optimal latent reasoning policy. So, we propose the \\underline{A}c\\underline{t}ive Latent \\underline{P}lanning method (ATP-Latent), which models the supervision process of latent tokens as a conditional variational auto-encoder (VAE) to obtain a smoother latent space. Moreover, to facilitate the most reasonable latent reasoning policy, ATP-Latent conducts reinforcement learning (RL) with an auxiliary coherence reward, which is calculated based on the consistency between VAE-decoded contents of latent tokens, enabling a guided RL process. In experiments on LLaMA-1B, ATP-Latent demonstrates +4.1\\% accuracy and -3.3\\% tokens on four benchmarks compared to advanced baselines. Codes are available on https://github.com/zz1358m/ATP-Latent-master.",
    "title_zh": "超越模仿：用于主动潜在规划的强化学习",
    "abstract_zh": "针对高效且密集的思维链（Chain-of-Thought, CoT）推理，隐式推理方法通过微调大语言模型（LLMs），用连续的隐变量 token 替代离散的语言 token。与传统的语言 CoT 推理相比，这类方法消耗更少的 token，并具备在密集隐空间中进行规划的潜力。然而，当前的隐式 token 通常基于模仿语言标签进行监督学习。考虑到一个问题可能存在多个等价但多样化的 CoT 标签，被动地模仿任意一个标签可能导致隐式 token 表示和隐式推理策略质量下降，从而削弱其潜在的规划能力，并在训练与测试之间造成明显差距。\n\n在本工作中，我们强调在隐式 token 的表示空间中实现主动规划对于获得最优隐式推理策略的重要性。为此，我们提出了 **A**ctive **T**oken **P**lanning 方法（ATP-Latent）。该方法将隐式 token 的监督过程建模为条件变分自编码器（Conditional VAE），以获得更加平滑的隐空间。此外，为了促进最合理的隐式推理策略，ATP-Latent 引入了基于辅助一致性奖励的强化学习（Reinforcement Learning, RL）机制，该奖励根据 VAE 解码出的隐式 token 内容之间的一致性进行计算，从而实现引导式的强化学习过程。\n\n在 LLaMA-1B 上的实验表明，相较于先进基线方法，ATP-Latent 在四个基准测试中实现了 +4.1% 的准确率提升，同时 token 消耗降低 3.3%。代码已开源于 https://github.com/zz1358m/ATP-Latent-master。"
  },
  {
    "date": "2026-01-29",
    "title": "FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale",
    "authors": "Ajay Patel, Colin Raffel, Chris Callison-Burch",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.22146v1",
    "source": "arXiv",
    "abstract": "Due to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised \"predict the next word\" objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of \"instruction-tuning\" data comprised of supervised training examples of instructions and responses. To overcome the limited amount of supervised data, we propose a procedure that can transform the knowledge in internet-scale pre-training documents into billions of synthetic instruction and answer training pairs. The resulting dataset, called FineInstructions, uses ~18M instruction templates created from real user-written queries and prompts. These instruction templates are matched to and instantiated with human-written source documents from unstructured pre-training corpora. With \"supervised\" synthetic training data generated at this scale, an LLM can be pre-trained from scratch solely with the instruction-tuning objective, which is far more in-distribution with the expected downstream usage of LLMs (responding to user prompts). We conduct controlled token-for-token training experiments and find pre-training on FineInstructions outperforms standard pre-training and other proposed synthetic pre-training techniques on standard benchmarks measuring free-form response quality. Our resources can be found at https://huggingface.co/fineinstructions .",
    "title_zh": "精细指令：将合成指令扩展至预训练规模",
    "abstract_zh": "由于监督训练数据有限，大型语言模型（LLMs）通常通过自监督的“预测下一个词”目标，在海量非结构化文本数据上进行预训练。为了使模型对用户更具实用性，还需在数量远少得多的“指令微调”数据上进一步训练，这些数据由人工标注的指令与对应回答组成。为克服监督数据量不足的问题，我们提出了一种方法，可将互联网规模预训练文档中的知识转化为数十亿条合成的指令-回答训练样本。由此生成的数据集名为 FineInstructions，其包含约1800万条来自真实用户查询和提示的指令模板。这些模板与来自非结构化预训练语料库中的人类撰写的源文档相匹配并实例化。借助如此大规模的“监督式”合成训练数据，我们可以仅使用指令微调目标从头开始对大语言模型进行预训练，这与下游实际使用场景（即响应用户提示）更加一致。我们进行了受控的逐标记训练实验，结果表明，在衡量自由格式回答质量的标准基准测试中，基于 FineInstructions 进行预训练的表现优于标准预训练方法及其他已提出的合成预训练技术。相关资源可在 https://huggingface.co/fineinstructions 获取。"
  },
  {
    "date": "2026-01-29",
    "title": "Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers",
    "authors": "Xin Chen, Feng Jiang, Yiqian Zhang, Hardy Chen, Shuo Yan, Wenya Xie, Min Yang, Shujian Huang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.22139v1",
    "source": "arXiv",
    "abstract": "Reasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \\emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous. We propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from passive solvers into proactive inquirers that interleave reasoning with clarification. Unlike existing search- or tool-based frameworks that primarily address knowledge uncertainty by querying external environments, PIR targets premise- and intent-level uncertainty through direct interaction with the user. PIR is implemented via two core components: (1) an uncertainty-aware supervised fine-tuning procedure that equips models with interactive reasoning capability, and (2) a user-simulator-based policy optimization framework driven by a composite reward that aligns model behavior with user intent. Extensive experiments on mathematical reasoning, code generation, and document editing demonstrate that PIR consistently outperforms strong baselines, achieving up to 32.70\\% higher accuracy, 22.90\\% higher pass rate, and 41.36 BLEU improvement, while reducing nearly half of the reasoning computation and unnecessary interaction turns. Further reliability evaluations on factual knowledge, question answering, and missing-premise scenarios confirm the strong generalization and robustness of PIR. Model and code are publicly available at: \\href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1}",
    "title_zh": "提问中的推理：将推理型大语言模型从被动求解者转变为积极的探询者",
    "abstract_zh": "以推理为导向的大语言模型（LLMs）在链式思维（Chain-of-Thought, CoT）提示技术的推动下取得了显著进展，但其仍受限于一种“盲目自我思考”的范式：即使关键信息缺失或模糊，也仍会进行大量内部推理。为此，我们提出了主动交互式推理（Proactive Interactive Reasoning, PIR），这是一种全新的推理范式，将大语言模型从被动求解者转变为积极提问者，实现推理与澄清的交替进行。与现有主要通过查询外部环境来应对知识不确定性的搜索或工具型框架不同，PIR 专注于通过与用户直接互动来解决前提和意图层面的不确定性。\n\nPIR 的实现依赖于两个核心组件：（1）一种具备不确定性感知能力的监督微调方法，使模型获得交互式推理能力；（2）基于用户模拟器的策略优化框架，采用复合奖励机制，引导模型行为与用户意图保持一致。在数学推理、代码生成和文档编辑等多个任务上的大量实验表明，PIR 持续优于多个强基线模型，准确率最高提升达 32.70%，通过率提高 22.90%，BLEU 值提升 41.36，同时将推理计算量减少近一半，并显著降低不必要的交互轮次。对事实知识、问答任务及前提缺失场景的进一步可靠性评估，验证了 PIR 强大的泛化能力和鲁棒性。\n\n相关模型与代码已公开发布于：\\href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1}"
  },
  {
    "date": "2026-01-29",
    "title": "SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents",
    "authors": "Yifeng Ding, Lingming Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.22129v1",
    "source": "arXiv",
    "abstract": "Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools. In this paper, we introduce SWE-Replay, the first efficient and generalizable test-time scaling technique for modern agents without reliance on potentially noisy value estimates. SWE-Replay optimizes the scaling process by recycling trajectories from prior trials, dynamically choosing to either explore from scratch or exploit archived experience by branching at critical intermediate steps. This selection of intermediate steps is driven by the potential and reasoning significance of repository exploration, rather than external LLM-based quality estimates. Our evaluation shows that, on SWE-Bench Verified, SWE-Replay consistently outperforms naive scaling, reducing costs by up to 17.4% while maintaining or even improving performance by up to 3.8%. Further evaluation on SWE-Bench Pro and Multilingual validates the generalizability of SWE-Replay, establishing it as a robust foundation for efficient test-time scaling of software engineering agents.",
    "title_zh": "SWE-Replay：面向软件工程智能体的高效测试时扩展",
    "abstract_zh": "测试时缩放（Test-time scaling）已被广泛采用，以提升大型语言模型（LLM）代理在软件工程（SWE）任务中的能力。然而，标准方法通过反复从头采样轨迹，计算成本较高。尽管近期一些方法尝试通过使用专用价值代理来降低开销，但这些方法可能因模型校准不当而表现不佳，并且难以泛化到现代代理——这些代理会将自定义的 Bash 脚本作为工具进行合成。本文提出 SWE-Replay，这是首个无需依赖潜在噪声较大的价值估计、且适用于现代代理的高效且可泛化的测试时缩放技术。SWE-Replay 通过复用先前试验中的轨迹来优化缩放过程，在关键中间步骤动态决定是重新探索还是利用已存档的经验，从而实现探索与利用的平衡。这种中间步骤的选择基于代码库探索的潜力和推理意义，而非依赖外部 LLM 的质量评估。实验结果表明，在 SWE-Bench Verified 上，SWE-Replay 始终优于简单的缩放方法，最多可降低 17.4% 的计算成本，同时性能提升最高达 3.8%。进一步在 SWE-Bench Pro 和多语言数据集上的评估验证了 SWE-Replay 的泛化能力，确立其作为高效软件工程代理测试时缩放的坚实基础。"
  },
  {
    "date": "2026-01-29",
    "title": "Value-Based Pre-Training with Downstream Feedback",
    "authors": "Shuqi Ke, Giulia Fanti",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.22108v1",
    "source": "arXiv",
    "abstract": "Can a small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes a fixed proxy objective (e.g., next-token prediction), which can misallocate compute away from downstream capabilities of interest. We introduce V-Pretraining: a value-based, modality-agnostic method for controlled continued pretraining in which a lightweight task designer reshapes the pretraining task to maximize the value of each gradient step. For example, consider self-supervised learning (SSL) with sample augmentation. The V-Pretraining task designer selects pretraining tasks (e.g., augmentations) for which the pretraining loss gradient is aligned with a gradient computed over a downstream task (e.g., image segmentation). This helps steer pretraining towards relevant downstream capabilities. Notably, the pretrained model is never updated on downstream task labels; they are used only to shape the pretraining task. Under matched learner update budgets, V-Pretraining of 0.5B--7B language models improves reasoning (GSM8K test Pass@1) by up to 18% relative over standard next-token prediction using only 12% of GSM8K training examples as feedback. In vision SSL, we improve the state-of-the-art results on ADE20K by up to 1.07 mIoU and reduce NYUv2 RMSE while improving ImageNet linear accuracy, and we provide pilot evidence of improved token efficiency in continued pretraining.",
    "title_zh": "基于价值的预训练与下游反馈",
    "abstract_zh": "少量经过验证的目标信息能否引导昂贵的自监督预训练过程？标准的预训练方法优化一个固定的代理目标（例如，下一个词预测），这可能导致计算资源被错误地分配，远离我们感兴趣的下游能力。为此，我们提出了V-Pretraining：一种基于价值、与模态无关的受控持续预训练方法，其中轻量级的任务设计者能够重塑预训练任务，以最大化每一步梯度更新的价值。例如，在使用样本增强的自监督学习（SSL）中，V-Pretraining的任务设计者会选择那些预训练损失梯度与下游任务（如图像分割）梯度对齐的预训练任务（如数据增强方式）。这种方法有助于将预训练过程引导至与下游任务相关的有效能力上。值得注意的是，预训练模型从未直接使用下游任务的标签进行更新；这些标签仅用于塑造预训练任务本身。在相同的更新预算下，V-Pretraining对0.5B至7B规模的语言模型进行微调，使推理能力（GSM8K测试Pass@1）相比标准的下一个词预测方法提升了最高达18%的相对性能，且仅需12%的GSM8K训练样本作为反馈信号。在视觉SSL领域，我们在ADE20K上实现了最高达1.07 mIoU的性能提升，同时降低了NYUv2的RMSE，并提高了ImageNet线性分类准确率，还提供了初步证据表明在持续预训练中具有更高的token效率。"
  },
  {
    "date": "2026-01-29",
    "title": "When \"Better\" Prompts Hurt: Evaluation-Driven Iteration for LLM Applications",
    "authors": "Daniel Commey",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.22025v1",
    "source": "arXiv",
    "abstract": "Evaluating Large Language Model (LLM) applications differs from traditional software testing because outputs are stochastic, high-dimensional, and sensitive to prompt and model changes. We present an evaluation-driven workflow - Define, Test, Diagnose, Fix - that turns these challenges into a repeatable engineering loop. We introduce the Minimum Viable Evaluation Suite (MVES), a tiered set of recommended evaluation components for (i) general LLM applications, (ii) retrieval-augmented generation (RAG), and (iii) agentic tool-use workflows. We also synthesize common evaluation methods (automated checks, human rubrics, and LLM-as-judge) and discuss known judge failure modes. In reproducible local experiments (Ollama; Llama 3 8B Instruct and Qwen 2.5 7B Instruct), we observe that a generic \"improved\" prompt template can trade off behaviors: on our small structured suites, extraction pass rate decreased from 100% to 90% and RAG compliance from 93.3% to 80% for Llama 3 when replacing task-specific prompts with generic rules, while instruction-following improved. These findings motivate evaluation-driven prompt iteration and careful claim calibration rather than universal prompt recipes. All test suites, harnesses, and results are included for reproducibility.",
    "title_zh": "“更好”提示词的负面影响：基于评估的迭代在大语言模型应用中的应用",
    "abstract_zh": "评估大型语言模型（LLM）应用与传统软件测试存在显著差异，因为其输出具有随机性、高维度特征，并对提示词和模型变化高度敏感。为此，我们提出了一种以评估驱动的工作流程——“定义、测试、诊断、修复”（Define, Test, Diagnose, Fix），将这些挑战转化为可重复的工程闭环。我们引入了最小可行评估套件（Minimum Viable Evaluation Suite, MVES），该套件采用分层设计，为三类典型场景提供推荐的评估组件：(i) 通用LLM应用，(ii) 检索增强生成（RAG），以及(iii) 代理式工具使用工作流。同时，我们整合了常见的评估方法（自动化检查、人工评分标准、LLM作为评判者），并讨论了评判者常见的失效模式。在可复现的本地实验中（基于Ollama平台，使用Llama 3 8B Instruct和Qwen 2.5 7B Instruct模型），我们发现，一个通用的“优化版”提示模板会带来行为权衡：在我们的小型结构化测试套件中，当用通用规则替代任务特定提示时，Llama 3的提取通过率从100%下降至90%，RAG合规性从93.3%降至80%，但指令遵循能力反而有所提升。这些发现强调了应以评估驱动的方式进行提示迭代，并谨慎校准性能声明，而非依赖普适的提示模板。所有测试套件、测试框架及实验结果均已公开，确保研究可复现。"
  },
  {
    "date": "2026-01-29",
    "title": "Token-Guard: Towards Token-Level Hallucination Control via Self-Checking Decoding",
    "authors": "Yifan Zhu, Huiqiang Rong, Haoran Luo",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21969v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) often hallucinate, generating content inconsistent with the input. Retrieval-Augmented Generation (RAG) and Reinforcement Learning with Human Feedback (RLHF) can mitigate hallucinations but require resource-intensive retrieval or large-scale fine-tuning. Decoding-based methods are lighter yet lack explicit hallucination control. To address this, we present Token-Guard, a token-level hallucination control method based on self-checking decoding. Token-Guard performs internal verification at each reasoning step to detect hallucinated tokens before they propagate. Candidate fragments are further evaluated in a latent space with explicit hallucination risk scoring, while iterative pruning and regeneration dynamically correct detected errors. Experiments on HALU datasets show Token-Guard substantially reduces hallucinations and improves generation accuracy, offering a scalable, modular solution for reliable LLM outputs. Our code is publicly available.",
    "title_zh": "Token-Guard：通过自我检查解码实现令牌级幻觉控制",
    "abstract_zh": "大型语言模型（LLMs）常常会产生幻觉，生成与输入不符的内容。检索增强生成（RAG）和基于人类反馈的强化学习（RLHF）虽能在一定程度上缓解幻觉问题，但需要消耗大量资源的检索或大规模微调。而基于解码的方法虽然开销较小，却缺乏对幻觉的显式控制。为解决这一问题，我们提出 Token-Guard，一种基于自检解码的 token 级幻觉控制方法。Token-Guard 在每个推理步骤中进行内部验证，提前检测出可能产生幻觉的 token，防止其传播。候选片段在潜在空间中进一步评估，并通过显式的幻觉风险评分机制进行判断；同时，通过迭代的剪枝与重生成动态修正已发现的错误。在 HALU 数据集上的实验表明，Token-Guard 显著减少了幻觉现象并提升了生成准确性，提供了一种可扩展、模块化的可靠 LLM 输出解决方案。我们的代码已公开。"
  },
  {
    "date": "2026-01-29",
    "title": "Evolution of Benchmark: Black-Box Optimization Benchmark Design through Large Language Model",
    "authors": "Chen Wang, Sijie Ma, Zeyuan Ma, Yue-Jiao Gong",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21877v1",
    "source": "arXiv",
    "abstract": "Benchmark Design in Black-Box Optimization (BBO) is a fundamental yet open-ended topic. Early BBO benchmarks are predominantly human-crafted, introducing expert bias and constraining diversity. Automating this design process can relieve the human-in-the-loop burden while enhancing diversity and objectivity. We propose Evolution of Benchmark (EoB), an automated BBO benchmark designer empowered by the large language model (LLM) and its program evolution capability. Specifically, we formulate benchmark design as a bi-objective optimization problem towards maximizing (i) landscape diversity and (ii) algorithm-differentiation ability across a portfolio of BBO solvers. Under this paradigm, EoB iteratively prompts LLM to evolve a population of benchmark programs and employs a reflection-based scheme to co-evolve the landscape and its corresponding program. Comprehensive experiments validate our EoB is a competitive candidate in multi-dimensional usages: 1) Benchmarking BBO algorithms; 2) Training and testing learning-assisted BBO algorithms; 3) Extending proxy for expensive real-world problems.",
    "title_zh": "基准的演进：通过大语言模型进行黑箱优化基准设计",
    "abstract_zh": "黑箱优化（BBO）中的基准设计是一个基础性但开放性极强的研究课题。早期的BBO基准大多由人工精心设计，这不可避免地引入了专家偏见，并限制了问题景观的多样性。自动化这一设计过程不仅能够减轻人工参与的负担，还能提升基准的多样性和客观性。为此，我们提出了“基准演化”（Evolution of Benchmark, EoB），一种基于大语言模型（LLM）及其程序演化能力的自动化BBO基准设计方法。具体而言，我们将基准设计建模为一个双目标优化问题，旨在同时最大化：(i) 问题景观的多样性；以及 (ii) 在一组BBO求解器中区分不同算法性能的能力。在此框架下，EoB通过迭代调用LLM来演化基准程序种群，并采用基于反思的机制协同进化问题景观与其对应的程序实现。大量实验验证了EoB在多维度应用中的卓越表现：1）用于BBO算法的基准测试；2）用于学习辅助型BBO算法的训练与测试；3）作为昂贵真实世界问题的高效代理。"
  },
  {
    "date": "2026-01-29",
    "title": "Migrating Esope to Fortran 2008 using model transformations",
    "authors": "Younoussa Sow, Nicolas Anquetil, Léandre Brault, Stéphane Ducasse",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21755v1",
    "source": "arXiv",
    "abstract": "Legacy programming languages such as FORTRAN 77 still play a vital role in many industrial applications. Maintaining and modernizing these languages is challenging, especially when migrating to newer standards such as Fortran 2008. This is exacerbated in the presence of legacy proprietary extensions on such legacy languages, because their semantics are often based on old context (limits of legacy language, domain logic,...). This paper presents an approach for automatically migrating FORTRAN 77 with a proprietary extension, named Esope, to Fortran 2008. We introduce a tool that converts Esope source code to Fortran 2008. While supporting readability of the generated code, we want to maintain the level of abstraction provided by Esope. Our method uses model-driven engineering techniques, with transformations to generate a target model from which we export easy-to-read Fortran 2008 source code. We discuss the advantages, limitations, and maintainability considerations of our approach and provide insights into its scalability and adaptability to evolving requirements.",
    "title_zh": "使用模型转换将Esope迁移至Fortran 2008",
    "abstract_zh": "像FORTRAN 77这样的遗留编程语言在许多工业应用中仍然发挥着至关重要的作用。维护和现代化这些语言极具挑战性，尤其是在向更现代的标准（如Fortran 2008）迁移时。当这些遗留语言中存在专有的扩展时，这种挑战更加突出，因为这些扩展的语义往往基于过时的上下文（如旧语言的限制、领域逻辑等）。本文提出了一种自动将带有专有扩展Esope的FORTRAN 77代码迁移至Fortran 2008的方法。我们开发了一款工具，可将Esope源代码转换为Fortran 2008代码。在确保生成代码可读性的前提下，我们还致力于保持Esope所提供的抽象层次。我们的方法采用模型驱动工程技术，通过一系列转换从源模型生成目标模型，再从中导出易于阅读的Fortran 2008源代码。本文还讨论了该方法的优势、局限性以及可维护性方面的考量，并对方法的可扩展性和适应不断变化需求的能力提供了深入见解。"
  },
  {
    "date": "2026-01-29",
    "title": "FBS: Modeling Native Parallel Reading inside a Transformer",
    "authors": "Tongxi Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21708v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) excel across many tasks, yet inference is still dominated by strictly token-by-token autoregression. Existing acceleration methods largely patch this pipeline and miss core human-reading ingredients: content-adaptive foresight, chunk-structure-aware compute allocation, and train--test consistency for preview/skimming. We propose the \\textbf{Fovea-Block-Skip Transformer} (FBS), which injects a causal, trainable loop into Transformers via Parafovea-Attention Window (PAW), Chunk-Head (CH), and Skip-Gate (SG). Across diverse benchmarks, FBS improves the quality-efficiency trade-off without increasing parameters, and ablations show the three modules are complementary.",
    "title_zh": "FBS：在Transformer中建模原生并行阅读",
    "abstract_zh": "大型语言模型（LLMs）在众多任务中表现出色，但推理过程仍主要依赖于严格的逐标记自回归机制。现有的加速方法大多只是对这一流程进行修补，忽略了人类阅读的核心要素：内容自适应的前瞻能力、对文本块结构敏感的计算资源分配，以及预览/略读时训练与测试的一致性。我们提出了**Fovea-Block-Skip Transformer**（FBS），通过引入一种因果可训练的循环机制，将Parafovea注意力窗口（PAW）、块头（CH）和跳过门（SG）三个模块嵌入Transformer架构中。在多种基准测试中，FBS在不增加参数量的前提下显著提升了质量与效率之间的权衡表现，消融实验也表明这三个模块具有互补性。"
  },
  {
    "date": "2026-01-29",
    "title": "Scale-Dependent Semantic Dynamics Revealed by Allan Deviation",
    "authors": "Debayan Dasgupta",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21678v1",
    "source": "arXiv",
    "abstract": "While language progresses through a sequence of semantic states, the underlying dynamics of this progression remain elusive. Here, we treat the semantic progression of written text as a stochastic trajectory in a high-dimensional state space. We utilize Allan deviation, a tool from precision metrology, to analyze the stability of meaning by treating ordered sentence embeddings as a displacement signal. Our analysis reveals two distinct dynamical regimes: short-time power-law scaling, which differentiates creative literature from technical texts, and a long-time crossover to a stability-limited noise floor. We find that while large language models successfully mimic the local scaling statistics of human text, they exhibit a systematic reduction in their stability horizon. These results establish semantic coherence as a measurable physical property, offering a framework to differentiate the nuanced dynamics of human cognition from the patterns generated by algorithmic models.",
    "title_zh": "由阿兰偏差揭示的尺度相关语义动态",
    "abstract_zh": "当语言在一系列语义状态中演进时，其背后的动力学机制仍不清晰。本文将书面文本的语义演进视为高维状态空间中的随机轨迹。我们采用精密计量学中的阿兰偏差（Allan deviation）工具，将有序句向量视为位移信号，以分析意义的稳定性。分析结果揭示出两种截然不同的动力学行为：短时间内的幂律标度，能够区分创造性文学与技术性文本；以及长时间尺度上向稳定性受限的噪声底限的转变。我们发现，尽管大型语言模型能够成功模仿人类文本的局部标度统计特征，但其稳定性持续时间却系统性地缩短。这些结果确立了语义连贯性作为一种可测量的物理属性，为区分人类认知的细微动态与算法模型生成的模式提供了新的框架。"
  },
  {
    "date": "2026-01-29",
    "title": "Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening",
    "authors": "Xiaotong Ji, Rasul Tutunov, Matthieu Zimmer, Haitham Bou Ammar",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21590v1",
    "source": "arXiv",
    "abstract": "Reinforcement learning (RL) post-training is a dominant approach for improving the reasoning performance of large language models (LLMs), yet growing evidence suggests that its gains arise primarily from distribution sharpening rather than the acquisition of new capabilities. Recent work has shown that sampling from the power distribution of LLMs using Markov chain Monte Carlo (MCMC) can recover performance comparable to RL post-training without relying on external rewards; however, the high computational cost of MCMC makes such approaches impractical for widespread adoption. In this work, we propose a theoretically grounded alternative that eliminates the need for iterative MCMC. We derive a novel formulation showing that the global power distribution can be approximated by a token-level scaled low-temperature one, where the scaling factor captures future trajectory quality. Leveraging this insight, we introduce a training-free and verifier-free algorithm that sharpens the base model's generative distribution autoregressively. Empirically, we evaluate our method on math, QA, and code tasks across four LLMs, and show that our method matches or surpasses one-shot GRPO without relying on any external rewards, while reducing inference latency by over 10x compared to MCMC-based sampling.",
    "title_zh": "可扩展的功率采样：通过分布锐化解锁无需训练的高效大模型推理",
    "abstract_zh": "强化学习（RL）后训练是提升大语言模型（LLM）推理性能的主流方法，然而越来越多的证据表明，其性能提升主要源于分布的锐化，而非获得新的能力。近期研究发现，通过马尔可夫链蒙特卡洛（MCMC）采样LLM的幂分布，可以在不依赖外部奖励的情况下实现与RL后训练相当的性能；然而，MCMC的高计算开销使其难以大规模应用。在本工作中，我们提出了一种理论基础坚实的替代方案，彻底消除了对迭代式MCMC的需求。我们推导出一种新公式，表明全局幂分布可通过一个逐标记缩放的低温度分布近似，其中缩放因子捕捉了未来轨迹的质量。基于这一洞察，我们提出了一种无需训练、也无需验证器的算法，能够自回归地锐化基础模型的生成分布。在四个不同LLM上，我们在数学、问答和代码任务上进行了实验评估，结果表明，我们的方法在不依赖任何外部奖励的前提下，达到了甚至超过了单次GRPO的性能，同时相比基于MCMC的采样，推理延迟降低了10倍以上。"
  },
  {
    "date": "2026-01-29",
    "title": "Chain Of Thought Compression: A Theoritical Analysis",
    "authors": "Juncai Li, Ru Li, Yuxiang Zhou, Boxiang Ma, Jeff Z. Pan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21576v1",
    "source": "arXiv",
    "abstract": "Chain-of-Thought (CoT) has unlocked advanced reasoning abilities of Large Language Models (LLMs) with intermediate steps, yet incurs prohibitive computational costs due to generation of extra tokens. Recent studies empirically show that compressing reasoning steps into latent states, or implicit CoT compression, offers a token-efficient alternative. However, the mechanism behind CoT compression remains unclear. In this paper, we provide the first theoretical analysis of the difficulty of learning to internalize intermediate reasoning steps. By introducing Order-r Interaction, we prove that the learning signal for high-order logical dependencies exponentially decays to solve irreducible problem, where skipping intermediate steps inevitably leads to high-order interaction barriers. To empirically validate this, we introduce NatBool-DAG, a challenging benchmark designed to enforce irreducible logical reasoning and eliminate semantic shortcuts. Guided by our theoretical findings, we propose ALiCoT (Aligned Implicit CoT), a novel framework that overcomes the signal decay by aligning latent token distributions with intermediate reasoning states. Experimental results demonstrate that ALiCoT successfully unlocks efficient reasoning: it achieves a 54.4x speedup while maintaining performance comparable to explicit CoT.",
    "title_zh": "思维链压缩：一种理论分析",
    "abstract_zh": "思维链（Chain-of-Thought, CoT）通过引入中间推理步骤，激发了大型语言模型（LLMs）的高级推理能力，但其高昂的计算成本源于额外生成的token。近期研究 empirically 表明，将推理步骤压缩为隐状态，即隐式思维链压缩（implicit CoT compression），是一种更节省token的替代方案。然而，CoT压缩背后的机制仍不清晰。本文首次对内化中间推理步骤的学习难度进行了理论分析。通过引入“序r交互”（Order-r Interaction）概念，我们证明：对于不可约问题，高阶逻辑依赖关系的学习信号会呈指数级衰减，跳过中间步骤必然导致高阶交互障碍。为实证验证该理论，我们提出了NatBool-DAG——一个具有挑战性的基准测试集，旨在强制进行不可简化的逻辑推理，并消除语义捷径。基于上述理论发现，我们提出ALiCoT（Aligned Implicit CoT）框架，通过将隐状态的分布与中间推理状态对齐，有效克服了学习信号衰减问题。实验结果表明，ALiCoT成功实现了高效推理：在保持与显式CoT相当性能的同时，实现了54.4倍的速度提升。"
  },
  {
    "date": "2026-01-29",
    "title": "EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots",
    "authors": "Zixing Lei, Genjia Liu, Yuanshuo Zhang, Qipeng Liu, Chuan Wen, Shanghang Zhang, Wenzhao Lian, Siheng Chen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21570v1",
    "source": "arXiv",
    "abstract": "The field of Embodied AI is witnessing a rapid evolution toward general-purpose robotic systems, fueled by high-fidelity simulation and large-scale data collection. However, this scaling capability remains severely bottlenecked by a reliance on labor-intensive manual oversight from intricate reward shaping to hyperparameter tuning across heterogeneous backends. Inspired by LLMs' success in software automation and science discovery, we introduce \\textsc{EmboCoach-Bench}, a benchmark evaluating the capacity of LLM agents to autonomously engineer embodied policies. Spanning 32 expert-curated RL and IL tasks, our framework posits executable code as the universal interface. We move beyond static generation to assess a dynamic closed-loop workflow, where agents leverage environment feedback to iteratively draft, debug, and optimize solutions, spanning improvements from physics-informed reward design to policy architectures such as diffusion policies. Extensive evaluations yield three critical insights: (1) autonomous agents can qualitatively surpass human-engineered baselines by 26.5\\% in average success rate; (2) agentic workflow with environment feedback effectively strengthens policy development and substantially narrows the performance gap between open-source and proprietary models; and (3) agents exhibit self-correction capabilities for pathological engineering cases, successfully resurrecting task performance from near-total failures through iterative simulation-in-the-loop debugging. Ultimately, this work establishes a foundation for self-evolving embodied intelligence, accelerating the paradigm shift from labor-intensive manual tuning to scalable, autonomous engineering in embodied AI field.",
    "title_zh": "EmboCoach-Bench：面向具身机器人开发的AI智能体基准测试",
    "abstract_zh": "具身人工智能领域正迅速向通用机器人系统演进，这一进程得益于高保真度仿真与大规模数据采集技术的发展。然而，这种扩展能力仍严重受限于对人工密集型监督的依赖，从复杂的奖励函数设计到异构后端的超参数调优，均需大量人力投入。受大型语言模型（LLM）在软件自动化与科学发现中取得成功的启发，我们提出了 \\textsc{EmboCoach-Bench}——一个用于评估LLM智能体自主构建具身策略能力的基准测试框架。该框架涵盖32个由专家精心设计的强化学习（RL）与模仿学习（IL）任务，以可执行代码作为统一接口。我们突破了静态生成的局限，转而评估一种动态的闭环工作流：智能体通过利用环境反馈，迭代地完成方案起草、调试与优化，覆盖从物理感知的奖励设计到扩散策略等各类策略架构的改进。大量实验揭示出三个关键洞见：（1）自主智能体在平均成功率上可比人工设计基线提升26.5%；（2）结合环境反馈的智能体工作流能有效增强策略开发能力，显著缩小开源模型与专有模型之间的性能差距；（3）智能体具备自我修正能力，能够针对异常工程问题进行修复，通过循环式“仿真-调试”迭代，成功将濒临失败的任务表现重新恢复。最终，本研究为自演化具身智能奠定了基础，推动具身人工智能领域实现从耗时费力的人工调优向可扩展、自主化工程范式的根本性转变。"
  },
  {
    "date": "2026-01-29",
    "title": "Meta Context Engineering via Agentic Skill Evolution",
    "authors": "Haoran Ye, Xuning He, Vincent Arak, Haonan Dong, Guojie Song",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21557v1",
    "source": "arXiv",
    "abstract": "The operational efficacy of large language models relies heavily on their inference-time context. This has established Context Engineering (CE) as a formal discipline for optimizing these inputs. Current CE methods rely on manually crafted harnesses, such as rigid generation-reflection workflows and predefined context schemas. They impose structural biases and restrict context optimization to a narrow, intuition-bound design space. To address this, we introduce Meta Context Engineering (MCE), a bi-level framework that supersedes static CE heuristics by co-evolving CE skills and context artifacts. In MCE iterations, a meta-level agent refines engineering skills via agentic crossover, a deliberative search over the history of skills, their executions, and evaluations. A base-level agent executes these skills, learns from training rollouts, and optimizes context as flexible files and code. We evaluate MCE across five disparate domains under offline and online settings. MCE demonstrates consistent performance gains, achieving 5.6--53.8% relative improvement over state-of-the-art agentic CE methods (mean of 16.9%), while maintaining superior context adaptability, transferability, and efficiency in both context usage and training.",
    "title_zh": "通过代理技能演进实现元上下文工程",
    "abstract_zh": "大型语言模型的运行效率在很大程度上依赖于其推理时的上下文。这使得上下文工程（Context Engineering, CE）成为一门专门优化输入的正式学科。然而，现有的CE方法主要依赖于人工设计的框架，例如固定的生成-反思工作流和预定义的上下文结构。这些方法引入了结构性偏差，将上下文优化限制在狭窄且依赖直觉的设计空间内。为解决这一问题，我们提出了元上下文工程（Meta Context Engineering, MCE），这是一种双层框架，通过协同进化上下文工程技能与上下文产物，取代了传统的静态CE启发式方法。在MCE的迭代过程中，元层级代理通过“代理交叉”（agentic crossover）——即对技能历史、执行过程及其评估结果进行深思熟虑的搜索——来不断精炼工程技能；而基础层级代理则执行这些技能，通过训练回放学习，并将上下文优化为灵活的文件和代码形式。我们在五个不同的领域中，分别在离线和在线设置下对MCE进行了评估。结果表明，MCE在各项任务中均表现出持续的性能提升，在相对改进幅度上达到5.6%至53.8%（平均提升16.9%），同时在上下文适应性、可迁移性以及上下文使用与训练效率方面均展现出卓越表现。"
  },
  {
    "date": "2026-01-29",
    "title": "Chasing Elusive Memory Bugs in GPU Programs",
    "authors": "Anubhab Ghosh, Ajay Nayak, Dhananjay Rao Thallikar Shyam, Arkaprava Basu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21552v1",
    "source": "arXiv",
    "abstract": "Memory safety bugs, such as out-of-bound accesses (OOB) in GPU programs, can compromise the security and reliability of GPU-accelerated software. We report the existence of input-dependent OOBs in the wild that manifest only under specific inputs. All existing tools to detect OOBs in GPU programs rely on runtime techniques that require an OOB to manifest for detection. Thus, input-dependent OOBs elude them. We also discover intra-allocation OOBs that arise in the presence of logical partitioning of a memory allocation into multiple data structures. Existing techniques are oblivious to the possibility of such OOBs. We make a key observation that the presence (or absence) of semantic relations among program variables, which determines the size of allocations (CPU code) and those calculating offsets into memory allocations (GPU code), helps identify the absence (or presence) of OOBs. We build SCuBA, a first-of-its-kind compile-time technique that analyzes CPU and GPU code to capture such semantic relations (if present). It uses a SAT solver to check if an OOB access is possible under any input, given the captured relations expressed as constraints. It further analyzes GPU code to track logical partitioning of memory allocations for detecting intra-allocation OOB. Compared to NVIDIA's Compute Sanitizer that misses 45 elusive memory bugs across 20 programs, SCuBA misses none with no false alarms.",
    "title_zh": "在GPU程序中追踪难以捉摸的内存错误",
    "abstract_zh": "内存安全漏洞，例如GPU程序中的越界访问（OOB），可能危及GPU加速软件的安全性和可靠性。我们发现，在实际应用中存在一种输入依赖型的越界访问，它们仅在特定输入条件下才会显现。目前所有用于检测GPU程序中越界访问的工具都依赖于运行时技术，这些技术需要越界行为实际发生才能被检测到，因此无法发现这类输入依赖型的越界问题。此外，我们还发现了存在于内存分配逻辑划分情况下的“分配内越界”（intra-allocation OOB）——即一个内存分配被逻辑划分为多个数据结构时产生的越界问题。现有技术对这种越界形式完全缺乏识别能力。\n\n我们提出一个关键观察：程序变量之间的语义关系（或缺乏语义关系）决定了CPU代码中内存分配的大小，以及GPU代码中计算内存偏移的逻辑。这种语义关系的存在与否，有助于判断越界访问是否可能发生。基于此，我们开发了SCuBA——首个面向编译时分析的创新技术，能够同时分析CPU与GPU代码，以捕获这些潜在的语义关系（若存在）。SCuBA利用SAT求解器，将所捕获的关系表示为约束条件，并验证在任意输入下是否存在越界访问的可能性。此外，它还能进一步分析GPU代码，追踪内存分配的逻辑划分情况，从而有效检测分配内的越界问题。\n\n与NVIDIA的Compute Sanitizer相比，后者在20个程序中共漏检了45个隐蔽的内存错误，而SCuBA则实现了零漏报且无误报。"
  },
  {
    "date": "2026-01-29",
    "title": "World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems",
    "authors": "Lakshya Gupta, Litao Li, Yizhe Liu, Sriram Ganapathi Subramanian, Kaheer Suleman, Zichen Zhang, Haoye Lu, Sumit Pasupalak",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.22130v1",
    "source": "arXiv",
    "abstract": "Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a benchmark of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities. We reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub for setting up and evaluating WoW.",
    "title_zh": "工作流的世界：将世界模型引入企业系统的基准测试",
    "abstract_zh": "前沿大型语言模型（LLMs）在多个领域作为自主代理表现出色，但在复杂的企事业单位系统中仍缺乏充分验证。这类系统中隐藏的工作流程会在相互关联的数据库间引发级联效应。现有的企业基准测试仅评估表面层次的代理任务完成情况，类似于通用消费者基准测试，忽略了企业环境中真正的挑战，例如可观测性受限、数据库状态庞大，以及存在级联副作用的隐藏工作流。\n\n为此，我们提出了“工作流世界”（World of Workflows, WoW），一个基于ServiceNow的真实环境，内嵌超过4,000条业务规则和55条活跃工作流。同时，我们推出了WoW-bench基准测试，包含234项任务，用于评估代理在约束条件下的任务完成能力及对企业动态建模的能力。我们的研究揭示了两大关键发现：（1）前沿LLMs存在“动态盲视”问题，始终无法预测其行为所引发的不可见级联副作用，从而导致无声的约束违规；（2）在不透明系统中实现可靠性，需要基于现实的世界建模——代理必须在脑海中模拟隐藏的状态转换，以弥补可观测性差距，尤其是在高保真反馈不可用的情况下。\n\n为了构建可靠且实用的企业级智能代理，WoW提出了一种新范式：显式学习系统动态。我们已将相关代码开源，可通过GitHub获取，以支持WoW环境的搭建与评估。"
  },
  {
    "date": "2026-01-29",
    "title": "SINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence",
    "authors": "Saoud Aldowaish, Yashwanth Karumanchi, Kai-Chen Chiang, Soroosh Noorzad, Morteza Fayazi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.22114v1",
    "source": "arXiv",
    "abstract": "Current methods for converting circuit schematic images into machine-readable netlists struggle with component recognition and connectivity inference. In this paper, we present SINA, an open-source, fully automated circuit schematic image-to-netlist generator. SINA integrates deep learning for accurate component detection, Connected-Component Labeling (CCL) for precise connectivity extraction, and Optical Character Recognition (OCR) for component reference designator retrieval, while employing a Vision-Language Model (VLM) for reliable reference designator assignments. In our experiments, SINA achieves 96.47% overall netlist-generation accuracy, which is 2.72x higher than state-of-the-art approaches.",
    "title_zh": "SINA：一种基于人工智能的电路原理图图像到网表生成器",
    "abstract_zh": "当前将电路原理图图像转换为机器可读网表的方法在元件识别和连接关系推断方面仍存在困难。本文提出SINA，一个开源的、全自动的电路原理图图像到网表生成工具。SINA结合了深度学习以实现精确的元件检测，采用连通域标记（CCL）技术进行精准的连接关系提取，并利用光学字符识别（OCR）获取元件的参考标识符，同时借助视觉-语言模型（VLM）实现可靠的参考标识符分配。实验结果表明，SINA的总体网表生成准确率达到96.47%，较现有最先进方法提升了2.72倍。"
  },
  {
    "date": "2026-01-29",
    "title": "LANCER: LLM Reranking for Nugget Coverage",
    "authors": "Jia-Huei Ju, François G. Landry, Eugene Yang, Suzan Verberne, Andrew Yates",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.22008v1",
    "source": "arXiv",
    "abstract": "Unlike short-form retrieval-augmented generation (RAG), such as factoid question answering, long-form RAG requires retrieval to provide documents covering a wide range of relevant information. Automated report generation exemplifies this setting: it requires not only relevant information but also a more elaborate response with comprehensive information. Yet, existing retrieval methods are primarily optimized for relevance ranking rather than information coverage. To address this limitation, we propose LANCER, an LLM-based reranking method for nugget coverage. LANCER predicts what sub-questions should be answered to satisfy an information need, predicts which documents answer these sub-questions, and reranks documents in order to provide a ranked list covering as many information nuggets as possible. Our empirical results show that LANCER enhances the quality of retrieval as measured by nugget coverage metrics, achieving higher $α$-nDCG and information coverage than other LLM-based reranking methods. Our oracle analysis further reveals that sub-question generation plays an essential role.",
    "title_zh": "LANCER：用于实体覆盖的LLM重排序",
    "abstract_zh": "与短文本检索增强生成（RAG）不同，例如事实型问答任务，长文本RAG需要检索提供涵盖广泛相关资讯的文档。自动化报告生成正是这一场景的典型例子：它不仅需要相关信息，还要求生成更为详尽、全面的回答。然而，现有的检索方法主要针对相关性排序进行优化，而非信息覆盖度。为解决这一局限，我们提出了LANCER——一种基于大语言模型（LLM）的重排序方法，旨在提升信息要点（nugget）的覆盖能力。LANCER能够预测满足特定信息需求所需回答的子问题，识别哪些文档可解答这些子问题，并据此对文档进行重排序，以生成一个尽可能覆盖更多信息要点的排序列表。实验结果表明，LANCER在信息要点覆盖率指标上显著提升了检索质量，其α-nDCG和信息覆盖率均优于其他基于LLM的重排序方法。此外，通过“理想情况”分析进一步揭示了子问题生成在其中的关键作用。"
  },
  {
    "date": "2026-01-29",
    "title": "Optimal Software Pipelining using an SMT-Solver",
    "authors": "Jan-Willem Roorda",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21842v1",
    "source": "arXiv",
    "abstract": "Software Pipelining is a classic and important loop-optimization for VLIW processors. It improves instruction-level parallelism by overlapping multiple iterations of a loop and executing them in parallel. Typically, it is implemented using heuristics. In this paper, we present an optimal software pipeliner based on a Satisfiability Modulo Theories (SMT) Solver. We show that our approach significantly outperforms heuristic algorithms and hand-optimization. Furthermore, we show how the solver can be used to give feedback to programmers and processor designers on why a software pipelined schedule of a certain initiation interval is not feasible.",
    "title_zh": "使用SMT求解器进行最优软件流水",
    "abstract_zh": "软件流水是一种经典的、重要的VLIW处理器循环优化技术。它通过重叠循环的多个迭代并行执行，从而提升指令级并行性。通常，该技术采用启发式方法实现。本文提出了一种基于可满足性模理论（SMT）求解器的最优软件流水器。我们证明，该方法显著优于现有的启发式算法以及手工优化。此外，我们还展示了如何利用求解器为程序员和处理器设计者提供反馈，解释为何某个特定启动间隔的软件流水调度不可行。"
  },
  {
    "date": "2026-01-29",
    "title": "StepShield: When, Not Whether to Intervene on Rogue Agents",
    "authors": "Gloria Felicia, Michael Eniolade, Jinfeng He, Zitha Sasindran, Hemant Kumar, Milan Hussain Angati, Sandeep Bandarupalli",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.22136v1",
    "source": "arXiv",
    "abstract": "Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.",
    "title_zh": "步骤防护：对失控代理人干预的时机，而非是否干预",
    "abstract_zh": "现有的智能体安全评估基准仅报告二元准确率，将早期干预与事后分析混为一谈。一个在第8步就标记违规行为的检测器能够实现及时干预；而一个在第48步才报告违规的检测器，仅具有事后取证价值。这种区别至关重要，但现有基准无法衡量。为此，我们提出StepShield——首个关注违规行为何时被发现而非仅仅是否被发现的评估基准。StepShield包含9,213条代码智能体轨迹，其中包括1,278对精心标注的训练样本和一个包含7,935条轨迹的测试集，其真实“越界”行为比例为8.1%。这些越界行为基于六大类真实世界安全事件构建。我们提出了三项新颖的时间维度指标：早期干预率（EIR）、干预延迟差（Intervention Gap）和节省的Token数。令人惊讶的是，我们的评估显示，基于大语言模型（LLM）的评判器达到59%的EIR，而静态分析器仅为26%，性能差距高达2.3倍——这一差异在传统准确率指标下完全不可见。此外，我们还证明了早期检测具有直接的经济价值：我们的级联式HybridGuard检测器可将监控成本降低75%，在企业规模下五年内预计累计节省达1.08亿美元。通过将评估重点从“是否”转向“何时”，StepShield为构建更安全、更具经济可行性的AI智能体奠定了全新基础。相关代码与数据已按Apache 2.0许可开源。"
  },
  {
    "date": "2026-01-29",
    "title": "SWE-Spot: Building Small Repo-Experts with Repository-Centric Learning",
    "authors": "Jinjun Peng, Magnus Saebo, Tianjun Zhong, Yi-Jie Cheng, Junfeng Yang, Baishakhi Ray, Simin Chen, Yangruibo Ding",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21649v1",
    "source": "arXiv",
    "abstract": "The deployment of coding agents in privacy-sensitive and resource-constrained environments drives the demand for capable open-weight Small Language Models (SLMs). However, they suffer from a fundamental capability gap: unlike frontier large models, they lack the inference-time strong generalization to work with complicated, unfamiliar codebases. We identify that the prevailing Task-Centric Learning (TCL) paradigm, which scales exposure across disparate repositories, fails to address this limitation. In response, we propose Repository-Centric Learning (RCL), a paradigm shift that prioritizes vertical repository depth over horizontal task breadth, suggesting SLMs must internalize the \"physics\" of a target software environment through parametric knowledge acquisition, rather than attempting to recover it via costly inference-time search. Following this new paradigm, we design a four-unit Repository-Centric Experience, transforming static codebases into interactive learning signals, to train SWE-Spot-4B, a family of highly compact models built as repo-specialized experts that breaks established scaling trends, outperforming open-weight models up to larger (e.g., CWM by Meta, Qwen3-Coder-30B) and surpassing/matching efficiency-focused commercial models (e.g., GPT-4.1-mini, GPT-5-nano) across multiple SWE tasks. Further analysis reveals that RCL yields higher training sample efficiency and lower inference costs, emphasizing that for building efficient intelligence, repository mastery is a distinct and necessary dimension that complements general coding capability.",
    "title_zh": "SWE-Spot：基于仓库中心学习构建小型代码库专家",
    "abstract_zh": "在隐私敏感且资源受限的环境中部署编码代理，推动了对强大开源权重小型语言模型（SLMs）的需求。然而，这些模型存在一个根本性的能力短板：与前沿大型模型不同，它们缺乏在复杂、陌生代码库中进行推理时的强泛化能力。我们发现，当前主流的“任务中心学习”（TCL）范式——通过广泛覆盖不同仓库来扩大训练数据暴露——无法解决这一局限性。为此，我们提出“仓库中心学习”（RCL）的新范式，强调垂直深度而非水平广度，主张小型语言模型应通过参数化知识获取，内化目标软件环境的“内在规律”，而非依赖代价高昂的推理时搜索来重建这些知识。\n\n遵循这一新范式，我们设计了一套四单元“仓库中心体验”机制，将静态代码库转化为动态交互式学习信号，从而训练出SWE-Spot-4B系列高度紧凑的模型。这些模型作为针对特定仓库优化的专业专家，打破了既有的规模扩展规律，在多项软件工程任务中表现超越或媲美更大规模的开源模型（如Meta的CWM、Qwen3-Coder-30B），同时在效率方面优于或匹配专注于性能的商业模型（如GPT-4.1-mini、GPT-5-nano）。\n\n进一步分析表明，RCL不仅提升了训练样本的利用效率，还显著降低了推理成本。这凸显出：在构建高效智能系统时，对仓库的深入掌握是一个独立且不可或缺的维度，它与通用编程能力相辅相成，共同构成真正高效的代码理解与生成能力。"
  },
  {
    "date": "2026-01-29",
    "title": "MoCo: A One-Stop Shop for Model Collaboration Research",
    "authors": "Shangbin Feng, Yuyang Bai, Ziyuan Yang, Yike Wang, Zhaoxuan Tan, Jiajie Yan, Zhenyu Lei, Wenxuan Ding, Weijia Shi, Haojin Wang, Zhenting Qi, Yuru Jiang, Heng Wang, Chengsong Huang, Yu Fei, Jihan Yao, Yilun Du, Luke Zettlemoyer, Yejin Choi, Yulia Tsvetkov",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21257v1",
    "source": "arXiv",
    "abstract": "Advancing beyond single monolithic language models (LMs), recent research increasingly recognizes the importance of model collaboration, where multiple LMs collaborate, compose, and complement each other. Existing research on this topic has mostly been disparate and disconnected, from different research communities, and lacks rigorous comparison. To consolidate existing research and establish model collaboration as a school of thought, we present MoCo: a one-stop Python library of executing, benchmarking, and comparing model collaboration algorithms at scale. MoCo features 26 model collaboration methods, spanning diverse levels of cross-model information exchange such as routing, text, logit, and model parameters. MoCo integrates 25 evaluation datasets spanning reasoning, QA, code, safety, and more, while users could flexibly bring their own data. Extensive experiments with MoCo demonstrate that most collaboration strategies outperform models without collaboration in 61.0% of (model, data) settings on average, with the most effective methods outperforming by up to 25.8%. We further analyze the scaling of model collaboration strategies, the training/inference efficiency of diverse methods, highlight that the collaborative system solves problems where single LMs struggle, and discuss future work in model collaboration, all made possible by MoCo. We envision MoCo as a valuable toolkit to facilitate and turbocharge the quest for an open, modular, decentralized, and collaborative AI future.",
    "title_zh": "MoCo：模型协作研究的一站式平台",
    "abstract_zh": "超越单一的庞大语言模型（LM），近期研究越来越认识到模型协作的重要性，即多个语言模型通过协同、组合与互补来共同工作。然而，现有相关研究大多分散且彼此孤立，来自不同研究领域，缺乏严谨的对比评估。为整合现有研究成果，并推动模型协作成为一种系统性的研究范式，我们提出了MoCo：一个集执行、基准测试与比较于一体的Python一站式工具库，可大规模实现模型协作算法的部署与评估。MoCo包含26种模型协作方法，覆盖了从路由、文本、logit到模型参数等不同层次的跨模型信息交互。该工具库集成25个评估数据集，涵盖推理、问答、代码生成、安全性等多个任务领域，同时支持用户灵活引入自定义数据。通过在MoCo上开展的大规模实验表明，在平均61.0%的（模型，数据）组合中，大多数协作策略的表现优于无协作的单模型，最优方法甚至提升了25.8%。我们进一步分析了模型协作策略的扩展性，比较了各类方法在训练与推理效率上的差异，揭示出协作系统能够解决单个语言模型难以应对的问题，并探讨了未来在模型协作方向的研究展望——这一切都得益于MoCo的强大支持。我们期待MoCo能成为推动开放、模块化、去中心化和协作式人工智能未来的宝贵工具包。"
  },
  {
    "date": "2026-01-29",
    "title": "Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification",
    "authors": "Yiju Guo, Tianyi Hu, Zexu Sun, Yankai Lin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21244v1",
    "source": "arXiv",
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has advanced LLM reasoning, but remains constrained by inefficient exploration under limited rollout budgets, leading to low sampling success and unstable training in complex tasks. We find that many exploration failures arise not from problem difficulty, but from a small number of prompt tokens that introduce interference. Building on this insight, we propose the Less Noise Sampling Framework (LENS), which first prompts by identifying and removing interference tokens. then transfers successful rollouts from the purification process to supervise policy optimization on the original noisy prompts, enabling the model to learn to ignore interference in the real-world, noisy prompting settings. Experimental results show that LENS significantly outperforms GRPO, delivering higher performance and faster convergence, with a 3.88% average gain and over 1.6$\\times$ speedup. Our work highlights the critical role of pruning interference tokens in improving rollout efficiency, offering a new perspective for RLVR research.",
    "title_zh": "更少噪声，更多声音：通过指令净化实现推理的强化学习",
    "abstract_zh": "基于可验证奖励的强化学习（RLVR）已显著提升了大语言模型（LLM）的推理能力，但在有限的回溯预算下仍受限于低效的探索过程，导致复杂任务中采样成功率低且训练不稳定。我们发现，许多探索失败并非源于问题本身的难度，而是由少数几个引入干扰的提示词（prompt tokens）所引起。基于这一洞察，我们提出了“少噪声采样框架”（Less Noise Sampling, LENS）：首先通过识别并移除干扰性提示词进行提示净化，随后将净化过程中获得的成功回溯轨迹迁移至原始含噪提示上，用于监督策略优化，从而使模型在真实世界中存在噪声提示的环境下学会忽略这些干扰。实验结果表明，LENS显著优于GRPO方法，在性能上平均提升3.88%，收敛速度提升超过1.6倍。我们的工作凸显了剔除干扰性提示词在提升回溯效率中的关键作用，为RLVR研究提供了新的视角。"
  },
  {
    "date": "2026-01-29",
    "title": "TIDE: Tuning-Integrated Dynamic Evolution for LLM-Based Automated Heuristic Design",
    "authors": "Chentong Chen, Mengyuan Zhong, Ye Fan, Jialong Shi, Jianyong Sun",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21239v1",
    "source": "arXiv",
    "abstract": "Although Large Language Models have advanced Automated Heuristic Design, treating algorithm evolution as a monolithic text generation task overlooks the coupling between discrete algorithmic structures and continuous numerical parameters. Consequently, existing methods often discard promising algorithms due to uncalibrated constants and suffer from premature convergence resulting from simple similarity metrics. To address these limitations, we propose TIDE, a Tuning-Integrated Dynamic Evolution framework designed to decouple structural reasoning from parameter optimization. TIDE features a nested architecture where an outer parallel island model utilizes Tree Similarity Edit Distance to drive structural diversity, while an inner loop integrates LLM-based logic generation with a differential mutation operator for parameter tuning. Additionally, a UCB-based scheduler dynamically prioritizes high-yield prompt strategies to optimize resource allocation. Extensive experiments across nine combinatorial optimization problems demonstrate that TIDE discovers heuristics that significantly outperform state-of-the-art baselines in solution quality while achieving improved search efficiency and reduced computational costs.",
    "title_zh": "TIDE：面向基于大语言模型的自动启发式设计的集成调优动态演化方法",
    "abstract_zh": "尽管大型语言模型在自动化启发式设计方面取得了进展，但将算法演化视为一个单一的文本生成任务，忽略了离散算法结构与连续数值参数之间的耦合关系。因此，现有方法常常因常数未校准而丢弃有潜力的算法，并由于采用简单的相似性度量而导致过早收敛。为解决这些局限性，我们提出TIDE（Tuning-Integrated Dynamic Evolution）框架，该框架旨在将结构推理与参数优化解耦。TIDE采用嵌套式架构：外层采用并行岛屿模型，利用树形相似性编辑距离（Tree Similarity Edit Distance）促进结构多样性；内层则结合基于大语言模型的逻辑生成与差分变异算子，实现参数调优。此外，基于UCB的调度器可动态优先选择高产出的提示策略，以优化资源分配。在九个组合优化问题上的大量实验表明，TIDE发现的启发式方法在解的质量上显著优于当前最先进基线方法，同时实现了更高的搜索效率和更低的计算成本。"
  },
  {
    "date": "2026-01-29",
    "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models",
    "authors": "Hong Liu, Jiaqi Zhang, Chao Wang, Xing Hu, Linkun Lyu, Jiaqi Sun, Xurui Yang, Bo Wang, Fengcun Li, Yulei Qian, Lingtong Si, Yerui Sun, Rumei Li, Peng Pei, Yuchen Xie, Xunliang Cai",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21204v1",
    "source": "arXiv",
    "abstract": "While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.",
    "title_zh": "扩展嵌入优于扩展专家在语言模型中的表现",
    "abstract_zh": "尽管混合专家（Mixture-of-Experts, MoE）架构已成为大型语言模型中稀疏性扩展的标准方案，但其正面临收益递减和系统级瓶颈的挑战。在本工作中，我们探索了嵌入扩展作为一种强大且正交的稀疏性扩展维度。通过全面的分析与实验，我们识别出在特定场景下，嵌入扩展相较于专家扩展能够实现更优的帕累托前沿。我们系统地刻画了决定该方法有效性的关键架构因素——从参数预算分配到与模型宽度和深度之间的相互作用。此外，通过引入量身定制的系统优化技术以及推测性解码策略，我们成功将这种稀疏性转化为实际的推理加速效果。基于这些洞见，我们提出了 LongCat-Flash-Lite，一个从零训练的 685 亿参数模型，其中约 30 亿参数处于激活状态。尽管有超过 300 亿参数被分配给嵌入层，LongCat-Flash-Lite 不仅超越了同等参数规模的 MoE 基线模型，还在同类规模模型中展现出极强的竞争力，尤其在智能体任务和代码生成领域表现尤为突出。"
  },
  {
    "date": "2026-01-29",
    "title": "From Logic to Toolchains: An Empirical Study of Bugs in the TypeScript Ecosystem",
    "authors": "TianYi Tang, Saba Alimadadi, Nick Sumner",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21186v1",
    "source": "arXiv",
    "abstract": "TypeScript has rapidly become a popular language for modern web development, yet its effect on software faults remains poorly understood. This paper presents the first large-scale empirical study of bugs in real-world TypeScript projects. We analyze 633 bug reports from 16 popular open-source repositories to construct a taxonomy of fault types, quantify their prevalence, and relate them to project characteristics such as size, domain, and dependency composition. Our results reveal a fault landscape dominated not by logic or syntax errors but by tooling and configuration faults, API misuses, and asynchronous error-handling issues. We show that these categories correlate strongly with build complexity and dependency heterogeneity, indicating that modern failures often arise at integration and orchestration boundaries rather than within algorithmic logic. A longitudinal comparison with JavaScript studies shows that while static typing in TypeScript has reduced traditional runtime and type errors, it has shifted fragility toward build systems and toolchains. These findings offer new insight into how language design and ecosystem evolution reshape the fault profiles of large-scale software systems.",
    "title_zh": "从逻辑到工具链：对 TypeScript 生态系统中缺陷的实证研究",
    "abstract_zh": "TypeScript 已迅速成为现代 Web 开发中广受欢迎的语言，但其对软件缺陷的影响仍缺乏深入理解。本文首次对真实世界 TypeScript 项目中的缺陷进行了大规模实证研究。我们分析了来自 16 个流行开源仓库的 633 个缺陷报告，构建了一个缺陷类型的分类体系，量化了各类缺陷的出现频率，并将其与项目特征（如规模、领域和依赖项组成）相关联。研究结果揭示，当前的缺陷格局并非由逻辑或语法错误主导，而是主要集中在工具链与配置错误、API 使用不当以及异步错误处理问题上。我们发现这些类别与构建复杂性和依赖项异质性存在显著相关性，表明现代软件故障往往出现在集成与协调边界，而非算法逻辑内部。与 JavaScript 研究的纵向对比显示，尽管 TypeScript 的静态类型系统有效减少了传统的运行时错误和类型错误，但也导致系统的脆弱性向构建系统和工具链转移。这些发现为语言设计与生态系统演进如何重塑大型软件系统的缺陷特征提供了新的洞见。"
  },
  {
    "date": "2026-01-29",
    "title": "Output-Space Search: Targeting LLM Generations in a Frozen Encoder-Defined Output Space",
    "authors": "Tobias Materzok",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21169v1",
    "source": "arXiv",
    "abstract": "We introduce Output-Space Search (OS-Search), which turns LLM generation into endpoint search. An outer loop selects a target z* in a frozen encoder-defined 3D output space Z, and a retrieval-grounded policy trained with sequence-level RL generates outputs whose coordinates land near z* under standard autoregressive decoding. This enables parallel sweeps and black-box optimization in Z without path-dependent token/program search. On stories, sweeping Z (text) yields 3.1x higher LLM-scored diversity than prompt-chaining. On code, Bayesian optimization over Z (code) improves an objective withheld from the controller under matched inference budgets while preserving validity.",
    "title_zh": "输出空间搜索：针对冻结编码器定义的输出空间中的大语言模型生成",
    "abstract_zh": "我们提出了输出空间搜索（OS-Search），将大语言模型（LLM）的生成过程转化为端点搜索。外层循环在由固定编码器定义的三维输出空间 Z 中选择一个目标 z*，而基于检索的策略则通过序列级强化学习进行训练，生成在标准自回归解码下坐标接近 z* 的输出。这种方法实现了在 Z 空间中无需依赖路径的并行扫描与黑箱优化，避免了对 token 或程序路径的逐次搜索。在故事生成任务中，对 Z（文本）空间的扫描所获得的 LLM 评分多样性比提示链（prompt-chaining）高出 3.1 倍；在代码生成任务中，基于 Z（代码）空间的贝叶斯优化，在保持相同推理预算的前提下，提升了控制器无法访问的目标性能，同时保证了生成代码的有效性。"
  },
  {
    "date": "2026-01-29",
    "title": "The Path of Least Resistance: Guiding LLM Reasining Trajectories with Prefix Consensus",
    "authors": "Ishan Jindal, Sai Prashanth Akuthota, Jayant Taneja, Sachin Dev Sharma",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21494v1",
    "source": "arXiv",
    "abstract": "Large language models achieve strong reasoning performance, but inference strategies such as Self-Consistency (SC) are computationally expensive, as they fully expand all reasoning traces. We introduce PoLR (Path of Least Resistance), the first inference-time method to leverage prefix consistency for compute-efficient reasoning. PoLR clusters short prefixes of reasoning traces, identifies the dominant cluster, and expands all paths in that cluster, preserving the accuracy benefits of SC while substantially reducing token usage and latency. Our theoretical analysis, framed via mutual information and entropy, explains why early reasoning steps encode strong signals predictive of final correctness. Empirically, PoLR consistently matches or exceeds SC across GSM8K, MATH500, AIME24/25, and GPQA-DIAMOND, reducing token usage by up to 60% and wall-clock latency by up to 50%. Moreover, PoLR is fully complementary to adaptive inference methods (e.g., Adaptive Consistency, Early-Stopping SC) and can serve as a drop-in pre-filter, making SC substantially more efficient and scalable without requiring model fine-tuning.",
    "title_zh": "阻力最小之路：通过前缀共识引导大语言模型的推理轨迹",
    "abstract_zh": "大型语言模型在推理性能上表现强劲，但诸如自一致（Self-Consistency, SC）之类的推理策略计算开销巨大，因为它们需要完全展开所有的推理路径。我们提出了PoLR（Least Resistance Path），这是首个在推理阶段利用前缀一致性实现计算高效推理的方法。PoLR对推理路径的短前缀进行聚类，识别出主导簇，并仅扩展该簇中的所有路径，从而在保持SC带来的准确率优势的同时，显著降低token使用量和延迟。我们的理论分析基于互信息与熵的框架，解释了为何早期推理步骤蕴含强烈信号，可有效预测最终结果的正确性。实证结果表明，PoLR在GSM8K、MATH500、AIME24/25以及GPQA-DIAMOND等多个基准上始终达到或超越SC的表现，token消耗减少高达60%，实际运行时间降低最多达50%。此外，PoLR与自适应推理方法（如自适应一致性、早停式SC）完全兼容，可作为即插即用的预过滤器，使SC在无需模型微调的前提下实现显著更高的效率与可扩展性。"
  },
  {
    "date": "2026-01-29",
    "title": "Adaptive Confidence Gating in Multi-Agent Collaboration for Efficient and Optimized Code Generation",
    "authors": "Haoji Zhang, Yuzhe Li, Zhenqiang Liu, Chenyang Liu, Shenyang Zhang, Yi Zhou",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21469v1",
    "source": "arXiv",
    "abstract": "While Large Language Models (LLMs) have catalyzed breakthroughs in automated code generation, Small Language Models (SLMs) often encounter reasoning bottlenecks and failure loops when addressing complex logical requirements. To overcome these challenges, we propose DebateCoder, a multi-agent collaborative framework designed to improve the reasoning ability of SLMs (e.g., Pangu-1B) in resource-constrained environments. DebateCoder uses a structured role-playing protocol with three agents: User Agent (A_UA), Technical Agent (A_TA), and Quality Assurance Agent (A_QA). It also includes an Adaptive Confidence Gating mechanism with a 95% threshold to balance accuracy and inference efficiency. In addition, we introduce a multi-turn deliberation module and a reviewer-guided analytical debugging loop for orthogonal pre-generation debate and post-generation refinement. Experiments on HumanEval and MBPP show that DebateCoder achieves 70.12% Pass@1 on HumanEval, outperforming MapCoder while reducing API overhead by about 35%. These results indicate that collaborative protocols can mitigate limitations of small-parameter models and provide a scalable, efficient approach to high-quality automated software engineering.",
    "title_zh": "多智能体协作中自适应置信度门控的高效优化代码生成",
    "abstract_zh": "尽管大型语言模型（LLMs）在自动化代码生成方面取得了突破性进展，小型语言模型（SLMs）在处理复杂逻辑需求时仍常面临推理瓶颈和失败循环的问题。为克服这些挑战，我们提出 DebateCoder——一种多智能体协作框架，旨在提升资源受限环境下小型语言模型（如 Pangu-1B）的推理能力。DebateCoder 采用结构化的角色扮演协议，包含三个智能体：用户代理（A_UA）、技术代理（A_TA）和质量保障代理（A_QA）。同时，系统引入自适应置信度门控机制，设定95%的阈值，以在准确率与推理效率之间取得平衡。此外，我们设计了多轮讨论模块以及由评审引导的分析式调试循环，分别实现生成前的正交辩论与生成后的优化精炼。在 HumanEval 和 MBPP 数据集上的实验表明，DebateCoder 在 HumanEval 上实现了 70.12% 的 Pass@1 得分，优于 MapCoder，且 API 调用开销降低了约 35%。结果表明，协作式协议能够有效缓解小参数模型的局限性，为高质量自动化软件工程提供了一种可扩展、高效率的解决方案。"
  },
  {
    "date": "2026-01-29",
    "title": "System 1&2 Synergy via Dynamic Model Interpolation",
    "authors": "Chenxu Yang, Qingyi Si, Chong Tian, Xiyu Liu, Dingyu Yao, Chuanyu Qin, Zheng Lin, Weiping Wang, Jiaqi Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21414v1",
    "source": "arXiv",
    "abstract": "Training a unified language model that adapts between intuitive System 1 and deliberative System 2 remains challenging due to interference between their cognitive modes. Recent studies have thus pursued making System 2 models more efficient. However, these approaches focused on output control, limiting what models produce. We argue that this paradigm is misaligned: output length is merely a symptom of the model's cognitive configuration, not the root cause. In this work, we shift the focus to capability control, which modulates \\textit{how models think} rather than \\textit{what they produce}. To realize this, we leverage existing Instruct and Thinking checkpoints through dynamic parameter interpolation, without additional training. Our pilot study establishes that linear interpolation yields a convex, monotonic Pareto frontier, underpinned by representation continuity and structural connectivity. Building on this, we propose \\textbf{DAMI} (\\textbf{D}yn\\textbf{A}mic \\textbf{M}odel \\textbf{I}nterpolation), a framework that estimates a query-specific Reasoning Intensity $λ(q)$ to configure cognitive depth. For training-based estimation, we develop a preference learning method encoding accuracy and efficiency criteria. For zero-shot deployment, we introduce a confidence-based method leveraging inter-model cognitive discrepancy. Experiments on five mathematical reasoning benchmarks demonstrate that DAMI achieves higher accuracy than the Thinking model while remaining efficient, effectively combining the efficiency of System 1 with the reasoning depth of System 2.",
    "title_zh": "系统1与系统2通过动态模型插值实现协同",
    "abstract_zh": "训练一个能够灵活在直觉式系统1与深思熟虑式系统2之间切换的统一语言模型，仍然面临认知模式间相互干扰的挑战。近期研究致力于提升系统2模型的效率，但这些方法主要聚焦于输出控制，仅限制了模型生成的内容。我们认为这一范式存在偏差：输出长度只是模型认知配置的表象，而非根本原因。在本工作中，我们转向能力控制——即调节模型“如何思考”，而非“产生什么”。为实现这一目标，我们通过动态参数插值的方式利用现有的指令（Instruct）与思维（Thinking）检查点，无需额外训练。我们的初步研究表明，线性插值可生成凸且单调的帕累托前沿，其背后是表示连续性和结构连通性的支撑。基于此，我们提出**DAMI**（**D**ynamic **A**gentic **M**odel **I**nterpolation）框架，通过估计与查询相关的推理强度 $λ(q)$ 来配置认知深度。针对基于训练的估计，我们设计了一种偏好学习方法，综合考虑准确率与效率标准；对于零样本部署，我们引入一种基于置信度的方法，利用不同模型间的认知差异进行推断。在五个数学推理基准上的实验表明，DAMI在保持高效的同时，实现了比思维模型更高的准确率，成功融合了系统1的高效性与系统2的推理深度。"
  },
  {
    "date": "2026-01-29",
    "title": "Statsformer: Validated Ensemble Learning with LLM-Derived Semantic Priors",
    "authors": "Erica Zhang, Naomi Sagan, Danny Tse, Fangzhao Zhang, Mert Pilanci, Jose Blanchet",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21410v1",
    "source": "arXiv",
    "abstract": "We introduce Statsformer, a principled framework for integrating large language model (LLM)-derived knowledge into supervised statistical learning. Existing approaches are limited in adaptability and scope: they either inject LLM guidance as an unvalidated heuristic, which is sensitive to LLM hallucination, or embed semantic information within a single fixed learner. Statsformer overcomes both limitations through a guardrailed ensemble architecture. We embed LLM-derived feature priors within an ensemble of linear and nonlinear learners, adaptively calibrating their influence via cross-validation. This design yields a flexible system with an oracle-style guarantee that it performs no worse than any convex combination of its in-library base learners, up to statistical error. Empirically, informative priors yield consistent performance improvements, while uninformative or misspecified LLM guidance is automatically downweighted, mitigating the impact of hallucinations across a diverse range of prediction tasks.",
    "title_zh": "Statsformer：基于大语言模型生成语义先验的验证集成学习",
    "abstract_zh": "我们提出Statsformer，这是一种将大语言模型（LLM）生成的知识系统性地融入监督统计学习的原理性框架。现有方法在适应性和应用范围上存在局限：要么以未经验证的启发式方式注入LLM指导，容易受到LLM幻觉的影响；要么仅将语义信息嵌入单一固定的学习器中。Statsformer通过一种受控的集成架构克服了上述两种局限。我们将LLM生成的特征先验信息嵌入由线性和非线性学习器组成的集成模型中，并通过交叉验证自适应地校准各学习器的影响。这种设计构建了一个灵活的系统，具备类似“预言机”的保证——其性能不会劣于其内部基学习器任意凸组合的表现，最多仅受统计误差影响。实证结果表明，具有信息量的先验可带来持续的性能提升，而无信息或错误指定的LLM指导则会被自动弱化，从而有效缓解幻觉问题，在多种预测任务中均表现出稳健性。"
  },
  {
    "date": "2026-01-29",
    "title": "Self-Improving Pretraining: using post-trained models to pretrain better models",
    "authors": "Ellen Xiaoqing Tan, Shehzaad Dhuliawala, Jing Xu, Ping Yu, Sainbayar Sukhbaatar, Jason Weston, Olga Golovneva",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21343v1",
    "source": "arXiv",
    "abstract": "Ensuring safety, factuality and overall quality in the generations of large language models is a critical challenge, especially as these models are increasingly deployed in real-world applications. The prevailing approach to addressing these issues involves collecting expensive, carefully curated datasets and applying multiple stages of fine-tuning and alignment. However, even this complex pipeline cannot guarantee the correction of patterns learned during pretraining. Therefore, addressing these issues during pretraining is crucial, as it shapes a model's core behaviors and prevents unsafe or hallucinated outputs from becoming deeply embedded. To tackle this issue, we introduce a new pretraining method that streams documents and uses reinforcement learning (RL) to improve the next K generated tokens at each step. A strong, post-trained model judges candidate generations -- including model rollouts, the original suffix, and a rewritten suffix -- for quality, safety, and factuality. Early in training, the process relies on the original and rewritten suffixes; as the model improves, RL rewards high-quality rollouts. This approach builds higher quality, safer, and more factual models from the ground up. In experiments, our method gives 36.2% and 18.5% relative improvements over standard pretraining in terms of factuality and safety, and up to 86.3% win rate improvements in overall generation quality.",
    "title_zh": "自我提升的预训练：利用后训练模型来预训练更优的模型",
    "abstract_zh": "确保大型语言模型生成内容的安全性、真实性和整体质量，是一个至关重要的挑战，尤其是在这些模型日益应用于现实场景的背景下。目前主流的解决方法是收集昂贵且精心筛选的数据集，并通过多阶段微调与对齐来应对这些问题。然而，即使采用如此复杂的流程，也无法完全纠正预训练过程中所习得的错误模式。因此，在预训练阶段就解决这些问题至关重要，因为这将决定模型的核心行为，防止不安全或虚构输出在模型中根深蒂固。\n\n为应对这一挑战，我们提出一种新的预训练方法：通过流式处理文档，并利用强化学习（RL）在每一步优化接下来 K 个生成词元的质量。一个经过充分训练的强模型负责评估候选生成结果——包括模型的推演路径、原始后缀以及重写后的后缀——以判断其质量、安全性和真实性。在训练初期，系统主要依赖原始后缀和重写后缀进行反馈；随着模型性能提升，强化学习开始奖励高质量的推演路径。这种方法从源头构建出更高质量、更安全、更真实的语言模型。\n\n实验表明，相较于标准预训练方法，该方法在事实性方面提升了36.2%，在安全性方面提升了18.5%，在整体生成质量上的胜率最高提升了86.3%。"
  },
  {
    "date": "2026-01-29",
    "title": "Detecting Multiple Semantic Concerns in Tangled Code Commits",
    "authors": "Beomsu Koh, Neil Walkinshaw, Donghwan Shin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21298v1",
    "source": "arXiv",
    "abstract": "Code commits in a version control system (e.g., Git) should be atomic, i.e., focused on a single goal, such as adding a feature or fixing a bug. In practice, however, developers often bundle multiple concerns into tangled commits, obscuring intent and complicating maintenance. Recent studies have used Conventional Commits Specification (CCS) and Language Models (LMs) to capture commit intent, demonstrating that Small Language Models (SLMs) can approach the performance of Large Language Models (LLMs) while maintaining efficiency and privacy. However, they do not address tangled commits involving multiple concerns, leaving the feasibility of using LMs for multi-concern detection unresolved. In this paper, we frame multi-concern detection in tangled commits as a multi-label classification problem and construct a controlled dataset of artificially tangled commits based on real-world data. We then present an empirical study using SLMs to detect multiple semantic concerns in tangled commits, examining the effects of fine-tuning, concern count, commit-message inclusion, and header-preserving truncation under practical token-budget limits. Our results show that a fine-tuned 14B-parameter SLM is competitive with a state-of-the-art LLM for single-concern commits and remains usable for up to three concerns. In particular, including commit messages improves detection accuracy by up to 44% (in terms of Hamming Loss) with negligible latency overhead, establishing them as important semantic cues.",
    "title_zh": "检测纠缠代码提交中的多个语义关切",
    "abstract_zh": "版本控制系统（如 Git）中的代码提交应具备原子性，即每个提交应聚焦于单一目标，例如添加功能或修复缺陷。然而，在实际开发中，开发者常常将多个不同意图的更改合并到同一个提交中，导致提交内容混乱，难以理解其真实目的，从而增加维护难度。近期研究通过采用常规提交规范（Conventional Commits Specification, CCS）与语言模型（Language Models, LMs），尝试捕捉提交的语义意图，结果表明小型语言模型（Small Language Models, SLMs）在性能上可接近大型语言模型（Large Language Models, LLMs），同时保持更高的效率和更好的隐私保护。然而，这些研究并未解决涉及多重语义关切的“纠缠提交”（tangled commits）问题，因此利用语言模型进行多关切检测的可行性仍不明确。\n\n本文将纠缠提交中的多关切检测问题建模为多标签分类任务，并基于真实世界数据构建了一个经过控制的人工纠缠提交数据集。随后，我们开展了一项实证研究，使用 SLMs 检测纠缠提交中的多个语义关切，系统考察了微调策略、关切数量、是否包含提交信息（commit message）、以及保留头部的截断方法在实际 token 预算限制下的影响。实验结果表明，经过微调的 140 亿参数 SLM 在单关切提交上的表现已可与当前最先进的 LLM 相媲美，并且在处理最多三个关切时仍具有可用性。特别地，引入提交信息可使检测准确率提升高达 44%（以 Hamming Loss 衡量），且几乎不增加延迟开销，证明提交信息是极具价值的语义线索。"
  },
  {
    "date": "2026-01-29",
    "title": "EGAM: Extended Graph Attention Model for Solving Routing Problems",
    "authors": "Licheng Wang, Yuzi Yan, Mingtao Huang, Yuan Shen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21281v1",
    "source": "arXiv",
    "abstract": "Neural combinatorial optimization (NCO) solvers, implemented with graph neural networks (GNNs), have introduced new approaches for solving routing problems. Trained with reinforcement learning (RL), the state-of-the-art graph attention model (GAM) achieves near-optimal solutions without requiring expert knowledge or labeled data. In this work, we generalize the existing graph attention mechanism and propose the extended graph attention model (EGAM). Our model utilizes multi-head dot-product attention to update both node and edge embeddings, addressing the limitations of the conventional GAM, which considers only node features. We employ an autoregressive encoder-decoder architecture and train it with policy gradient algorithms that incorporate a specially designed baseline. Experiments show that EGAM matches or outperforms existing methods across various routing problems. Notably, the proposed model demonstrates exceptional performance on highly constrained problems, highlighting its efficiency in handling complex graph structures.",
    "title_zh": "EGAM：用于解决路由问题的扩展图注意力模型",
    "abstract_zh": "基于图神经网络（GNN）实现的神经组合优化（NCO）求解器为解决路由问题带来了新的方法。通过强化学习（RL）训练，当前最先进的图注意力模型（GAM）能够在无需专家知识或标注数据的情况下，获得接近最优的解。在本研究中，我们对现有的图注意力机制进行了扩展，提出了一种改进的图注意力模型（EGAM）。该模型采用多头点积注意力机制，同时更新节点和边的嵌入表示，克服了传统GAM仅考虑节点特征的局限性。我们采用自回归编码器-解码器架构，并结合专门设计的基线项，使用策略梯度算法进行训练。实验结果表明，EGAM在多种路由问题上表现与现有方法相当或更优。尤其值得注意的是，该模型在高度约束的问题上展现出卓越性能，凸显其在处理复杂图结构方面的高效性。"
  },
  {
    "date": "2026-01-29",
    "title": "TBDFiltering: Sample-Efficient Tree-Based Data Filtering",
    "authors": "Robert Istvan Busa-Fekete, Julian Zimmert, Anne Xiangyi Zheng, Claudio Gentile, Andras Gyorgy",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.22016v1",
    "source": "arXiv",
    "abstract": "The quality of machine learning models depends heavily on their training data. Selecting high-quality, diverse training sets for large language models (LLMs) is a difficult task, due to the lack of cheap and reliable quality metrics. While querying existing LLMs for document quality is common, this is not scalable to the large number (billions) of documents used in training. Instead, practitioners often use classifiers trained on sparse quality signals. In this paper, we propose a text-embedding-based hierarchical clustering approach that adaptively selects the documents to be evaluated by the LLM to estimate cluster quality. We prove that our method is query efficient: under the assumption that the hierarchical clustering contains a subtree such that each leaf cluster in the tree is pure enough (i.e., it mostly contains either only good or only bad documents), with high probability, the method can correctly predict the quality of each document after querying a small number of documents. The number of such documents is proportional to the size of the smallest subtree with (almost) pure leaves, without the algorithm knowing this subtree in advance. Furthermore, in a comprehensive experimental study, we demonstrate the benefits of our algorithm compared to other classifier-based filtering methods.",
    "title_zh": "TBDFiltering：高效采样的基于树的数据过滤",
    "abstract_zh": "机器学习模型的质量在很大程度上取决于其训练数据。为大型语言模型（LLMs）选择高质量且多样化的训练数据集是一项艰巨的任务，原因在于缺乏廉价且可靠的评估质量指标。虽然通过查询现有的LLM来评估文档质量是一种常见做法，但这种方法无法扩展到训练中使用的数十亿份文档。因此，从业者通常依赖于基于稀疏质量信号训练的分类器。本文提出了一种基于文本嵌入的分层聚类方法，该方法能够自适应地选择由LLM评估以估计聚类质量的文档。我们证明了该方法具有查询效率：在假设分层聚类中存在一个子树，其中每个叶节点聚类都足够纯净（即，主要包含优质或劣质文档）的前提下，该方法以高概率仅需查询少量文档即可正确预测每篇文档的质量。所需查询的文档数量与最小纯叶节点子树的规模成正比，而算法在事前并不需要知道该子树的具体信息。此外，在一项全面的实验研究中，我们展示了该算法相较于其他基于分类器的过滤方法所具有的显著优势。"
  },
  {
    "date": "2026-01-29",
    "title": "Language-based Trial and Error Falls Behind in the Era of Experience",
    "authors": "Haoyu Wang, Guozheng Ma, Shugang Cui, Yilun Kong, Haotian Luo, Li Shen, Mengya Gao, Yichao Wu, Xiaogang Wang, Dacheng Tao",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21754v1",
    "source": "arXiv",
    "abstract": "While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight \"scouts\" (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.",
    "title_zh": "基于语言的试错法在经验时代落后了",
    "abstract_zh": "尽管大型语言模型（LLMs）在基于语言的代理任务中表现出色，但其在未见过的非语言环境（如符号或空间任务）中的应用仍存在局限。以往研究将这一性能差距归因于预训练分布与测试分布之间的不匹配。在本工作中，我们指出主要瓶颈在于探索成本过高：掌握这些任务需要大量的试错过程，而这种高维语义空间中的持续探索对参数量庞大的LLM而言在计算上是不可持续的。为解决该问题，我们提出SCOUT（Sub-Scale Collaboration On Unseen Tasks）——一种新型框架，通过将探索与利用解耦来提升效率。我们采用轻量级“侦察兵”（例如小型MLP）以远超LLM的速度和规模探测环境动态。收集到的轨迹数据用于通过监督微调（SFT）引导LLM，随后通过多轮强化学习（RL）激活其潜在的世界知识。实验表明，SCOUT使Qwen2.5-3B-Instruct模型平均得分达到0.86，显著优于多个专有模型，包括Gemini-2.5-Pro（得分为0.60），同时节省了约60%的GPU计算时长。"
  },
  {
    "date": "2026-01-29",
    "title": "E-mem: Multi-agent based Episodic Context Reconstruction for LLM Agent Memory",
    "authors": "Kaixiang Wang, Yidan Lin, Jiong Lou, Zhaojiacheng Zhou, Bunyod Suvonov, Jie Li",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21714v1",
    "source": "arXiv",
    "abstract": "The evolution of Large Language Model (LLM) agents towards System~2 reasoning, characterized by deliberative, high-precision problem-solving, requires maintaining rigorous logical integrity over extended horizons. However, prevalent memory preprocessing paradigms suffer from destructive de-contextualization. By compressing complex sequential dependencies into pre-defined structures (e.g., embeddings or graphs), these methods sever the contextual integrity essential for deep reasoning. To address this, we propose E-mem, a framework shifting from Memory Preprocessing to Episodic Context Reconstruction. Inspired by biological engrams, E-mem employs a heterogeneous hierarchical architecture where multiple assistant agents maintain uncompressed memory contexts, while a central master agent orchestrates global planning. Unlike passive retrieval, our mechanism empowers assistants to locally reason within activated segments, extracting context-aware evidence before aggregation. Evaluations on the LoCoMo benchmark demonstrate that E-mem achieves over 54\\% F1, surpassing the state-of-the-art GAM by 7.75\\%, while reducing token cost by over 70\\%.",
    "title_zh": "E-mem：基于多智能体的 episodic 上下文重建用于大语言模型智能体记忆",
    "abstract_zh": "大型语言模型（LLM）代理向系统2推理的演进，即具备深思熟虑、高精度问题解决能力的推理模式，要求在较长的时间跨度内保持严格的逻辑完整性。然而，当前普遍采用的记忆预处理范式存在破坏性去上下文化的问题。通过将复杂的序列依赖关系压缩为预定义结构（如嵌入向量或图结构），这些方法割裂了深度推理所必需的上下文连贯性。为解决这一问题，我们提出E-mem框架，实现从“记忆预处理”到“情景上下文重建”的范式转变。受生物记忆痕迹（engrams）的启发，E-mem采用异构分层架构：多个辅助代理负责维护未压缩的记忆上下文，而中央主代理则统筹全局规划。与被动检索不同，我们的机制使辅助代理能够在激活的上下文片段中进行局部推理，提取具有上下文感知性的证据，再进行聚合。在LoCoMo基准测试中的评估表明，E-mem的F1得分超过54%，较当前最先进的GAM模型提升7.75%，同时token消耗降低超过70%。"
  },
  {
    "date": "2026-01-29",
    "title": "Do Not Waste Your Rollouts: Recycling Search Experience for Efficient Test-Time Scaling",
    "authors": "Xinglin Wang, Jiayi Shi, Shaoxiong Feng, Peiwen Yuan, Yiwei Li, Yueqi Zhang, Chuyi Tan, Ji Zhang, Boyuan Pan, Yao Hu, Kan Li",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21684v1",
    "source": "arXiv",
    "abstract": "Test-Time Scaling enhances the reasoning capabilities of Large Language Models by allocating additional inference compute to broaden the exploration of the solution space. However, existing search strategies typically treat rollouts as disposable samples, where valuable intermediate insights are effectively discarded after each trial. This systemic memorylessness leads to massive computational redundancy, as models repeatedly re-derive discovered conclusions and revisit known dead ends across extensive attempts. To bridge this gap, we propose \\textbf{Recycling Search Experience (RSE)}, a self-guided, training-free strategy that turns test-time search from a series of isolated trials into a cumulative process. By actively distilling raw trajectories into a shared experience bank, RSE enables positive recycling of intermediate conclusions to shortcut redundant derivations and negative recycling of failure patterns to prune encountered dead ends. Theoretically, we provide an analysis that formalizes the efficiency gains of RSE, validating its advantage over independent sampling in solving complex reasoning tasks. Empirically, extensive experiments on HMMT24, HMMT25, IMO-Bench, and HLE show that RSE consistently outperforms strong baselines with comparable computational cost, achieving state-of-the-art scaling efficiency.",
    "title_zh": "不要浪费你的部署机会：通过复用搜索体验实现高效的测试时扩展",
    "abstract_zh": "测试时缩放（Test-Time Scaling）通过分配额外的推理计算资源，扩展了大语言模型对解空间的探索范围，从而增强了其推理能力。然而，现有的搜索策略通常将每次推演（rollout）视为一次性消耗的样本，导致宝贵的中间洞察在每次尝试后被彻底丢弃。这种系统性的“无记忆”特性造成了巨大的计算冗余：模型在大量尝试中反复重新推导已发现的结论，并重复踏入已知的死胡同。\n\n为弥合这一差距，我们提出**回收搜索经验（Recycling Search Experience, RSE）**——一种自引导、无需训练的策略，将测试时的搜索过程从一系列孤立的试验转变为累积式的学习过程。RSE通过主动将原始轨迹提炼为共享的经验库，实现正向回收：利用中间结论加速冗余推导的跳过；以及负向回收：识别并剔除已知的失败模式，避免重蹈覆辙。\n\n理论上，我们提供了对RSE效率增益的形式化分析，证明其在解决复杂推理任务时相较于独立采样的优势。实证方面，在HMMT24、HMMT25、IMO-Bench和HLE等多个基准上的大量实验表明，RSE在与基线相当的计算成本下，持续超越多个强基线方法，实现了当前最优的缩放效率。"
  },
  {
    "date": "2026-01-29",
    "title": "Breaking the Overscaling Curse: Thinking Parallelism Before Parallel Thinking",
    "authors": "Yiming Wang, Zhuosheng Zhang, Rui Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21619v1",
    "source": "arXiv",
    "abstract": "Parallel thinking enhances LLM reasoning by multi-path sampling and aggregation. In system-level evaluations, a global parallelism level N is allocated to all samples, typically set large to maximize overall dataset accuracy. However, due to sample heterogeneity, some samples can achieve comparable performance with a smaller N'< N, causing budget redundancy. This incompatibility between system-level efficacy and sample-level efficiency constitutes the overscaling curse. In this paper, we formalize and quantify the overscaling curse, showing its universality and severity in practice, and analyze its trigger mechanism. We then propose a lightweight method, T2, to break the overscaling curse, which utilizes latent representations to estimate the optimal parallelism level for each sample before decoding. Experiments show that T2 significantly reduces cost while maintaining comparable performance, enabling more efficient parallel thinking.",
    "title_zh": "打破过度扩展的诅咒：在并行思考之前先思考并行性",
    "abstract_zh": "并行思维通过多路径采样与聚合提升了大语言模型的推理能力。在系统级评估中，所有样本均被分配一个全局并行度 N，通常设置为较大值以最大化整体数据集的准确率。然而，由于样本之间的异质性，部分样本在较小的并行度 N' < N 下即可达到相近的性能，导致计算资源的冗余浪费。这种系统级效能与样本级效率之间的不匹配构成了“过度扩展诅咒”。本文首次对这一现象进行了形式化和量化分析，揭示了其在实际应用中的普遍性和严重性，并深入探讨了其触发机制。随后，我们提出一种轻量级方法 T2，以打破过度扩展诅咒：该方法利用潜在表示，在解码前估计每个样本的最优并行度。实验结果表明，T2 在显著降低计算成本的同时保持了相当的性能，从而实现了更高效的并行思维。"
  },
  {
    "date": "2026-01-29",
    "title": "Semantic Content Determines Algorithmic Performance",
    "authors": "Martiño Ríos-García, Nawaf Alampara, Kevin Maik Jablonka",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21618v1",
    "source": "arXiv",
    "abstract": "Counting should not depend on what is being counted; more generally, any algorithm's behavior should be invariant to the semantic content of its arguments. We introduce WhatCounts to test this property in isolation. Unlike prior work that conflates semantic sensitivity with reasoning complexity or prompt variation, WhatCounts is atomic: count items in an unambiguous, delimited list with no duplicates, distractors, or reasoning steps for different semantic types. Frontier LLMs show over 40% accuracy variation depending solely on what is being counted - cities versus chemicals, names versus symbols. Controlled ablations rule out confounds. The gap is semantic, and it shifts unpredictably with small amounts of unrelated fine-tuning. LLMs do not implement algorithms; they approximate them, and the approximation is argument-dependent. As we show with an agentic example, this has implications beyond counting: any LLM function may carry hidden dependencies on the meaning of its inputs.",
    "title_zh": "语义内容决定算法性能",
    "abstract_zh": "计数不应依赖于被计数对象的性质；更一般地，任何算法的行为都应与其参数的语义内容无关。我们提出了 WhatCounts 来单独测试这一特性。与以往将语义敏感性与推理复杂度或提示变化混淆的研究不同，WhatCounts 是原子性的：在清晰、明确分隔且无重复项、干扰项或推理步骤的列表中对项目进行计数，且不区分不同的语义类型。前沿大语言模型在仅因计数对象不同（如城市 vs 化学物质，名字 vs 符号）时，准确率差异超过 40%。通过受控的消融实验排除了混杂因素。这种差距本质上是语义相关的，并且在引入少量无关的微调后会不可预测地发生变化。大语言模型并未真正实现算法，而只是对算法的近似，且这种近似是依赖于输入参数的。正如我们通过一个代理型示例所展示的，这一问题的影响远不止于计数：任何大语言模型的功能都可能隐含地依赖于其输入的语义含义。"
  },
  {
    "date": "2026-01-29",
    "title": "Is My RPC Response Reliable? Detecting RPC Bugs in Ethereum Blockchain Client under Context",
    "authors": "Zhijie Zhong, Yuhong Nan, Mingxi Ye, Qing Xue, Jiashui Wang, Xinlei Ying, Long Liu, Zibin Zheng",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21593v1",
    "source": "arXiv",
    "abstract": "Blockchain clients are fundamental software for running blockchain nodes. They provide users with various RPC (Remote Procedure Call) interfaces to interact with the blockchain. These RPC methods are expected to follow the same specification across different blockchain nodes, providing users with seamless interaction. However, there have been continuous reports on various RPC bugs that can cause unexpected responses or even Denial of Service weakness. Existing studies on blockchain RPC bug detection mainly focus on generating the RPC method calls for testing blockchain clients. However, a wide range of the reported RPC bugs are triggered in various blockchain contexts. To the best of our knowledge, little attention is paid to generating proper contexts that can trigger these context-dependent RPC bugs. In this work, we propose EthCRAFT, a Context-aware RPC Analysis and Fuzzing Tool for client RPC bug detection. EthCRAFT first proposes to explore the state transition program space of blockchain clients and generate various transactions to construct the context. EthCRAFT then designs a context-aware RPC method call generation method to send RPC calls to the blockchain clients. The responses of 5 different client implementations are used as cross-referring oracles to detect the RPC bugs. We evaluate EthCRAFT on real-world RPC bugs collected from the GitHub issues of Ethereum client implementations. Experiment results show that EthCRAFT outperforms existing client RPC detectors by detecting more RPC bugs. Moreover, EthCRAFT has found six new bugs in major Ethereum clients and reported them to the developers. One of the bug fixes has been written into breaking changes in the client's updates. Three of our bug reports have been offered a vulnerability bounty by the Ethereum Foundation.",
    "title_zh": "我的RPC响应可靠吗？在上下文环境下检测以太坊区块链客户端中的RPC错误",
    "abstract_zh": "区块链客户端是运行区块链节点的基础软件，为用户提供多种RPC（远程过程调用）接口以与区块链进行交互。这些RPC方法本应遵循跨不同区块链节点一致的规范，从而实现用户无缝交互。然而，持续有报告指出各类RPC漏洞，可能导致意外响应甚至引发拒绝服务（DoS）攻击。现有的区块链RPC漏洞检测研究主要集中在生成RPC方法调用以测试区块链客户端。然而，大量已报告的RPC漏洞实际上是在特定区块链上下文中被触发的。据我们所知，目前很少有工作关注于生成能够触发这些上下文依赖型RPC漏洞的适当上下文。在本研究中，我们提出了EthCRAFT——一种面向客户端RPC漏洞检测的上下文感知RPC分析与模糊测试工具。EthCRAFT首先提出探索区块链客户端的状态转换程序空间，并生成多种交易以构建上下文；随后设计了一种上下文感知的RPC方法调用生成机制，向区块链客户端发送RPC请求。我们利用5种不同客户端实现的响应作为交叉验证的“预言机”来检测RPC漏洞。我们在从以太坊客户端GitHub问题中收集的真实世界RPC漏洞上对EthCRAFT进行了评估。实验结果表明，EthCRAFT在检测RPC漏洞方面优于现有客户端RPC检测工具。此外，EthCRAFT在主流以太坊客户端中发现了6个新漏洞，并已提交给开发者。其中一个漏洞修复已被纳入客户端更新中的“破坏性变更”说明。另有三个漏洞报告获得了以太坊基金会提供的漏洞赏金。"
  },
  {
    "date": "2026-01-29",
    "title": "Shaping capabilities with token-level data filtering",
    "authors": "Neil Rathi, Alec Radford",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21571v1",
    "source": "arXiv",
    "abstract": "Current approaches to reducing undesired capabilities in language models are largely post hoc, and can thus be easily bypassed by adversaries. A natural alternative is to shape capabilities during pretraining itself. On the proxy task of removing medical capabilities, we show that the simple intervention of filtering pretraining data is highly effective, robust, and inexpensive at scale. Inspired by work on data attribution, we show that filtering tokens is more effective than filtering documents, achieving the same hit to undesired capabilities at a lower cost to benign ones. Training models spanning two orders of magnitude, we then demonstrate that filtering gets more effective with scale: for our largest models, token filtering leads to a 7000x compute slowdown on the forget domain. We also show that models trained with token filtering can still be aligned on the forget domain. Along the way, we introduce a methodology for labeling tokens with sparse autoencoders and distilling cheap, high-quality classifiers. We also demonstrate that filtering can be robust to noisy labels with sufficient pretraining compute.",
    "title_zh": "通过令牌级数据过滤来塑造能力",
    "abstract_zh": "当前减少语言模型中不希望具备能力的方法大多是事后的，因此很容易被攻击者绕过。一个自然的替代方案是在预训练阶段就主动塑造模型的能力。在移除医疗相关能力这一代理任务上，我们发现，仅通过过滤预训练数据这一简单干预措施，就能实现高效、稳健且可大规模扩展的效果。受数据归因研究的启发，我们进一步证明，相较于过滤文档，过滤词元（tokens）更为有效：在对不良能力造成相同影响的同时，对良性能力的损害更小。我们在涵盖两个数量级规模的模型上进行训练，结果表明，随着模型规模的增大，过滤策略的效果也愈发显著：对于最大规模的模型，词元过滤在“遗忘”领域导致了7000倍的计算延迟。此外，我们还证明，经过词元过滤训练的模型仍可在“遗忘”领域保持对齐。在此过程中，我们提出了一种使用稀疏自编码器标注词元的方法，并开发出低成本、高质量的分类器蒸馏技术。同时，我们还展示了当预训练计算资源充足时，过滤方法对噪声标签具有较强的鲁棒性。"
  },
  {
    "date": "2026-01-29",
    "title": "ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas",
    "authors": "Xiaoyu Tian, Haotian Wang, Shuaiting Chen, Hao Zhou, Kaichi Yu, Yudian Zhang, Jade Ouyang, Junxi Yin, Jiong Chen, Baoyan Guo, Lei Zhang, Junjie Tao, Yuansheng Song, Ming Cui, Chengwei Liu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21558v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, a fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning. ASTRA integrates two complementary components. First, a pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed question-answer traces into independent, code-executable, and rule-verifiable environments, enabling deterministic multi-turn RL. Based on this method, we develop a unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra.",
    "title_zh": "Astra：智能体轨迹与强化领域自动化生成",
    "abstract_zh": "大型语言模型（LLMs）在多步骤决策任务中越来越多地被用作工具增强型智能体，然而训练鲁棒的工具使用智能体仍面临诸多挑战。现有方法仍需人工干预，依赖不可验证的模拟环境，仅依赖监督微调（SFT）或强化学习（RL），且难以实现稳定、长周期、多轮次的学习。为解决这些问题，我们提出ASTRA——一种通过可扩展数据合成与可验证强化学习实现全自动端到端训练工具增强型语言模型智能体的框架。ASTRA集成了两个互补组件：首先，一个利用工具调用图静态拓扑结构生成多样化、结构化轨迹的流水线，从而赋予智能体广泛且可迁移的工具使用能力；其次，一种环境合成框架，能够捕捉人类语义推理的丰富组合结构，将分解后的问答轨迹转化为独立、可执行代码且规则可验证的环境，从而支持确定性的多轮强化学习。基于该方法，我们构建了一种统一的训练范式，将SFT与在线强化学习相结合，采用轨迹级奖励来平衡任务完成度与交互效率。在多个代理工具使用基准上的实验表明，经ASTRA训练的模型在相近规模下达到当前最优性能，接近闭源系统水平，同时保持了核心推理能力。相关完整流程、环境及训练好的模型已开源至 https://github.com/LianjiaTech/astra。"
  },
  {
    "date": "2026-01-29",
    "title": "ARGORA: Orchestrated Argumentation for Causally Grounded LLM Reasoning and Decision Making",
    "authors": "Youngjin Jin, Hanna Kim, Kwanwoo Kim, Chanhee Lee, Seungwon Shin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21533v1",
    "source": "arXiv",
    "abstract": "Existing multi-expert LLM systems gather diverse perspectives but combine them through simple aggregation, obscuring which arguments drove the final decision. We introduce ARGORA, a framework that organizes multi-expert discussions into explicit argumentation graphs showing which arguments support or attack each other. By casting these graphs as causal models, ARGORA can systematically remove individual arguments and recompute outcomes, identifying which reasoning chains were necessary and whether decisions would change under targeted modifications. We further introduce a correction mechanism that aligns internal reasoning with external judgments when they disagree. Across diverse benchmarks and an open-ended use case, ARGORA achieves competitive accuracy and demonstrates corrective behavior: when experts initially disagree, the framework resolves disputes toward correct answers more often than it introduces new errors, while providing causal diagnostics of decisive arguments.",
    "title_zh": "ARGORA：用于因果基础的大型语言模型推理与决策的协同论证",
    "abstract_zh": "现有的多专家大模型系统虽然能够汇集多种观点，但通常通过简单的聚合方式融合这些观点，导致难以追溯最终决策背后的推理依据。我们提出了ARGORA框架，将多专家的讨论组织为显式的论证图，清晰展示各论点之间的支持或反驳关系。通过将这些论证图建模为因果模型，ARGORA能够系统性地移除单个论点并重新计算结果，从而识别出哪些推理链条是关键性的，并判断在特定修改下决策是否会改变。此外，我们还引入了一种校正机制，在内部推理与外部判断不一致时，使内部推理与正确答案对齐。在多种基准测试及一个开放性应用场景中，ARGORA表现出具有竞争力的准确性，并展现出良好的修正能力：当专家初始意见不一致时，该框架更倾向于引导争议走向正确答案，而非引入新的错误，同时还能提供对决定性论点的因果诊断。"
  },
  {
    "date": "2026-01-29",
    "title": "Scaling Reasoning Hop Exposes Weaknesses: Demystifying and Improving Hop Generalization in Large Language Models",
    "authors": "Zhaoyi Li, Jiatong Li, Gangwei Jiang, Linqi Song, Defu Lian, Ying Wei",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21214v1",
    "source": "arXiv",
    "abstract": "Chain-of-thought (CoT) reasoning has become the standard paradigm for enabling Large Language Models (LLMs) to solve complex problems. However, recent studies reveal a sharp performance drop in reasoning hop generalization scenarios, where the required number of reasoning steps exceeds training distributions while the underlying algorithm remains unchanged. The internal mechanisms driving this failure remain poorly understood. In this work, we conduct a systematic study on tasks from multiple domains, and find that errors concentrate at token positions of a few critical error types, rather than being uniformly distributed. Closer inspection reveals that these token-level erroneous predictions stem from internal competition mechanisms: certain attention heads, termed erroneous processing heads (ep heads), tip the balance by amplifying incorrect reasoning trajectories while suppressing correct ones. Notably, removing individual ep heads during inference can often restore the correct predictions. Motivated by these insights, we propose test-time correction of reasoning, a lightweight intervention method that dynamically identifies and deactivates ep heads in the reasoning process. Extensive experiments across different tasks and LLMs show that it consistently improves reasoning hop generalization, highlighting both its effectiveness and potential.",
    "title_zh": "推理跳跃的扩展暴露了弱点：揭秘并改进大语言模型中的跳跃泛化能力",
    "abstract_zh": "思维链（Chain-of-thought, CoT）推理已成为大语言模型（LLMs）解决复杂问题的标准范式。然而，近期研究发现，在推理步数泛化场景中，当所需推理步骤超出训练数据的分布范围，而底层算法保持不变时，模型性能会出现急剧下降。这一失败现象背后的内在机制仍不清晰。在本研究中，我们对多个领域的任务进行了系统性分析，发现错误主要集中于少数关键错误类型的标记位置，而非均匀分布。进一步观察表明，这些标记级别的错误预测源于模型内部的竞争机制：某些特定注意力头（称为错误处理头，ep heads），通过放大错误的推理路径同时抑制正确的路径，从而破坏了推理平衡。值得注意的是，在推理阶段移除单个ep头，往往能够恢复正确的预测结果。基于这些发现，我们提出了一种轻量级的推理过程测试时修正方法——在推理过程中动态识别并禁用ep头。在多种任务和不同LLM上的大量实验表明，该方法能持续提升推理步数的泛化能力，充分展现了其有效性与应用潜力。"
  },
  {
    "date": "2026-01-29",
    "title": "Conversation for Non-verifiable Learning: Self-Evolving LLMs through Meta-Evaluation",
    "authors": "Yuan Sui, Bryan Hooi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21464v1",
    "source": "arXiv",
    "abstract": "Training large language models (LLMs) for non-verifiable tasks, such as creative writing, dialogue, and ethical reasoning, remains challenging due to the absence of ground-truth labels. While LLM-as-Judge approaches offer a scalable alternative to human feedback, they face a fundamental limitation: performance is constrained by the evaluator's own quality. If the judge cannot recognize good solutions, it cannot provide useful training signals, and evaluation biases (e.g., favoring verbosity over quality) remain unaddressed. This motivates meta-evaluation: the ability to evaluate and improve the evaluator itself. We introduce CoNL, a framework that unifies generation, evaluation, and meta-evaluation through multi-agent self-play. Our key insight: critique quality can be measured by whether it helps others improve their solutions. In CoNL, multiple agents sharing the same policy engage in structured conversations to propose, critique, and revise solutions. Critiques that enable solution improvements earn a diagnostic reward, creating explicit supervision for meta-evaluation and enabling joint optimization of generation and judging capabilities through self-play, without external judges or ground truth. Experiments on five benchmarks show that CoNL achieves consistent improvements over self-rewarding baselines while maintaining stable training.",
    "title_zh": "不可验证学习的对话：通过元评估实现自演化的大规模语言模型",
    "abstract_zh": "训练大型语言模型（LLM）完成无法验证的任务，如创意写作、对话生成和伦理推理，仍然面临巨大挑战，因为缺乏真实标签。尽管“LLM作为评判者”（LLM-as-Judge）的方法为人类反馈提供了一种可扩展的替代方案，但其存在根本性局限：性能受限于评判者自身的质量。如果评判者无法识别优质解决方案，就无法提供有效的训练信号，同时评估中的偏见（例如偏好冗长而非高质量内容）也难以消除。这促使我们提出“元评估”（meta-evaluation）——即对评估者自身进行评估与改进的能力。我们提出了CoNL框架，通过多智能体自博弈，将生成、评估与元评估统一起来。我们的核心洞察是：批判的质量可以通过其是否有助于他人改进解决方案来衡量。在CoNL中，共享同一策略的多个智能体通过结构化对话，共同提出、批评并修订解决方案。那些能够促进解决方案改进的批评将获得诊断奖励，从而为元评估提供明确的监督信号，并实现生成与评判能力的联合优化，整个过程无需外部评判者或真实标签。在五个基准测试上的实验表明，CoNL在保持训练稳定的同时，持续优于自奖励基线方法。"
  },
  {
    "date": "2026-01-29",
    "title": "Mitigating Overthinking in Large Reasoning Models via Difficulty-aware Reinforcement Learning",
    "authors": "Qian Wan, Ziao Xu, Luona Wei, Xiaoxuan Shen, Jianwen Sun",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21418v1",
    "source": "arXiv",
    "abstract": "Large Reasoning Models (LRMs) achieve explicit chain-of-thought expansion by imitating deep thinking behaviors of humans, demonstrating excellent performance in complex task scenarios. However, the deep-thinking mode often leads to unnecessarily lengthy reasoning and resource inefficiency when handling simple tasks. This overthinking phenomenon may arise from the generation preference triggered by the reward function during post-training. Existing research attempts to mitigate overthinking from the perspective of prompt design or model training, but generally underestimates the importance of task difficulty awareness, which makes it difficult for LRMs to effectively allocate reasoning resources. In this paper, we propose Difficulty-aware Policy Optimization (DiPO), a reinforcement learning-based LRM training framework. DiPO encourages LRM to spontaneously model task complexity, and integrates them into reinforcement learning framework to adjust the generation preferences introduced by post-training. A difficulty modeling method based on model self-reasoning is proposed, which significantly reduces the dependence on manual annotation and formalize task complexity. We further develop a difficulty-signal-enhanced reward function that incorporates a penalty for lengthy reasoning while considering reasoning performance and output format. Experimental results indicate that DiPO enables the model to spontaneously adjust inference overhead, significantly reducing redundant tokens without losing performance due to thought compression.",
    "title_zh": "通过难度感知强化学习缓解大型推理模型中的过度思考",
    "abstract_zh": "大型推理模型（LRMs）通过模仿人类深度思考的行为，实现了显式的思维链扩展，在复杂任务场景中表现出色。然而，在处理简单任务时，这种深度思考模式常常导致不必要的冗长推理和资源浪费。这种过度思考现象可能源于后训练阶段奖励函数所引发的生成偏好。现有研究尝试从提示设计或模型训练的角度缓解过度思考问题，但普遍低估了任务难度感知的重要性，使得LRM难以有效分配推理资源。本文提出一种基于强化学习的LRM训练框架——难度感知策略优化（DiPO），该方法促使LRM自发建模任务复杂度，并将其融入强化学习框架中，以调整后训练阶段引入的生成偏好。我们提出了一种基于模型自我推理的难度建模方法，显著降低了对人工标注的依赖，并将任务复杂度形式化表达。此外，我们进一步设计了一种增强难度信号的奖励函数，在考虑推理表现和输出格式的同时，对过长的推理过程施加惩罚。实验结果表明，DiPO使模型能够自主调节推理开销，在不因思维压缩而损失性能的前提下，显著减少了冗余token的生成。"
  },
  {
    "date": "2026-01-29",
    "title": "The Compliance Paradox: Semantic-Instruction Decoupling in Automated Academic Code Evaluation",
    "authors": "Devanshu Sahoo, Manish Prasad, Vasudev Majhi, Arjun Neekhra, Yash Sinha, Murari Mandal, Vinay Chamola, Dhruv Kumar",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21360v1",
    "source": "arXiv",
    "abstract": "The rapid integration of Large Language Models (LLMs) into educational assessment rests on the unverified assumption that instruction following capability translates directly to objective adjudication. We demonstrate that this assumption is fundamentally flawed. Instead of evaluating code quality, models frequently decouple from the submission's logic to satisfy hidden directives, a systemic vulnerability we term the Compliance Paradox, where models fine-tuned for extreme helpfulness are vulnerable to adversarial manipulation. To expose this, we introduce the Semantic-Preserving Adversarial Code Injection (SPACI) Framework and the Abstract Syntax Tree-Aware Semantic Injection Protocol (AST-ASIP). These methods exploit the Syntax-Semantics Gap by embedding adversarial directives into syntactically inert regions (trivia nodes) of the Abstract Syntax Tree. Through a large-scale evaluation of 9 SOTA models across 25,000 submissions in Python, C, C++, and Java, we reveal catastrophic failure rates (>95%) in high-capacity open-weights models like DeepSeek-V3, which systematically prioritize hidden formatting constraints over code correctness. We quantify this failure using our novel tripartite framework measuring Decoupling Probability, Score Divergence, and Pedagogical Severity to demonstrate the widespread \"False Certification\" of functionally broken code. Our findings suggest that current alignment paradigms create a \"Trojan\" vulnerability in automated grading, necessitating a shift from standard RLHF toward domain-specific Adjudicative Robustness, where models are conditioned to prioritize evidence over instruction compliance. We release our complete dataset and injection framework to facilitate further research on the topic.",
    "title_zh": "合规悖论：自动化学术代码评估中的语义-指令解耦",
    "abstract_zh": "大型语言模型（LLMs）在教育评估中的快速融合，建立在一种未经验证的假设之上：即指令遵循能力能够直接转化为客观评判。我们证明这一假设从根本上存在缺陷。与真正评估代码质量不同，这些模型经常脱离提交代码的逻辑，转而迎合隐藏指令，这种系统性漏洞我们称之为“合规悖论”——那些为极致帮助性而微调的模型，极易受到对抗性操纵的影响。为揭示这一问题，我们提出了语义保持型对抗代码注入（SPACI）框架以及抽象语法树感知语义注入协议（AST-ASIP）。这些方法利用“语法-语义鸿沟”，将对抗性指令嵌入抽象语法树中语法上无意义的区域（即“ trivia 节点”）。通过对 9 种当前最先进的模型，在 Python、C、C++ 和 Java 中共 25,000 个代码提交上的大规模评估，我们发现像 DeepSeek-V3 这类高容量开源权重模型的灾难性失败率超过 95%，它们系统性地优先满足隐藏的格式约束，而非代码正确性。我们通过全新的三分法评估框架，量化了这种失效现象，该框架衡量解耦概率、评分偏差和教学严重性，从而揭示了功能损坏代码被广泛“虚假认证”的普遍问题。我们的研究结果表明，当前的对齐范式在自动评分系统中制造了一种“特洛伊木马”式漏洞，亟需从传统的强化学习人类反馈（RLHF）转向领域特定的“裁判鲁棒性”机制，使模型更注重证据本身，而非单纯服从指令。我们已公开全部数据集与注入框架，以推动该领域的进一步研究。"
  },
  {
    "date": "2026-01-29",
    "title": "Not All Code Is Equal: A Data-Centric Study of Code Complexity and LLM Reasoning",
    "authors": "Lukas Twist, Shu Yang, Hanqi Yan, Jingzhi Gong, Di Wang, Helen Yannakoudakis, Jie M. Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21894v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) increasingly exhibit strong reasoning abilities, often attributed to their capacity to generate chain-of-thought-style intermediate reasoning. Recent work suggests that exposure to code can further enhance these skills, but existing studies largely treat code as a generic training signal, leaving open the question of which properties of code actually contribute to improved reasoning. To address this gap, we study the structural complexity of code, which captures control flow and compositional structure that may shape how models internalise multi-step reasoning during fine-tuning. We examine two complementary settings: solution-driven complexity, where complexity varies across multiple solutions to the same problem, and problem-driven complexity, where complexity reflects variation in the underlying tasks. Using cyclomatic complexity and logical lines of code to construct controlled fine-tuning datasets, we evaluate a range of open-weight LLMs on diverse reasoning benchmarks. Our findings show that although code can improve reasoning, structural properties strongly determine its usefulness. In 83% of experiments, restricting fine-tuning data to a specific structural complexity range outperforms training on structurally diverse code, pointing to a data-centric path for improving reasoning beyond scaling.",
    "title_zh": "并非所有代码都相同：一项以数据为中心的代码复杂性与大语言模型推理研究",
    "abstract_zh": "大型语言模型（LLMs）日益展现出强大的推理能力，这通常归因于其生成“思维链”式中间推理过程的能力。近期研究指出，代码的接触可以进一步提升这些能力，但现有研究大多将代码视为一种通用的训练信号，尚未明确究竟是代码的哪些特性真正促进了推理能力的提升。为填补这一空白，我们研究了代码的结构复杂性，该特性捕捉了控制流和组合结构，可能影响模型在微调过程中内化多步推理的方式。我们考察了两种互补的情境：以解决方案为导向的复杂性，即同一问题的不同解法之间存在复杂性差异；以及以问题为导向的复杂性，即复杂性反映了底层任务之间的差异。通过使用环路复杂度和逻辑代码行数构建受控的微调数据集，我们在多种推理基准上评估了一系列开源权重的LLM。研究发现，尽管代码确实有助于提升推理能力，但其结构特性强烈决定了这种提升的有效性。在83%的实验中，将微调数据限制在特定结构复杂性范围内，表现优于使用结构多样化的代码进行训练，这表明，通过数据层面的优化，可以在不依赖规模扩展的前提下，有效提升模型的推理能力。"
  },
  {
    "date": "2026-01-29",
    "title": "CoFrGeNet: Continued Fraction Architectures for Language Generation",
    "authors": "Amit Dhurandhar, Vijil Chenthamarakshan, Dennis Wei, Tejaswini Pedapati, Karthikeyan Natesan Ramamurthy, Rahul Nair",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.21766v1",
    "source": "arXiv",
    "abstract": "Transformers are arguably the preferred architecture for language generation. In this paper, inspired by continued fractions, we introduce a new function class for generative modeling. The architecture family implementing this function class is named CoFrGeNets - Continued Fraction Generative Networks. We design novel architectural components based on this function class that can replace Multi-head Attention and Feed-Forward Networks in Transformer blocks while requiring much fewer parameters. We derive custom gradient formulations to optimize the proposed components more accurately and efficiently than using standard PyTorch-based gradients. Our components are a plug-in replacement requiring little change in training or inference procedures that have already been put in place for Transformer-based models thus making our approach easy to incorporate in large industrial workflows. We experiment on two very different transformer architectures GPT2-xl (1.5B) and Llama3 (3.2B), where the former we pre-train on OpenWebText and GneissWeb, while the latter we pre-train on the docling data mix which consists of nine different datasets. Results show that the performance on downstream classification, Q\\& A, reasoning and text understanding tasks of our models is competitive and sometimes even superior to the original models with $\\frac{2}{3}$ to $\\frac{1}{2}$ the parameters and shorter pre-training time. We believe that future implementations customized to hardware will further bring out the true potential of our architectures.",
    "title_zh": "CoFrGeNet：用于语言生成的连分数架构",
    "abstract_zh": "Transformer 无疑是语言生成任务中的首选架构。在本文中，受连分数的启发，我们提出了一种新的函数类用于生成建模。实现该函数类的架构家族被命名为 CoFrGeNets（Continued Fraction Generative Networks，连分数生成网络）。我们基于这一函数类设计了新颖的架构组件，可替代 Transformer 模块中的多头注意力机制和前馈网络，同时所需参数量大幅减少。我们推导出专门的梯度计算方法，以比使用标准 PyTorch 梯度更准确、高效地优化所提出的组件。这些组件可作为即插即用的替换模块，几乎无需修改已有的 Transformer 模型训练或推理流程，因此能够轻松集成到大规模工业工作流中。我们在两种截然不同的 Transformer 架构上进行了实验：GPT2-xl（15亿参数）和 Llama3（32亿参数）。其中，GPT2-xl 在 OpenWebText 和 GneissWeb 数据集上进行预训练，而 Llama3 则在包含九个不同数据集的 docling 数据混合集上进行预训练。实验结果表明，我们的模型在下游分类、问答、推理和文本理解等任务上的表现具有竞争力，甚至在某些情况下优于原始模型，且仅需原模型 $\\frac{2}{3}$ 到 $\\frac{1}{2}$ 的参数量，并拥有更短的预训练时间。我们相信，未来针对特定硬件进行定制化实现将进一步释放我们架构的真正潜力。"
  },
  {
    "date": "2026-01-29",
    "title": "RedSage: A Cybersecurity Generalist LLM",
    "authors": "Naufal Suryanto, Muzammal Naseer, Pengfei Li, Syed Talal Wasim, Jinhui Yi, Juergen Gall, Paolo Ceravolo, Ernesto Damiani",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.22159v1",
    "source": "arXiv",
    "abstract": "Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.",
    "title_zh": "RedSage：一名网络安全通才大模型",
    "abstract_zh": "网络安全运营需要能够支持多样化工作流程且不泄露敏感数据的辅助大语言模型。现有解决方案要么依赖存在隐私风险的专有API，要么使用缺乏领域适配能力的开源模型。为弥合这一差距，我们通过大规模网络过滤和高质量资源的人工收集，精心构建了118亿token的网络安全持续预训练数据集，涵盖28,600份文档，内容覆盖安全框架、攻击技术及安全工具等多个方面。基于此数据集，我们设计了一种智能体增强流水线，模拟专家工作流程，生成26.6万条多轮次网络安全样本，用于监督式微调。结合通用开源大模型数据，这些资源使我们得以训练出RedSage——一个开源、可本地部署的网络安全助手，具备领域感知的预训练与后训练能力。为严格评估模型性能，我们提出了RedSage-Bench基准测试，包含3万道多项选择题和240个开放问答题，全面覆盖网络安全知识、技能与工具应用能力。此外，RedSage还在多个成熟网络安全基准（如CTI-Bench、CyberMetric、SECURE）以及通用大模型基准上进行了评估，以检验其泛化能力。在8B规模下，RedSage表现稳定优异，在网络安全基准上相较基线模型最高提升5.59分，在Open LLM Leaderboard任务中最高提升5.05分。研究结果表明，领域感知的智能体增强方法与预训练/后训练策略不仅能显著提升网络安全专业能力，还能有效促进通用推理与指令遵循能力的提升。所有模型、数据集及代码均已公开发布。"
  },
  {
    "date": "2026-1-29",
    "title": "A fast and parallel framework for massive sensitive data detection based on big data technologies and large language models (LLMs)",
    "authors": "Xingsen Zhang, Guanyu Su, Daopeng Zhu, Jiaxi Huang, Yanrui Yang, Guohui Li, Shengan Che, Bolu Zhang",
    "publish": "Proceedings of the 2025 7th International Conference on Big-data Service and Intelligent Computation",
    "url": "https://doi.org/10.1145/3778265.3778274",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "一种基于大数据技术和大语言模型（LLMs）的快速并行敏感数据检测框架",
    "abstract_zh": "None"
  }
]