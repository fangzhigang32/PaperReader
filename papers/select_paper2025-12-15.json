[
  {
    "date": "2025-12-15",
    "title": "AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning",
    "authors": "Jiaru Zou, Ling Yang, Yunzhe Qi, Sirui Chen, Mengting Ai, Ke Shen, Jingrui He, Mengdi Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.13278v1",
    "source": "arXiv",
    "abstract": "Agentic reinforcement learning has advanced large language models (LLMs) to reason through long chain-of-thought trajectories while interleaving external tool use. Existing approaches assume a fixed inventory of tools, limiting LLM agents' adaptability to new or evolving toolsets. We present AutoTool, a framework that equips LLM agents with dynamic tool-selection capabilities throughout their reasoning trajectories. We first construct a 200k dataset with explicit tool-selection rationales across 1,000+ tools and 100+ tasks spanning mathematics, science, code generation, and multimodal reasoning. Building on this data foundation, AutoTool employs a dual-phase optimization pipeline: (i) supervised and RL-based trajectory stabilization for coherent reasoning, and (ii) KL-regularized Plackett-Luce ranking to refine consistent multi-step tool selection. Across ten diverse benchmarks, we train two base models, Qwen3-8B and Qwen2.5-VL-7B, with AutoTool. With fewer parameters, AutoTool consistently outperforms advanced LLM agents and tool-integration methods, yielding average gains of 6.4% in math & science reasoning, 4.5% in search-based QA, 7.7% in code generation, and 6.9% in multimodal understanding. In addition, AutoTool exhibits stronger generalization by dynamically leveraging unseen tools from evolving toolsets during inference.",
    "title_zh": "AutoTool：面向智能体推理的动态工具选择与集成",
    "abstract_zh": "代理强化学习已推动大型语言模型（LLMs）在推理过程中通过长链式思维轨迹，并实现外部工具的交替使用。现有方法通常假设工具库是固定的，限制了LLM代理对新出现或不断演化的工具集的适应能力。我们提出了AutoTool框架，赋予LLM代理在其推理过程中动态选择工具的能力。我们首先构建了一个包含20万条数据的语料库，涵盖1000多个工具和100多项任务，覆盖数学、科学、代码生成及多模态推理等领域，并为每项工具选择提供了明确的理由说明。基于这一数据基础，AutoTool采用双阶段优化流程：(i) 通过监督学习与强化学习相结合的方式稳定推理轨迹，确保思维连贯性；(ii) 利用KL正则化的Plackett-Luce排序方法，优化多步工具选择的一致性。我们在十个不同的基准测试中，使用AutoTool训练了两个基础模型——Qwen3-8B和Qwen2.5-VL-7B。尽管参数量更少，AutoTool在性能上持续优于先进的LLM代理及工具集成方法，在数学与科学推理方面平均提升6.4%，在基于搜索的问答任务中提升4.5%，在代码生成方面提升7.7%，在多模态理解任务中提升6.9%。此外，AutoTool在推理过程中展现出更强的泛化能力，能够动态利用演化工具集中未见过的新工具。"
  },
  {
    "date": "2025-12-15",
    "title": "Towards Effective Model Editing for LLM Personalization",
    "authors": "Baixiang Huang, Limeng Cui, Jiapeng Liu, Haoran Wang, Jiawei Xu, Zhuiyue Tan, Yutong Chen, Chen Luo, Yi Liu, Kai Shu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.13676v1",
    "source": "arXiv",
    "abstract": "Personalization is becoming indispensable for LLMs to align with individual user preferences and needs. Yet current approaches are often computationally expensive, data-intensive, susceptible to catastrophic forgetting, and prone to performance degradation in multi-turn interactions or when handling implicit queries. To address these challenges, we conceptualize personalization as a model editing task and introduce Personalization Editing, a framework that applies localized edits guided by clustered preference representations. This design enables precise preference-aligned updates while preserving overall model capabilities. In addition, existing personalization benchmarks frequently rely on persona-based dialogs between LLMs rather than user-LLM interactions, or focus primarily on stylistic imitation while neglecting information-seeking tasks that require accurate recall of user-specific preferences. We introduce User Preference Question Answering (UPQA), a short-answer QA dataset constructed from in-situ user queries with varying levels of difficulty. Unlike prior benchmarks, UPQA directly evaluates a model's ability to recall and apply specific user preferences. Across experimental settings, Personalization Editing achieves higher editing accuracy and greater computational efficiency than fine-tuning, while outperforming prompting-based baselines in multi-turn conversations and implicit preference questions settings.",
    "title_zh": "面向大语言模型个性化的有效模型编辑方法",
    "abstract_zh": "个性化正日益成为大语言模型（LLM）与个体用户偏好和需求对齐不可或缺的手段。然而，当前的方法往往计算成本高昂、数据依赖性强，容易导致灾难性遗忘，并在多轮交互或处理隐含查询时出现性能下降。为应对这些挑战，我们提出将个性化视为一种模型编辑任务，并引入“个性化编辑”（Personalization Editing）框架——该框架基于聚类后的偏好表示进行局部化编辑，从而实现精准的偏好对齐更新，同时保持模型整体能力不受影响。此外，现有个性化评估基准通常依赖于大模型之间的角色扮演式对话，而非真实用户与大模型的互动；且多聚焦于风格模仿，忽视了需要准确回忆用户特定偏好的信息获取类任务。为此，我们提出了“用户偏好问答”（User Preference Question Answering, UPQA）数据集，该数据集基于实际用户在使用过程中提出的各类难度不一的即时查询构建而成。与以往基准不同，UPQA直接评估模型在回忆并应用具体用户偏好方面的能力。在多种实验设置下，个性化编辑在编辑准确性上优于微调方法，且具有更高的计算效率；同时，在多轮对话及隐含偏好问题场景中，其表现也显著超越基于提示（prompting）的基线方法。"
  },
  {
    "date": "2025-12-15",
    "title": "SIGMA: An AI-Empowered Training Stack on Early-Life Hardware",
    "authors": "Lei Qu, Lianhai Ren, Peng Cheng, Rui Gao, Ruizhe Wang, Tianyu Chen, Xiao Liu, Xingjian Zhang, Yeyun Gong, Yifan Xiong, Yucheng Ding, Yuting Jiang, Zhenghao Lin, Zhongxin Guo, Ziyue Yang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.13488v1",
    "source": "arXiv",
    "abstract": "An increasing variety of AI accelerators is being considered for large-scale training. However, enabling large-scale training on early-life AI accelerators faces three core challenges: frequent system disruptions and undefined failure modes that undermine reliability; numerical errors and training instabilities that threaten correctness and convergence; and the complexity of parallelism optimization combined with unpredictable local noise that degrades efficiency. To address these challenges, SIGMA is an open-source training stack designed to improve the reliability, stability, and efficiency of large-scale distributed training on early-life AI hardware. The core of this initiative is the LUCIA TRAINING PLATFORM (LTP), the system optimized for clusters with early-life AI accelerators. Since its launch in March 2025, LTP has significantly enhanced training reliability and operational productivity. Over the past five months, it has achieved an impressive 94.45% effective cluster accelerator utilization, while also substantially reducing node recycling and job-recovery times. Building on the foundation of LTP, the LUCIA TRAINING FRAMEWORK (LTF) successfully trained SIGMA-MOE, a 200B MoE model, using 2,048 AI accelerators. This effort delivered remarkable stability and efficiency outcomes, achieving 21.08% MFU, state-of-the-art downstream accuracy, and encountering only one stability incident over a 75-day period. Together, these advances establish SIGMA, which not only tackles the critical challenges of large-scale training but also establishes a new benchmark for AI infrastructure and platform innovation, offering a robust, cost-effective alternative to prevailing established accelerator stacks and significantly advancing AI capabilities and scalability. The source code of SIGMA is available at https://github.com/microsoft/LuciaTrainingPlatform.",
    "title_zh": "SIGMA：一种基于早期硬件的AI赋能训练栈",
    "abstract_zh": "越来越多的AI加速器正被考虑用于大规模训练。然而，在早期阶段的AI加速器上实现大规模训练面临三大核心挑战：频繁的系统中断和未定义的故障模式，严重影响可靠性；数值误差和训练不稳定性，威胁正确性与收敛性；以及并行优化的复杂性与不可预测的局部噪声相结合，导致效率下降。为应对这些挑战，SIGMA是一个开源训练栈，旨在提升早期AI硬件上大规模分布式训练的可靠性、稳定性和效率。该计划的核心是LUCIA TRAINING PLATFORM（LTP），一个专为配备早期AI加速器的集群优化的系统。自2025年3月发布以来，LTP显著提升了训练的可靠性和运营生产率。在过去的五个月中，其有效集群加速器利用率达到了惊人的94.45%，同时大幅减少了节点回收和任务恢复时间。基于LTP的基础，LUCIA TRAINING FRAMEWORK（LTF）成功使用2,048个AI加速器训练了SIGMA-MOE——一个2000亿参数的MoE模型。此次实践取得了卓越的稳定性和效率成果，实现了21.08%的MFU（模型利用率）、业界领先的下游准确率，并在长达75天的训练周期中仅出现一次稳定性问题。这些进展共同确立了SIGMA，不仅有效解决了大规模训练中的关键挑战，更树立了AI基础设施与平台创新的新标杆，为现有主流加速器栈提供了一个强大、经济高效的替代方案，显著推动了AI能力与可扩展性的进步。SIGMA的源代码可在 https://github.com/microsoft/LuciaTrainingPlatform 获取。"
  },
  {
    "date": "2025-12-15",
    "title": "Database Research needs an Abstract Relational Query Language",
    "authors": "Wolfgang Gatterbauer, Diandre Miguel Sabale",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.12957v1",
    "source": "arXiv",
    "abstract": "For decades, SQL has been the default language for composing queries, but it is increasingly used as an artifact to be read and verified rather than authored. With Large Language Models (LLMs), queries are increasingly machine-generated, while humans read, validate, and debug them. This shift turns relational query languages into interfaces for back-and-forth communication about intent, which will lead to a rethinking of relational language design, and more broadly, relational interface design. We argue that this rethinking needs support from an Abstract Relational Query Language (ARQL): a semantics-first reference metalanguage that separates query intent from user-facing syntax and makes underlying relational patterns explicit and comparable across user-facing languages. An ARQL separates a query into (i) a relational core (the compositional structure that determines intent), (ii) modalities (alternative representations of that core tailored to different audiences), and (iii) conventions (orthogonal environment-level semantic parameters under which the core is interpreted, e.g., set vs. bag semantics, or treatment of null values). Usability for humans or machines then depends less on choosing a particular language and more on choosing an appropriate modality. Comparing languages becomes a question of which relational patterns they support and what conventions they choose. We introduce Abstract Relational Calculus (ARC), a strict generalization of Tuple Relational Calculus (TRC), as a concrete instance of ARQL. ARC comes in three modalities: (i) a comprehension-style textual notation, (ii) an Abstract Language Tree (ALT) for machine reasoning about meaning, and (iii) a diagrammatic hierarchical graph (higraph) representation for humans. ARC provides the missing vocabulary and acts as a Rosetta Stone for relational querying.",
    "title_zh": "数据库研究需要一种抽象的关系型查询语言。",
    "abstract_zh": "数十年来，SQL一直是编写查询的默认语言，但如今它越来越多地被视为一种需要被阅读和验证的“遗留物”，而非由人类直接编写的语言。随着大型语言模型（LLMs）的发展，查询正日益由机器生成，而人类则负责阅读、验证和调试这些查询。这一转变使得关系型查询语言演变为一种用于反复沟通意图的接口，这将促使我们重新思考关系型语言的设计，乃至更广泛的关系型接口设计。\n\n我们认为，这种重新思考需要依托一种抽象关系查询语言（ARQL）：一种以语义为先的参考性元语言，它将查询意图与面向用户的语法分离开来，并使底层的关系模式显式化，从而在不同用户界面语言之间实现可比性。ARQL将一个查询分解为三个部分：(i) 关系核心（决定查询意图的组合结构），(ii) 模态（针对不同受众量身定制的核心表达形式），以及(iii) 约定（独立于环境层面的语义参数，例如集合与多重集语义，或对空值的处理方式）。因此，对于人类或机器而言，可用性不再取决于选择某种特定语言，而更多取决于选择合适的模态。不同语言之间的比较，也转化为对它们所支持的关系模式以及所采用约定的比较。\n\n我们提出了抽象关系演算（ARC），作为ARQL的一个具体实例——它是元组关系演算（TRC）的严格推广。ARC包含三种模态：(i) 一种类似集合构造的文本记法，(ii) 用于机器推理意义的抽象语言树（ALT），以及(iii) 供人类使用的图示化分层图（higraph）表示。ARC填补了现有语言中缺失的表达词汇，成为关系查询领域的“罗塞塔石碑”。"
  },
  {
    "date": "2025-12-15",
    "title": "Modeling Collaborative Problem Solving Dynamics from Group Discourse: A Text-Mining Approach with Synergy Degree Model",
    "authors": "Jianjun Xiao, Cixiao Wang, Wenmei Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.13061v1",
    "source": "arXiv",
    "abstract": "Measuring collaborative problem solving (CPS) synergy remains challenging in learning analytics, as classical manual coding cannot capture emergent system-level dynamics. This study introduces a computational framework that integrates automated discourse analysis with the Synergy Degree Model (SDM) to quantify CPS synergy from group communication. Data were collected from 52 learners in 12 groups during a 5-week connectivist MOOC (cMOOC) activity. Nine classification models were applied to automatically identify ten CPS behaviors across four interaction levels: operation, wayfinding, sense-making, and creation. While BERT achieved the highest accuracy, GPT models demonstrated superior precision suitable for human-AI collaborative coding. Within the SDM framework, each interaction level was treated as a subsystem to compute group-level order parameters and derive synergy degrees. Permutation tests showed automated measures preserve construct validity, despite systematic biases at the subsystem level. Statistical analyses revealed significant task-type differences: survey study groups exhibited higher creation-order than mode study groups, suggesting \"controlled disorder\" may benefit complex problem solving. Importantly, synergy degree distinguished collaborative quality, ranging from excellent to failing groups. Findings establish synergy degree as a sensitive indicator of collaboration and demonstrate the feasibility of scaling fine-grained CPS analytics through AI-in-the-loop approaches.",
    "title_zh": "基于群体话语的协作问题解决动态建模：一种结合协同度模型的文本挖掘方法",
    "abstract_zh": "在学习分析领域，协作式问题解决（CPS）协同效应的测量仍面临挑战，因为传统的手工编码方法难以捕捉系统层面涌现的动力学特征。本研究提出一种计算框架，将自动话语分析与协同度模型（SDM）相结合，从群体交流中量化CPS协同效应。研究数据来自一个为期五周的连接主义型大规模开放在线课程（cMOOC）活动中12个小组共52名学习者的表现。研究应用了九种分类模型，自动识别四种交互层次（操作、导航、意义建构和创造）下的十类CPS行为。尽管BERT模型取得了最高准确率，但GPT系列模型表现出更优的精确度，更适合用于人机协同编码。在SDM框架下，每个交互层次被视作一个子系统，用于计算群体层面的序参量并推导协同度。置换检验表明，尽管在子系统层面存在系统性偏差，自动化测量仍保持了构念效度。统计分析显示，任务类型存在显著差异：调查研究组的“创造-秩序”水平显著高于模式研究组，提示“受控的混乱”可能有利于复杂问题的解决。尤为重要的是，协同度能够有效区分协作质量，涵盖从优秀到失败的不同群体。研究结果确立了协同度作为协作质量敏感指标的有效性，并展示了通过“AI在环”方法实现细粒度CPS分析规模化可行性的前景。"
  },
  {
    "date": "2025-12-15",
    "title": "Design in Tiles: Automating GEMM Deployment on Tile-Based Many-PE Accelerators",
    "authors": "Aofeng Shen, Chi Zhang, Yakup Budanaz, Alexandru Calotoiu, Torsten Hoefler, Luca Benini",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.13638v1",
    "source": "arXiv",
    "abstract": "Tile-based many-Processing Element (PE) accelerators can achieve competitive performance on General Matrix Multiplication (GEMM), but they are extremely hard to program, as their optimal software mapping is deeply coupled with hardware design which is unwieldy to manual deployment. We propose \"Design in Tiles (DiT)\", an automated framework connecting a deployment toolchain with a configurable executable model for these accelerators. For evaluation, we apply our framework to GEMM targeting a large acceleration configuration (e.g., 32x32 tiles, 1979 TFLOPS@FP8, 4 TB/s Bandwidth) comparable to an NVIDIA GH200. We achieve higher PE utilization than GH200 with its expert-tuned GEMM libraries, achieving 1.2-2.0x speedup across diverse matrix shapes.",
    "title_zh": "基于Tile的设计：在基于Tile的多PE加速器上自动化GEMM部署",
    "abstract_zh": "基于瓦片（Tile）的多处理单元（PE）加速器在通用矩阵乘法（GEMM）上可实现具有竞争力的性能，但其编程极为困难，因为最优软件映射与硬件设计深度耦合，手动部署十分繁琐。为此，我们提出了“瓦片内设计（Design in Tiles, DiT）”——一种将部署工具链与可配置可执行模型相连接的自动化框架。为验证该框架的有效性，我们将其应用于GEMM，针对一个大规模加速配置（例如：32×32瓦片、FP8下达1979 TFLOPS、4 TB/s带宽），其性能水平可与NVIDIA GH200相当。在多种矩阵形状下，我们的方法实现了比GH200使用专家调优GEMM库更高的PE利用率，整体速度提升达1.2至2.0倍。"
  },
  {
    "date": "2025-12-15",
    "title": "Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models",
    "authors": "Boxin Wang, Chankyu Lee, Nayeon Lee, Sheng-Chieh Lin, Wenliang Dai, Yang Chen, Yangyi Chen, Zhuolin Yang, Zihan Liu, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.13607v1",
    "source": "arXiv",
    "abstract": "Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.",
    "title_zh": "Nemotron-Cascade：面向通用推理模型的级联强化学习扩展",
    "abstract_zh": "使用强化学习（RL）构建通用推理模型面临显著的跨领域异质性问题，包括推理时响应长度和验证延迟的巨大差异。这种变异性使得强化学习基础设施设计复杂化，减缓了训练速度，并使训练课程设计（如响应长度扩展）和超参数选择变得困难。在本研究中，我们提出了一种级联式领域专属强化学习（Cascade RL），用于开发通用推理模型Nemotron-Cascade，该模型可在指令模式和深度思考模式下运行。与传统方法将不同领域的异构提示混合处理不同，Cascade RL采用顺序化的、按领域划分的强化学习策略，有效降低了工程复杂度，并在广泛的基准测试中实现了顶尖性能。值得注意的是，当以RLHF（基于人类反馈的强化学习）作为预处理步骤时，不仅能实现对齐优化，更显著提升了模型的推理能力；后续的领域专属RLVR阶段极少会降低先前领域所达到的基准性能，甚至可能进一步提升（详见图1示例）。我们的140亿参数模型在经过强化学习优化后，在LiveCodeBench v5/v6/Pro上超越了其SFT教师模型DeepSeek-R1-0528，并在2025年国际信息学奥林匹克竞赛（IOI）中取得了银牌成绩。我们已公开透明地分享了完整的训练流程与数据配方。"
  },
  {
    "date": "2025-12-15",
    "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding",
    "authors": "Jia-Nan Li, Jian Guan, Wei Wu, Chongxuan Li",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.13586v1",
    "source": "arXiv",
    "abstract": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\\times$ average speedup.",
    "title_zh": "ReFusion：一种具有并行自回归解码的扩散型大语言模型",
    "abstract_zh": "自回归模型（ARMs）受限于缓慢的顺序推理过程。尽管掩码扩散模型（MDMs）提供了一种并行推理的替代方案，但其存在显著缺陷：由于无法使用键值（KV）缓存，导致计算开销过高；同时，由于在难以处理的词元组合空间中学习依赖关系，生成内容往往缺乏连贯性。为解决这些局限性，我们提出了ReFusion——一种新型掩码扩散模型，通过将并行解码从词元层面提升至更高层级的“槽位”（slot）层面，实现了更优的性能与效率。每个槽位是一个固定长度、连续的子序列。该方法采用迭代式的“规划-填充”解码流程：首先通过基于扩散的规划步骤识别一组弱相关槽位，随后利用自回归方式并行解码这些选定的槽位。这种基于槽位的设计，不仅在统一的因果框架下实现了完整的KV缓存复用，还将学习复杂度从难以处理的词元组合空间降低到可管理的槽位级排列空间。在七个多样化的基准测试中进行的大量实验表明，ReFusion不仅显著超越了以往的MDMs，在平均性能上提升了34%，速度提升超过18倍，同时缩小了与强大自回归模型之间的性能差距，并保持了平均2.33倍的速度优势。"
  },
  {
    "date": "2025-12-15",
    "title": "Async Control: Stress-testing Asynchronous Control Measures for LLM Agents",
    "authors": "Asa Cooper Stickland, Jan Michelfeit, Arathi Mani, Charlie Griffin, Ollie Matthews, Tomek Korbak, Rogan Inglis, Oliver Makins, Alan Cooney",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.13526v1",
    "source": "arXiv",
    "abstract": "LLM-based software engineering agents are increasingly used in real-world development tasks, often with access to sensitive data or security-critical codebases. Such agents could intentionally sabotage these codebases if they were misaligned. We investigate asynchronous monitoring, in which a monitoring system reviews agent actions after the fact. Unlike synchronous monitoring, this approach does not impose runtime latency, while still attempting to disrupt attacks before irreversible harm occurs. We treat monitor development as an adversarial game between a blue team (who design monitors) and a red team (who create sabotaging agents). We attempt to set the game rules such that they upper bound the sabotage potential of an agent based on Claude 4.1 Opus. To ground this game in a realistic, high-stakes deployment scenario, we develop a suite of 5 diverse software engineering environments that simulate tasks that an agent might perform within an AI developer's internal infrastructure. Over the course of the game, we develop an ensemble monitor that achieves a 6% false negative rate at 1% false positive rate on a held out test environment. Then, we estimate risk of sabotage at deployment time by extrapolating from our monitor's false negative rate. We describe one simple model for this extrapolation, present a sensitivity analysis, and describe situations in which the model would be invalid. Code is available at: https://github.com/UKGovernmentBEIS/async-control.",
    "title_zh": "异步控制：对大型语言模型代理的异步控制措施进行压力测试",
    "abstract_zh": "基于大语言模型（LLM）的软件工程代理在现实开发任务中日益普及，通常需要访问敏感数据或安全关键型代码库。如果这些代理出现对齐偏差，便可能故意破坏代码库。本文研究了一种异步监控机制，即监控系统在代理行为发生后进行事后审查。与同步监控不同，该方法不会引入运行时延迟，同时仍试图在造成不可逆损害前阻止攻击行为。\n\n我们将监控系统的开发视为一场对抗性博弈：蓝队（负责设计监控系统）与红队（负责构建具有破坏意图的代理）之间的对抗。我们尝试设定游戏规则，以对Claude 4.1 Opus模型所具备的潜在破坏能力进行上界约束。为使这一博弈场景贴近真实、高风险的部署环境，我们构建了五个多样化的软件工程仿真环境，模拟代理在AI开发者内部基础设施中可能执行的任务。\n\n在整个博弈过程中，我们开发出一个集成式监控系统，在独立测试环境中实现了1%的误报率下6%的漏报率。随后，我们通过外推监控系统的漏报率来评估实际部署时的破坏风险。文中提出一种简化的外推模型，并进行了敏感性分析，同时指出了该模型失效的典型情境。\n\n相关代码已公开于：https://github.com/UKGovernmentBEIS/async-control"
  },
  {
    "date": "2025-12-15",
    "title": "UCRBench: Benchmarking LLMs on Use Case Recovery",
    "authors": "Shuyuan Xiao, Yiran Zhang, Weisong Sun, Xiaohong Chen, Yang Liu, Zhi Jin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.13360v1",
    "source": "arXiv",
    "abstract": "Use cases are widely employed to specify functional requirements, yet existing benchmarks are scarce and face the risk of being misaligned with actual system behavior, similarly limiting the rigorous evaluation of large language models (LLMs) in generating use cases from source code. We address this gap by introducing code-aligned use case benchmarks, constructed through manual validation of both user-goal and subfunction use cases across nine real-world software projects. Using this benchmark, we conduct the first systematic study of LLMs and propose a hierarchical evaluation protocol that assesses actor correctness, name accuracy, path fidelity, and behavioral coverage. The results show that while LLMs can partially reconstruct system functionality, their performance varies significantly across projects, with particularly noticeable shortcomings in domain-specific and multi-module systems. The models also exhibit high omission rates and struggle to maintain consistent abstraction when aggregating subfunctions into user-goal use cases, highlighting both the potential and current limitations of LLM-based use case reverse engineering.",
    "title_zh": "UCRBench：面向用例恢复的大型语言模型基准测试",
    "abstract_zh": "用例被广泛用于描述功能需求，然而现有的基准数据集十分稀缺，且存在与实际系统行为不一致的风险，这同样限制了大语言模型（LLMs）在从源代码生成用例时的严谨评估。为填补这一空白，我们提出了基于代码对齐的用例基准，该基准通过人工验证九个真实软件项目中的用户目标用例和子功能用例构建而成。基于此基准，我们开展了首个针对LLMs的系统性研究，并提出了一种分层评估协议，用于衡量角色正确性、名称准确性、路径保真度以及行为覆盖度。实验结果表明，尽管LLMs能够部分还原系统功能，但其表现随项目差异显著，在领域特定及多模块系统中尤为明显地暴露出不足。此外，模型在子功能聚合为用户目标用例时表现出较高的遗漏率，且难以保持抽象层级的一致性，凸显了基于LLM的用例逆向工程所蕴含的潜力与当前局限。"
  },
  {
    "date": "2025-12-15",
    "title": "Error-Driven Prompt Optimization for Arithmetic Reasoning",
    "authors": "Árpád Pándy, Róbert Lakatos, András Hajdu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.13323v1",
    "source": "arXiv",
    "abstract": "Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner.",
    "title_zh": "基于错误驱动的提示优化用于算术推理",
    "abstract_zh": "近年来，人工智能的快速发展激发了人们对工业级智能代理的兴趣，这些代理能够支持金融、医疗等受监管领域中的分析师，在表格数据工作流中完成任务。此类系统的一项关键能力是在处理结构化数据时执行精确的算术运算，同时确保敏感信息始终留在安全的本地环境中，不外泄。本文提出了一种基于错误驱动的优化框架，用于提升算术推理能力，并将其应用于代码生成代理（CGA），特别针对本地部署的小型语言模型（SLMs）。通过对当前领先的SLM（Qwen3 4B）进行系统性评估，我们发现基础模型在算术任务上存在固有局限性；而我们提出的错误驱动方法通过聚类错误预测结果，迭代优化提示规则，显著提升了模型性能，使准确率大幅提升至70.8%。研究结果表明，通过系统性的、基于错误驱动的提示优化，而非昂贵的微调，即可实现可靠、可解释且适用于工业场景的AI助手开发。这一方法使得小型模型在符合隐私保护要求的前提下，甚至超越了大型语言模型（如GPT-3.5 Turbo）的表现。"
  },
  {
    "date": "2025-12-15",
    "title": "Sharpen the Spec, Cut the Code: A Case for Generative File System with SYSSPEC",
    "authors": "Qingyuan Liu, Zou Mo, Hengbin Zhang, Dong Du, Yubin Xia, Haibo Chen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.13047v1",
    "source": "arXiv",
    "abstract": "File systems are critical OS components that require constant evolution to support new hardware and emerging application needs. However, the traditional paradigm of developing features, fixing bugs, and maintaining the system incurs significant overhead, especially as systems grow in complexity. This paper proposes a new paradigm, generative file systems, which leverages Large Language Models (LLMs) to generate and evolve a file system from prompts, effectively addressing the need for robust evolution. Despite the widespread success of LLMs in code generation, attempts to create a functional file system have thus far been unsuccessful, mainly due to the ambiguity of natural language prompts. This paper introduces SYSSPEC, a framework for developing generative file systems. Its key insight is to replace ambiguous natural language with principles adapted from formal methods. Instead of imprecise prompts, SYSSPEC employs a multi-part specification that accurately describes a file system's functionality, modularity, and concurrency. The specification acts as an unambiguous blueprint, guiding LLMs to generate expected code flexibly. To manage evolution, we develop a DAG-structured patch that operates on the specification itself, enabling new features to be added without violating existing invariants. Moreover, the SYSSPEC toolchain features a set of LLM-based agents with mechanisms to mitigate hallucination during construction and evolution. We demonstrate our approach by generating SPECFS, a concurrent file system. SPECFS passes hundreds of regression tests, matching a manually-coded baseline. We further confirm its evolvability by seamlessly integrating 10 real-world features from Ext4. Our work shows that a specification-guided approach makes generating and evolving complex systems not only feasible but also highly effective.",
    "title_zh": "精炼规范，裁剪代码：基于 SYSSPEC 的生成式文件系统之案例",
    "abstract_zh": "文件系统是操作系统中至关重要的组成部分，需要持续演进以支持新型硬件和不断涌现的应用需求。然而，传统的功能开发、缺陷修复与系统维护模式带来了巨大的开销，尤其在系统复杂性日益增长的背景下更为显著。本文提出了一种新的范式——生成式文件系统（Generative File Systems），该范式利用大型语言模型（LLMs）根据提示（prompts）自动生成并演化文件系统，从而有效应对系统持续演进的需求。尽管大语言模型在代码生成方面已取得广泛应用成功，但此前尝试构建可运行的文件系统均未成功，主要原因在于自然语言提示的模糊性。为此，本文提出了SYSSPEC框架，用于开发生成式文件系统。其核心思想是用源自形式化方法的原则替代模糊的自然语言描述。SYSSPEC不再依赖不精确的提示，而是采用多部分规范（multi-part specification），精准刻画文件系统的功能、模块化结构以及并发行为。这一规范作为清晰无歧义的设计蓝图，引导LLM灵活生成预期代码。为支持系统演化，我们设计了一种基于有向无环图（DAG）结构的补丁机制，直接作用于规范本身，确保新增功能不会破坏已有不变量。此外，SYSSPEC工具链集成了多个基于LLM的智能体（agents），具备抑制生成过程中的“幻觉”现象的机制。我们通过生成SPECFS——一个支持并发的文件系统，验证了该方法的有效性：SPECFS通过了数百项回归测试，性能与手工编码的基准系统相当。我们进一步通过无缝集成Ext4中的10个真实世界特性，证实了其良好的可演化能力。本研究表明，以规范为导向的方法不仅使复杂系统的生成与演化成为可能，而且具有极高的有效性。"
  },
  {
    "date": "2025-12-15",
    "title": "Behavior and Representation in Large Language Models for Combinatorial Optimization: From Feature Extraction to Algorithm Selection",
    "authors": "Francesca Da Ros, Luca Di Gaspero, Kevin Roitero",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.13374v1",
    "source": "arXiv",
    "abstract": "Recent advances in Large Language Models (LLMs) have opened new perspectives for automation in optimization. While several studies have explored how LLMs can generate or solve optimization models, far less is understood about what these models actually learn regarding problem structure or algorithmic behavior. This study investigates how LLMs internally represent combinatorial optimization problems and whether such representations can support downstream decision tasks. We adopt a twofold methodology combining direct querying, which assesses LLM capacity to explicitly extract instance features, with probing analyses that examine whether such information is implicitly encoded within their hidden layers. The probing framework is further extended to a per-instance algorithm selection task, evaluating whether LLM-derived representations can predict the best-performing solver. Experiments span four benchmark problems and three instance representations. Results show that LLMs exhibit moderate ability to recover feature information from problem instances, either through direct querying or probing. Notably, the predictive power of LLM hidden-layer representations proves comparable to that achieved through traditional feature extraction, suggesting that LLMs capture meaningful structural information relevant to optimization performance.",
    "title_zh": "大型语言模型在组合优化中的行为与表征：从特征提取到算法选择",
    "abstract_zh": "近年来，大型语言模型（LLMs）的进展为优化领域的自动化开辟了新的前景。尽管已有若干研究探讨了LLMs在生成或求解优化模型方面的潜力，但关于这些模型究竟学习到了多少关于问题结构或算法行为的知识，目前仍知之甚少。本研究旨在探究LLMs如何内部表征组合优化问题，以及这种表征是否能够支持下游决策任务。我们采用双重方法：一方面通过直接查询，评估LLMs显式提取实例特征的能力；另一方面通过探针分析，检验这些信息是否隐含地编码在其隐藏层中。该探针框架进一步拓展至针对单个实例的算法选择任务，以评估LLMs生成的表征能否预测最优求解器。实验覆盖四个基准优化问题及三种实例表示方式。结果表明，LLMs在通过直接查询或探针分析恢复问题实例特征信息方面表现出中等程度的能力。值得注意的是，LLMs隐藏层表征的预测能力与传统特征提取方法相当，表明LLMs确实捕捉到了与优化性能相关的重要结构信息。"
  },
  {
    "date": "2025-12-15",
    "title": "Fine-tuned LLM-based Code Migration Framework",
    "authors": "Oleg Grynets, Vasyl Lyashkevych, Dmytro Baran, Maksym Orliansky, Taras Zelenyy, Markiian Leshchyshyn",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.13515v1",
    "source": "arXiv",
    "abstract": "The study presents the outcomes of research and experimental validation in the domain of automated codebase migration, with a focus on addressing challenges in transitioning SQL-based systems. The proposed method for migration essentially appears as a framework that leverages the best aspects of traditional software engineering techniques and provides an iterative, scalable, precise and efficient solution for modern database transformations. The central piece of the approach is the integration of a fine-tuned Large Language Model to address critical issues in SQL code conversion, such as syntax mapping, resolving discrepancies between Oracle PL/SQL and PostgreSQL, and optimising database elements such as stored procedures, triggers, views, and overall database logic. Thus, the method involves a trade-off between fine-tuning and prompt engineering. Special attention is given to a fine-tuning approach, which enhances the adaptability and compatibility with migration requirements across the entire database. According to the achieved results, fine-tuning plays a very important role. The study employs targeted evaluation methodologies along with computational metrics to measure the success of iterative conversion cycles. Core innovations include automated SQL feature detection, semi-supervised error analysis and integration of Subject Matter Experts feedback within a systematic migration workflow. The methodology achieves significant reductions in Syntax Error Rates, enhances feature alignment throughout migration iterations, and leverages dataset sampling to ensure continual improvement. By embedding GAI into the migration process, the framework facilitates precise feature mapping, semi-automated error resolution, and data-driven optimisation loops, improving workflow efficiency.",
    "title_zh": "微调的基于大语言模型的代码迁移框架",
    "abstract_zh": "该研究展示了在自动化代码库迁移领域中，针对SQL系统迁移所面临挑战的研究成果与实验验证。所提出的迁移方法本质上构成一个框架，融合了传统软件工程方法的诸多优势，为现代数据库转换提供了迭代性、可扩展性、精确性和高效性的解决方案。该方法的核心在于引入经过微调的大语言模型，以应对SQL代码转换中的关键问题，如语法映射、Oracle PL/SQL与PostgreSQL之间的差异处理，以及对存储过程、触发器、视图和整体数据库逻辑等数据库元素的优化。因此，该方法在微调与提示工程之间进行了权衡。研究特别关注一种微调策略，显著提升了系统在整个数据库迁移过程中对迁移需求的适应性与兼容性。根据实验结果，微调在迁移过程中起到了至关重要的作用。研究采用针对性的评估方法与计算指标，衡量多轮转换周期的成功程度。核心创新包括：自动SQL特性检测、半监督错误分析，以及将领域专家反馈整合进系统化的迁移工作流。该方法显著降低了语法错误率，在各次迁移迭代中增强了功能对齐，并通过数据集采样确保持续改进。通过将生成式人工智能（GAI）嵌入迁移流程，该框架实现了精准的功能映射、半自动化错误修复以及数据驱动的优化循环，从而有效提升了工作流的整体效率。"
  },
  {
    "date": "2025-12-15",
    "title": "An Optimal Alignment-Driven Iterative Closed-Loop Convergence Framework for High-Performance Ultra-Large Scale Layout Pattern Clustering",
    "authors": "Shuo Liu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.13133v1",
    "source": "arXiv",
    "abstract": "With the aggressive scaling of VLSI technology, the explosion of layout patterns creates a critical bottleneck for DFM applications like OPC. Pattern clustering is essential to reduce data complexity, yet existing methods struggle with computational prohibitiveness ($O(N^2)$ comparisons), sub-optimal discrete sampling for center alignment, and difficult speed-quality trade-offs. To address these, we propose an Optimal Alignment-Driven Iterative Closed-Loop Convergence Framework. First, to resolve alignment ambiguity, we introduce a hybrid suite of high-performance algorithms: an FFT-based Phase Correlation method for cosine similarity constraints, and a Robust Geometric Min-Max strategy for edge displacement constraints that analytically solves for the global optimum. Second, we model clustering as a Set Cover Problem (SCP) using a Surprisal-Based Lazy Greedy heuristic within a coarse-to-fine iterative refinement loop to ensure convergence. Additionally, a multi-stage pruning mechanism filters over 99% of redundant computations. Experimental results on the 2025 China Postgraduate EDA Elite Challenge benchmark demonstrate a 93.4% compression ratio relative to raw inputs and an over 100x speedup compared to the official baseline, effectively handling tens of thousands of patterns in seconds. Securing First Place among 77 teams, this approach proves its superiority in solving the NP-Hard layout clustering problem with an optimal balance of scalability and precision.",
    "title_zh": "一种面向高性能超大规模版图模式聚类的最优对齐驱动迭代闭合环收敛框架",
    "abstract_zh": "随着VLSI技术的迅猛发展，版图模式的急剧增长为DFM应用（如OPC）带来了关键瓶颈。模式聚类对于降低数据复杂度至关重要，但现有方法在计算开销（$O(N^2)$次比较）、中心对齐的次优离散采样以及速度与质量之间的权衡方面仍面临挑战。为解决这些问题，我们提出了一种**最优对齐驱动的迭代闭合环收敛框架**。首先，为消除对齐歧义，我们引入了一套高性能混合算法：基于FFT的相位相关法用于满足余弦相似性约束，以及鲁棒几何极小-极大策略用于处理边缘位移约束，并能解析求解全局最优解。其次，我们将聚类建模为集合覆盖问题（SCP），结合基于意外度的懒惰贪心启发式算法，在粗到精的迭代优化循环中确保收敛性。此外，多阶段剪枝机制可过滤超过99%的冗余计算。在2025年中国研究生EDA精英挑战赛基准测试中的实验结果表明，该方法相对于原始输入实现了93.4%的压缩率，且相比官方基线提速超过100倍，可在数秒内高效处理数万级版图模式。在77支参赛队伍中荣获第一名，充分证明了该方法在求解NP难的版图聚类问题上，兼具卓越的可扩展性与精度平衡能力。"
  },
  {
    "date": "2025-12-15",
    "title": "Socratic Students: Teaching Language Models to Learn by Asking Questions",
    "authors": "Rajeev Bhatt Ambati, Tianyi Niu, Aashu Singh, Shlok Mishra, Shashank Srivastava, Snigdha Chaturvedi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.13102v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.",
    "title_zh": "苏格拉底式学生：教会语言模型通过提问来学习",
    "abstract_zh": "大型语言模型（LLMs）在静态交互中表现出色，能够通过检索其参数中编码的知识来回答用户问题。然而，在许多现实场景中，如教育辅导或医疗协助，相关知识并不直接可用，必须通过动态交互主动获取。一个具备互动能力的智能体应能识别自身知识的不确定性，提出有针对性的问题，并高效地保留新获得的知识。以往的研究主要关注教师如何有效指导学生，即教师识别学生的知识盲点并提供引导。本文则将焦点转向学生，探讨学生主动向教师提问以获取有用信息的有效策略。在数学和编程基准测试中，当基线学生模型初始表现接近于零时，我们发现以学生为主导的方法相较于静态基线，始终实现了至少0.5的绝对Pass@k提升。为了提高提问质量，我们采用直接偏好优化（DPO）方法，结合自我反馈或更强学生模型的指导来训练学生。研究发现，这种有指导的训练使小型模型能够学会提出更高质量的问题，从而进一步提升了学习效率。"
  },
  {
    "date": "2025-12-15",
    "title": "OptHQC: Optimize HQC for High-Performance Post-Quantum Cryptography",
    "authors": "Ben Dong, Hui Feng, Qian Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.12904v1",
    "source": "arXiv",
    "abstract": "As post-quantum cryptography (PQC) becomes increasingly critical for securing future communication systems, the performance overhead introduced by quantum-resistant algorithms presents a major computing challenge. HQC (Hamming Quasi-Cyclic) is a newly standardized code-based PQC scheme designed to replace classical key exchange methods. In this paper, we propose OptHQC, an optimized implementation of the HQC scheme to deliver high-performance cryptographic operations. Our approach provides a comprehensive analysis of each computational blocks in HQC and introduces optimizations across all three stages: key generation, encryption, and decryption. We first exploit data-level sparsity in vector multiplication to accelerate polynomial operations during vector generation. We then leverage instruction-level acceleration (e.g., AVX2) in hash computation to further improve performance. Last, we transform multiplication into lookup table indexing and optimize memory access patterns in syndrome computation and error vector recovery, which are the most computationally intensive operations in HQC. Overall, OptHQC achieves an average 55% speedup over the reference HQC implementation on CPU.",
    "title_zh": "OptHQC：面向高性能后量子密码学的HQC优化",
    "abstract_zh": "随着后量子密码学（PQC）在保障未来通信系统安全方面变得日益关键，抗量子算法带来的性能开销已成为一项重大的计算挑战。HQC（汉明准循环）是一种新近被标准化的基于编码的后量子密码方案，旨在取代传统的密钥交换方法。本文提出OptHQC，一种针对HQC方案的优化实现，以提供高性能的密码运算。我们的方法对HQC中的每一项计算模块进行了全面分析，并在密钥生成、加密和解密三个阶段均引入了优化措施。首先，我们利用向量乘法中的数据级稀疏性，加速向量生成过程中的多项式运算；其次，通过在哈希计算中采用指令级加速（如AVX2），进一步提升性能；最后，我们将乘法运算转换为查表索引，并优化了综合征计算与错误向量恢复中的内存访问模式，这两者是HQC中最耗时的计算操作。总体而言，OptHQC在CPU上相较于参考实现平均提升了55%的运行速度。"
  },
  {
    "date": "2025-12-15",
    "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management",
    "authors": "Weizhou Shen, Ziyi Yang, Chenliang Li, Zhiyuan Lu, Miao Peng, Huashan Sun, Yingcheng Shi, Shengyi Liao, Shaopeng Lai, Bo Zhang, Dayiheng Liu, Fei Huang, Jingren Zhou, Ming Yan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.12967v1",
    "source": "arXiv",
    "abstract": "We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.",
    "title_zh": "QwenLong-L1.5：长上下文推理与记忆管理的后训练方案",
    "abstract_zh": "我们推出了QwenLong-L1.5，该模型通过系统性的后训练创新，在长上下文推理能力方面取得了卓越突破。QwenLong-L1.5的关键技术突破包括以下三点：  \n（1）长上下文数据合成流水线：我们构建了一套系统化的合成框架，能够生成需要在全局分布证据上进行多跳推理的高难度任务。通过对文档进行原子事实及其内在关系的拆解，并程序化地组合出可验证的推理问题，我们的方法实现了大规模高质量训练数据的生成，显著超越了简单的检索任务，真正赋予模型长距离推理能力。  \n（2）面向长上下文训练的稳定强化学习：为解决长上下文强化学习中的关键不稳定性问题，我们引入了任务平衡采样与任务特异性优势估计，有效缓解奖励偏差；同时提出自适应熵控制策略优化（AEPO），动态调节探索与利用之间的权衡，显著提升了训练稳定性与性能。  \n（3）面向超长上下文的记忆增强架构：考虑到即使扩展上下文窗口也无法容纳任意长度序列，我们设计了一种多阶段融合的强化学习训练记忆管理框架，实现单次遍历推理与迭代式基于记忆的处理无缝结合，支持超过400万token的超长任务处理。  \n\n基于Qwen3-30B-A3B-Thinking模型，QwenLong-L1.5在长上下文推理基准测试中表现媲美GPT-5和Gemini-2.5-Pro，平均性能较基线提升9.90分。在超长任务（100万至400万token）场景下，其记忆代理框架相较基线实现9.48分的显著提升。此外，所获得的长上下文推理能力也有效迁移至通用领域，显著增强了科学推理、记忆工具使用及长对话等任务的表现。"
  },
  {
    "date": "2025-12-15",
    "title": "Toward Self-Healing Networks-on-Chip: RL-Driven Routing in 2D Torus Architectures",
    "authors": "Mohammad Walid Charrwi, Zaid Hussain",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.13096v1",
    "source": "arXiv",
    "abstract": "We investigate adaptive minimal routing in 2D torus networks on chip NoCs under node fault conditions comparing a reinforcement learning RL based strategy to an adaptive routing baseline A torus topology is used for its low diameter high connectivity properties The RL approach models each router as an agent that learns to forward packets based on network state while the adaptive scheme uses fixed minimal paths with simple rerouting around faults We implement both methods in simulation injecting up to 50 node faults uniformly at random Key metrics are measured 1 throughput vs offered load at fault density 02 2 packet delivery ratio PDR vs fault density and 3 a fault adaptive score FT vs fault density Experimental results show the RL method achieves significantly higher throughput at high load approximately 2030 gain and maintains higher reliability under increasing faults The RL router delivers more packets per cycle and adapts to faults by exploiting path diversity whereas the adaptive scheme degrades sharply as faults accumulate In particular the RL approach preserves end to end connectivity longer PDR remains above 90 until approximately 3040 faults while adaptive PDR drops to approximately 70 at the same point The fault adaptive score likewise favors RL routing Thus RL based adaptive routing demonstrates clear advantages in throughput and fault resilience for torus NoCs",
    "title_zh": "面向自愈网络芯片：基于强化学习的二维环面架构路由",
    "abstract_zh": "我们研究了在节点故障条件下，二维环形网络（2D torus）片上网络（NoC）中的自适应最小路由策略，将基于强化学习（RL）的方法与一种自适应路由基准方法进行了对比。环形拓扑因其低直径和高连通性而被采用。在RL方法中，每个路由器被建模为一个智能体，根据网络状态学习如何转发数据包；而自适应方案则采用固定的最短路径，并在遇到故障时进行简单的绕行处理。我们在仿真中实现了这两种方法，并随机注入最多50个节点故障。关键性能指标包括：1）在故障密度为0.2时，吞吐量随负载的变化；2）分组交付率（PDR）随故障密度的变化；3）故障适应度评分（FT）随故障密度的变化。实验结果表明，在高负载情况下，RL方法的吞吐量显著更高，提升幅度约为20%–30%，并且在故障数量增加时仍保持更高的可靠性。RL路由器每周期能传输更多数据包，通过利用路径多样性实现对故障的自适应调整；而自适应方案在故障累积后性能急剧下降。特别地，RL方法能够更长时间维持端到端的连通性——当故障数达到约30–40时，其PDR仍高于90%，而自适应方案在同一故障水平下PDR已降至约70%。故障适应度评分也显示出对RL路由的明显优势。因此，基于强化学习的自适应路由在吞吐量和容错能力方面，展现出对环形NoC系统明显的优越性。"
  },
  {
    "date": "2025-12-15",
    "title": "Wait, Wait, Wait... Why Do Reasoning Models Loop?",
    "authors": "Charilaos Pipis, Shivam Garg, Vasilis Kontonis, Vaishnavi Shrivastava, Akshay Krishnamurthy, Dimitris Papailiopoulos",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.12895v1",
    "source": "arXiv",
    "abstract": "Reasoning models (e.g., DeepSeek-R1) generate long chains of thought to solve harder problems, but they often loop, repeating the same text at low temperatures or with greedy decoding. We study why this happens and what role temperature plays. With open reasoning models, we find that looping is common at low temperature. Larger models tend to loop less, and distilled students loop significantly even when their teachers rarely do. This points to mismatches between the training distribution and the learned model, which we refer to as errors in learning, as a key cause. To understand how such errors cause loops, we introduce a synthetic graph reasoning task and demonstrate two mechanisms. First, risk aversion caused by hardness of learning: when the correct progress-making action is hard to learn but an easy cyclic action is available, the model puts relatively more probability on the cyclic action and gets stuck. Second, even when there is no hardness, Transformers show an inductive bias toward temporally correlated errors, so the same few actions keep being chosen and loops appear. Higher temperature reduces looping by promoting exploration, but it does not fix the errors in learning, so generations remain much longer than necessary at high temperature; in this sense, temperature is a stopgap rather than a holistic solution. We end with a discussion of training-time interventions aimed at directly reducing errors in learning.",
    "title_zh": "等等，等等，等等……为什么推理模型会陷入循环？",
    "abstract_zh": "推理模型（如 DeepSeek-R1）通过生成较长的思维链来解决更复杂的问题，但它们常常陷入循环——在低温或贪婪解码时重复相同的文本。我们研究了这种现象发生的原因以及温度参数所起的作用。借助开放的推理模型，我们发现：在低温度下，循环现象非常普遍。较大的模型倾向于较少循环，而经过蒸馏得到的学生模型即使其教师模型几乎不循环，也仍会显著出现循环。这表明，训练数据分布与模型实际学习到的内容之间存在不匹配，我们称之为“学习错误”，这是导致循环的关键原因。\n\n为了理解这些学习错误如何引发循环，我们引入了一个合成的图推理任务，并揭示了两种机制。第一种是学习困难带来的规避风险倾向：当正确推进问题的行动难以学习，而一个简单的循环动作却容易习得时，模型会相对更倾向于选择这个循环动作，从而陷入僵局。第二种情况是，即便不存在学习难度，Transformer 模型本身也表现出一种对时间相关错误的归纳偏置，使得少数特定动作反复被选择，最终形成循环。\n\n提高温度可以在一定程度上减少循环，因为它促进了探索性行为。然而，温度并不能从根本上解决学习错误，因此在高温下生成内容往往仍然远超必要长度。从这个角度看，温度只是一种临时缓解手段，而非根本性的解决方案。最后，我们讨论了在训练阶段采取干预措施，以直接减少学习错误的可能性，从而从根源上改善模型的推理质量。"
  },
  {
    "date": "2025-12-15",
    "title": "Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation",
    "authors": "Richard J. Young",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.13655v1",
    "source": "arXiv",
    "abstract": "Safety alignment mechanisms in large language models prevent responses to harmful queries through learned refusal behavior, yet these same mechanisms impede legitimate research applications including cognitive modeling, adversarial testing, and security analysis. While abliteration techniques enable surgical removal of refusal representations through directional orthogonalization, the relative effectiveness of available implementations remains uncharacterized. This study evaluates four abliteration tools (Heretic, DECCP, ErisForge, FailSpy) across sixteen instruction-tuned models (7B-14B parameters), reporting tool compatibility on all 16 models and quantitative metrics on subsets dictated by tool support. Single-pass methods demonstrated superior capability preservation on the benchmarked subset (avg GSM8K change across three models: ErisForge -0.28 pp; DECCP -0.13 pp), while Bayesian-optimized abliteration produced variable distribution shift (KL divergence: 0.043-1.646) with model-dependent capability impact. These findings provide researchers with evidence-based selection criteria for abliteration tool deployment across diverse model architectures. The principal finding indicates that mathematical reasoning capabilities exhibit the highest sensitivity to abliteration interventions, with GSM8K change ranging from +1.51 pp to -18.81 pp (-26.5% relative) depending on tool selection and model architecture.",
    "title_zh": "大语言模型消融方法的比较分析：跨架构评估",
    "abstract_zh": "大型语言模型中的安全对齐机制通过习得的拒绝行为来防止对有害查询作出回应，但这些机制同样会阻碍合法的研究应用，如认知建模、对抗性测试和安全分析。尽管消融技术可通过方向正交化实现对拒绝表征的精准移除，但现有实现方案的有效性仍缺乏系统评估。本研究在16个指令微调模型（参数规模7B-14B）上评估了四种消融工具（Heretic、DECCP、ErisForge、FailSpy），报告了所有16个模型的工具兼容性，并基于各工具支持情况对子集进行了定量分析。单次遍历方法在基准子集上表现出更优的能力保留效果（在三个模型上的平均GSM8K得分变化：ErisForge为-0.28个百分点；DECCP为-0.13个百分点）；而贝叶斯优化的消融方法则导致了可变的分布偏移（KL散度范围：0.043–1.646），且对模型能力的影响具有显著的模型依赖性。这些发现为研究人员在不同模型架构中部署消融工具提供了基于证据的选择依据。主要发现表明，数学推理能力对消融干预最为敏感，其GSM8K得分变化范围从+1.51个百分点到-18.81个百分点（相对变化达-26.5%），具体取决于所选工具与模型架构。"
  },
  {
    "date": "2025-12-15",
    "title": "Scaling Laws for Code: Every Programming Language Matters",
    "authors": "Jian Yang, Shawn Guo, Lin Jing, Wei Zhang, Aishan Liu, Chuan Hao, Zhoujun Li, Wayne Xin Zhao, Xianglong Liu, Weifeng Lv, Bryan Dai",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.13472v1",
    "source": "arXiv",
    "abstract": "Code large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget.",
    "title_zh": "代码的缩放定律：每种编程语言都至关重要",
    "abstract_zh": "代码大语言模型（Code LLMs）虽然功能强大，但其训练成本高昂。现有的缩放定律能够根据模型规模、数据量和计算资源预测模型性能。然而，不同编程语言（PLs）在预训练过程中对基础模型性能的影响各不相同，这种差异显著影响了性能预测的准确性。此外，现有研究多集中于语言无关的设置，忽视了现代软件开发固有的多语言特性。因此，首先需要系统研究各类编程语言的缩放规律，再进一步分析它们之间的相互影响，最终构建出适用于多语言环境的综合缩放定律。\n\n本文首次对多语言代码预训练的缩放规律进行了系统性探索，覆盖超过1000次实验（相当于336,000多小时的H800算力），涉及多种编程语言、模型规模（从0.2B到14B参数）以及数据集规模（达1万亿token）。我们建立了跨多种编程语言的代码大模型全面缩放定律，发现解释型语言（如Python）在模型规模和数据量增加时获得的性能提升，明显优于编译型语言（如Rust）。\n\n研究还表明，多语言预训练具有协同增益效应，尤其在语法结构相似的语言之间表现更为显著。此外，采用并行配对策略（将代码片段与其对应翻译拼接进行训练）能显著增强跨语言能力，并展现出良好的缩放特性。最后，我们提出一种依赖比例的多语言缩放定律，通过优先分配高价值编程语言（如Python）、平衡高协同效应语言对（如JavaScript与TypeScript），同时减少对快速饱和语言（如Rust）的资源投入，在相同计算预算下，实现了所有编程语言上平均性能的显著提升，优于均匀分配策略。"
  },
  {
    "date": "2025-12-15",
    "title": "Lyra: A Hardware-Accelerated RISC-V Verification Framework with Generative Model-Based Processor Fuzzing",
    "authors": "Juncheng Huo, Yunfan Gao, Xinxin Liu, Sa Wang, Yungang Bao, Xitong Gao, Kan Shi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.13686v1",
    "source": "arXiv",
    "abstract": "As processor designs grow more complex, verification remains bottlenecked by slow software simulation and low-quality random test stimuli. Recent research has applied software fuzzers to hardware verification, but these rely on semantically blind random mutations that may generate shallow, low-quality stimuli unable to explore complex behaviors. These limitations result in slow coverage convergence and prohibitively high verification costs. In this paper, we present Lyra, a heterogeneous RISC-V verification framework that addresses both challenges by pairing hardware-accelerated verification with an ISA-aware generative model. Lyra executes the DUT and reference model concurrently on an FPGA SoC, enabling high-throughput differential checking and hardware-level coverage collection. Instead of creating verification stimuli randomly or through simple mutations, we train a domain-specialized generative model, LyraGen, with inherent semantic awareness to generate high-quality, semantically rich instruction sequences. Empirical results show Lyra achieves up to $1.27\\times$ higher coverage and accelerates end-to-end verification by up to $107\\times$ to $3343\\times$ compared to state-of-the-art software fuzzers, while consistently demonstrating lower convergence difficulty.",
    "title_zh": "Lyra：一种基于生成模型驱动的处理器模糊测试的硬件加速RISC-V验证框架",
    "abstract_zh": "随着处理器设计日益复杂，验证工作仍受限于软件仿真速度缓慢以及随机测试激励质量低下。近期研究已将软件模糊测试技术应用于硬件验证，但这些方法依赖于语义无感知的随机变异，生成的激励往往浅显且质量不高，难以探索复杂的系统行为。这一局限导致覆盖率收敛缓慢，验证成本过高。本文提出 Lyra——一种异构 RISC-V 验证框架，通过结合硬件加速验证与具备指令集架构（ISA）感知能力的生成模型，同时应对上述挑战。Lyra 在 FPGA SoC 上并行执行待测设计（DUT）与参考模型，实现高吞吐量的差异性检查和硬件级覆盖率收集。不同于传统的随机生成或简单变异方式，我们训练了一个领域专用的生成模型 LyraGen，其具备内在的语义感知能力，可生成高质量、语义丰富的指令序列。实验结果表明，与当前最先进的软件模糊测试工具相比，Lyra 的覆盖率最高提升达 1.27 倍，端到端验证速度加速高达 107 倍至 3343 倍，同时始终表现出更低的收敛难度。"
  },
  {
    "date": "2025-12-15",
    "title": "Automated assessment of open-ended computer science questions using large language models",
    "authors": "Miroslav Ölvecký, Miroslav Beňo, Pablo Díaz",
    "publish": "2025 International Conference on Emerging eLearning Technologies and Applications (ICETA)",
    "url": "https://doi.org/10.1109/iceta67772.2025.11280262",
    "source": "IEEE",
    "abstract": "This study examines the use of various machine learning algorithms and large language models (LLM) to evaluate student answers in a computer science learning environment. To capture semantic nuances, models such as MiniLM and SlovakBert were used to convert text into vector embeddings and assess similarity using cosine measures. More advanced LLM, including various Llama and Gemma architectures, were integrated via an API to classify answers as either correct or incorrect. Experimental evaluation of a dataset comprising 50 diverse student responses revealed that, while rule-based methods achieved only moderate accuracy, semantic and LLM-based models significantly improved performance. The largest Llama 3 (70B) model achieved near-perfect accuracy. Despite these promising results, the crucial role of teachers in validating and supervising assessment outcomes was emphasized, particularly given the potential for students to misuse AI.",
    "title_zh": "使用大型语言模型对开放式计算机科学问题进行自动化评估",
    "abstract_zh": "本研究探讨了在计算机科学学习环境中，使用多种机器学习算法及大型语言模型（LLM）对学生作答进行评估的可行性。为捕捉文本的语义细微差别，研究采用了MiniLM和SlovakBert等模型将文本转换为向量嵌入，并通过余弦相似度衡量其相似性。同时，通过API集成更先进的大型语言模型，如不同架构的Llama和Gemma系列，用于将学生答案分类为正确或错误。对包含50个多样化学生作答的数据集进行实验评估后发现，尽管基于规则的方法仅达到中等准确率，但采用语义分析与LLM的方法显著提升了性能，其中最大的Llama 3（70B）模型几乎达到了完美的准确率。尽管结果令人鼓舞，研究仍强调教师在验证和监督评估结果方面的重要作用，尤其是在学生可能滥用人工智能的潜在风险背景下。"
  },
  {
    "date": "2025-12-15",
    "title": "Prompt Inject Detection with Generative Explanation as an Investigative Tool",
    "authors": "Jonathan Pan, Swee Liang Wong, Yidi Yuan, Xin Wei Chia",
    "publish": "2025 International Conference on Machine Learning and Cybernetics (ICMLC)",
    "url": "https://doi.org/10.1109/icmlc66258.2025.11280014",
    "source": "IEEE",
    "abstract": "Large Language Models (LLMs) are vulnerable to adversarial prompt based injects. These injects could jailbreak or exploit vulnerabilities within these models with explicit prompt requests leading to undesired responses. In the context of investigating prompt injects, the challenge is the sheer volume of input prompts involved that are likely to be largely benign. This investigative challenge is further complicated by the semantics and subjectivity of the input prompts involved in the LLM conversation with its user and the context of the environment to which the conversation is being carried out. Hence, the challenge for AI security investigators would be twofold. The first is to identify adversarial prompt injects and then to assess whether the input prompt is contextually benign or adversarial. For the first step, this could be done using existing AI security solutions like guardrails to detect and protect the LLMs. Guardrails have been developed using a variety of approaches. A popular approach is to use signature based. Another popular approach to develop AI models to classify such prompts include the use of NLP based models like a language model. However, in the context of conducting an AI security investigation of prompt injects, these guardrails lack the ability to aid investigators in triaging or assessing the identified input prompts. In this applied research exploration, we explore the use of a text generation capabilities of LLM to detect prompt injects and generate explanation for its detections to aid AI security investigators in assessing and triaging of such prompt inject detections. The practical benefit of such a tool is to ease the task of conducting investigation into prompt injects.",
    "title_zh": "提示注入检测与生成式解释作为调查工具",
    "abstract_zh": "大型语言模型（LLMs）容易受到基于对抗性提示的注入攻击。这些注入可能通过明确的提示请求，导致模型“越狱”或利用其内部漏洞，从而产生非预期的响应。在调查提示注入问题时，面临的挑战在于输入提示的数量极为庞大，其中绝大多数提示实际上是无害的。这一调查难题进一步因提示内容的语义复杂性和主观性而加剧——这些提示涉及与用户交互的上下文，以及对话所处的具体环境。因此，AI安全调查人员面临双重挑战：首先，需要识别出对抗性提示注入；其次，判断该输入提示在具体语境中是属于良性还是恶意。\n\n针对第一项任务，可借助现有的AI安全解决方案（如“护栏”机制）来检测并保护语言模型。目前，“护栏”技术已发展出多种实现方式。一种流行的方法是基于签名的检测，即通过预定义的恶意模式匹配来识别潜在威胁。另一种常见方法是利用自然语言处理（NLP）模型，例如训练专门的语言模型以分类此类提示。然而，在实际开展AI安全调查时，这些现有“护栏”系统往往缺乏对调查人员进行优先级排序或评估已识别提示的能力，难以有效支持深入分析。\n\n在本项应用研究探索中，我们探讨了利用大型语言模型自身的文本生成能力，来检测提示注入，并自动生成解释说明，以辅助AI安全调查人员对检测结果进行评估和优先级排序。此类工具的实际价值在于显著减轻调查人员在排查提示注入事件时的工作负担，提升调查效率与准确性。"
  },
  {
    "date": "2025-12-15",
    "title": "Safety-Oriented Evaluation of Embedded C Coding Methodologies for Automotive Software Using MISRA C and ISO 26262 Standards",
    "authors": "Rachid Latif, Moussa Bellaou, Lahoucaine Amenzouy, Abdessamade Elboudani",
    "publish": "2025 International Conference on Electrical Systems &amp;amp; Automation (ICESA)",
    "url": "https://doi.org/10.1109/icesa66763.2025.11281351",
    "source": "IEEE",
    "abstract": "The growing complexity of automotive systems has led to an increased reliance on embedded software, emphasizing the need for safe, reliable, and maintainable coding practises. This paper presents a comparative evaluation of manual and model-based C code development approaches for automotive systems, focusing on MISRA C and ISO 26262 compliance. The analysis covers two automotive use cases—a Battery Management System and a Wiper Controller—evaluated across safety, maintainability, and traceability metrics. Results demonstrate that standards-compliant manual coding achieved 92.4% ASIL-B coverage with only 3 MISRA violations and 63.6% smaller memory footprint (28.7KB vs 78.9KB) compared to model-based approaches, while maintaining competitive fault detection rates (89.2% vs 95.8%). Although model-based methods reached slightly higher coverage (97.1% ASIL-B), they required 3.2× more code (1,742 vs 548 LOC). These findings establish standards-compliant manual coding as a practical alternative to commercial model-based tools while providing a comparative evaluation methodology for safety-critical automotive software development.",
    "title_zh": "基于MISRA C和ISO 26262标准的汽车软件嵌入式C编程方法的安全性评估",
    "abstract_zh": "汽车系统日益复杂的趋势导致对嵌入式软件的依赖程度不断提高，凸显了安全、可靠且可维护的编码实践的重要性。本文对汽车系统中手动编程与基于模型的C代码开发方法进行了对比评估，重点考察其在符合MISRA C和ISO 26262标准方面的表现。分析涵盖两个汽车应用场景——电池管理系统和雨刷控制器，从安全性、可维护性及可追溯性等多个维度进行评价。结果表明，符合标准的手动编码方法在ASIL-B级别实现了92.4%的覆盖率，仅存在3处MISRA违规，内存占用比基于模型的方法减少63.6%（28.7KB vs 78.9KB），同时保持了具有竞争力的故障检测率（89.2% vs 95.8%）。尽管基于模型的方法达到了更高的覆盖率（97.1% ASIL-B），但其代码量增加了3.2倍（1,742行 vs 548行LOC）。研究结果确立了符合标准的手动编码作为一种可行的商业模型化工具替代方案，并为安全关键型汽车软件开发提供了一种可比较的评估方法。"
  },
  {
    "date": "2025-12-15",
    "title": "AI-Powered System Cybersecurity Operations and Incident Response",
    "authors": "Pranita Chaudhary, Varad Sawant, Anurag Karpe, Lokesh Kad, Pratham Kubetkar",
    "publish": "2025 9th International Conference on Computing, Communication, Control and Automation (ICCCBEA)",
    "url": "https://doi.org/10.1109/iccubea65967.2025.11283848",
    "source": "IEEE",
    "abstract": "The intensity of cyber security hazards is increasing, smart, integrated solutions require which can immediately identify, classify and respond immediately. This paper presents a comprehensive AI-powered cyber security system that integrates modules for simplified monitoring and a centralized dashboard for coordination, detection of malware, network infiltration and email classification. To increase the accuracy of detecting danger and reducing false positivity, the system appoints machine machine learning and intensive learning methods. The email classification system uses supervised learning and natural language processing (NLP) to effectively identify spam and fishing email. The malware detection system appoints nerve network classifier with stable and dynamic analysis methods to identify malicious programs. Network infiltration detection system (NIDS) examines the network traffic behavior through models detecting discrepancy to identify suspicious activities. The dashboard compiles insight from each module, providing real -time analytics, alert management and user response tools to increase the system flexibility. Experimental findings encourage classification accuracy in various modules, in which email classifier reaches more than 90 % of accuracy. Even though delay and throwput assessment was not done, modular and scalable design guarantees flexibility of the system in various network settings. This study highlights the ability of AI-operated cyber security systems to identify the active danger in the corporate environment and improve the event management.",
    "title_zh": "基于人工智能的系统网络安全运营与事件响应",
    "abstract_zh": "网络安全威胁的强度正在不断加剧，迫切需要智能、集成的解决方案，能够立即识别、分类并迅速响应。本文提出了一套全面的基于人工智能的网络安全系统，集成了简化监控模块和集中式仪表板，用于协调管理恶意软件检测、网络入侵检测以及电子邮件分类。为提高危险检测的准确性并降低误报率，系统采用了机器学习与深度学习方法。电子邮件分类系统利用监督学习与自然语言处理（NLP）技术，有效识别垃圾邮件和钓鱼邮件。恶意软件检测系统采用神经网络分类器结合静态与动态分析方法，以识别恶意程序。网络入侵检测系统（NIDS）通过行为模型分析网络流量，检测异常模式，从而识别可疑活动。仪表板整合各模块的分析结果，提供实时数据分析、告警管理及用户响应工具，显著提升了系统的灵活性。实验结果表明，各模块的分类准确率表现良好，其中电子邮件分类器的准确率超过90%。尽管尚未进行延迟与吞吐量评估，但系统的模块化与可扩展设计确保了其在不同网络环境中的适应性。本研究凸显了人工智能驱动的网络安全系统在企业环境中识别主动威胁及提升事件管理能力的潜力。"
  },
  {
    "date": "2025-12-15",
    "title": "TinyML vs LLMs: A Survey of Extreme Scales in Machine Learning",
    "authors": "Ismail Lamaakal, Chaymae Yahyati, Khalid El Makkaoui, Yassine Maleh, Ibrahim Ouahbi",
    "publish": "2025 International Conference on Electrical Systems &amp;amp; Automation (ICESA)",
    "url": "https://doi.org/10.1109/icesa66763.2025.11281142",
    "source": "IEEE",
    "abstract": "ML now works with an unmatched range of computational scales, from microcontrollers with models that are only a few kilobytes big to cloud infrastructures with Large Language Models (LLMs) that have trillions of parameters. These extremes show how AI has made progress in different ways: Tiny Machine Learning (TinyML) is great for green apps that don’t have a lot of resources because it focuses on real-time, ultra-low-power inference on edge devices. LLMs, on the other hand, are the best at understanding, reasoning, and generating language, but they often use a lot of energy and hurt the environment. This research provides a comprehensive analysis of TinyML and LLMs, highlighting their importance in the context of green and sustainable technology. We offer a systematic taxonomy that evaluates both paradigms based on critical factors such as model size, latency, energy consumption, memory footprint, training methodologies, privacy considerations, and deployment architecture. There are talks about how to use TinyML in smart agriculture, IoT, and wearable health monitoring, and how to use LLMs in cloud-based text generation, code synthesis, and decision support. We carefully consider the pros and cons of centralization and decentralization, scalability and sustainability, and performance and efficiency. Finally, we talk about what might happen in the future, like quantized TinyLLMs, federated learning for privacy-aware sustainability, and hybrid edge-cloud systems where TinyML acts as a green intelligence filter for LLM backends. The purpose of this poll is to get people to share their ideas on how to make AI systems that are both helpful and good for the environment.",
    "title_zh": "TinyML 与大模型：机器学习极端规模的综述",
    "abstract_zh": "机器学习（ML）如今已能够适应从仅几KB大小的微型模型到拥有数万亿参数的大型语言模型（LLMs）等各类计算规模。这些极端案例展现了人工智能在不同方向上的进步：微型机器学习（TinyML）在资源受限的绿色应用中表现出色，它专注于在边缘设备上实现实时、超低功耗的推理；而大型语言模型则在理解、推理和生成语言方面表现卓越，但通常能耗巨大，对环境造成较大影响。本研究对TinyML与LLMs进行了全面分析，强调了它们在绿色与可持续技术背景下的重要性。我们提出了一套系统化的分类体系，从模型大小、延迟、能耗、内存占用、训练方法、隐私考量以及部署架构等多个关键因素出发，对两种范式进行评估。文中还探讨了TinyML在智能农业、物联网（IoT）及可穿戴健康监测中的应用潜力，以及LLMs在基于云的文本生成、代码合成和决策支持中的实际用途。我们深入分析了集中化与去中心化、可扩展性与可持续性、性能与效率之间的权衡关系。最后，我们展望了未来的发展趋势，包括量化后的TinyLLMs、注重隐私保护的联邦学习以实现可持续发展，以及融合边缘与云端的混合系统——其中TinyML作为绿色智能过滤器，为LLM后端提供高效支持。本文旨在激发公众思考，共同探索如何构建既实用又环保的人工智能系统。"
  },
  {
    "date": "2025-12-15",
    "title": "A Survey of Machine Learning Approaches in Logic Synthesis",
    "authors": "Miao Liu, Liwei Ni, Junfeng Liu, Xingyu Meng, Rui Wang, Xiaoze Lin, Xinhua Lai, Xingquan Li, Jungang Xu",
    "publish": "ACM Transactions on Design Automation of Electronic Systems",
    "url": "https://doi.org/10.1145/3785362",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "逻辑综合中机器学习方法的综述",
    "abstract_zh": "None"
  },
  {
    "date": "2025-12-15",
    "title": "Automated Answer Paper Evaluation (AAPE)",
    "authors": "C K Roopa, D Ashwin, B Adithya, C Ajith, J Chandan",
    "publish": "2025 First International Conference on Intelligent Computing and Communication Systems (CICCS)",
    "url": "https://doi.org/10.1109/ciccs66437.2025.11280115",
    "source": "IEEE",
    "abstract": "To ensure accuracy and fairness in education, grading is crucial. However, conventional techniques for assessing answer sheets can be laborious and frequently influenced by bias. By updating the entire procedure, the Automated Answer Paper Evaluation (AAPE) system solves these problems. Using optical character recognition (OCR), it first scans and reads handwritten responses before comparing them to model responses. Our system uses Natural Language Processing (NLP), which includes cosine metrics to measure similarity and WordNet to expand synonyms, to increase the accuracy of this comparison. Our system additionally integrates Large Language Models (LLMs) to enhance comprehension of the content. Additionally, it does not ignore visual information; to guarantee that answers are graded fairly, diagrams are examined using YOLO-based object detection.Altogether, this smart approach speeds up grading, reduces bias, and gives students more meaningful feedback.",
    "title_zh": "自动答题纸评分（AAPE）",
    "abstract_zh": "为确保教育的准确性和公平性，评分至关重要。然而，传统的答卷评估方法往往耗时费力，且容易受到主观偏见的影响。通过全面优化整个流程，自动答卷评阅系统（AAPE）有效解决了这些问题。该系统首先利用光学字符识别（OCR）技术扫描并读取手写答案，随后将其与标准答案进行比对。我们的系统采用自然语言处理（NLP）技术，包括余弦相似度度量以评估文本相似性，以及WordNet扩展同义词，从而提升比对的准确性。此外，系统还引入大型语言模型（LLMs），以增强对内容语义的理解。同时，系统并未忽视视觉信息——通过基于YOLO的目标检测技术，对答题中的图表进行分析，以确保评分公正。总体而言，这一智能方法显著加快了评分速度，减少了人为偏见，并为学生提供更具价值的反馈。"
  },
  {
    "date": "2025-12-15",
    "title": "Design and Verification of Programmable UART with AXI in Application-Specific Integrated Circuits",
    "authors": "Jagadeesh Basavaiah, Poornima H S, Audre Arlene Anthony, Naveen Kumar H N, Mahadevaswamy Mahadevaswamy, Chandrashekar Mohan Patil",
    "publish": "2025 First International Conference on Intelligent Computing and Communication Systems (CICCS)",
    "url": "https://doi.org/10.1109/ciccs66437.2025.11280043",
    "source": "IEEE",
    "abstract": "Modern embedded systems demand high-speed, reliable communication between IP cores, necessitating the integration of robust bus protocols and interfaces. To meet this need, our work focuses on designing reusable, synthesizable modules that ensure performance and verification completeness. This study presents the design and verification of a programmable UART, and an AXI interface tailored for ASIC-based embedded systems. The UART module supports serial data transmission, while AXI provides high-speed bus communication for SoCs. Functional simulation and waveform analysis validate the successful data transmission of UART, and AXI’s performance is confirmed using write/read transactions with proper handshaking. The verification is conducted using Universal Verification Methodology (UVM), achieving 90% functional coverage and confirming protocol compliance. The proposed work ensures reliable, low-latency communication, suitable for high-performance embedded applications.",
    "title_zh": "面向特定应用集成电路的可编程UART与AXI接口的设计与验证",
    "abstract_zh": "现代嵌入式系统要求IP核之间实现高速、可靠的通信，因此必须集成强大且稳健的总线协议与接口。为满足这一需求，本研究致力于设计可重用、可综合的模块，以确保性能和验证完整性。本文提出了一种可编程UART的设计与验证，以及专为基于ASIC的嵌入式系统定制的AXI接口。其中，UART模块支持串行数据传输，而AXI则为片上系统（SoC）提供高速总线通信能力。通过功能仿真与波形分析，验证了UART成功实现数据传输；AXI接口的性能则通过带有正确握手机制的读写事务得到确认。验证过程采用通用验证方法学（UVM），实现了90%的功能覆盖率，并验证了协议合规性。所提出的方案确保了可靠且低延迟的通信，适用于高性能嵌入式应用场景。"
  },
  {
    "date": "2025-12-15",
    "title": "Automating Software Test Case Generation with Pre-trained Large Language Models",
    "authors": "Moughit Imane, Hafidi Imad, Mouhassine Najib",
    "publish": "2025 5th International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)",
    "url": "https://doi.org/10.1109/iceccme64568.2025.11277902",
    "source": "IEEE",
    "abstract": "the rise of Large Language Models (LLMs) has significantly impacted software engineering based on their capability to comprehend and create natural language. Traditional test-case automation techniques, whether they depend only on textual descriptions or exclusively on focal methods often fail to express expected behavior, capture edge conditions, and expose faults. Such limitations make them illsuited for Test-Driven Development (TDD). To overcome these challenges, we present an approach that combines text and methods for test case generation. Using prompt engineering techniques, we evaluate two LLMs: Meta-LLaMA-3-8B (base) and Mistral-7B-Instruct on a dataset of $\\mathbf{1 3 6, 0 6 0}$ entries, resulting in the generation of 7,606- unit test cases from opensource projects to assess whether instruction tuning or an increase in parameter count has a greater impact on test case generation quality. Results show that instruction tuning outperforms a larger base model, especially in zero-shot settings, while few-shot prompting reduces syntax errors by $\\boldsymbol{\\sim} \\mathbf{9 0 \\%}$ for both.",
    "title_zh": "使用预训练大型语言模型自动化软件测试用例生成",
    "abstract_zh": "大型语言模型（LLMs）的兴起显著影响了软件工程领域，这得益于其在理解与生成自然语言方面的能力。传统的测试用例自动化技术，无论是仅依赖文本描述，还是仅采用焦点方法，往往难以准确表达预期行为、捕捉边缘情况，也无法有效暴露缺陷。这些局限性使其不适合用于测试驱动开发（TDD）。为克服上述挑战，我们提出了一种结合文本与方法的测试用例生成方法。通过提示工程（prompt engineering）技术，我们在包含 $\\mathbf{136,060}$ 条数据的语料库上评估了两种 LLM：Meta-LLaMA-3-8B（基础模型）和 Mistral-7B-Instruct，从开源项目中生成了 7,606 个单元测试用例，以评估指令微调与参数量增加对测试用例生成质量的影响。实验结果表明，在零样本（zero-shot）场景下，指令微调的表现优于参数量更大的基础模型；而采用少样本提示（few-shot prompting）可使语法错误减少约 $\\boldsymbol{\\sim} \\mathbf{90\\%}$，且在两种模型上均取得类似效果。"
  },
  {
    "date": "2025-12-15",
    "title": "Securing the Internet of Things: The Role of Artificial Intelligence in Agricultural Cyber Defense",
    "authors": "Dora Konstantinou, Teodora Stoyanova, Stoyan Stoyanov, Radostin Rafailov",
    "publish": "2025 10th International Conference on Energy Efficiency and Agricultural Engineering (EE&amp;amp;AE)",
    "url": "https://doi.org/10.1109/eeae65901.2025.11273781",
    "source": "IEEE",
    "abstract": "This paper aims to examine the developing landscape of Internet of Things (IoT) technology from a practical perspective. Exploring IoT's implications in various domains including agriculture, the paper focuses on the security challenges that arise from its fast adoption and growth both in domestic and industrial domains. Furthermore, identifying and preventing these critical security vulnerabilities is highly essential. The increased number of attacks on IoT systems, especially the last two years (2023-2024), led us to explore the potential of automated security systems utilizing AI, apart from the conventional ways of security. In response to this, this particular study aims to explore the role of artificial intelligence (AI) in enhancing IoT systems' security with an emphasis on agriculture. The study specifically explores and presents the way AI can be applied to minimize security risks along with a comprehensive review of current AI-driven platforms and solutions designed to strengthen IoT security frameworks in the particular domain.",
    "title_zh": "保障物联网安全：人工智能在农业网络防御中的作用",
    "abstract_zh": "本文旨在从实践角度审视物联网（IoT）技术的发展格局。通过探讨物联网在农业等多个领域的应用，本文重点关注其在家庭和工业领域快速普及与增长所带来的安全挑战。此外，识别并防范这些关键的安全漏洞至关重要。近年来，尤其是2023至2024年这两年间，针对物联网系统的攻击事件显著增加，促使我们探索除传统安全手段外，利用人工智能（AI）构建自动化安全系统所具备的潜力。为此，本研究特别聚焦于人工智能在提升物联网系统安全性方面的作用，尤其关注农业领域。研究具体探讨并展示了人工智能如何应用于降低安全风险，并对当前用于强化物联网安全框架的各类AI驱动平台与解决方案进行了全面综述。"
  },
  {
    "date": "2025-12-15",
    "title": "LLMMutation: Mutation Testing for Large Language Models",
    "authors": "Zhiwei Liu, Xinhong Duan, Zhanqi Cui, Zheng Zeng",
    "publish": "2025 IEEE 37th International Conference on Tools with Artificial Intelligence (ICTAI)",
    "url": "https://doi.org/10.1109/ictai66417.2025.00053",
    "source": "IEEE",
    "abstract": "The performance of Large Language Models (LLMs) heavily depends on the quality of their training data, while their capabilities are primarily evaluated through taskspecific test datasets. This evaluation approach makes the quality of test datasets a critical factor in ensuring LLM reliability. Even high-accuracy LLMs may exhibit weak generalizability and insufficient robustness when they are evaluated using flawed test sets with insufficient sample sizes or limited diversity. In traditional software testing, mutation testing is a well-established technique for assessing test suite effectiveness by measuring the ability to detect artificially injected faults. However, fundamental differences exist between traditional software and Transformerbased LLMs, preventing the direct application of classical mutation testing techniques. To address this, we propose LLMMutation, a dedicated test dataset quality evaluation method for LLMs, which applies multi-level mutation operators to introduce perturbations into the model. Experimental results demonstrate that models perturbed by different mutation operators exhibit significant accuracy degradation, which validate LLMMutation's effectiveness in fault injection. In addition, different test datasets exhibit distinct mutation score distributions under LLMMutation perturbations, which is positively correlated with the quality of the datasets. This confirms that the mutation score can effectively distinguish the quality of datasets used for the same task.",
    "title_zh": "LLMMutation：大型语言模型的变异测试",
    "abstract_zh": "大型语言模型（LLMs）的性能在很大程度上依赖于其训练数据的质量，而其能力主要通过特定任务的测试数据集进行评估。这种评估方式使得测试数据集的质量成为确保LLM可靠性的重要因素。即使某些LLM在准确率上表现优异，若使用样本量不足或多样性有限的有缺陷测试集进行评估，仍可能表现出较差的泛化能力和不足的鲁棒性。在传统软件测试中，变异测试（mutation testing）是一种成熟的技术，通过测量测试用例发现人为注入缺陷的能力来评估测试套件的有效性。然而，传统软件与基于Transformer架构的LLM之间存在根本性差异，导致经典变异测试方法无法直接应用。为解决这一问题，我们提出了LLMMutation——一种专用于LLM测试数据集质量评估的方法。该方法采用多层次的变异算子对测试数据引入扰动。实验结果表明，经不同变异算子扰动后的模型均出现显著的准确率下降，验证了LLMMutation在故障注入方面的有效性。此外，在LLMMutation扰动下，不同测试数据集呈现出不同的变异得分分布，且该分布与数据集质量呈正相关。这表明，变异得分能够有效区分同一任务所使用的不同数据集的质量优劣。"
  },
  {
    "date": "2025-12-15",
    "title": "Silent Data Corruption– Optimal Mitigation Strategies for Datacenter Computing",
    "authors": "Nirmal Saxena, Saurabh Hukerikar, Siva Hari",
    "publish": "IEEE Micro",
    "url": "https://doi.org/10.1109/mm.2025.3643799",
    "source": "IEEE",
    "abstract": "Given current datacenter compute scales, extended job durations, and existing detection mechanisms, the risk of silent data corruption (SDC) exposure is substantial. Leveraging existing detection coverage and SDC characteristics, we propose optimized SDC mitigation strategies that balance availability and reliability. These optimization strategies strive to achieve Shannon’s like bounds on the overhead required for reliable (meaning SDC mitigation) compute channels. While error-correcting codes effectively address transient faults in memory, storage, and high-speed links, residual SDC risks persist. We advocate for algorithm-based error detection (ABED) to bridge this gap and stress the need for ongoing ABED research to adapt to evolving compute patterns. Low-level fault simulation for SDC assessment in long-running jobs is impractical; instead, near-real-time evaluation is essential. Although our approach partially addresses systematic faults, robust design verification and manufacturing tests remain critical for comprehensive fault management. This work highlights the urgency of scalable, efficient SDC detection in large-scale systems.",
    "title_zh": "静默数据损坏——数据中心计算中的最佳缓解策略",
    "abstract_zh": "在当前数据中心的计算规模、任务执行时间延长以及现有检测机制的背景下，隐性数据损坏（SDC）暴露的风险极为显著。基于现有的检测覆盖范围和SDC特性，我们提出了优化的SDC缓解策略，在系统可用性与可靠性之间取得平衡。这些优化策略力求在实现可靠（即SDC缓解）计算通道所需开销方面达到类似香农极限的性能边界。尽管纠错码能够有效应对内存、存储和高速链路中的瞬态故障，但残余的SDC风险依然存在。因此，我们主张采用基于算法的错误检测（ABED）来弥补这一差距，并强调必须持续推动ABED研究，以适应不断演进的计算模式。对于长时间运行任务中的SDC评估，低层次的故障仿真不切实际；取而代之的是，近实时的评估机制至关重要。虽然我们的方法部分解决了系统性故障问题，但健全的设计验证和制造测试仍是实现全面故障管理的关键。本研究凸显了在大规模系统中实现可扩展、高效率SDC检测的紧迫性。"
  },
  {
    "date": "2025-12-15",
    "title": "Twin Pipeline: AI Sibling of CI/CD",
    "authors": "Yash Salvi, Isha Kanade",
    "publish": "2025 9th International Conference on Computing, Communication, Control and Automation (ICCCBEA)",
    "url": "https://doi.org/10.1109/iccubea65967.2025.11283703",
    "source": "IEEE",
    "abstract": "CI/CD pipelines have become an integral part of today's software development processes, where they help in rapid integration and deployment of code changes. Despite that, they are associated with long execution times, flaky tests and high resource consumption which lead them to become less efficient. Even with state of the art AI driven optimizations, developers frequently need to rerun pipelines to diagnose and correct problems, which results in time being wasted while waiting for pipeline execution to finish and unnecessary resource consumptions. These inefficiencies slow down the development cycle and increase the cost of infrastructure, making CI/CD less effective at scale. This paper introduces the AI Twin Pipeline, a predictive system that simulates CI/CD execution outcomes before running the actual pipeline. By analyzing past pipeline executions and applying predictive modeling, it identifies potential failure points, offers corrective insights to address detected issues, and estimates the execution time of the actual CI/CD pipeline. Developers can debug and resolve issues upfront, reducing unnecessary pipeline runs and improving development efficiency. Unlike existing AI-optimized CI/CD solutions that focus on optimizations after the pipeline is triggered, the AI Twin Pipeline proactively identifies failures before triggering CI/CD thus, minimizing redundant submissions and optimizing resource utilization. This novel approach empowers developers with faster feedback, ensuring more reliable code submissions while significantly improving CI/CD pipeline performance.",
    "title_zh": "双管道：CI/CD 的人工智能兄弟",
    "abstract_zh": "持续集成/持续部署（CI/CD）流水线已成为当今软件开发流程中不可或缺的一部分，有助于快速整合和部署代码变更。然而，这些流水线常伴随着执行时间长、测试结果不稳定以及资源消耗高等问题，导致其效率下降。即使采用最先进的AI驱动优化技术，开发者仍需频繁重新运行流水线以诊断和修复问题，这不仅浪费了等待流水线完成的时间，还造成了不必要的资源消耗。这些低效因素拖慢了开发周期，增加了基础设施成本，使CI/CD在大规模应用时效果大打折扣。\n\n本文提出了一种名为“AI孪生流水线”（AI Twin Pipeline）的预测系统，该系统可在实际运行流水线之前，模拟其执行结果。通过分析历史流水线执行数据并应用预测建模，该系统能够识别潜在的失败点，提供针对性的修正建议以解决发现的问题，并预估实际CI/CD流水线的执行时间。开发者可提前进行调试和问题修复，从而减少不必要的流水线重跑，显著提升开发效率。\n\n与现有AI优化CI/CD方案主要在流水线触发后进行优化不同，AI孪生流水线能够在触发前主动识别可能的失败，从而最大限度减少重复提交，优化资源利用。这一创新方法为开发者提供了更快的反馈机制，确保代码提交更加可靠，同时大幅提升了CI/CD流水线的整体性能。"
  },
  {
    "date": "2025-12-15",
    "title": "Empowering Conversational Systems Through AI Agents Specialized in Reusable APIs Across Software Ecosystems",
    "authors": "Rayfran Rocha Lima, Irineu Evangelista Cruz de Brito, Jardel da Cunha Nascimento, Elias Nascimento Nogueira",
    "publish": "2025 IEEE 37th International Conference on Tools with Artificial Intelligence (ICTAI)",
    "url": "https://doi.org/10.1109/ictai66417.2025.00191",
    "source": "IEEE",
    "abstract": "Software reuse is a key strategy in modern engineering practices, yet it remains underexploited in enterprise environments due to technical barriers and limited accessibility to existing functionalities. This paper presents a novel architecture that integrates LLMs with AI agents to enhance conversational systems, enabling intelligent reuse of enterprise APIs through natural language commands. To test the feasibility of this solution, a multi-agent system with three roles was designed - LLM interpreter, recommender agent, and executor agent - deployed in a real-world setting. The solution evaluation combined automated accuracy tests, a TAM-based user satisfaction survey, and focus groups with developers. The bge-m3 embedding model achieved the highest top-5 recommendation accuracy. Users reported increased perceived usefulness and ease of use. Developers valued the modularity of the architecture, but expressed concerns about operational risks. The proposed architecture effectively connects user intent to actionable enterprise APIs, enhancing reuse, accessibility, and adaptability while pointing to future improvements in security, learning, and transparency.",
    "title_zh": "通过专用于跨软件生态系统的可重用API的AI代理，赋能对话式系统",
    "abstract_zh": "软件复用是现代工程实践中的关键策略，但在企业环境中仍因技术障碍和现有功能难以获取而未被充分应用。本文提出一种创新架构，将大语言模型（LLMs）与AI代理相结合，以增强对话系统，通过自然语言指令实现企业API的智能复用。为验证该方案的可行性，设计了一个包含三个角色的多代理系统——LLM解释器、推荐代理和执行代理，并在真实场景中部署。解决方案评估结合了自动化准确率测试、基于技术接受模型（TAM）的用户满意度调查以及开发人员焦点小组讨论。实验结果表明，bge-m3嵌入模型在Top-5推荐准确率方面表现最佳。用户报告感知有用性和易用性均有提升。开发人员认可该架构的模块化设计，但对操作风险表示担忧。所提出的架构有效实现了用户意图到可执行企业API的连接，在提升复用性、可访问性和适应性的同时，也指出了未来在安全性、学习能力及透明度方面的改进方向。"
  },
  {
    "date": "2025-12-15",
    "title": "Layered Minimums and Virtual Labs in the Service of Industrial Transferability",
    "authors": "Edina Popof, Zoltán Gömböcz, Zoltán Illés, Gergely Bencsik",
    "publish": "2025 International Conference on Emerging eLearning Technologies and Applications (ICETA)",
    "url": "https://doi.org/10.1109/iceta67772.2025.11280253",
    "source": "IEEE",
    "abstract": "We present an AI-assisted simulated laboratory environment where the Python and C# ecosystem and entry-level ESP32/Arduino edge devices bridge the gap between educational practice and industrial expectations. The virtual lab provides a cost-effective, safe setting for students to model industrial processes and to practice fault diagnosis and remediation, while lowering the maintenance overhead of physical labs. The system is built on realistic Programmable logic controller (PLC) and process simulations, with measurable Definition-of-Done (DoD) gates and telemetry and observability indicators such as Time-to-First-Success (TTFS), Time-to-Diagnosis (ToD) and a /health endpoint. An LLM-based coach delivers adaptive, personalized feedback and dynamically tunes the training content to learners’ behavioral and cognitive patterns, thereby supporting differentiated instruction. Our result is a reference architecture and methodology optimized for metrics and industrial applicability, which enables rapid prototyping and standardizable quality assurance in an educational environment. The study contributes to the future of educational practice, where the coordinated application of simulation, IoT devices, and artificial intelligence creates a bridge between school and industry, promoting the development of digital competences and labor market integration.",
    "title_zh": "面向工业可转移性的分层最低标准与虚拟实验室",
    "abstract_zh": "我们提出一个由人工智能辅助的虚拟实验室环境，该环境通过Python和C#生态系统，以及入门级ESP32/Arduino边缘设备，弥合了教育实践与工业需求之间的差距。该虚拟实验室为学生提供了一个成本低廉、安全可靠的平台，使其能够模拟工业流程，练习故障诊断与修复，同时显著降低物理实验室的维护成本。系统基于真实的可编程逻辑控制器（PLC）和过程仿真，配备可量化的“完成定义”（DoD）评估节点，以及诸如首次成功时间（TTFS）、故障诊断时间（ToD）和/health健康检查接口等可观测性指标。基于大语言模型（LLM）的智能导师能够提供自适应、个性化的反馈，并根据学习者的认知与行为模式动态调整教学内容，从而实现差异化教学支持。本研究构建了一套优化于关键绩效指标与工业适用性的参考架构与方法论，支持教育环境中快速原型开发与标准化质量保障。该成果推动了教育实践的未来发展，通过仿真技术、物联网设备与人工智能的协同应用，搭建起学校与产业之间的桥梁，促进数字能力培养与劳动力市场融合。"
  },
  {
    "date": "2025-12-15",
    "title": "Synergizing Human-AI Cooperation in Automated Software Testing: A Balance for Enhanced Efficiency, Reliability, and Trust",
    "authors": "Micheal Tuape, Jussi Kasurinen, Maria T Najjemba, Aminul D Islam",
    "publish": "2025 5th International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)",
    "url": "https://doi.org/10.1109/iceccme64568.2025.11277750",
    "source": "IEEE",
    "abstract": "Achieving reliable and trustworthy automated software testing requires a synergistic approach that leverages both human expertise and AI capabilities. This position paper argues that active human oversight is crucial for addressing key challenges such as the test oracle problem and training data bias. By fostering human-AI cooperation, potential biases and inaccuracies can be mitigated, leading to more efficient, reliable, and trustworthy automated testing. This approach also aligns with principles from the EU AI Act, which emphasizes the need for effective human control over AI systems. The paper concludes by outlining a future research agenda focused on developing frameworks to foster seamless human-AI collaboration and presenting several open research questions vital for advancing the field.",
    "title_zh": "人机协同在自动化软件测试中的融合：提升效率、可靠性与信任的平衡之道",
    "abstract_zh": "实现可靠且可信的自动化软件测试，需要一种融合人类专业知识与人工智能能力的协同方法。本文主张，积极的人类监督对于应对测试判定问题（test oracle problem）和训练数据偏差等关键挑战至关重要。通过促进人机协作，可以有效缓解潜在的偏见与不准确性，从而提升自动化测试的效率、可靠性与可信度。这一方法也符合欧盟《人工智能法案》（EU AI Act）的原则，该法案强调必须对人工智能系统实施有效的人员控制。本文最后提出了一个未来研究议程，旨在开发促进无缝人机协作的框架，并列举了若干亟待解决的关键开放性研究问题，以推动该领域的发展。"
  },
  {
    "date": "2025-12-15",
    "title": "From Retrieval to Response: Tracing the Impact of Embedding Quality in RAG Systems",
    "authors": "Orlando Amaral Cejas, Yuejun Guo, Qiang Tang",
    "publish": "IEEE Access",
    "url": "https://doi.org/10.1109/access.2025.3644595",
    "source": "IEEE",
    "abstract": "Recently, Retrieval Augmented Generation (RAG) has emerged as a powerful paradigm for enhancing the capability of Large Language Models (LLMs) by incorporating external, up-to-date, or domain-specific information through retrieval mechanisms. In their implementation, RAG systems usually rely on vector embedding to identify relevant context from an underlying knowledge base, and have gained widespread adoption across tasks such as question answering and summarization due to their superior performance. Surprisingly, the interplay between embedding quality and RAG performance has not received much attention so far, even though it is commonly perceived as the key factor for influencing retrieval accuracy and the trustworthiness of the generated answer. To fill in this gap, in this paper, we systematically investigate the impact of embedding quality on the overall performance of RAG-based applications. Through a comprehensive performance evaluation of 12 embeddings over two datasets (i.e., a public dataset SQuAD and a private Telecom dataset), we reveal quite different retrieval accuracy levels for these embeddings, while following the same trend on both datasets. For the public dataset SQuAD, there is a strong positive correlation between retrieval accuracy and RAG performance, while this relationship is not as strong in the private Telecom dataset. Our findings highlight that embedding quality is a fundamental yet tricky determinant of RAG systems’ ability to generate trustworthy responses and it should be examined deeply on a case-by-case basis. We open source our full implementation at: https://github.com/Yuejun-GUO/RAG-exp.",
    "title_zh": "从检索到响应：追踪嵌入质量在RAG系统中的影响",
    "abstract_zh": "最近，检索增强生成（Retrieval Augmented Generation, RAG）作为一种强大的范式，通过检索机制引入外部、最新或领域特定的信息，显著提升了大型语言模型（LLMs）的能力。在实际应用中，RAG系统通常依赖于向量嵌入技术，从底层知识库中识别相关上下文，因此在问答和摘要等任务中得到了广泛应用，表现出优异的性能。然而，令人意外的是，尽管普遍认为嵌入质量是影响检索准确性和生成答案可信度的关键因素，但其与RAG性能之间的相互作用却尚未受到足够关注。为填补这一研究空白，本文系统地探究了嵌入质量对基于RAG的应用整体性能的影响。通过对12种不同嵌入方法在两个数据集（即公开数据集SQuAD和私有电信数据集）上的全面性能评估，我们发现这些嵌入方法在检索准确率上表现差异显著，且在两个数据集上呈现出一致的趋势。在公开数据集SQuAD上，检索准确率与RAG性能之间存在强烈的正相关关系；而在私有电信数据集上，这种关联性则相对较弱。我们的研究结果表明，嵌入质量是决定RAG系统生成可信回答能力的根本性但又复杂多变的因素，必须针对具体应用场景进行深入分析。我们已将完整的实现代码开源，地址为：https://github.com/Yuejun-GUO/RAG-exp。"
  },
  {
    "date": "2025-12-15",
    "title": "Training Generative Judge with Hard Negative Mining: A Metric Learning Perspective",
    "authors": "Changxin Chen",
    "publish": "2025 IEEE 37th International Conference on Tools with Artificial Intelligence (ICTAI)",
    "url": "https://doi.org/10.1109/ictai66417.2025.00112",
    "source": "IEEE",
    "abstract": "Prior SFT-based LLM-as-a-Judge suffers from a distributional mismatch between GPT-4 and the smaller opensource language models (SLMs), often leading to overfitting. On-policy methods, such as Group Relative Policy Optimization (GRPO), attempt to address this limitation by training models using their own outputs. However, conventional GRPO typically rely on randomly sampled data, often selecting negative samples with low model-assigned probabilities-samples that provide weak gradient signals and thus limit learning efficacy. To overcome this, we first establish a theoretical connection by proving that GRPO is equivalent to the margin ranking loss commonly used in metric learning. Building on this, we introduce the first approach to optimize GRPO through hard negative mining, a well-established technique in metric learning. We define hard negatives within the context of LLM-as-a-Judge and propose a simple yet effective strategy for identifying them. We evaluate our method on three out-of-domain datasets and one in-domain dataset. Compared to SFT-based methods, our approach demonstrates superior scalability, which is becoming increasingly important for training on modern, largescale datasets. Specifically, while SFT-based methods struggle to improve performance when scaling from 10 % to 100 % of the training data, our method exhibits consistent improvements across this range. This suggests that current SFT methods may not be well-suited for training generative judges. Furthermore, empirical results show that our approach consistently outperforms larger open-source baselines.",
    "title_zh": "基于困难负样本挖掘的生成式判别器训练：一种度量学习视角",
    "abstract_zh": "基于监督微调（SFT）的大型语言模型作为评判者（LLM-as-a-Judge）存在GPT-4与小型开源语言模型（SLMs）之间的分布差异问题，常导致过拟合。在策略上的方法，如组相对策略优化（GRPO），试图通过使用模型自身输出进行训练来缓解这一局限性。然而，传统的GRPO通常依赖于随机采样的数据，常常选取模型赋予低概率的负样本——这些样本提供的梯度信号较弱，从而限制了学习效率。为克服这一问题，我们首先从理论上建立了联系，证明GRPO等价于度量学习中常用的边界排序损失（margin ranking loss）。在此基础上，我们提出了首个通过硬负样本挖掘（hard negative mining）优化GRPO的方法，而硬负样本挖掘是度量学习中的成熟技术。我们针对LLM-as-a-Judge的场景定义了“硬负样本”，并提出了一种简单但高效的方法来识别它们。我们在三个跨领域数据集和一个领域内数据集上评估了该方法。与基于SFT的方法相比，我们的方法展现出更优的可扩展性，这在现代大规模数据集训练中变得日益重要。具体而言，当训练数据规模从10%扩大到100%时，SFT方法难以提升性能，而我们的方法在整个范围内均表现出持续的性能改进。这表明当前的SFT方法可能并不适合用于训练生成式评判模型。此外，实证结果还显示，我们的方法始终优于更大的开源基线模型。"
  },
  {
    "date": "2025-12-15",
    "title": "Integrating Large Language Models with Formal Planning to Automate the Design and Validation of Biosignal Processing Pipelines",
    "authors": "João Areias Saraiva, Martin Dyrba, Thomas Kirste",
    "publish": "2025 IEEE 37th International Conference on Tools with Artificial Intelligence (ICTAI)",
    "url": "https://doi.org/10.1109/ictai66417.2025.00117",
    "source": "IEEE",
    "abstract": "Robust analysis of biosignals hinges on well-crafted processing pipelines, yet assembling them is still a slow, errorprone exercise, requiring both domain expertise and programming skills. While Large Language Models (LLMs) offer promising assistance, they fundamentally lack the combinatorial reasoning capabilities needed for designing reliable and reproducible processing pipelines. We present an innovative hybrid system that harnesses LLMs' natural language understanding while leveraging classical AI planning for logical reasoning and validation. A retrieval-augmented model parses natural language pipeline descriptions into implementation-independent atomic biosignal processing operations, which follow to a Hierarchical Task Network (HTN) domain that can plan and validate processing workflows, as well as flag violations of biosignal processing best practices. Evaluation on a preliminary corpus of scenarios demonstrates <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathbf{0. 8 9 - 0. 9 8}$</tex> precision in mapping natural language to processing blocks and 0.97-0.98 F1-score in generating valid 2-15 step pipelines using SHOP3 planner. A drag-and-drop pipeline design interface allows users to build and formally validate their pipeline ideas from scratch, showing potential to democratize and expedite biosignal analysis for scientists of different backgrounds.",
    "title_zh": "将大型语言模型与形式化规划相结合以自动化生物信号处理流程的设计与验证",
    "abstract_zh": "生物信号的稳健分析依赖于精心设计的处理流程，然而构建这些流程仍是一项耗时且易出错的工作，既需要领域专业知识，又要求编程技能。尽管大型语言模型（LLMs）提供了有前景的帮助，但它们在设计可靠、可复现的处理流程方面，本质上缺乏必要的组合推理能力。本文提出了一种创新的混合系统，该系统结合了LLM在自然语言理解方面的优势，同时利用经典人工智能规划技术实现逻辑推理与验证。一个基于检索增强的模型将自然语言描述的处理流程解析为与实现无关的原子生物信号处理操作，并将其转化为层次任务网络（HTN）领域，从而能够规划和验证处理工作流，同时识别违反生物信号处理最佳实践的情况。在初步场景语料库上的评估表明，该系统在将自然语言映射到处理模块时达到0.89–0.98的精确率，在使用SHOP3规划器生成2至15步的有效流程时，F1分数达到0.97–0.98。此外，通过拖拽式流程设计界面，用户可以从零开始构建并正式验证其流程构想，展现出为不同背景的科研人员普及并加速生物信号分析的巨大潜力。"
  },
  {
    "date": "2025-12-15",
    "title": "Human-AI Collaborative Design of ReActNet-XGBoost Hardware Accelerator for Personalized Wearable ECG Monitoring",
    "authors": "Ching-Han Chen, Ke-Shan Lin, Ju-Shan Hsiao",
    "publish": "2025 IEEE 37th International Conference on Tools with Artificial Intelligence (ICTAI)",
    "url": "https://doi.org/10.1109/ictai66417.2025.00215",
    "source": "IEEE",
    "abstract": "The rapid expansion of personalized healthcare has positioned wearable electrocardiogram (ECG) monitoring systems as a pivotal solution for continuous cardiac assessment. However, achieving real-time signal classification under strict power and resource constraints remains a core challenge for edge-based deployment. While convolutional neural networks (CNNs) offer excellent feature extraction capabilities, their intensive computational and memory demands impede efficient implementation on lightweight hardware platforms. To overcome these limitations, we propose a hybrid hardware accelerator integrating a binarized ReActNet backbone for low-overhead feature extraction with an XGBoost decision tree classifier that provides interpretable and hardware-efficient inference. The ReActNet module leverages binary convolution operations to reduce arithmetic complexity, while the XGBoost component replaces dense layers, enhancing model transparency and reducing logic resource usage. To further streamline deployment, we introduce a generative hardware synthesis methodology based on large language models (LLMs), enabling automatic translation of trained XGBoost models into synthesizable VHDL code from their JSON representation. This LLM-assisted workflow significantly reduces development time, eliminates manual RTL coding, and promotes modular, reusable classifier generation adaptable to evolving clinical models. The proposed system is evaluated through RTL-level simulation and synthesis targeting low-power, resource-constrained FPGA platforms, demonstrating its architectural feasibility for real-time ECG classification at the edge. These findings highlight a unified and scalable framework that fuses binary neural networks, interpretable classifiers, and generative AI techniques to accelerate hardware-software co-design and broaden the deployment of edge AI in wearable health applications.",
    "title_zh": "面向个性化可穿戴心电监测的人机协同设计ReActNet-XGBoost硬件加速器",
    "abstract_zh": "个性化医疗的快速发展使可穿戴心电图（ECG）监测系统成为持续心脏评估的关键解决方案。然而，在严格的功耗和资源限制下实现边缘计算环境中的实时信号分类，仍是其部署面临的核心挑战。尽管卷积神经网络（CNN）具备出色的特征提取能力，但其高计算与内存需求阻碍了在轻量级硬件平台上的高效实现。为克服这些局限，我们提出一种混合型硬件加速器：采用二值化ReActNet作为骨干网络，实现低开销的特征提取；同时结合XGBoost决策树分类器，提供可解释性强且硬件友好的推理能力。ReActNet模块通过二值卷积操作降低算术复杂度，而XGBoost组件则替代传统密集层，提升模型透明度并减少逻辑资源占用。为进一步简化部署流程，我们引入一种基于大语言模型（LLM）的生成式硬件综合方法，能够将训练完成的XGBoost模型从JSON表示自动转换为可综合的VHDL代码。该LLM辅助的工作流显著缩短开发周期，避免手动编写RTL代码，并支持模块化、可复用的分类器生成，便于适应不断演进的临床模型需求。所提出的系统通过针对低功耗、资源受限FPGA平台的RTL级仿真与综合进行评估，验证了其在边缘端实现实时ECG分类的架构可行性。研究结果表明，该方案构建了一个统一且可扩展的框架，融合二值神经网络、可解释性分类器与生成式人工智能技术，有效加速软硬件协同设计进程，推动边缘人工智能在可穿戴健康应用中的广泛应用。"
  },
  {
    "date": "2025-12-15",
    "title": "Standardized Logging Approach for Software Code-bases Using Natural Language Processing",
    "authors": "Brian Kipchumba Tum, Eunice Njeri Mwangi, Annette Waithira Irungu, Isaac Nyabisa Oteyo",
    "publish": "2025 International Conference on Information and Communication Technology for Development for Africa (ICT4DA)",
    "url": "https://doi.org/10.1109/ict4da67218.2025.11282517",
    "source": "IEEE",
    "abstract": "Software development efforts are shifting from localized to global collaboration ventures. This shift introduces linguistic disparity challenges, such as software developers using the same natural language differently. More concretely, in software development, linguistic disparity is experienced in how software developers describe anticipated error messages (in log statements) within code bases. This disparity negatively impacts software debugging activities. To address this challenge, in this paper, we propose a standardized logging approach using a natural language processing model. The model uses a large corpus of well-structured log entries to suggest semantically similar log entries during log instrumentation. Our approach ensures that software debugging is done in a uniform and standardized natural language, which streamlines collaboration in software development teams. To validate our approach, we trained our model using LogHub<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> log files. The validation shows promising results when tested using log statements in the English language.",
    "title_zh": "基于自然语言处理的软件代码库标准化日志方法",
    "abstract_zh": "软件开发工作正从本地化协作转向全球协作。这一转变带来了语言差异的挑战，例如，使用相同自然语言的软件开发者在表达上存在差异。具体而言，在软件开发中，语言差异体现在开发者对代码库中预期错误消息（日志语句）的描述方式上。这种差异对软件调试活动产生了负面影响。为应对这一挑战，本文提出一种基于自然语言处理模型的标准化日志记录方法。该模型利用大量结构良好的日志条目语料库，在日志注入过程中推荐语义相似的日志条目。我们的方法确保了软件调试采用统一且标准化的自然语言，从而提升了软件开发团队之间的协作效率。为验证该方法的有效性，我们使用LogHub<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> 的日志文件训练了该模型。测试结果表明，该方法在英文日志语句上的表现具有良好的前景。"
  },
  {
    "date": "2025-12-15",
    "title": "AI-Based Real-Time Feedback System for Automated Assignment Evaluation and Personalized Quiz Generation in Computer Engineering Education",
    "authors": "Priya Surana, Sushma Vispute, Harsha Talele, Ashish Suryawanshi, Aditya Vedpathak, Irfan Tadvi, Atharva Toke",
    "publish": "2025 9th International Conference on Computing, Communication, Control and Automation (ICCCBEA)",
    "url": "https://doi.org/10.1109/iccubea65967.2025.11284234",
    "source": "IEEE",
    "abstract": "This article proposes a novel AI-based system that aims to offer real-time feedback and customized quizzes for computer engineering problems. The system focuses on one of the main issues of contemporary education: the time and effort required for manual grading. Through the utilization of Generative AI, our solution automatizes the process of feedback, resulting in a drastic reduction of the time spent by educators on manual grading. One of the main features of the system is its capability to process varied assignment types, both digital and hand-written submissions, with the inclusion of Optical Character Recognition (OCR). The system examines student responses by applying deep learning models to detect particular errors and knowledge gaps. It then uses Generative AI to create personalized quizzes automatically to reinforce principal concepts and enhance student comprehension. This method not only simplifies the grading process but also maximizes the learning process by giving students immediate, personalized feedback. Our testing of the system validates its effectiveness, with higher accuracy, faster operation rates, and higher user satisfaction.",
    "title_zh": "基于人工智能的实时反馈系统在计算机工程教育中的自动作业评估与个性化测验生成",
    "abstract_zh": "本文提出了一种基于人工智能的新系统，旨在为计算机工程问题提供实时反馈和个性化测验。该系统聚焦于当代教育中的一个核心难题：人工批改作业所耗费的时间与精力。通过运用生成式人工智能，我们的解决方案实现了反馈过程的自动化，大幅减少了教师在人工批改上所花费的时间。系统的主要功能之一是能够处理多种类型的作业，包括电子提交和手写作业，并集成了光学字符识别（OCR）技术。系统利用深度学习模型分析学生的作答情况，识别特定错误和知识盲点，随后借助生成式人工智能自动生成个性化的练习题，以强化核心概念并提升学生的理解能力。这一方法不仅简化了批改流程，还通过即时、个性化的反馈最大化了学习效果。我们对系统的测试验证了其有效性，结果显示其具有更高的准确率、更快的运行速度以及更高的用户满意度。"
  },
  {
    "date": "2025-12-15",
    "title": "Redefining Open-ended Assessment Authorship: Empirical Insights on Mind versus Machine Assessments in Computer Science",
    "authors": "Benson Kituku, Eric Araka, Elizaphan Maina",
    "publish": "2025 International Conference on Information and Communication Technology for Development for Africa (ICT4DA)",
    "url": "https://doi.org/10.1109/ict4da67218.2025.11282535",
    "source": "IEEE",
    "abstract": "Manual preparation and design of high-quality and practical assessments can be challenging, time-consuming, labor-intensive, and require significant training alongside experience on the part of the educator. Currently, multiple-choice question (MCQ) automatic assessment methods are being employed to solve these challenges. However, MCQs are deficient in cultivating critical thinking, advanced cognitive skills and skills of analytical problem-solving as compared to open-ended questions, which are currently administered manually. Furthermore, there is insufficient empirical evidence to demonstrate the effectiveness of automatic question generation for open-ended assessments. Consequently, using a quasi-experimental approach, this research examined the latent potential of machine (ChatGPT-4) in supporting the design of open-ended assessments. The study designed both machine- and human-generated exams, assessed students, graded their responses, and compared the two sets of assessments and scores using external examiner reviews and statistical analysis in the computer science domain. The findings indicated equivalence between machine- and human-generated assessments, particularly when practical prompt engineering, moderation, and human oversight are applied to machine-generated assessments that align with course content and learning outcomes. Therefore, the study concluded that a hybrid approach, combining Generative artificial intelligence (Gen AI) and human expertise, offers a promising direction for developing credible and practical assessments.",
    "title_zh": "重新定义开放性评估的作者身份：计算机科学领域关于人脑与机器评估的实证洞察",
    "abstract_zh": "高质量且实用的评估题目的手动设计与准备往往具有挑战性，耗时耗力，需要教育者具备充分的训练和经验。目前，多项选择题（MCQ）的自动评分方法被广泛采用，以应对这些挑战。然而，相较于需人工评阅的开放性问题，多项选择题在培养学生批判性思维、高阶认知能力以及分析解决问题的能力方面存在明显不足。此外，目前尚缺乏充分的实证证据来证明自动生成开放性试题的有效性。因此，本研究采用准实验方法，考察了机器（ChatGPT-4）在支持开放性评估设计方面的潜在能力。研究设计了由机器与人类分别生成的考试试卷，对学生进行测评，对作答结果进行评分，并通过外部评审员评价及统计分析，在计算机科学领域对两组评估结果及其得分进行了比较。研究结果表明，在应用有效的提示工程（prompt engineering）、适度调整及人工监督的前提下，机器生成的评估与人工生成的评估在质量上具有等效性，尤其当机器生成的内容与课程内容和学习目标相一致时。因此，研究得出结论：结合生成式人工智能（Gen AI）与人类专业经验的混合模式，为开发可信且实用的评估体系提供了极具前景的发展方向。"
  },
  {
    "date": "2025-12-15",
    "title": "EvoCAD: Evolutionary CAD Code Generation with Vision Language Models",
    "authors": "Tobias Preintner, Weixuan Yuan, Adrian König, Thomas Bäck, Elena Raponi, Niki Van Stein",
    "publish": "2025 IEEE 37th International Conference on Tools with Artificial Intelligence (ICTAI)",
    "url": "https://doi.org/10.1109/ictai66417.2025.00075",
    "source": "IEEE",
    "abstract": "Combining large language models with evolutionary computation algorithms represents a promising research direction leveraging the remarkable generative and in-context learning capabilities of LLMs with the strengths of evolutionary algorithms. In this work, we present EvoCAD, a method for generating computer-aided design (CAD) objects through their symbolic representations using vision language models and evolutionary optimization. Our method samples multiple CAD objects, which are then optimized using an evolutionary approach with vision language and reasoning language models. We assess our method using GPT-4V and GPT-4o, evaluating it on the CAD-Prompt benchmark dataset and comparing it to prior methods. Additionally, we introduce two new metrics based on topological properties defined by the Euler characteristic, which capture a form of semantic similarity between 3D objects. Our results demonstrate that EvoCAD outperforms previous approaches on multiple metrics, particularly in generating topologically correct objects, which can be efficiently evaluated using our two novel metrics that complement existing spatial metrics.",
    "title_zh": "EvoCAD：基于视觉语言模型的进化式CAD代码生成",
    "abstract_zh": "将大型语言模型与进化计算算法相结合，代表了一个有前景的研究方向，该方向充分利用了大语言模型卓越的生成能力和上下文学习能力，同时融合了进化算法的优势。在本研究中，我们提出了EvoCAD，一种通过视觉语言模型和进化优化方法，基于符号化表示生成计算机辅助设计（CAD）对象的方法。我们的方法首先采样多个CAD对象，随后利用视觉语言模型和推理语言模型结合进化优化策略进行优化。我们采用GPT-4V和GPT-4o对所提方法进行了评估，并在CAD-Prompt基准数据集上与先前方法进行了对比。此外，我们引入了两个基于欧拉特征数定义的拓扑属性的新度量指标，用于捕捉三维物体之间的某种语义相似性。实验结果表明，EvoCAD在多项指标上优于以往方法，尤其在生成拓扑结构正确的对象方面表现突出。这些新提出的度量指标能够高效评估拓扑正确性，与现有的空间度量指标形成良好互补。"
  },
  {
    "date": "2025-12-15",
    "title": "Implementation of RISC-V Processor along with Radix-2 Booth Multiplier Using Verilog",
    "authors": "Karthik E, Karamdeep Singh",
    "publish": "2025 First International Conference on Intelligent Computing and Communication Systems (CICCS)",
    "url": "https://doi.org/10.1109/ciccs66437.2025.11280057",
    "source": "IEEE",
    "abstract": "This paper presents the integration of a Radix-2 Booth multiplier within a RISC-V soft-core processor architecture, implemented in Verilog. Unlike traditional approaches that treat the multiplier as an isolated module, our design embeds the Booth multiplier directly into the execution stage of the pipeline, optimizing signed multiplications with reduced cycle latency. We introduce a control-path optimization technique for conditional activation of the multiplier unit, minimizing dynamic power consumption. Verified on FPGA, our implementation demonstrates superior performance compared to multiple open-source RISC-V variants, achieving a delay of 5.95 ns, power consumption of 0.103 µW, and area efficiency with 1123 LUTs and 426 registers. This study highlights the advantages of a tightly coupled Booth multiplier in lightweight, pipelined RISC-V architectures, enhancing performance for edge computing and embedded applications.",
    "title_zh": "基于Verilog的RISC-V处理器与Radix-2 Booth乘法器的实现",
    "abstract_zh": "本文提出了一种在RISC-V软核处理器架构中集成Radix-2 Booth乘法器的设计，采用Verilog语言实现。与传统将乘法器视为独立模块的方法不同，本设计将其直接嵌入流水线的执行阶段，通过降低周期延迟来优化有符号乘法运算。我们引入了一种控制路径优化技术，实现乘法单元的条件激活，从而有效降低动态功耗。在FPGA上的验证结果表明，该设计相较于多个开源RISC-V版本表现出更优的性能：延迟仅为5.95 ns，功耗为0.103 µW，面积效率方面使用了1123个LUT和426个寄存器。本研究凸显了紧密耦合的Booth乘法器在轻量级、流水线化RISC-V架构中的优势，显著提升了边缘计算与嵌入式应用中的处理性能。"
  },
  {
    "date": "2025-12-15",
    "title": "System Verilog-Based Design and Verification of an Optimized AXI4 Interface for Low-Power FPGA Applications",
    "authors": "Swaroop S Harithsa, Vaishnavi Sankar",
    "publish": "2025 First International Conference on Intelligent Computing and Communication Systems (CICCS)",
    "url": "https://doi.org/10.1109/ciccs66437.2025.11280154",
    "source": "IEEE",
    "abstract": "The proposed work demonstrates the implementation and verification of a power-efficient, high-throughput Advanced eXtensible Interface 4 (AXI4) for FPGA (Field Programmable Gate Array) based system on chip designs. Both the AXI Master and Slave modules have been implemented in Verilog and System Verilog with support for burst mode and aggressive transactions. These modules also incorporate power-saving strategies, such as clock gating and adaptive clocking. Compared to standard AXI IP cores, the design delivers approximately 21% lower total power consumption and 32% lower average latency, highlighting its optimization efficiency. The design is rigorously verified using a constrained-random testbench, achieving over 98% functional and 100% assertion coverage. Standard tools such as Questa Sim and EDA Playground are used for simulation and waveform analysis, which validate low-latency, pipelined data handling and verify protocol compliance. The design is validated using randomized constrained testing in System Verilog and synthesized in Xilinx, making it suitable for high-performance embedded systems, including AI, DSP, and real-time computing domains.",
    "title_zh": "基于System Verilog的低功耗FPGA应用优化AXI4接口设计与验证",
    "abstract_zh": "所提出的工作展示了面向基于FPGA（现场可编程门阵列）的片上系统设计的一种高效能、高吞吐量的Advanced eXtensible Interface 4（AXI4）接口的实现与验证。AXI主控（Master）和从机（Slave）模块均采用Verilog和System Verilog语言实现，支持突发传输模式及高效率事务处理，并集成了多种省电策略，如时钟门控和自适应时钟技术。与标准AXI IP核相比，该设计在总功耗方面降低了约21%，平均延迟降低32%，充分体现了其优化效率。设计通过约束随机测试平台进行了严格验证，功能覆盖率超过98%，断言覆盖率达100%。采用Questa Sim和EDA Playground等标准工具进行仿真与波形分析，验证了低延迟、流水线式数据处理能力以及协议合规性。该设计在System Verilog中通过随机约束测试验证，并在Xilinx平台上完成综合，适用于高性能嵌入式系统，包括人工智能（AI）、数字信号处理（DSP）及实时计算等领域。"
  },
  {
    "date": "2025-12-15",
    "title": "An Updated Assessment of Reinforcement Learning for Macro Placement",
    "authors": "Chung-Kuan Cheng, Andrew B. Kahng, Sayak Kundu, Yucheng Wang, Zhiang Wang",
    "publish": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
    "url": "https://doi.org/10.1109/tcad.2025.3644293",
    "source": "IEEE",
    "abstract": "We provide an improved assessment of Google Brain’s deep reinforcement learning approach to macro placement [29] and its updated Circuit Training (<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">CT</i>) implementation in GitHub [53]. A stronger simulated annealing (SA) baseline leverages the “go-with-the-winners” metaheuristic [3] and a multi-threading implementation. We develop and release new public benchmarks in sub-10nm technology: LEF/DEF for Google’s 7nm TSMC Ariane protobuf and scaled variants, as well as testcases implemented in the open-source ASAP7 7nm research enablement. We evaluate from-scratch training and fine-tuning results for the latest “AlphaChip” release of Circuit Training, alongside multiple alternative macro placers. We also study the recently-published pre-training guidance in [53]. A commercial place-and-route tool is used to provide “true reward” post-route power, performance and area metrics. All data, evaluation flows and related scripts are publicly available in the <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">MacroPlacement</i> GitHub repository [63]. Our study affords insights into reproducibility and reporting in the research literature, and points out still-missing confirmations (e.g., of CT’s scalability and pre-training methodology) that remain open questions for the research community.",
    "title_zh": "强化学习在宏观布局中的最新评估",
    "abstract_zh": "我们对 Google Brain 的深度强化学习宏单元布局方法 [29] 进行了改进评估，并对其在 GitHub 上更新的 Circuit Training（CT）实现 [53] 进行了分析。我们构建了一个更强的模拟退火（SA）基线方法，该方法结合了“胜者恒胜”元启发式策略 [3] 与多线程实现。我们开发并发布了面向亚10纳米工艺的新公共基准：包括 Google 7nm TSMC Ariane 的 protobuf 格式及其缩放变体的 LEF/DEF 文件，以及基于开源 ASAP7 7nm 研究使能平台实现的测试用例。我们评估了最新发布的 “AlphaChip” 版本 Circuit Training 的从头训练和微调结果，并对比了多种其他宏单元布局工具。此外，我们还研究了近期发表论文 [53] 中提出的预训练指导策略。采用商业级布局布线工具，以获取“真实奖励”指标——即后布线的功耗、性能和面积数据。所有数据、评估流程及相关脚本均已公开发布于 MacroPlacement GitHub 仓库 [63]。我们的研究为学术文献中的可复现性与报告规范提供了深入见解，并指出了当前仍缺乏验证的关键问题（例如 CT 方法的可扩展性及预训练方法的有效性），这些问题仍是研究社区亟待解决的开放性课题。"
  },
  {
    "date": "2025-12-15",
    "title": "Automated PCB Defect Detection and Quality Assurance System Using Image Processing",
    "authors": "Sumedh S. Deshmukh, Yogesh S. Nirve, Sharad A. Tichkule, Rohan G. Pawar, Archana Dongardive, Kiran Patil",
    "publish": "2025 9th International Conference on Computing, Communication, Control and Automation (ICCCBEA)",
    "url": "https://doi.org/10.1109/iccubea65967.2025.11284002",
    "source": "IEEE",
    "abstract": "This project, “Automated PCB Defect Detection and Quality Assurance System using Image Processing,” is an AI-based solution that detects defects in Printed Circuit Boards (PCBs) at high speed and precision. The system is industrial in nature, where users can <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\log$</tex> in and upload images of PCBs for checking. The system starts with checking if the uploaded image is of a PCB or not using the YOLO (You Only Look Once) algorithm, which supports real-time object detection. The image is then transformed to grayscale, which simplifies and speeds up processing. Template matching is then implemented to match the reference defect-free image with the input image to determine areas of mismatch. To prevent unwanted data and false positives, heuristic filtering is implemented. YOLO Class Map Check is then employed to label and classify certain defect types, i.e., missing holes, spurious copper, open circuits, and shorts, on the output image directly. One of the most important features of the system is its AI advisory module, which gives insight into potential reasons for each defect and recommends the necessary corrective measures, helping quality control engineers in decision-making. The analytics dashboard provides real-time metrics on total scans, OK and defective PCBs, and a detailed defect report table exportable as PDF for audits and records. This project is significant since it automates a historically manual, timeconsuming, and error-prone inspection process. The system proved highly accurate and efficient during testing and showed great promise for real-time use in PCB production lines. Through minimizing human error and maximizing throughput, the solution plays an important role in smart manufacturing and product reliability improvement.",
    "title_zh": "基于图像处理的自动化PCB缺陷检测与质量保证系统",
    "abstract_zh": "本项目“基于图像处理的自动化PCB缺陷检测与质量保证系统”是一项基于人工智能的解决方案，能够以高速度和高精度检测印刷电路板（PCBs）中的缺陷。该系统具有工业应用特性，用户可登录后上传待检测的PCB图像。系统首先利用YOLO（You Only Look Once）算法判断上传的图像是否为PCB，该算法支持实时目标检测。随后，图像被转换为灰度图，从而简化处理流程并提升处理速度。接着采用模板匹配技术，将参考无缺陷图像与输入图像进行比对，以识别不一致区域。为避免无关数据及误报，系统引入了启发式过滤机制。随后，通过YOLO类别映射检查功能，直接在输出图像上对特定缺陷类型（如缺失孔洞、多余铜箔、开路和短路）进行标注与分类。系统最核心的功能之一是其AI辅助决策模块，能够分析每种缺陷的潜在成因，并提出相应的纠正措施，为质量控制工程师提供决策支持。此外，分析仪表盘可实时展示总扫描数、合格与不合格PCB数量，并生成可导出为PDF格式的详细缺陷报告，便于审计与存档。该项目意义重大，因为它自动化了以往依赖人工、耗时且易出错的检测流程。在测试过程中，系统表现出极高的准确性和效率，展现出在PCB生产线中实现实时应用的巨大潜力。通过减少人为错误并最大化生产吞吐量，该解决方案在智能制造和产品质量提升方面发挥着重要作用。"
  }
]