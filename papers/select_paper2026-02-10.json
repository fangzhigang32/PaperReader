[
  {
    "date": "2026-02-10",
    "title": "Code2World: A GUI World Model via Renderable Code Generation",
    "authors": "Yuhao Zheng, Li'an Zhong, Yi Wang, Rui Dai, Kaikui Liu, Xiangxiang Chu, Linyuan Lv, Philip Torr, Kevin Qinghong Lin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09856v1",
    "source": "arXiv",
    "abstract": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.",
    "title_zh": "Code2World：通过可渲染代码生成的GUI世界模型",
    "abstract_zh": "自主的GUI智能体通过感知界面并执行动作来与环境交互。作为虚拟沙盒，GUI World模型通过实现动作条件下的预测，赋予智能体类人的前瞻能力。然而，现有的基于文本和像素的方法难以同时实现高视觉保真度和细粒度的结构可控性。为此，我们提出了Code2World，一种视觉-语言编码器，通过生成可渲染的代码来模拟下一视觉状态。具体而言，为解决数据稀缺问题，我们构建了AndroidCode数据集，将GUI轨迹转换为高保真的HTML，并通过视觉反馈修正机制优化生成的代码，最终获得超过8万对高质量的屏幕-动作配对数据。为了将现有视觉语言模型（VLMs）适配为代码预测任务，我们首先进行监督微调（SFT）以实现格式布局的初步遵循，随后进一步引入“渲染感知强化学习”（Render-Aware Reinforcement Learning），利用渲染结果作为奖励信号，强制保证视觉语义一致性和动作一致性。大量实验表明，Code2World-8B在下一UI预测任务中表现卓越，性能媲美GPT-5和Gemini-3-Pro-Image等先进模型。值得注意的是，Code2World以灵活方式显著提升了下游导航任务的成功率，在AndroidWorld导航任务中使Gemini-2.5-Flash的性能提升了+9.5%。代码已开源，地址为：https://github.com/AMAP-ML/Code2World。"
  },
  {
    "date": "2026-02-10",
    "title": "LLM Reasoning Predicts When Models Are Right: Evidence from Coding Classroom Discourse",
    "authors": "Bakhtawar Ahtisham, Kirk Vanacore, Zhuqian Zhou, Jinsook Lee, Rene F. Kizilcec",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09832v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) are increasingly deployed to automatically label and analyze educational dialogue at scale, yet current pipelines lack reliable ways to detect when models are wrong. We investigate whether reasoning generated by LLMs can be used to predict the correctness of a model's own predictions. We analyze 30,300 teacher utterances from classroom dialogue, each labeled by multiple state-of-the-art LLMs with an instructional move construct and an accompanying reasoning. Using human-verified ground-truth labels, we frame the task as predicting whether a model's assigned label for a given utterance is correct. We encode LLM reasoning using Term Frequency-Inverse Document Frequency (TF-IDF) and evaluate five supervised classifiers. A Random Forest classifier achieves an F1 score of 0.83 (Recall = 0.854), successfully identifying most incorrect predictions and outperforming baselines. Training specialist detectors for specific instructional move constructs further improves performance on difficult constructs, indicating that error detection benefits from construct-specific linguistic cues. Using the Linguistic Inquiry and Word Count (LIWC) framework, we examine four linguistic markers of correctness: Causation, Differentiation, Tentativeness, and Insight. Correct predictions exhibit grounded causal language (e.g., because, therefore), while incorrect reasoning is substantially more likely to rely on epistemic hedging (e.g., might, could) and performative metacognition (e.g., think, realize). Syntactic complexity does not distinguish correct from incorrect reasoning, and longer reasoning is not more reliable. These findings demonstrate that reasoning-based error detection offers a practical and scalable approach to quality control in automated educational dialogue analysis.",
    "title_zh": "大语言模型推理预测模型何时正确：来自编程课堂对话的证据",
    "abstract_zh": "大型语言模型（LLMs）正被越来越多地用于大规模自动标注和分析教育对话，然而当前的处理流程缺乏可靠的方法来检测模型何时出错。本文探究了LLM生成的推理过程是否可用于预测其自身预测的正确性。我们分析了来自课堂对话的30,300条教师发言，每条发言均由多个先进的LLM标注了教学行为类别，并附有相应的推理过程。基于人工验证的真值标签，我们将任务定义为预测模型对某条发言所分配的标签是否正确。我们采用词频-逆文档频率（TF-IDF）对LLM的推理进行编码，并评估了五种监督分类器。其中，随机森林分类器取得了0.83的F1分数（召回率=0.854），能够有效识别大多数错误预测，优于各类基线方法。针对特定教学行为类别训练专用检测器，进一步提升了在复杂类别上的表现，表明错误检测可从类别特定的语言线索中获益。通过语言调查与词频分析（LIWC）框架，我们考察了四种与正确性相关的语言标记：因果性、区分性、不确定性以及洞察力。正确预测往往表现出基于因果的语言特征（如“因为”、“因此”），而错误推理则更倾向于使用认知上的模糊表达（如“可能”、“可以”）以及表现性元认知语言（如“认为”、“意识到”）。句法复杂度无法区分正确与错误的推理，且更长的推理过程也并不意味着更高的可靠性。这些发现表明，基于推理的错误检测为自动化教育对话分析提供了实用且可扩展的质量控制方法。"
  },
  {
    "date": "2026-02-10",
    "title": "Flexible Entropy Control in RLVR with Gradient-Preserving Perspective",
    "authors": "Kun Chen, Peng Shi, Fanfan Liu, Haibo Qiu, Zhixiong Zeng, Siqi Yang, Wenji Mao",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09782v1",
    "source": "arXiv",
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a critical method for enhancing the reasoning capabilities of Large Language Models (LLMs). However, continuous training often leads to policy entropy collapse, characterized by a rapid decay in entropy that results in premature overconfidence, reduced output diversity, and vanishing gradient norms that inhibit learning. Gradient-Preserving Clipping is a primary factor influencing these dynamics, but existing mitigation strategies are largely static and lack a framework connecting clipping mechanisms to precise entropy control. This paper proposes reshaping entropy control in RL from the perspective of Gradient-Preserving Clipping. We first theoretically and empirically verify the contributions of specific importance sampling ratio regions to entropy growth and reduction. Leveraging these findings, we introduce a novel regulation mechanism using dynamic clipping threshold to precisely manage entropy. Furthermore, we design and evaluate dynamic entropy control strategies, including increase-then-decrease, decrease-increase-decrease, and oscillatory decay. Experimental results demonstrate that these strategies effectively mitigate entropy collapse, and achieve superior performance across multiple benchmarks.",
    "title_zh": "基于梯度保持视角的RLVR中灵活的熵控制",
    "abstract_zh": "基于可验证奖励的强化学习（RLVR）已成为提升大语言模型（LLMs）推理能力的关键方法。然而，持续训练常导致策略熵坍塌，表现为熵值迅速下降，引发过早的过度自信、输出多样性降低以及梯度范数消失，从而抑制学习过程。梯度保持裁剪（Gradient-Preserving Clipping）是影响这一动态过程的主要因素，但现有缓解策略大多为静态设置，缺乏将裁剪机制与精确熵控制相联系的理论框架。本文从梯度保持裁剪的视角重新思考强化学习中的熵控制问题。我们首先通过理论分析与实证研究，验证了特定重要性采样比率区域对熵增长与衰减的贡献。基于这些发现，我们提出一种基于动态裁剪阈值的新调控机制，以实现对熵的精确管理。此外，我们设计并评估了多种动态熵控制策略，包括先增后减、先减后增再减以及振荡衰减等模式。实验结果表明，这些策略能有效缓解熵坍塌问题，并在多个基准测试中取得更优性能。"
  },
  {
    "date": "2026-02-10",
    "title": "AgentCgroup: Understanding and Controlling OS Resources of AI Agents",
    "authors": "Yusheng Zheng, Jiakun Fan, Quanzhi Fu, Yiwei Yang, Wei Zhang, Andi Quinn",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09345v1",
    "source": "arXiv",
    "abstract": "AI agents are increasingly deployed in multi-tenant cloud environments, where they execute diverse tool calls within sandboxed containers, each call with distinct resource demands and rapid fluctuations. We present a systematic characterization of OS-level resource dynamics in sandboxed AI coding agents, analyzing 144 software engineering tasks from the SWE-rebench benchmark across two LLM models. Our measurements reveal that (1) OS-level execution (tool calls, container and agent initialization) accounts for 56-74% of end-to-end task latency; (2) memory, not CPU, is the concurrency bottleneck; (3) memory spikes are tool-call-driven with a up to 15.4x peak-to-average ratio; and (4) resource demands are highly unpredictable across tasks, runs, and models. Comparing these characteristics against serverless, microservice, and batch workloads, we identify three mismatches in existing resource controls: a granularity mismatch (container-level policies vs. tool-call-level dynamics), a responsiveness mismatch (user-space reaction vs. sub-second unpredictable bursts), and an adaptability mismatch (history-based prediction vs. non-deterministic stateful execution). We propose AgentCgroup , an eBPF-based resource controller that addresses these mismatches through hierarchical cgroup structures aligned with tool-call boundaries, in-kernel enforcement via sched_ext and memcg_bpf_ops, and runtime-adaptive policies driven by in-kernel monitoring. Preliminary evaluation demonstrates improved multi-tenant isolation and reduced resource waste.",
    "title_zh": "AgentCgroup：理解与控制AI代理的OS资源",
    "abstract_zh": "AI代理正越来越多地部署在多租户云环境中，它们在沙箱容器中执行各种工具调用，每次调用具有不同的资源需求且波动迅速。本文系统地分析了沙箱化AI编程代理在操作系统层面的资源动态行为，基于SWE-rebench基准测试对144个软件工程任务进行了分析，涵盖两种大语言模型（LLM）。我们的测量结果揭示了以下发现：（1）操作系统层面的执行开销（包括工具调用、容器及代理初始化）占端到端任务延迟的56%至74%；（2）内存而非CPU是并发执行的瓶颈；（3）内存峰值由工具调用驱动，峰值与平均值之比最高可达15.4倍；（4）资源需求在不同任务、不同运行实例以及不同模型之间表现出高度不可预测性。将这些特性与无服务器（serverless）、微服务和批处理工作负载进行对比后，我们识别出现有资源控制机制存在的三大不匹配：（1）粒度不匹配（容器级策略与工具调用级动态不一致）；（2）响应性不匹配（用户态响应机制无法应对亚秒级突发的不可预测负载）；（3）适应性不匹配（基于历史数据的预测机制难以应对非确定性的有状态执行）。为此，我们提出AgentCgroup，一种基于eBPF的资源控制器，通过与工具调用边界对齐的分层cgroup结构、利用sched_ext和memcg_bpf_ops在内核中强制执行策略，以及基于内核监控的运行时自适应策略，有效解决了上述问题。初步评估表明，该方案显著提升了多租户隔离性并减少了资源浪费。"
  },
  {
    "date": "2026-02-10",
    "title": "On A Parameterized Theory of Dynamic Logic for Operationally-based Programs",
    "authors": "Yuanrui Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09307v1",
    "source": "arXiv",
    "abstract": "Applying dynamic logics to program verifications is a challenge, because their axiomatic rules for regular expressions can be difficult to be adapted to different program models. We present a novel dynamic logic, called DLp, which supports reasoning based on programs' operational semantics. For those programs whose transitional behaviours are their standard or natural semantics, DLp makes their verifications easier since one can directly apply the program transitions for reasoning, without the need of re-designing and validating new rules as in most other dynamic logics. DLp is parametric. It provides a model-independent framework consisting of a relatively small set of inference rules, which depends on a given set of trustworthy rules for the operational semantics. These features of DLp let multiple models easily compared in its framework and makes it compatible with existing dynamic-logic theories. DLp supports cyclic reasoning, providing an incremental derivation process for recursive programs, making it more convenient to reason about without prior program transformations. We analyze and prove the soundness and completeness of DLp under certain conditions. Several case studies illustrate the features of DLp and fully demonstrate its potential usage.",
    "title_zh": "基于操作语义的程序的动态逻辑参数化理论",
    "abstract_zh": "将动态逻辑应用于程序验证是一项挑战，因为其正则表达式的公理规则往往难以适应不同的程序模型。本文提出了一种新型动态逻辑——DLp，它支持基于程序操作语义的推理。对于那些其转移行为即为其标准或自然语义的程序，DLp 使得验证过程更加简便：用户可以直接利用程序的转移步骤进行推理，而无需像大多数其他动态逻辑那样重新设计和验证新的规则。DLp 具有参数化特性，提供了一个与模型无关的框架，该框架仅包含一组相对较少的推理规则，其有效性依赖于一组可信的操作语义规则。DLp 的这些特性使得不同模型在该框架下能够轻松比较，并且与现有的动态逻辑理论具有良好的兼容性。DLp 支持循环推理，为递归程序提供了一种增量式的推导过程，从而在无需预先对程序进行变换的情况下，更方便地进行推理。本文在一定条件下分析并证明了 DLp 的可靠性和完备性。多个案例研究展示了 DLp 的各项特性，并充分证明了其潜在的应用价值。"
  },
  {
    "date": "2026-02-10",
    "title": "ReSIM: Re-ranking Binary Similarity Embeddings to Improve Function Search Performance",
    "authors": "Gianluca Capozzi, Anna Paola Giancaspro, Fabio Petroni, Leonardo Querzoni, Giuseppe Antonio Di Luna",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09548v1",
    "source": "arXiv",
    "abstract": "Binary Function Similarity (BFS), the problem of determining whether two binary functions originate from the same source code, has been extensively studied in recent research across security, software engineering, and machine learning communities. This interest arises from its central role in developing vulnerability detection systems, copyright infringement analysis, and malware phylogeny tools. Nearly all binary function similarity systems embed assembly functions into real-valued vectors, where similar functions map to points that lie close to each other in the metric space. These embeddings enable function search: a query function is embedded and compared against a database of candidate embeddings to retrieve the most similar matches. Despite their effectiveness, such systems rely on bi-encoder architectures that embed functions independently, limiting their ability to capture cross-function relationships and similarities. To address this limitation, we introduce ReSIM, a novel and enhanced function search system that complements embedding-based search with a neural re-ranker. Unlike traditional embedding models, our reranking module jointly processes query-candidate pairs to compute ranking scores based on their mutual representation, allowing for more accurate similarity assessment. By re-ranking the top results from embedding-based retrieval, ReSIM leverages fine-grained relation information that bi-encoders cannot capture. We evaluate ReSIM across seven embedding models on two benchmark datasets, demonstrating consistent improvements in search effectiveness, with average gains of 21.7% in terms of nDCG and 27.8% in terms of Recall.",
    "title_zh": "ReSIM：通过重排序二进制相似性嵌入以提升函数搜索性能",
    "abstract_zh": "二元函数相似性（Binary Function Similarity, BFS）问题，即判断两个二进制函数是否源自同一段源代码，在近年来的安全、软件工程和机器学习领域受到了广泛关注。这一研究的重要性源于其在漏洞检测系统、版权侵权分析以及恶意软件演化关系推断等关键应用中的核心作用。目前几乎所有二元函数相似性系统都将汇编函数嵌入到实值向量空间中，使得语义相近的函数在度量空间中映射为彼此接近的点。这种嵌入方式支持函数搜索：将查询函数进行嵌入后，与候选函数的嵌入向量库进行比对，从而检索出最相似的结果。然而，这类系统普遍依赖双编码器（bi-encoder）架构，即独立地对每个函数进行编码，难以捕捉函数之间的交叉关系与深层相似性。\n\n为克服上述局限，我们提出 ReSIM——一种新颖且增强型的函数搜索系统，通过引入神经重排序模块，弥补传统基于嵌入的搜索方法的不足。与传统嵌入模型不同，我们的重排序模块能够联合处理查询函数与候选函数的配对，基于二者之间的相互表示来计算排名得分，从而实现更精准的相似性评估。ReSIM 通过对嵌入式检索返回的前若干结果进行重排序，有效利用了双编码器无法建模的细粒度关系信息。\n\n我们在两个基准数据集上，针对七种不同的嵌入模型对 ReSIM 进行了全面评估，结果表明其在搜索效果上实现了持续提升：平均在 nDCG 指标上提高 21.7%，在 Recall 指标上提升达 27.8%。"
  },
  {
    "date": "2026-02-10",
    "title": "UniARM: Towards a Unified Autoregressive Reward Model for Multi-Objective Test-Time Alignment",
    "authors": "Hongyan Xie, Yikun Ban, Ruiyu Fang, Zixuan Huang, Deqing Wang, Jianxin Li, Yitong Yao, Chao Wang, Shuangyong Song",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09538v1",
    "source": "arXiv",
    "abstract": "Multi-objective alignment aims to align LLM responses with multiple human preference objectives. Among existing methods, guiding the generation of frozen LLMs through autoregressive reward models (ARMs) to accomplish multi-objective test-time alignment is a low-cost solution. However, these methods typically rely on independent parameters for each preference objective, either by training ARMs independently across preference dimensions, which neglects interactions among preference features, or by training a single ARM with separate feature extraction modules for each preference, which can cause feature entanglement. Both strategies can result in misalignment between generated outputs and user preferences. To address this limitation, we propose Preference-Modulated \\& Shared Low-Rank Adaptation (MoSLoRA) for ARM training, which first extracts shared features via a preference-agnostic module and then applies affine transformations to shared features via a preference modulation module conditioned on mixed preference vectors. This design mitigates feature entanglement and enables precise control over preference trade-offs during inference. Building on this, we introduce the Unified Autoregressive Reward Model (UniARM), a novel framework for multi-objective test-time alignment. UniARM jointly models all preference dimensions in a single parameter space, eliminating the need for independent parameters for each preference objective. es on larger-scale LLMs, enhancing its practical usability.",
    "title_zh": "UniARM：面向多目标测试时对齐的统一自回归奖励模型",
    "abstract_zh": "多目标对齐旨在使大语言模型（LLM）的输出与多个用户偏好目标保持一致。在现有方法中，通过自回归奖励模型（ARMs）引导冻结的LLM生成，以实现多目标测试时对齐，是一种低成本的解决方案。然而，这些方法通常为每个偏好目标单独使用独立参数，要么在不同偏好维度上独立训练ARMs，从而忽略了偏好特征之间的相互作用；要么训练单一ARM，但为每个偏好维度配备独立的特征提取模块，这可能导致特征纠缠。这两种策略都可能造成生成结果与用户偏好之间的错位。为解决这一局限，我们提出了一种新的ARM训练方法——偏好调制与共享低秩适配（MoSLoRA）。该方法首先通过一个与偏好无关的模块提取共享特征，再通过一个基于混合偏好向量条件化的偏好调制模块，对共享特征施加仿射变换。这种设计有效缓解了特征纠缠问题，并在推理阶段实现了对偏好权衡的精确控制。基于此，我们进一步提出了统一的自回归奖励模型（UniARM），这是一种全新的多目标测试时对齐框架。UniARM在单一参数空间中联合建模所有偏好维度，无需为每个偏好目标配置独立参数，从而显著提升了在更大规模LLM上的适用性，增强了其实际应用价值。"
  },
  {
    "date": "2026-02-10",
    "title": "SWE-AGI: Benchmarking Specification-Driven Software Construction with MoonBit in the Era of Autonomous Agents",
    "authors": "Zhirui Zhang, Hongbo Zhang, Haoxiang Fei, Zhiyuan Bao, Yubin Chen, Zhengyu Lei, Ziyue Liu, Yixuan Sun, Mingkun Xiao, Zihang Ye, Yu Zhang, Hongcheng Zhu, Yuxiang Wen, Heung-Yeung Shum",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09447v1",
    "source": "arXiv",
    "abstract": "Although large language models (LLMs) have demonstrated impressive coding capabilities, their ability to autonomously build production-scale software from explicit specifications remains an open question. We introduce SWE-AGI, an open-source benchmark for evaluating end-to-end, specification-driven construction of software systems written in MoonBit. SWE-AGI tasks require LLM-based agents to implement parsers, interpreters, binary decoders, and SAT solvers strictly from authoritative standards and RFCs under a fixed API scaffold. Each task involves implementing 1,000-10,000 lines of core logic, corresponding to weeks or months of engineering effort for an experienced human developer. By leveraging the nascent MoonBit ecosystem, SWE-AGI minimizes data leakage, forcing agents to rely on long-horizon architectural reasoning rather than code retrieval. Across frontier models, gpt-5.3-codex achieves the best overall performance (solving 19/22 tasks, 86.4%), outperforming claude-opus-4.6 (15/22, 68.2%), and kimi-2.5 exhibits the strongest performance among open-source models. Performance degrades sharply with increasing task difficulty, particularly on hard, specification-intensive systems. Behavioral analysis further reveals that as codebases scale, code reading, rather than writing, becomes the dominant bottleneck in AI-assisted development. Overall, while specification-driven autonomous software engineering is increasingly viable, substantial challenges remain before it can reliably support production-scale development.",
    "title_zh": "SWE-AGI：在自主代理时代，基于规范驱动的软件构建基准测试——以MoonBit为例",
    "abstract_zh": "尽管大型语言模型（LLMs）已展现出令人瞩目的编程能力，但其能否从明确规范中自主构建生产级软件系统，仍是悬而未决的问题。为此，我们提出了 SWE-AGI——一个开源基准测试，用于评估基于规范驱动的端到端软件系统构建能力，所有系统均使用 MoonBit 语言编写。SWE-AGI 的任务要求基于 LLM 的智能体仅依据权威标准和 RFC 文档，严格在固定 API 框架下实现解析器、解释器、二进制解码器以及 SAT 求解器。每项任务需实现 1,000 至 10,000 行核心逻辑，相当于经验丰富的开发者数周甚至数月的工程投入。通过利用新兴的 MoonBit 生态系统，SWE-AGI 有效减少了数据泄露风险，迫使智能体依赖长周期的架构推理，而非简单的代码检索。在前沿模型中，gpt-5.3-codex 表现最佳（成功解决 22 项中的 19 项，准确率 86.4%），优于 claude-opus-4.6（15/22，68.2%），而 kimi-2.5 在开源模型中表现最强。随着任务难度增加，尤其是面对高度依赖规范的复杂系统时，性能显著下降。行为分析进一步表明，随着代码库规模扩大，代码理解（code reading）而非代码生成，成为 AI 辅助开发中的主要瓶颈。总体而言，虽然基于规范的自主软件工程正变得日益可行，但在可靠支持生产级开发之前，仍面临诸多重大挑战。"
  },
  {
    "date": "2026-02-10",
    "title": "Long Chain-of-Thought Compression via Fine-Grained Group Policy Optimization",
    "authors": "Xinchen Han, Hossam Afifi, Michel Marot, Xilu Wang, Lu Yin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.10048v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) often generate unnecessarily verbose Chain-of-Thought (CoT) reasoning that increases computational costs and latency without proportional performance gains. In this paper, we propose \\textbf{F}ine-grained \\textbf{G}roup policy \\textbf{O}ptimization (\\textbf{FGO}), a Reinforcement Learning (RL) algorithm that refines group responses by subdividing them and assigning appropriate weights based on length and entropy, thereby enabling effective CoT compression. Meanwhile, as an enhanced variant of Group Relative Policy Optimization (GRPO), FGO successfully addresses two major limitations of the GRPO: inefficient data utilization and entropy collapse. We evaluate FGO on multiple reasoning LLMs and benchmarks, including MATH500, AIME24, AMC23, and Minerva. Experimental results show that FGO achieves efficient CoT compression without degrading performance, and simultaneously resolves the key limitations of GRPO.",
    "title_zh": "通过细粒度分组策略优化实现长链式思维压缩",
    "abstract_zh": "大型语言模型（LLMs）在生成链式思维（Chain-of-Thought, CoT）推理时，常常产生冗长且不必要的内容，导致计算成本和延迟增加，而性能提升却并不显著。本文提出了一种新的强化学习算法——**细粒度分组策略优化**（Fine-grained Group policy Optimization, FGO），通过将群体响应进行细分，并根据长度和熵分配合适的权重，实现高效的CoT压缩。同时，作为分组相对策略优化（GRPO）的增强版本，FGO有效解决了GRPO存在的两大关键问题：数据利用效率低和熵崩溃。我们在多个推理类LLM及基准测试集（包括MATH500、AIME24、AMC23和Minerva）上对FGO进行了评估。实验结果表明，FGO能够在不损害模型性能的前提下实现高效的CoT压缩，并成功克服了GRPO的核心缺陷。"
  },
  {
    "date": "2026-02-10",
    "title": "Why Do AI Agents Systematically Fail at Cloud Root Cause Analysis?",
    "authors": "Taeyoon Kim, Woohyeok Park, Hoyeong Yun, Kyungyong Lee",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09937v1",
    "source": "arXiv",
    "abstract": "Failures in large-scale cloud systems incur substantial financial losses, making automated Root Cause Analysis (RCA) essential for operational stability. Recent efforts leverage Large Language Model (LLM) agents to automate this task, yet existing systems exhibit low detection accuracy even with capable models, and current evaluation frameworks assess only final answer correctness without revealing why the agent's reasoning failed. This paper presents a process level failure analysis of LLM-based RCA agents. We execute the full OpenRCA benchmark across five LLM models, producing 1,675 agent runs, and classify observed failures into 12 pitfall types across intra-agent reasoning, inter-agent communication, and agent-environment interaction. Our analysis reveals that the most prevalent pitfalls, notably hallucinated data interpretation and incomplete exploration, persist across all models regardless of capability tier, indicating that these failures originate from the shared agent architecture rather than from individual model limitations. Controlled mitigation experiments further show that prompt engineering alone cannot resolve the dominant pitfalls, whereas enriching the inter-agent communication protocol reduces communication-related failures by up to 15 percentage points. The pitfall taxonomy and diagnostic methodology developed in this work provide a foundation for designing more reliable autonomous agents for cloud RCA.",
    "title_zh": "为什么AI代理在云根因分析中系统性地失败？",
    "abstract_zh": "大规模云系统中的故障会导致巨大的财务损失，因此自动化根本原因分析（RCA）对于保障系统运行稳定性至关重要。近期的研究尝试利用大语言模型（LLM）代理来实现该任务的自动化，但现有系统即使在使用能力较强的模型时仍表现出较低的检测准确率；此外，当前的评估框架仅关注最终答案的正确性，而无法揭示代理推理失败的具体原因。本文对基于LLM的RCA代理进行了过程级故障分析。我们在五个LLM模型上完整执行了OpenRCA基准测试，共生成1,675次代理运行，并将观察到的故障归纳为12种陷阱类型，涵盖代理内部推理、代理间通信以及代理与环境交互三个层面。分析结果表明，最普遍的陷阱——如虚构数据解读和探索不充分——在所有模型中均普遍存在，无论其能力水平高低，这说明这些故障源于共享的代理架构本身，而非单个模型的局限性。受控缓解实验进一步显示，仅通过提示工程无法有效解决主要陷阱，而优化代理间的通信协议可使与通信相关的故障率降低最多达15个百分点。本文提出的陷阱分类体系与诊断方法，为设计更可靠的云环境自主RCA代理奠定了基础。"
  },
  {
    "date": "2026-02-10",
    "title": "Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning",
    "authors": "Zhaoyang Wang, Canwen Xu, Boyi Liu, Yite Wang, Siwei Han, Zhewei Yao, Huaxiu Yao, Yuxiong He",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.10090v1",
    "source": "arXiv",
    "abstract": "Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.",
    "title_zh": "代理世界模型：用于代理强化学习的无限合成环境",
    "abstract_zh": "大型语言模型（LLM）的最新进展使自主智能体能够执行需要与工具和环境进行多轮交互的复杂任务。然而，这类智能体训练的扩展受到缺乏多样化且可靠环境的限制。本文提出了一种全合成环境生成管道——智能体世界模型（Agent World Model, AWM）。通过该管道，我们构建了1000个涵盖日常场景的合成环境，每个环境中智能体可与丰富的工具集（平均每环境35个工具）交互，并获取高质量的观测信息。值得注意的是，这些环境由代码驱动并依托数据库运行，相比由LLM模拟的环境，其状态转移更加可靠且一致。此外，与从真实环境中收集轨迹相比，这些环境能实现更高效的智能体交互。为验证该资源的有效性，我们开展了大规模强化学习实验，针对多轮工具使用智能体进行训练。得益于完全可执行的环境和可访问的数据库状态，我们还能设计出可靠的奖励函数。在三个基准测试上的实验表明，仅在合成环境中训练的智能体，相比在特定基准环境训练的智能体，展现出更强的分布外泛化能力。代码已开源，地址为：https://github.com/Snowflake-Labs/agent-world-model。"
  },
  {
    "date": "2026-02-10",
    "title": "Chain of Mindset: Reasoning with Adaptive Cognitive Modes",
    "authors": "Tianyi Jiang, Arctanx An, Hengyi Feng, Naixin Zhai, Haodong Li, Xiaomin Yu, Jiahui Liu, Hanwen Du, Shuo Zhang, Zhi Yang, Jie Huang, Yuhua Li, Yongxin Ni, Huacan Wang, Ronghao Chen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.10063v1",
    "source": "arXiv",
    "abstract": "Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\\% and 4.72\\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at \\href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}.",
    "title_zh": "思维链：以自适应认知模式进行推理",
    "abstract_zh": "人类的问题解决从来不是单一思维模式的重复，这里的“思维模式”指的是特定的认知处理方式。在应对具体任务时，我们并不会依赖单一的思维模式，而是将多种思维模式整合进同一个解题过程中。然而，现有的大语言模型推理方法陷入了一个共同的误区：在所有推理步骤中都采用相同的固定思维模式，忽视了同一问题的不同解决阶段实际上需要根本不同的思维模式。这种单一思维的假设限制了模型迈向更高层次智能的可能性。为解决这一局限，我们提出了一种无需训练的代理式框架——思维链（Chain of Mindset, CoM），该框架能够实现步骤级的自适应思维模式协调。CoM将推理过程分解为四种功能异构的思维模式：空间思维、收敛思维、发散思维和算法思维。一个元代理（Meta-Agent）根据不断演化的推理状态动态选择最优思维模式，同时双向上下文门（Context Gate）对模块间的信息流动进行过滤，以确保推理的有效性与效率。在涵盖数学、代码生成、科学问答和空间推理等领域的六个挑战性基准测试中，CoM均取得了当前最优的性能表现，在Qwen3-VL-32B-Instruct和Gemini-2.0-Flash上分别比最强基线模型整体准确率提升4.96%和4.72%，同时保持了良好的推理效率。我们的代码已公开，地址为：\\href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}。"
  },
  {
    "date": "2026-02-10",
    "title": "Focus Session: LLM4PQC -- An Agentic Framework for Accurate and Efficient Synthesis of PQC Cores",
    "authors": "Buddhi Perera, Zeng Wang, Weihua Xiao, Mohammed Nabeel, Ozgur Sinanoglu, Johann Knechtel, Ramesh Karri",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09919v1",
    "source": "arXiv",
    "abstract": "The design of post-quantum cryptography (PQC) hardware is a complex and hierarchical process with many challenges. A primary bottleneck is the conversion of PQC reference codes from C to high-level synthesis (HLS) specifications, which requires extensive manual refactoring [1]-[3]. Another bottleneck is the scalability of synthesis for complex PQC primitives, including number theoretic transform (NTT) accelerators and wide memory interfaces. While large language models (LLMs) have shown remarkable results for coding in general-purpose languages like Python, coding for hardware design is more challenging; feedback-driven and agentic integration are key principles of successful state-of-the-art approaches. Here, we propose LLM4PQC, an LLM-based agentic framework that refactors high-level PQC specifications and reference C codes into HLS-ready and synthesizable C code. Our framework generates and verifies the resulting RTL code. For correctness, we leverage a hierarchy of checks, covering fast C compilation and simulation as well as RTL simulation. Case studies on NIST PQC reference designs demonstrate a reduction in manual effort and accelerated design-space exploration compared to traditional flows. Overall, LLM4PQC provides a powerful and efficient pathway for synthesizing complex hardware accelerators.",
    "title_zh": "焦点研讨：LLM4PQC —— 一种用于精确高效合成PQC核心的智能体框架",
    "abstract_zh": "后量子密码学（PQC）硬件的设计是一个复杂且分层的过程，面临诸多挑战。其中主要瓶颈之一是将PQC的参考C代码转换为高层次综合（HLS）规范，这一过程需要大量手动重构工作[1]-[3]。另一个瓶颈在于复杂PQC原语的可扩展性问题，包括数论变换（NTT）加速器和宽内存接口的综合。尽管大型语言模型（LLMs）在通用编程语言（如Python）的代码生成方面已展现出卓越成果，但硬件设计编程更具挑战性；反馈驱动与代理式集成是当前先进方法成功的关键原则。本文提出LLM4PQC，一种基于LLM的代理式框架，能够将高层PQC规范和参考C代码自动重构为HLS就绪且可综合的C代码。我们的框架可生成并验证相应的RTL代码。为确保正确性，我们采用多层次验证机制，涵盖快速C编译与仿真以及RTL仿真。对NIST PQC参考设计的案例研究显示，与传统设计流程相比，LLM4PQC显著减少了人工工作量，并加速了设计空间探索。总体而言，LLM4PQC为复杂硬件加速器的综合提供了一条强大而高效的路径。"
  },
  {
    "date": "2026-02-10",
    "title": "CoFEH: LLM-driven Feature Engineering Empowered by Collaborative Bayesian Hyperparameter Optimization",
    "authors": "Beicheng Xu, Keyao Ding, Wei Liu, Yupeng Lu, Bin Cui",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09851v1",
    "source": "arXiv",
    "abstract": "Feature Engineering (FE) is pivotal in automated machine learning (AutoML) but remains a bottleneck for traditional methods, which treat it as a black-box search, operating within rigid, predefined search spaces and lacking domain awareness. While Large Language Models (LLMs) offer a promising alternative by leveraging semantic reasoning to generate unbounded operators, existing methods fail to construct free-form FE pipelines, remaining confined to isolated subtasks such as feature generation. Most importantly, they are rarely optimized jointly with hyperparameter optimization (HPO) of the ML model, leading to greedy \"FE-then-HPO\" workflows that cannot capture strong FE-HPO interactions. In this paper, we present CoFEH, a collaborative framework that interleaves LLM-based FE and Bayesian HPO for robust end-to-end AutoML. CoFEH uses an LLM-driven FE optimizer powered by Tree of Thought (ToT) to explore flexible FE pipelines, a Bayesian optimization (BO) module to solve HPO, and a dynamic optimizer selector that realizes interleaved optimization by adaptively scheduling FE and HPO steps. Crucially, we introduce a mutual conditioning mechanism that shares context between LLM and BO, enabling mutually informed decisions. Experiments show that CoFEH not only outperforms traditional and LLM-based FE baselines, but also achieves superior end-to-end performance under joint optimization.",
    "title_zh": "CoFEH：由协作贝叶斯超参数优化赋能的LLM驱动特征工程",
    "abstract_zh": "特征工程（Feature Engineering, FE）在自动化机器学习（AutoML）中至关重要，但传统方法将其视为黑箱搜索，受限于僵化的预定义搜索空间且缺乏领域感知能力，因而成为瓶颈。尽管大型语言模型（LLM）通过语义推理生成无边界特征操作符，展现出替代潜力，但现有方法仍难以构建自由形式的特征工程流水线，大多局限于孤立的子任务（如特征生成）。更重要的是，这些方法通常未与机器学习模型的超参数优化（HPO）联合优化，导致采用“先特征工程后超参数优化”的贪婪流程，无法捕捉特征工程与超参数优化之间的强交互关系。本文提出 CoFEH，一种协同框架，将基于 LLM 的特征工程与贝叶斯超参数优化（Bayesian HPO）进行交错执行，实现稳健的端到端 AutoML。CoFEH 采用基于思维树（Tree of Thought, ToT）的 LLM 驱动特征工程优化器，探索灵活的特征工程流水线；引入贝叶斯优化（BO）模块完成超参数优化；并通过动态优化器选择器，自适应地调度特征工程与超参数优化步骤，实现交错优化。关键在于，我们设计了一种互条件机制，使 LLM 与 BO 之间能够共享上下文信息，实现相互启发的决策。实验结果表明，CoFEH 不仅显著优于传统方法和基于 LLM 的特征工程基线，更在联合优化下实现了更优的端到端性能。"
  },
  {
    "date": "2026-02-10",
    "title": "GHS-TDA: A Synergistic Reasoning Framework Integrating Global Hypothesis Space with Topological Data Analysis",
    "authors": "Jiaquan Zhang, Chaoning Zhang, Shuxu Chen, Xudong Wang, Zhenzhen Huang, Pengcheng Zheng, Shuai Yuan, Sheng Zheng, Qigan Sun, Jie Zou, Lik-Hang Lee, Yang Yang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09794v1",
    "source": "arXiv",
    "abstract": "Chain-of-Thought (CoT) has been shown to significantly improve the reasoning accuracy of large language models (LLMs) on complex tasks. However, due to the autoregressive, step-by-step generation paradigm, existing CoT methods suffer from two fundamental limitations. First, the reasoning process is highly sensitive to early decisions: once an initial error is introduced, it tends to propagate and amplify through subsequent steps, while the lack of a global coordination and revision mechanism makes such errors difficult to correct, ultimately leading to distorted reasoning chains. Second, current CoT approaches lack structured analysis techniques for filtering redundant reasoning and extracting key reasoning features, resulting in unstable reasoning processes and limited interpretability. To address these issues, we propose GHS-TDA. GHS-TDA first constructs a semantically enriched global hypothesis graph to aggregate, align, and coordinate multiple candidate reasoning paths, thereby providing alternative global correction routes when local reasoning fails. It then applies topological data analysis based on persistent homology to capture stable multi-scale structures, remove redundancy and inconsistencies, and extract a more reliable reasoning skeleton. By jointly leveraging reasoning diversity and topological stability, GHS-TDA achieves self-adaptive convergence, produces high-confidence and interpretable reasoning paths, and consistently outperforms strong baselines in terms of both accuracy and robustness across multiple reasoning benchmarks.",
    "title_zh": "GHS-TDA：一种融合全局假设空间与拓扑数据分析的协同推理框架",
    "abstract_zh": "思维链（Chain-of-Thought, CoT）已被证明能显著提升大型语言模型（LLMs）在复杂任务上的推理准确性。然而，由于现有CoT方法采用自回归、逐步生成的范式，存在两个根本性局限。首先，推理过程对早期决策极为敏感：一旦引入初始错误，该错误往往会在后续步骤中传播并放大，而由于缺乏全局协调与修正机制，此类错误难以被纠正，最终导致推理链条严重扭曲。其次，当前的CoT方法缺乏结构化的分析技术来过滤冗余推理并提取关键推理特征，从而造成推理过程不稳定且可解释性有限。为解决上述问题，我们提出GHS-TDA。GHS-TDA首先构建一个语义丰富的全局假设图，用于聚合、对齐和协调多个候选推理路径，从而在局部推理失败时提供替代的全局修正路径。随后，基于持久同调的拓扑数据分析方法，捕捉稳定且多尺度的结构特征，消除冗余与不一致性，提取出更可靠的推理骨架。通过协同利用推理多样性与拓扑稳定性，GHS-TDA实现了自适应收敛，生成高置信度且可解释的推理路径，并在多个推理基准测试中持续优于强基线模型，在准确率与鲁棒性方面均表现出色。"
  },
  {
    "date": "2026-02-10",
    "title": "Circuit Fingerprints: How Answer Tokens Encode Their Geometrical Path",
    "authors": "Andres Saurez, Neha Sengar, Dongsoo Har",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09784v1",
    "source": "arXiv",
    "abstract": "Circuit discovery and activation steering in transformers have developed as separate research threads, yet both operate on the same representational space. Are they two views of the same underlying structure? We show they follow a single geometric principle: answer tokens, processed in isolation, encode the directions that would produce them. This Circuit Fingerprint hypothesis enables circuit discovery without gradients or causal intervention -- recovering comparable structure to gradient-based methods through geometric alignment alone. We validate this on standard benchmarks (IOI, SVA, MCQA) across four model families, achieving circuit discovery performance comparable to gradient-based methods. The same directions that identify circuit components also enable controlled steering -- achieving 69.8\\% emotion classification accuracy versus 53.1\\% for instruction prompting while preserving factual accuracy. Beyond method development, this read-write duality reveals that transformer circuits are fundamentally geometric structures: interpretability and controllability are two facets of the same object.",
    "title_zh": "电路指纹：答案标记如何编码其几何路径",
    "abstract_zh": "在变压器模型中，电路发现与激活引导分别发展为独立的研究方向，但二者均作用于相同的表征空间。它们是否为同一底层结构的两种视角？我们证明，二者遵循单一的几何原理：答案标记在独立处理时，编码了生成它们的方向。这一“电路指纹”假说使得无需梯度或因果干预即可实现电路发现——仅通过几何对齐即可恢复与基于梯度的方法相当的结构。我们在四个模型族的标准基准（IOI、SVA、MCQA）上验证了该方法，实现了与基于梯度的方法相当的电路发现性能。同样这些方向不仅能够识别电路组件，还能实现可控引导——在情感分类任务中达到69.8%的准确率，远超指令提示的53.1%，同时保持事实准确性。除了方法论的突破，这一读写对偶性揭示了变压器电路本质上是几何结构：可解释性与可控性是同一对象的两个方面。"
  },
  {
    "date": "2026-02-10",
    "title": "Toward Linking Declined Proposals and Source Code: An Exploratory Study on the Go Repository",
    "authors": "Sota Nakashima, Masanari Kondo, Mahmoud Alfadel, Aly Ahmad, Toshihiro Nakae, Hidenori Matsuzaki",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09467v1",
    "source": "arXiv",
    "abstract": "Traceability links are key information sources for software developers, connecting software artifacts (e.g., linking requirements to the corresponding source code). In open-source software (OSS) projects, such links play an important role, particularly between the contributions (e.g., GitHub issues) and the corresponding source code. Through these links, developers can trace the discussions in contributions and uncover design rationales, constraints, and security concerns. Previous studies have mainly examined accepted contributions, while those declined after discussion have been overlooked. The discussions behind declined contributions contain valuable design rationales and implicit knowledge about software decision-making, as the reasons behind the decline often reveal the criteria used to judge what should or should not be implemented. In this study, we present the first attempt to establish traceability links between declined contributions and related source code. We propose an initial linking approach and conduct an empirical analysis of the generated links to discuss factors affecting link generation. As our dataset, we use proposals from the official Go repository, which are GitHub issues used to propose new features or language changes. To link declined proposals to source code, we designed an LLM-driven pipeline. Our results showed that the pipeline selected the correct granularity for each declined proposal with an accuracy of 0.836, and generated correct links at that granularity with a mean precision of 0.643. To clarify the challenges of linking declined proposals, we performed a failure analysis. In the declined proposals where the pipeline failed to generate links, the discussions were often redundant and lacked concrete information (e.g., how the feature should be implemented).",
    "title_zh": "连接被拒绝的提案与源代码：对Go仓库的一项探索性研究",
    "abstract_zh": "可追溯性链接是软件开发人员的关键信息来源，用于连接软件制品（例如，将需求与相应的源代码关联起来）。在开源软件（OSS）项目中，这类链接尤为重要，尤其是在贡献（如GitHub问题）与相应源代码之间。通过这些链接，开发人员可以追溯贡献中的讨论内容，从而揭示设计决策的依据、约束条件以及安全问题。以往的研究主要关注已被接受的贡献，而对经过讨论后被拒绝的贡献则较少关注。然而，被拒绝贡献背后的讨论中蕴含着宝贵的设计 rationale 和关于软件决策的隐性知识，因为拒绝的原因往往揭示了判断某功能是否应被实现的标准。在本研究中，我们首次尝试建立被拒绝贡献与相关源代码之间的可追溯性链接。我们提出了一种初步的链接方法，并对生成的链接进行了实证分析，以探讨影响链接生成的因素。我们的数据集来自官方Go语言仓库的提案，即用于提议新功能或语言变更的GitHub问题。为了将被拒绝的提案与源代码关联，我们设计了一个基于大语言模型（LLM）的处理流程。实验结果表明，该流程在每个被拒绝提案上正确识别出合适粒度的准确率达到0.836，且在该粒度下生成正确链接的平均精确率为0.643。为进一步阐明链接被拒绝提案所面临的挑战，我们进行了失败分析。在那些流程未能成功生成链接的被拒绝提案中，讨论内容往往冗余且缺乏具体信息（例如，该功能应如何实现）。"
  },
  {
    "date": "2026-02-10",
    "title": "Accelerating Post-Quantum Cryptography via LLM-Driven Hardware-Software Co-Design",
    "authors": "Yuchao Liao, Tosiron Adegbija, Roman Lysecky",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09410v1",
    "source": "arXiv",
    "abstract": "Post-quantum cryptography (PQC) is crucial for securing data against emerging quantum threats. However, its algorithms are computationally complex and difficult to implement efficiently on hardware. In this paper, we explore the potential of Large Language Models (LLMs) to accelerate the hardware-software co-design process for PQC, with a focus on the FALCON digital signature scheme. We present a novel framework that leverages LLMs to analyze PQC algorithms, identify performance-critical components, and generate candidate hardware descriptions for FPGA implementation. We present the first quantitative comparison between LLM-driven synthesis and conventional HLS-based approaches for low-level compute-intensive kernels in FALCON, showing that human-in-the-loop LLM-generated accelerators can achieve up to 2.6x speedup in kernel execution time with shorter critical paths, while highlighting trade-offs in resource utilization and power consumption. Our results suggest that LLMs can minimize design effort and development time by automating FPGA accelerator design iterations for PQC algorithms, offering a promising new direction for rapid and adaptive PQC accelerator design on FPGAs.",
    "title_zh": "通过大语言模型驱动的软硬件协同设计加速后量子密码学",
    "abstract_zh": "后量子密码学（PQC）对于防范新兴的量子计算威胁至关重要。然而，其算法计算复杂度高，在硬件上实现时难以高效部署。本文探讨了大型语言模型（LLMs）在加速PQC硬件-软件协同设计过程中的潜力，重点聚焦于FALCON数字签名方案。我们提出了一种创新框架，利用LLMs分析PQC算法，识别性能关键组件，并生成适用于FPGA实现的候选硬件描述。我们首次对LLM驱动的综合方法与传统HLS（高层次综合）方法在FALCON中低层次计算密集型内核上的表现进行了定量比较，结果表明，采用人机协同的LLM生成加速器可使内核执行时间提升最高达2.6倍，同时具有更短的关键路径，但也揭示了在资源利用率和功耗方面的权衡。我们的研究结果表明，LLM能够通过自动化FPGA加速器设计迭代，显著降低PQC算法的设计工作量和开发周期，为FPGA上快速、自适应的PQC加速器设计提供了极具前景的新方向。"
  },
  {
    "date": "2026-02-10",
    "title": "AgentSkiller: Scaling Generalist Agent Intelligence through Semantically Integrated Cross-Domain Data Synthesis",
    "authors": "Zexu Sun, Bokai Ji, Hengyi Cai, Shuaiqiang Wang, Lei Wang, Guangxia Li, Xu Chen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09372v1",
    "source": "arXiv",
    "abstract": "Large Language Model agents demonstrate potential in solving real-world problems via tools, yet generalist intelligence is bottlenecked by scarce high-quality, long-horizon data. Existing methods collect privacy-constrained API logs or generate scripted interactions lacking diversity, which struggle to produce data requisite for scaling capabilities. We propose AgentSkiller, a fully automated framework synthesizing multi-turn interaction data across realistic, semantically linked domains. It employs a DAG-based architecture with explicit state transitions to ensure determinism and recoverability. The pipeline builds a domain ontology and Person-Centric Entity Graph, defines tool interfaces via Service Blueprints for Model Context Protocol servers, and populates environments with consistent databases and strict Domain Policies. A cross-domain fusion mechanism links services to simulate complex tasks. Finally, the pipeline creates user tasks by verifying solution paths, filtering via execution-based validation, and generating queries using a Persona-based Simulator for automated rollout. This produces reliable environments with clear state changes. To demonstrate effectiveness, we synthesized $\\approx$ 11K interaction samples; experimental results indicate that models trained on this dataset achieve significant improvements on function calling over baselines, particularly in larger parameter regimes.",
    "title_zh": "AgentSkiller：通过语义集成的跨域数据合成实现通用代理智能的扩展",
    "abstract_zh": "大型语言模型代理通过工具在解决现实世界问题方面展现出巨大潜力，但其通用智能的发展受限于高质量、长时序数据的稀缺。现有方法要么收集受隐私限制的API日志，要么生成缺乏多样性的脚本化交互，难以生成支撑能力扩展所需的数据。为此，我们提出AgentSkiller——一种完全自动化的框架，用于在真实且语义关联的多领域中合成多轮交互数据。该框架采用基于有向无环图（DAG）的架构，通过显式的状态转移机制确保过程的确定性与可恢复性。整个流程首先构建领域本体和以人物为中心的实体图谱，通过服务蓝图（Service Blueprints）定义工具接口，对接模型上下文协议（Model Context Protocol）服务器，并在环境中部署一致的数据库及严格的领域策略。跨域融合机制将不同服务连接起来，以模拟复杂任务。最后，系统通过验证解决方案路径、基于执行结果进行过滤，并利用基于角色的模拟器生成用户任务，实现自动化部署。该流程生成了具备清晰状态变化的可靠环境。为验证其有效性，我们共合成约11,000条交互样本；实验结果表明，基于该数据集训练的模型在函数调用能力上显著优于基线方法，尤其在参数量较大的模型中表现更为突出。"
  },
  {
    "date": "2026-02-10",
    "title": "CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs",
    "authors": "Richard Bornemann, Pierluigi Vito Amadori, Antoine Cully",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.10085v1",
    "source": "arXiv",
    "abstract": "Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is infeasible for open-ended skill discovery, where the set of meaningful skills is not known a priori. While recent methods have shown promising results towards automating reward function design, they remain limited to refining rewards for pre-defined tasks. To address this limitation, we introduce Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs (CODE-SHARP), a novel framework leveraging Foundation Models (FM) to open-endedly expand and refine a hierarchical skill archive, structured as a directed graph of executable reward functions in code. We show that a goal-conditioned agent trained exclusively on the rewards generated by the discovered SHARP skills learns to solve increasingly long-horizon goals in the Craftax environment. When composed by a high-level FM-based planner, the discovered skills enable a single goal-conditioned agent to solve complex, long-horizon tasks, outperforming both pretrained agents and task-specific expert policies by over $134$% on average. We will open-source our code and provide additional videos $\\href{https://sites.google.com/view/code-sharp/homepage}{here}$.",
    "title_zh": "CODE-SHARP：技能的连续开放式发现与演化作为分层奖励程序",
    "abstract_zh": "开发能够持续不断地发现并学习新技能的智能体，是人工智能领域的一项重大挑战。尽管强化学习为训练智能体掌握复杂技能提供了强大的框架，但其通常依赖于人工设计的奖励函数。然而，在开放式的技能发现场景中，有意义的技能集合在事前是未知的，因此人工设计奖励函数的方法难以适用。尽管近期一些方法在自动化奖励函数设计方面取得了令人鼓舞的进展，但它们仍局限于对预定义任务的奖励进行优化。为解决这一局限性，我们提出了连续开放式技能发现与进化框架——基于分层奖励程序的技能发现与进化（CODE-SHARP）。该框架利用基础模型（Foundation Models, FM）持续扩展并优化一个分层技能库，该技能库以有向图的形式组织，其中每个节点均为可执行的代码化奖励函数。我们证明，仅使用由所发现的SHARP技能生成的奖励进行训练的目标条件智能体，能够在Craftax环境中逐步解决越来越长时序的目标。当由基于基础模型的高层规划器进行组合时，这些发现的技能使单一目标条件智能体能够完成复杂且长时序的任务，其性能平均优于预训练智能体和任务专用专家策略超过134%。我们将在[此处](https://sites.google.com/view/code-sharp/homepage)开源代码并提供额外的演示视频。"
  },
  {
    "date": "2026-02-10",
    "title": "Artisan: Agentic Artifact Evaluation",
    "authors": "Doehyun Baek, Michael Pradel",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.10046v1",
    "source": "arXiv",
    "abstract": "Artifact evaluation has become standard practice in the software engineering community to ensure the reproducibility of research results. However, the current manual process is labor-intensive, and hence, done only as a one-time assessment for a subset of all papers. To support the artifact evaluation effort, we present Artisan, an automated LLM agent for reproducing research results given a paper and its artifact. The approach is enabled by two key contributions: First, we frame the reproduction problem as a code generation task where the goal is to generate a reproduction script that, when executed, reproduces the results reported in a paper. Unlike prior work on automatically reproducing research results in other domains, this formulation allows for running the script independently of the agent and for assessing the reproduction process at a fine-grained level. Second, we design automated judging mechanism that guides the agent toward the expected results without revealing them and that prevent trivial solutions, such as simply copying checked-in results. To evaluate Artisan, we introduce Artisan-Bench, the first benchmark assessing the ability to generate reproduction scripts and the first benchmark for automated artifact evaluation in software engineering. Artisan-Bench comprises 60 tasks derived from 23 software engineering papers, covering different research areas and programming languages. We validate all tasks in Artisan-Bench for reproducibility to ensure that the tasks are feasible. Our experiments show that Artisan is effective, producing 44/60 reproduction scripts and outperforming the best available baseline, a vanilla LLM agent (mini-swe-agent), by 3.14$\\times$ in terms of reproduction scripts generated while taking $0.45 and 48 minutes, on average per task. Artisan also helped uncover 20 new errors in either the paper or artifact.",
    "title_zh": "工匠：代理型人工制品评估",
    "abstract_zh": "成果评估已成为软件工程领域确保研究结果可复现的标准实践。然而，当前的评估过程主要依赖人工，耗时耗力，因此通常仅针对所有论文中的一小部分进行一次性评估。为支持成果评估工作，我们提出了 Artisan——一种基于大语言模型（LLM）的自动化代理，能够根据论文及其配套成果（artifact）自动复现研究结果。该方法基于两项关键贡献：第一，我们将复现问题建模为代码生成任务，目标是生成一个复现脚本，当执行该脚本时，能够复现论文中报告的结果。与以往在其他领域尝试自动复现研究结果的工作不同，这种建模方式使得生成的脚本可独立运行，并支持对复现过程进行细粒度评估。第二，我们设计了一种自动评判机制，能够引导代理向预期结果逼近，而无需直接透露结果本身，同时防止诸如直接复制已提交结果等“投机性”解决方案。\n\n为评估 Artisan 的性能，我们提出了 Artisan-Bench，这是首个用于评估生成复现脚本能力的基准，也是首个面向软件工程领域自动化成果评估的基准。Artisan-Bench 包含 60 个任务，源自 23 篇软件工程论文，覆盖多种研究领域和编程语言。我们对所有任务进行了可复现性验证，以确保任务的可行性。实验结果表明，Artisan 表现优异，成功生成了 44/60 个复现脚本，相比现有最佳基线（即普通的 LLM 代理 mini-swe-agent），在生成复现脚本数量上提升了 3.14 倍，且每个任务平均耗时仅 0.45 到 48 分钟。此外，Artisan 还帮助发现了 20 个存在于论文或配套成果中的新错误。"
  },
  {
    "date": "2026-02-10",
    "title": "A Task-Centric Theory for Iterative Self-Improvement with Easy-to-Hard Curricula",
    "authors": "Chenruo Liu, Yijun Dong, Yiqiu Shen, Qi Lei",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.10014v1",
    "source": "arXiv",
    "abstract": "Iterative self-improvement fine-tunes an autoregressive large language model (LLM) on reward-verified outputs generated by the LLM itself. In contrast to the empirical success of self-improvement, the theoretical foundation of this generative, iterative procedure in a practical, finite-sample setting remains limited. We make progress toward this goal by modeling each round of self-improvement as maximum-likelihood fine-tuning on a reward-filtered distribution and deriving finite-sample guarantees for the expected reward. Our analysis reveals an explicit feedback loop where better models accept more data per iteration, supporting sustained self-improvement while explaining eventual saturation of such improvement. Adopting a task-centric view by considering reasoning tasks with multiple difficulty levels, we further prove quantifiable conditions on model initialization, task difficulty, and sample budget where easy-to-hard curricula provably achieve better guarantees than training on fixed mixtures of tasks. Our analyses are validated via Monte-Carlo simulations and controlled experiments on graph-based reasoning tasks.",
    "title_zh": "以任务为中心的迭代自我提升理论：从易到难的课程设计",
    "abstract_zh": "迭代式自我改进通过利用语言模型自身生成的、经奖励验证的输出来微调自回归大语言模型（LLM）。与自我改进在实践中取得的显著成功形成对比的是，这种生成性、迭代式过程在实际的有限样本设置下的理论基础仍然较为薄弱。本文通过将每一轮自我改进建模为在奖励过滤分布上的最大似然微调，并推导出期望奖励的有限样本保证，从而在这一方向上取得了进展。我们的分析揭示了一个明确的反馈回路：模型性能越好，每轮迭代中可接受的数据量就越多，这支持了持续的自我改进，同时解释了改进最终趋于饱和的原因。通过引入以任务为中心的视角，考虑具有多种难度级别的推理任务，我们进一步证明了在模型初始化、任务难度和样本预算方面存在可量化的条件，使得从易到难的任务循序渐进式训练（curricula）能够优于固定任务混合的训练方式，并提供更优的保证。我们的理论分析通过蒙特卡洛模拟以及在基于图的推理任务上的受控实验得到了验证。"
  },
  {
    "date": "2026-02-10",
    "title": "ATTNPO: Attention-Guided Process Supervision for Efficient Reasoning",
    "authors": "Shuaiyi Nie, Siyu Ding, Wenyuan Zhang, Linhao Yu, Tianmeng Yang, Yao Chen, Tingwen Liu, Weichong Yin, Yu Sun, Hua Wu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09953v1",
    "source": "arXiv",
    "abstract": "Large reasoning models trained with reinforcement learning and verifiable rewards (RLVR) achieve strong performance on complex reasoning tasks, yet often overthink, generating redundant reasoning without performance gains. Existing trajectory-level length penalties often fail to effectively shorten reasoning length and degrade accuracy, as they uniformly treat all reasoning steps and lack fine-grained signals to distinguish redundancy from necessity. Meanwhile, process-supervised methods are typically resource-intensive and suffer from inaccurate credit assignment. To address these issues, we propose ATTNPO, a low-overhead process-supervised RL framework that leverages the model's intrinsic attention signals for step-level credit assignment. We first identify a set of special attention heads that naturally focus on essential steps while suppressing redundant ones. By leveraging the attention scores of these heads, We then employ two sub-strategies to mitigate overthinking by discouraging redundant steps while preserving accuracy by reducing penalties on essential steps. Experimental results show that ATTNPO substantially reduces reasoning length while significantly improving performance across 9 benchmarks.",
    "title_zh": "ATTNPO：用于高效推理的注意力引导过程监督",
    "abstract_zh": "通过强化学习与可验证奖励（RLVR）训练的大规模推理模型在复杂推理任务上表现出色，但常常出现过度思考的问题，即生成冗余的推理过程却未能带来性能提升。现有的轨迹级长度惩罚方法通常无法有效缩短推理长度，反而会降低准确率，因为它们对所有推理步骤一视同仁，缺乏细粒度信号来区分冗余与必要步骤。与此同时，过程监督方法通常资源消耗大，且存在信用分配不准确的问题。为解决上述问题，我们提出ATTPPO——一种低开销的过程监督强化学习框架，该框架利用模型内在的注意力信号实现步骤级的信用分配。我们首先识别出一组特殊的注意力头，这些注意力头天然聚焦于关键步骤，同时抑制冗余步骤。随后，基于这些注意力头的得分，我们采用两种子策略：一方面通过抑制冗余步骤来缓解过度思考，另一方面通过降低对关键步骤的惩罚来保持准确性。实验结果表明，ATTPPO在9个基准测试中显著减少了推理长度，同时大幅提升了模型性能。"
  },
  {
    "date": "2026-02-10",
    "title": "JMigBench: A Benchmark for Evaluating LLMs on Source Code Migration (Java 8 to Java 11)",
    "authors": "Nishil Amin, Zhiwei Fei, Xiang Li, Justyna Petke, He Ye",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09930v1",
    "source": "arXiv",
    "abstract": "We build a benchmark to evaluate large language models (LLMs) for source code migration tasks, specifically upgrading functions from Java 8 to Java 11. We first collected a dataset of function pairs from open-source repositories, but limitations in data quality led us to construct a refined dataset covering eight categories of deprecated APIs. Using this dataset, the Mistral Codestral model was evaluated with CodeBLEU and keyword-based metrics to measure lexical and semantic similarity as well as migration correctness. Results show that the evaluated model (Mistral Codestral) can handle trivial one-to-one API substitutions with moderate success, achieving identical migrations in 11.11% of the cases, but it struggles with more complex migrations such as CORBA or JAX-WS. These findings suggest Mistral Codestral can partially reduce developer effort by automating repetitive migration tasks but cannot yet replace humans within the scope of the JMigBench benchmark. The benchmark and analysis provide a foundation for future work on expanding datasets, refining prompting strategies, and improving migration performance across different LLMs.",
    "title_zh": "JMigBench：一个用于评估大语言模型在源代码迁移（从 Java 8 到 Java 11）任务中表现的基准测试",
    "abstract_zh": "我们构建了一个基准测试，用于评估大型语言模型（LLMs）在源代码迁移任务中的表现，具体针对将函数从 Java 8 升级至 Java 11 的场景。我们首先从开源仓库中收集了函数对数据集，但由于数据质量的局限性，我们进一步构建了一个经过精炼的数据集，涵盖八类已弃用的 API。利用该数据集，我们采用 CodeBLEU 和基于关键词的指标对 Mistral Codestral 模型进行了评估，以衡量其在词汇和语义相似性以及迁移正确性方面的表现。结果表明，所评估的模型（Mistral Codestral）在处理简单的、一对一的 API 替换任务时表现中等，仅在 11.11% 的案例中实现了完全一致的迁移；但在处理更复杂的迁移任务（如 CORBA 或 JAX-WS）时表现不佳。这些发现表明，Mistral Codestral 能够在一定程度上通过自动化重复性迁移任务减轻开发者的负担，但在 JMigBench 基准测试的范围内尚无法替代人工。该基准测试与分析为未来工作奠定了基础，包括扩展数据集、优化提示策略，以及提升不同大型语言模型在迁移任务中的性能。"
  },
  {
    "date": "2026-02-10",
    "title": "Tiny Moves: Game-based Hypothesis Refinement",
    "authors": "Agnieszka Dobrowolska, Rogier Hintzen, Martin Balla, Karl Gemayel, Sabine Reichert, Thomas Charman, Jen Ning Lim, Lindsay Edwards, Anna Gogleva",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09801v1",
    "source": "arXiv",
    "abstract": "Most machine learning approaches to scientific discovery frame hypotheses as end-to-end predictions, obscuring the incremental structure of scientific reasoning. We propose The Hypothesis Game, a symbolic formalism for hypothesis refinement in which LLM agents operate on a shared hypothesis state using a fixed grammar of reasoning moves. The framework is motivated by the observation that scientific progress often proceeds through small, localized revisions, grounded in domain context, rather than extensive rewrites. We instantiate a minimal game with LLM agents and evaluate it on pathway-level mechanistic refinement tasks. In the primary setting of corruption recovery, where hypotheses contain controlled errors, the game-based approach consistently removes more errors and achieves higher precision than strong prompting baselines, while preserving valid structure through incremental edits. In a secondary reconstruction setting from partial cues, it performs comparably to the strongest baseline, indicating that explicit move-based refinement remains competitive even when ground-truth recovery is difficult. These findings support game-based reasoning as a principled route to more controllable, interpretable, and transferable hypothesis refinement systems for scientific discovery.",
    "title_zh": "微小动作：基于游戏的假设优化",
    "abstract_zh": "大多数机器学习方法在科学发现中将假设视为端到端的预测，掩盖了科学推理的渐进式结构。我们提出了“假设博弈”（The Hypothesis Game），这是一种符号化形式化框架，用于假设的逐步优化，其中大语言模型（LLM）代理基于固定的推理动作语法，在共享的假设状态上协同操作。该框架的提出源于一个观察：科学进步通常通过小范围、局部化的修正实现，这些修正根植于具体领域背景，而非大规模重写。我们构建了一个最小化的博弈实例，使用LLM代理，并在通路级机制细化任务上进行了评估。在主要的“错误恢复”场景中，假设包含可控错误，基于博弈的方法始终能消除更多错误，并在精度上优于强提示基线，同时通过渐进式编辑保持了有效结构。在次要的“部分线索重建”场景中，其表现与最强基线相当，表明即使在难以恢复真实答案的情况下，基于显式动作的优化方法依然具有竞争力。这些发现支持了基于博弈的推理是一种有原则的路径，可实现更可控、可解释且可迁移的假设优化系统，从而推动科学发现的发展。"
  },
  {
    "date": "2026-02-10",
    "title": "When Less is More: The LLM Scaling Paradox in Context Compression",
    "authors": "Ruishan Guo, Yibing Liu, Guoxin Ma, Yan Wang, Yueyang Zhang, Long Xia, Kecheng Chen, Zhiyuan Sun, Daiting Shi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09789v1",
    "source": "arXiv",
    "abstract": "Scaling up model parameters has long been a prevalent training paradigm driven by the assumption that larger models yield superior generation capabilities. However, under lossy context compression in a compressor-decoder setup, we observe a Size-Fidelity Paradox: increasing the compressor size can lessen the faithfulness of reconstructed contexts though training loss decreases. Through extensive experiments across models from 0.6B to 90B, we coin this paradox arising from two dominant factors: 1) knowledge overwriting: larger models increasingly replace source facts with their own prior beliefs, e.g., ``the white strawberry'' $\\to$ ``the red strawberry''; and 2) semantic drift: larger models tend to paraphrase or restructure content instead of reproducing it verbatim, e.g., ``Alice hit Bob'' $\\to$ ``Bob hit Alice''. By holding model size fixed, we reflect on the emergent properties of compressed context representations. We show that the culprit is not parameter count itself, but the excessive semantic capacity and amplified generative uncertainty that accompany scaling. Specifically, the increased rank of context embeddings facilitates prior knowledge intrusion, whereas higher entropy over token prediction distributions promotes rewriting. Our results complement existing evaluations over context compression paradigm, underpinning a breakdown in scaling laws for faithful preservation in open-ended generation.",
    "title_zh": "少即是多：上下文压缩中的大语言模型缩放悖论",
    "abstract_zh": "长期以来，扩大模型参数规模一直是一种主流的训练范式，其背后假设是：更大的模型能够产生更优越的生成能力。然而，在压缩器-解码器架构下的有损上下文压缩场景中，我们观察到一种“规模-保真度悖论”：尽管训练损失下降，但增大压缩器规模反而会降低重建上下文的忠实度。通过在0.6B至90B规模的多个模型上进行广泛实验，我们揭示了这一悖论主要由两个关键因素导致：1）知识覆盖：更大的模型越来越倾向于用自身的先验信念取代原始事实，例如将“白色的草莓”错误地重构为“红色的草莓”；2）语义漂移：更大的模型更倾向于对内容进行改写或重构，而非逐字复现，例如将“爱丽丝打了鲍勃”变为“鲍勃打了爱丽丝”。在固定模型规模的前提下，我们深入探讨了压缩后上下文表征所涌现的特性。研究发现，问题的根源并非参数数量本身，而是随着模型规模扩大而带来的过度语义容量以及被放大的生成不确定性。具体而言，上下文嵌入的秩增加促进了先验知识的侵入，而标记预测分布的熵升高则加剧了内容重写行为。我们的研究结果补充了现有对上下文压缩范式的评估，揭示了在开放式生成任务中，忠实性保持的“扩展规律”已出现失效。"
  },
  {
    "date": "2026-02-10",
    "title": "On the Optimal Reasoning Length for RL-Trained Language Models",
    "authors": "Daisuke Nohara, Taishi Nakamura, Rio Yokota",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09591v1",
    "source": "arXiv",
    "abstract": "Reinforcement learning substantially improves reasoning in large language models, but it also tends to lengthen chain of thought outputs and increase computational cost during both training and inference. Though length control methods have been proposed, it remains unclear what the optimal output length is for balancing efficiency and performance. In this work, we compare several length control methods on two models, Qwen3-1.7B Base and DeepSeek-R1-Distill-Qwen-1.5B. Our results indicate that length penalties may hinder reasoning acquisition, while properly tuned length control can improve efficiency for models with strong prior reasoning. By extending prior work to RL trained policies, we identify two failure modes, 1) long outputs increase dispersion, and 2) short outputs lead to under-thinking.",
    "title_zh": "强化学习训练语言模型的最优推理长度研究",
    "abstract_zh": "强化学习显著提升了大语言模型的推理能力，但同时也往往导致思维链输出变长，并在训练和推理阶段增加计算开销。尽管已有长度控制方法被提出，但如何在效率与性能之间取得最佳平衡，仍不清楚最优输出长度究竟是多少。在本研究中，我们对比了多种长度控制方法在两个模型——Qwen3-1.7B Base 和 DeepSeek-R1-Distill-Qwen-1.5B 上的表现。结果表明，长度惩罚可能会阻碍推理能力的获取，而对具备较强先验推理能力的模型，经过适当调优的长度控制则能有效提升效率。通过将先前工作扩展至强化学习训练的策略，我们识别出两种失败模式：1）过长的输出会增加结果的离散性；2）过短的输出则会导致思考不足。"
  },
  {
    "date": "2026-02-10",
    "title": "Sci-VLA: Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments",
    "authors": "Yiwen Pang, Bo Zhou, Changjin Li, Xuanhao Wang, Shengxiang Xu, Deng-Bao Wang, Min-Ling Zhang, Shimin Di",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09430v1",
    "source": "arXiv",
    "abstract": "Robotic laboratories play a critical role in autonomous scientific discovery by enabling scalable, continuous experimental execution. Recent vision-language-action (VLA) models offer a promising foundation for robotic laboratories. However, scientific experiments typically involve long-horizon tasks composed of multiple atomic tasks, posing a fundamental challenge to existing VLA models. While VLA models fine-tuned for scientific tasks can reliably execute atomic experimental actions seen during training, they often fail to perform composite tasks formed by reordering and composing these known atomic actions. This limitation arises from a distributional mismatch between training-time atomic tasks and inference-time composite tasks, which prevents VLA models from executing necessary transitional operations between atomic tasks. To address this challenge, we propose an Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments. It introduces an LLM-based agentic inference mechanism that intervenes when executing sequential manipulation tasks. By performing explicit transition inference and generating transitional robotic action code, the proposed plugin guides VLA models through missing transitional steps, enabling reliable execution of composite scientific workflows without any additional training. This inference-only intervention makes our method computationally efficient, data-efficient, and well-suited for open-ended and long-horizon robotic laboratory tasks. We build 3D assets of scientific instruments and common scientific operating scenes within an existing simulation environment. In these scenes, we have verified that our method increases the average success rate per atomic task by 42\\% during inference. Furthermore, we show that our method can be easily transferred from the simulation to real scientific laboratories.",
    "title_zh": "Sci-VLA：面向科学实验中长时程任务的智能体VLA推理插件",
    "abstract_zh": "机器人实验室在自主科学发现中发挥着关键作用，能够实现可扩展、持续的实验执行。近年来，视觉-语言-动作（VLA）模型为机器人实验室提供了极具前景的基础。然而，科学实验通常涉及由多个原子任务组成的长时程任务，这对现有的VLA模型构成了根本性挑战。尽管针对科学任务微调过的VLA模型能够可靠地执行训练过程中见过的原子实验操作，但它们在执行由已知原子动作重新排序或组合而成的复合任务时往往表现不佳。这一局限源于训练阶段的原子任务与推理阶段的复合任务之间的分布差异，导致VLA模型无法完成原子任务之间的必要过渡操作。\n\n为解决这一挑战，我们提出了一种面向科学实验中长时程任务的代理式VLA推理插件（Agentic VLA Inference Plugin）。该插件引入了一种基于大语言模型（LLM）的代理式推理机制，在执行序列化操作任务时进行干预。通过显式地进行过渡推理并生成过渡性的机器人动作代码，该插件能够弥补VLA模型在原子任务之间缺失的过渡步骤，从而在无需任何额外训练的情况下，实现对复合科学工作流的可靠执行。这种仅依赖推理的干预方式使我们的方法在计算效率、数据效率方面表现优异，特别适用于开放性、长时程的机器人实验室任务。\n\n我们已在现有仿真环境中构建了科学仪器和常见科学操作场景的三维资产。在这些场景中，我们验证了所提方法在推理阶段将每个原子任务的平均成功率提升了42%。此外，我们还展示了该方法能够轻松从仿真环境迁移到真实的科学实验室中。"
  },
  {
    "date": "2026-02-10",
    "title": "QRS: A Rule-Synthesizing Neuro-Symbolic Triad for Autonomous Vulnerability Discovery",
    "authors": "George Tsigkourakos, Constantinos Patsakis",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09774v1",
    "source": "arXiv",
    "abstract": "Static Application Security Testing (SAST) tools are integral to modern DevSecOps pipelines, yet tools like CodeQL, Semgrep, and SonarQube remain fundamentally constrained: they require expert-crafted queries, generate excessive false positives, and detect only predefined vulnerability patterns. Recent work has explored augmenting SAST with Large Language Models (LLMs), but these approaches typically use LLMs to triage existing tool outputs rather than to reason about vulnerability semantics directly. We introduce QRS (Query, Review, Sanitize), a neuro-symbolic framework that inverts this paradigm. Rather than filtering results from static rules, QRS employs three autonomous agents that generate CodeQL queries from a structured schema definition and few-shot examples, then validate findings through semantic reasoning and automated exploit synthesis. This architecture enables QRS to discover vulnerability classes beyond predefined patterns while substantially reducing false positives. We evaluate QRS on full Python packages rather than isolated snippets. In 20 historical CVEs in popular PyPI libraries, QRS achieves 90.6% detection accuracy. Applied to the 100 most-downloaded PyPI packages, QRS identified 39 medium-to-high-severity vulnerabilities, 5 of which were assigned new CVEs, 5 received documentation updates, while the remaining 29 were independently discovered by concurrent researchers, validating both the severity and discoverability of these findings. QRS accomplishes this with low time overhead and manageable token costs, demonstrating that LLM-driven query synthesis and code review can complement manually curated rule sets and uncover vulnerability patterns that evade existing industry tools.",
    "title_zh": "QRS：一种用于自主漏洞发现的规则合成神经符号三元组",
    "abstract_zh": "静态应用安全测试（SAST）工具是现代 DevSecOps 流水线的核心组成部分，但诸如 CodeQL、Semgrep 和 SonarQube 等工具仍存在根本性局限：它们依赖专家手工编写的查询规则，产生大量误报，并且仅能检测预定义的漏洞模式。近期研究尝试将大型语言模型（LLMs）引入 SAST，但这些方法通常仅用 LLM 对现有工具的输出进行筛选，而非直接对漏洞语义进行推理。我们提出了 QRS（Query, Review, Sanitize）——一种神经符号框架，彻底颠覆了这一范式。与传统方法通过过滤静态规则结果不同，QRS 采用三个自主代理，基于结构化模式定义和少量示例自动生成 CodeQL 查询，随后通过语义推理和自动化漏洞利用生成验证发现结果。该架构使 QRS 能够发现超出预设模式的新型漏洞类别，同时显著降低误报率。\n\n我们在完整的 Python 包上评估 QRS，而非孤立的代码片段。在 20 个流行 PyPI 库中的历史 CVE 上，QRS 达到了 90.6% 的检测准确率。将其应用于下载量最高的 100 个 PyPI 包时，QRS 共识别出 39 个中高严重性漏洞，其中 5 个被分配了新的 CVE 编号，5 个获得项目文档更新，其余 29 个漏洞被同期独立研究者发现，充分验证了这些发现的严重性与可探测性。QRS 在实现高效检测的同时，保持了较低的时间开销和可控的 token 消耗，证明了基于 LLM 的查询生成与代码审查能够有效补充人工维护的规则集，揭示出当前行业工具难以捕捉的新型漏洞模式。"
  },
  {
    "date": "2026-02-10",
    "title": "Learning from the Irrecoverable: Error-Localized Policy Optimization for Tool-Integrated LLM Reasoning",
    "authors": "Qiao Liang, Yuke Zhu, Chao Ge, Lei Yang, Ying Shen, Bo Zheng, Sheng Guo",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09598v1",
    "source": "arXiv",
    "abstract": "Tool-integrated reasoning (TIR) enables LLM agents to solve tasks through planning, tool use, and iterative revision, but outcome-only reinforcement learning in this setting suffers from sparse, delayed rewards and weak step-level credit assignment. In long-horizon TIR trajectories, an early irrecoverable mistake can determine success or failure, making it crucial to localize the first irrecoverable step and leverage it for fine-grained credit assignment. We propose Error-Localized Policy Optimization (ELPO), which localizes the first irrecoverable step via binary-search rollout trees under a fixed rollout budget, converts the resulting tree into stable learning signals through hierarchical advantage attribution, and applies error-localized adaptive clipping to strengthen corrective updates on the critical step and its suffix. Across TIR benchmarks in math, science QA, and code execution, ELPO consistently outperforms strong Agentic RL baselines under comparable sampling budgets, with additional gains in Pass@K and Major@K scaling, rollout ranking quality, and tool-call efficiency. Our code will be publicly released soon.",
    "title_zh": "从不可挽回的错误中学习：面向工具集成大模型推理的误差定位策略优化",
    "abstract_zh": "工具集成推理（TIR）使大语言模型代理能够通过规划、工具使用和迭代修正来完成任务，但在该场景下仅基于结果的强化学习面临奖励稀疏、延迟严重以及步骤级信用分配能力弱的问题。在长时程TIR轨迹中，一个早期的不可挽回的错误可能直接决定任务成败，因此准确定位首个不可挽回的步骤，并利用该信息实现细粒度的信用分配至关重要。我们提出了一种误差定位策略优化方法（Error-Localized Policy Optimization, ELPO），该方法在固定回溯预算下，通过二分搜索回溯树定位首个不可挽回的步骤；将生成的树结构转化为稳定的学习信号，通过分层优势归因实现；并采用误差定位的自适应裁剪机制，强化对关键步骤及其后续部分的修正更新。在数学、科学问答和代码执行等多个TIR基准测试中，ELPO在与现有强代理强化学习基线相当的采样预算下，持续表现更优，且在Pass@K和Major@K的扩展性能、回溯排序质量以及工具调用效率方面均取得显著提升。我们的代码即将公开发布。"
  },
  {
    "date": "2026-02-10",
    "title": "Aligning Tree-Search Policies with Fixed Token Budgets in Test-Time Scaling of LLMs",
    "authors": "Sora Miyamoto, Daisuke Oba, Naoaki Okazaki",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09574v1",
    "source": "arXiv",
    "abstract": "Tree-search decoding is an effective form of test-time scaling for large language models (LLMs), but real-world deployment imposes a fixed per-query token budget that varies across settings. Existing tree-search policies are largely budget-agnostic, treating the budget as a termination condition, which can lead to late-stage over-branching or premature termination. We propose {Budget-Guided MCTS} (BG-MCTS), a tree-search decoding algorithm that aligns its search policy with the remaining token budget: it starts with broad exploration, then prioritizes refinement and answer completion as the budget depletes while reducing late-stage branching from shallow nodes. BG-MCTS consistently outperforms budget-agnostic tree-search baselines across different budgets on MATH500 and AIME24/25 with open-weight LLMs.",
    "title_zh": "在大语言模型测试时扩展中，将树搜索策略与固定令牌预算对齐",
    "abstract_zh": "树搜索解码是大型语言模型（LLMs）在测试时扩展的一种有效方法，但在实际部署中，每个查询的令牌预算通常是固定的，并且在不同场景下有所差异。现有的树搜索策略大多对预算不敏感，将预算视为终止条件，这可能导致后期过度分支或过早终止。我们提出了{预算引导的蒙特卡洛树搜索}（BG-MCTS），这是一种树搜索解码算法，其搜索策略与剩余的令牌预算相匹配：在初期进行广泛探索，随着预算逐渐耗尽，逐步转向精细化调整和答案完成，并减少浅层节点的后期分支。在使用开源权重的LLM时，BG-MCTS在MATH500以及AIME24/25数据集上，于不同预算条件下均持续优于对预算不敏感的树搜索基线方法。"
  },
  {
    "date": "2026-02-10",
    "title": "Listen to the Layers: Mitigating Hallucinations with Inter-Layer Disagreement",
    "authors": "Koduvayur Subbalakshmi, Sabbir Hossain Ujjal, Venkata Krishna Teja Mangichetty, Nastaran Jamalipour Soofi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09486v1",
    "source": "arXiv",
    "abstract": "Pretrained Large Language Models (LLMs) are prone to generating fluent yet factually incorrect text-a phenomenon known as hallucinations, undermining their reliability and utility in downstream tasks. We hypothesize that a generated text span's factuality is correlated with its representational instability across the model's internal layers. Based on this, we propose the CoCoA (Confusion and Consistency Aware) decoder, a novel, training-free decoding algorithm that mitigates hallucinations at inference time by listening to these signals in the middle layers. We propose two metrics to quantify this instability in the middle layers, and use it to penalize outputs that exhibit high internal confusion, thereby steering the model towards more internally consistent and factually grounded outputs. We further propose a self-information gated variant, CoCoA-SIG, that dynamically modulates this penalty to selectively target high-surprise, unstable generations. Extensive experiments on diverse tasks, including question-answering, summarization and code generation demonstrate that CoCoA significantly improves factual correctness across multiple model families (e.g., Llama-3, Qwen-2.5, Mistral). By leveraging model-intrinsic signals, CoCoA offers an effective and broadly applicable method for enhancing the trustworthiness of LLMs at inference time, without requiring any model retraining.",
    "title_zh": "倾听层级：通过层间分歧缓解幻觉",
    "abstract_zh": "预训练的大语言模型（LLMs）容易生成流畅但事实错误的文本——这一现象被称为“幻觉”，严重削弱了其在下游任务中的可靠性与实用性。我们假设，生成文本片段的事实性与其在模型内部各层中的表征不稳定性密切相关。基于这一假设，我们提出了 CoCoA（Confusion and Consistency Aware，混淆与一致性感知）解码器，这是一种无需训练的新型解码算法，通过在推理阶段监听模型中间层的信号，有效缓解幻觉问题。我们提出了两种度量方法，用于量化中间层中的表征不稳定性，并利用该度量对表现出高内部混淆的输出施加惩罚，从而引导模型生成更具内部一致性和事实依据的文本。此外，我们还提出了一个自信息门控变体 CoCoA-SIG，能够动态调节该惩罚机制，有针对性地作用于高意外性、不稳定的生成结果。在多种任务上的大量实验，包括问答、摘要生成和代码生成，均表明 CoCoA 在多个模型家族（如 Llama-3、Qwen-2.5、Mistral）中显著提升了事实正确性。通过利用模型内在的信号，CoCoA 提供了一种高效且普适的方法，在不需任何模型重训练的前提下，增强大语言模型在推理阶段的可信度。"
  },
  {
    "date": "2026-02-10",
    "title": "AlgoVeri: An Aligned Benchmark for Verified Code Generation on Classical Algorithms",
    "authors": "Haoyu Zhao, Ziran Yang, Jiawei Li, Deyuan He, Zenan Li, Chi Jin, Venugopal V. Veeravalli, Aarti Gupta, Sanjeev Arora",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09464v1",
    "source": "arXiv",
    "abstract": "Vericoding refers to the generation of formally verified code from rigorous specifications. Recent AI models show promise in vericoding, but a unified methodology for cross-paradigm evaluation is lacking. Existing benchmarks test only individual languages/tools (e.g., Dafny, Verus, and Lean) and each covers very different tasks, so the performance numbers are not directly comparable. We address this gap with AlgoVeri, a benchmark that evaluates vericoding of $77$ classical algorithms in Dafny, Verus, and Lean. By enforcing identical functional contracts, AlgoVeri reveals critical capability gaps in verification systems. While frontier models achieve tractable success in Dafny ($40.3$% for Gemini-3 Flash), where high-level abstractions and SMT automation simplify the workflow, performance collapses under the systems-level memory constraints of Verus ($24.7$%) and the explicit proof construction required by Lean (7.8%). Beyond aggregate metrics, we uncover a sharp divergence in test-time compute dynamics: Gemini-3 effectively utilizes iterative repair to boost performance (e.g., tripling pass rates in Dafny), whereas GPT-OSS saturates early. Finally, our error analysis shows that language design affects the refinement trajectory: while Dafny allows models to focus on logical correctness, Verus and Lean trap models in persistent syntactic and semantic barriers. All data and evaluation code can be found at https://github.com/haoyuzhao123/algoveri.",
    "title_zh": "AlgoVeri：面向经典算法的可验证代码生成对齐基准",
    "abstract_zh": "Vericoding 指的是从严格规范中生成经过形式化验证的代码。近年来，人工智能模型在 vericoding 方面展现出巨大潜力，但缺乏统一的跨范式评估方法。现有的基准测试仅针对单一语言或工具（如 Dafny、Verus 和 Lean），且各自涵盖的任务差异极大，导致性能指标无法直接比较。为此，我们提出了 AlgoVeri，一个在 Dafny、Verus 和 Lean 中评估 77 个经典算法 vericoding 能力的基准。通过强制执行相同的函数契约，AlgoVeri 揭示了验证系统中的关键能力差距：尽管前沿模型在 Dafny 中表现尚可（Gemini-3 Flash 达到 40.3% 的成功率），得益于高层抽象和 SMT 自动化带来的流程简化，但在 Verus 的系统级内存限制下性能急剧下降（24.7%），在 Lean 所需的显式证明构造中更是大幅下滑（仅 7.8%）。除了整体指标外，我们还发现测试阶段的计算动态存在显著差异：Gemini-3 能有效利用迭代修复机制提升性能（例如在 Dafny 中使通过率提升三倍），而 GPT-OSS 则早期即达到饱和。最后，我们的错误分析表明，语言设计深刻影响了模型的细化路径：Dafny 使模型能够聚焦于逻辑正确性，而 Verus 和 Lean 则使模型陷入持续存在的语法和语义障碍之中。所有数据和评估代码均可在 https://github.com/haoyuzhao123/algoveri 获取。"
  },
  {
    "date": "2026-02-10",
    "title": "Decoupled Reasoning with Implicit Fact Tokens (DRIFT): A Dual-Model Framework for Efficient Long-Context Inference",
    "authors": "Wenxuan Xie, Yujia Wang, Xin Tan, Chaochao Lu, Xia Hu, Xuhong Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.10021v1",
    "source": "arXiv",
    "abstract": "The integration of extensive, dynamic knowledge into Large Language Models (LLMs) remains a significant challenge due to the inherent entanglement of factual data and reasoning patterns. Existing solutions, ranging from non-parametric Retrieval-Augmented Generation (RAG) to parametric knowledge editing, are often constrained in practice by finite context windows, retriever noise, or the risk of catastrophic forgetting. In this paper, we propose DRIFT, a novel dual-model architecture designed to explicitly decouple knowledge extraction from the reasoning process. Unlike static prompt compression, DRIFT employs a lightweight knowledge model to dynamically compress document chunks into implicit fact tokens conditioned on the query. These dense representations are projected into the reasoning model's embedding space, replacing raw, redundant text while maintaining inference accuracy. Extensive experiments show that DRIFT significantly improves performance on long-context tasks, outperforming strong baselines among comparably sized models. Our approach provides a scalable and efficient paradigm for extending the effective context window and reasoning capabilities of LLMs. Our code is available at https://github.com/Lancelot-Xie/DRIFT.",
    "title_zh": "解耦推理与隐式事实标记（DRIFT）：一种用于高效长上下文推理的双模型框架",
    "abstract_zh": "将大量动态知识整合到大型语言模型（LLMs）中仍面临重大挑战，这主要源于事实性数据与推理模式之间的固有纠缠。现有的解决方案，从非参数化的检索增强生成（RAG）到参数化的知识编辑，往往在实际应用中受到有限上下文窗口、检索器噪声或灾难性遗忘风险的制约。本文提出了一种名为DRIFT的新颖双模型架构，旨在显式地将知识提取过程与推理过程解耦。与静态提示压缩不同，DRIFT采用一个轻量级知识模型，根据查询动态地将文档片段压缩为条件化的隐式事实标记。这些密集的表示被映射到推理模型的嵌入空间中，替代原始且冗余的文本，同时保持推理的准确性。大量实验表明，DRIFT在长上下文任务中显著提升了性能，在同等规模模型中优于多个强基线。我们的方法为扩展LLM的有效上下文窗口和推理能力提供了一种可扩展且高效的范式。代码已开源，地址为：https://github.com/Lancelot-Xie/DRIFT。"
  },
  {
    "date": "2026-02-10",
    "title": "Environment-in-the-Loop: Rethinking Code Migration with LLM-based Agents",
    "authors": "Xiang Li, Zhiwei Fei, Ying Ma, Jerry Zhang, Sarro Federica, He Ye",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09944v1",
    "source": "arXiv",
    "abstract": "Modern software systems continuously undergo code upgrades to enhance functionality, security, and performance, and Large Language Models (LLMs) have demonstrated remarkable capabilities in code migration tasks. However, while research on automated code migration which including refactoring, API adaptation, and dependency updates has advanced rapidly, the exploration of the automated environment interaction that must accompany it remains relatively scarce. In practice, code and its environment are intricately intertwined. Relying solely on static analysis of the environment leads to an inadequate understanding of the target setting, prolongs feedback cycles, and consequently causes significant rework and project delays, thereby reducing overall efficiency. We contend that successful software evolution demands a holistic perspective that integrates both code and environment migration. To understand the current landscape and challenges, we first provide an overview of the status of automated environment construction. We then propose a novel framework paradigm that tightly integrates automated environment setup with the code migration workflow. Finally, we explore the challenges and future directions for automated environment interaction within the code migration domain. Our findings emphasize that without automated environment interaction, the automation of code migration is only half complete.",
    "title_zh": "环境回路：基于大语言模型代理重新思考代码迁移",
    "abstract_zh": "现代软件系统持续进行代码升级，以提升功能、安全性和性能。大型语言模型（LLMs）在代码迁移任务中已展现出卓越的能力。然而，尽管自动化代码迁移（包括重构、API适配和依赖项更新）方面的研究进展迅速，与之相伴的自动化环境交互探索仍相对匮乏。实际上，代码与其运行环境密不可分。仅依赖对环境的静态分析，难以全面理解目标部署场景，导致反馈周期延长，进而引发大量返工和项目延期，显著降低整体效率。我们认为，成功的软件演进需要兼顾代码与环境迁移的全局视角。为厘清当前现状与挑战，我们首先概述了自动化环境构建的发展现状；随后提出一种新型框架范式，将自动化环境配置与代码迁移流程紧密集成；最后探讨了代码迁移领域中自动化环境交互所面临的关键挑战及未来方向。我们的研究发现表明：若缺乏自动化环境交互，代码迁移的自动化便只是完成了一半。"
  },
  {
    "date": "2026-02-10",
    "title": "QEMI: A Quantum Software Stacks Testing Framework via Equivalence Modulo Inputs",
    "authors": "Junjie Luo, Shangzhou Xia, Fuyuan Zhang, Jianjun Zhao",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09942v1",
    "source": "arXiv",
    "abstract": "As quantum algorithms and hardware continue to evolve, ensuring the correctness of the quantum software stack (QSS) has become increasingly important. However, testing QSSes remains challenging due to the oracle problem, i.e., the lack of a reliable ground truth for expected program behavior. Existing metamorphic testing approaches often rely on equivalent circuit transformations, backend modifications, or parameter tuning to address this issue. In this work, inspired by Equivalence Modulo Inputs (EMI), we propose Quantum EMI (QEMI), a new testing approach for QSSes. Our key contributions include: (1) a random quantum program generator that produces code with dead code based on quantum control-flow structures, and (2) an adaptation of the EMI technique from classical compiler testing to generate variants by removing dead code. By comparing the behavior of these variants, we can detect potential bugs in QSS implementations. We applied QEMI to Qiskit, Q#, and Cirq, and successfully identified 11 crash bugs and 1 behavioral inconsistency. QEMI expands the limited set of testing techniques available for quantum software stacks by going beyond structural transformations and incorporating semantics-preserving ones into quantum program analysis.",
    "title_zh": "QEMI：一种基于输入等价性的量子软件栈测试框架",
    "abstract_zh": "随着量子算法和硬件的不断发展，确保量子软件栈（QSS）的正确性变得日益重要。然而，由于“预言机问题”——即缺乏对程序预期行为的可靠基准——测试QSS仍然面临挑战。现有的元测试方法通常依赖于等价电路变换、后端修改或参数调优来应对这一难题。在本研究中，受“输入模等价”（EMI）的启发，我们提出了量子输入模等价（QEMI），一种针对QSS的新测试方法。我们的主要贡献包括：（1）一个随机量子程序生成器，能够基于量子控制流结构生成包含死代码的程序；（2）将经典编译器测试中的EMI技术适配到量子领域，通过移除死代码生成程序变体。通过比较这些变体的行为，可以检测出QSS实现中的潜在缺陷。我们将QEMI应用于Qiskit、Q#和Cirq，成功发现了11个导致崩溃的漏洞以及1个行为不一致问题。QEMI通过超越传统的结构化变换，引入语义保持型变换，拓展了量子软件栈测试技术的有限集合，为量子程序分析提供了新的思路。"
  },
  {
    "date": "2026-02-10",
    "title": "Immersion in the GitHub Universe: Scaling Coding Agents to Mastery",
    "authors": "Jiale Zhao, Guoxin Chen, Fanzhe Meng, Minghao Li, Jie Chen, Hui Xu, Yongshuai Sun, Xin Zhao, Ruihua Song, Yuan Zhang, Peng Wang, Cheng Chen, Jirong Wen, Kai Jia",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09892v1",
    "source": "arXiv",
    "abstract": "Achieving mastery in real world software engineering tasks is fundamentally bottlenecked by the scarcity of large scale, high quality training data. Scaling such data has been limited by the complexity of environment setup, unit test generation, and problem statement curation. In this paper, we propose ScaleSWE, an automated, sandboxed multi agent workflow designed to construct high quality SWE data at scale. The system coordinates three specialized agents for environment setup, test creation, and problem description synthesis to process 6 million pull requests across 5200 repositories, producing Scale SWE Data: 100k verified SWE instances, the largest such dataset to date. It substantially surpasses existing real world datasets in repository diversity and reflects realistic task complexity. We further demonstrate the dataset utility for training by distilling 71498 high quality trajectories and finetuning Qwen30BA3BInstruct to produce ScaleSWE Agent. Our agent achieves a 64 resolve rate on SWE Bench Verified a nearly three fold improvement over the base model. ScaleSWE provides a scalable, reproducible approach for data construction to advance LLM based software engineering. Scale SWE will be publicly available.",
    "title_zh": "沉浸于 GitHub 宇宙：将编码代理扩展至精通",
    "abstract_zh": "在现实世界软件工程任务中实现精通，其根本瓶颈在于大规模、高质量训练数据的稀缺。以往这类数据的扩展受限于环境配置、单元测试生成以及问题描述整理的复杂性。本文提出ScaleSWE，一种自动化、沙箱化的多智能体工作流，旨在大规模构建高质量的软件工程（SWE）数据。该系统协调三个专业智能体，分别负责环境搭建、测试用例生成和问题描述合成，处理了5200个代码仓库中的600万次拉取请求，最终生成了Scale SWE数据集：包含10万条经验证的SWE实例，是目前规模最大的同类数据集。该数据集在代码仓库多样性方面显著超越现有真实世界数据集，并更真实地反映了实际任务的复杂性。我们进一步通过提炼71,498条高质量执行轨迹，并对Qwen30BA3BInstruct模型进行微调，训练出ScaleSWE Agent。该智能体在SWE Bench Verified基准测试中实现了64%的修复率，较基线模型提升近三倍。ScaleSWE为基于大语言模型的软件工程研究提供了一种可扩展、可复现的数据构建方法。Scale SWE数据集将公开发布。"
  },
  {
    "date": "2026-02-10",
    "title": "Steer2Edit: From Activation Steering to Component-Level Editing",
    "authors": "Chung-En Sun, Ge Yan, Zimo Wang, Tsui-Wei Weng",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09870v1",
    "source": "arXiv",
    "abstract": "Steering methods influence Large Language Model behavior by identifying semantic directions in hidden representations, but are typically realized through inference-time activation interventions that apply a fixed, global modification to the model's internal states. While effective, such interventions often induce unfavorable attribute-utility trade-offs under strong control, as they ignore the fact that many behaviors are governed by a small and heterogeneous subset of model components. We propose Steer2Edit, a theoretically grounded, training-free framework that transforms steering vectors from inference-time control signals into diagnostic signals for component-level rank-1 weight editing. Instead of uniformly injecting a steering direction during generation, Steer2Edit selectively redistributes behavioral influence across individual attention heads and MLP neurons, yielding interpretable edits that preserve the standard forward pass and remain compatible with optimized parallel inference. Across safety alignment, hallucination mitigation, and reasoning efficiency, Steer2Edit consistently achieves more favorable attribute-utility trade-offs: at matched downstream performance, it improves safety by up to 17.2%, increases truthfulness by 9.8%, and reduces reasoning length by 12.2% on average. Overall, Steer2Edit provides a principled bridge between representation steering and weight editing by translating steering signals into interpretable, training-free parameter updates.",
    "title_zh": "Steer2Edit：从激活控制到组件级编辑",
    "abstract_zh": "转向方法通过识别隐藏表示中的语义方向来影响大语言模型的行为，但通常通过推理时的激活干预实现，即对模型内部状态施加固定且全局的修改。尽管有效，这类干预在强控制条件下往往导致不利的属性-效用权衡，因为它们忽略了大多数行为实际上由模型中少量且异质的组件所决定这一事实。我们提出 Steer2Edit，一种理论基础扎实、无需训练的框架，能够将推理时的转向向量从控制信号转化为组件级别的秩-1权重编辑诊断信号。与在生成过程中均匀注入转向方向不同，Steer2Edit 选择性地在各个注意力头和MLP神经元之间重新分配行为影响，从而实现可解释的参数修改，同时保持标准前向传播过程，并与优化的并行推理兼容。在安全对齐、幻觉缓解和推理效率方面，Steer2Edit 均表现出更优的属性-效用权衡：在下游性能相当的情况下，其安全性最高提升17.2%，真实性提高9.8%，推理长度平均减少12.2%。总体而言，Steer2Edit 通过将转向信号转化为可解释、无需训练的参数更新，为表示转向与权重编辑之间建立了一条原则性桥梁。"
  },
  {
    "date": "2026-02-10",
    "title": "SWE-Bench Mobile: Can Large Language Model Agents Develop Industry-Level Mobile Applications?",
    "authors": "Muxin Tian, Zhe Wang, Blair Yang, Zhenwei Tang, Kunlun Zhu, Honghua Dong, Hanchen Li, Xinni Xie, Guangjing Wang, Jiaxuan You",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09540v1",
    "source": "arXiv",
    "abstract": "Can large language model agents develop industry-level mobile applications? We introduce \\textbf{SWE-Bench Mobile}, a benchmark for evaluating coding agents on realistic software engineering tasks derived from a production iOS codebase. Unlike existing benchmarks that focus on isolated problems or bug fixes, SWE-Bench Mobile captures the full complexity of industrial development: multi-modal inputs (PRDs and Figma designs), a large-scale mixed Swift/Objective-C codebase, and comprehensive test suites. We evaluate 22 agent-model configurations across four coding agents -- three commercial (Cursor, Codex, Claude Code) and one open-source (OpenCode) -- and find that even the best configurations achieve only 12\\% task success rate. Our analysis reveals that (1) agent design matters as much as model capability -- the same model shows up to 6$\\times$ performance gap across agents, (2) commercial agents consistently outperform open-source alternatives, and (3) simple ``Defensive Programming'' prompts outperform complex ones by 7.4\\%. These findings highlight a significant gap between current agent capabilities and industrial requirements, while providing actionable insights for practitioners and researchers. We release SWE-Bench Mobile as a \\textit{hosted benchmark challenge} to prevent data contamination and ensure fair evaluation. The public leaderboard and development toolkit are available at https://swebenchmobile.com.",
    "title_zh": "SWE-Bench Mobile：大型语言模型智能体能否开发行业级移动应用？",
    "abstract_zh": "大型语言模型代理能否开发行业级别的移动应用？我们提出了 \\textbf{SWE-Bench Mobile}，这是一个针对编码代理在真实软件工程任务上进行评估的基准测试，其任务源自一个实际生产的 iOS 代码库。与现有基准测试主要关注孤立问题或缺陷修复不同，SWE-Bench Mobile 全面捕捉了工业级开发的复杂性：包含多模态输入（如PRD文档和Figma设计稿）、大规模混合的 Swift/Objective-C 代码库，以及完整的测试套件。我们评估了四种编码代理（三种商业产品：Cursor、Codex、Claude Code，以及一个开源项目 OpenCode）共22种代理-模型组合，结果发现即使表现最好的配置，任务成功率也仅有12\\%。我们的分析揭示了以下几点：（1）代理的设计与模型能力同样重要——同一模型在不同代理中性能差异最高可达6倍；（2）商业代理在整体上持续优于开源替代方案；（3）简单的“防御性编程”提示词比复杂的提示词效果高出7.4\\%。这些发现凸显了当前代理能力与工业实际需求之间的显著差距，同时为从业者和研究者提供了切实可行的改进方向。我们以 \\textit{托管基准挑战赛} 的形式发布 SWE-Bench Mobile，以防止数据污染并确保评估的公平性。公开排行榜和开发工具包可访问 https://swebenchmobile.com。"
  },
  {
    "date": "2026-02-10",
    "title": "Knowledge Integration Decay in Search-Augmented Reasoning of Large Language Models",
    "authors": "Sangwon Yu, Ik-hwan Kim, Donghun Kang, Bongkyu Hwang, Junhwa Choi, Suk-hoon Jung, Seungki Hong, Taehee Lee, Sungroh Yoon",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09517v1",
    "source": "arXiv",
    "abstract": "Modern Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks by employing search-augmented reasoning to incorporate external knowledge into long chains of thought. However, we identify a critical yet underexplored bottleneck in this paradigm, termed Knowledge Integration Decay (KID). Specifically, we observe that as the length of reasoning generated before search grows, models increasingly fail to integrate retrieved evidence into subsequent reasoning steps, limiting performance even when relevant information is available. To address this, we propose Self-Anchored Knowledge Encoding (SAKE), a training-free inference-time strategy designed to stabilize knowledge utilization. By anchoring retrieved knowledge at both the beginning and end of the reasoning process, SAKE prevents it from being overshadowed by prior context, thereby preserving its semantic integrity. Extensive experiments on multi-hop QA and complex reasoning benchmarks demonstrate that SAKE significantly mitigates KID and improves performance, offering a lightweight yet effective solution for knowledge integration in agentic LLMs.",
    "title_zh": "大语言模型搜索增强推理中的知识融合衰减",
    "abstract_zh": "现代大型语言模型（LLMs）通过采用搜索增强型推理，将外部知识融入长链思维，已在复杂任务中展现出卓越能力。然而，我们发现该范式中存在一个关键但尚未充分探索的瓶颈，称为知识融合衰减（Knowledge Integration Decay, KID）。具体而言，我们观察到：随着搜索前生成的推理长度增加，模型越来越难以将检索到的证据整合到后续的推理步骤中，即使相关知识已可用，性能仍受到限制。为解决这一问题，我们提出了一种无需训练的推理阶段策略——自锚定知识编码（Self-Anchored Knowledge Encoding, SAKE）。该方法通过在推理过程的开头和结尾同时锚定检索到的知识，防止其被先前的上下文所掩盖，从而有效保持知识的语义完整性。在多跳问答和复杂推理基准上的大量实验表明，SAKE能显著缓解KID问题，提升模型性能，为智能体式LLM中的知识融合提供了一种轻量且高效解决方案。"
  },
  {
    "date": "2026-02-10",
    "title": "Auditing Multi-Agent LLM Reasoning Trees Outperforms Majority Vote and LLM-as-Judge",
    "authors": "Wei Yang, Shixuan Li, Heng Ping, Peiyu Zhang, Paul Bogdan, Jesse Thomason",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09341v1",
    "source": "arXiv",
    "abstract": "Multi-agent systems (MAS) can substantially extend the reasoning capacity of large language models (LLMs), yet most frameworks still aggregate agent outputs with majority voting. This heuristic discards the evidential structure of reasoning traces and is brittle under the confabulation consensus, where agents share correlated biases and converge on the same incorrect rationale. We introduce AgentAuditor, which replaces voting with a path search over a Reasoning Tree that explicitly represents agreements and divergences among agent traces. AgentAuditor resolves conflicts by comparing reasoning branches at critical divergence points, turning global adjudication into efficient, localized verification. We further propose Anti-Consensus Preference Optimization (ACPO), which trains the adjudicator on majority-failure cases and rewards evidence-based minority selections over popular errors. AgentAuditor is agnostic to MAS setting, and we find across 5 popular settings that it yields up to 5% absolute accuracy improvement over a majority vote, and up to 3% over using LLM-as-Judge.",
    "title_zh": "审计多智能体LLM推理树优于多数投票和LLM作为裁判",
    "abstract_zh": "多智能体系统（MAS）能够显著扩展大型语言模型（LLM）的推理能力，然而大多数现有框架仍采用多数投票的方式聚合智能体输出。这种启发式方法忽略了推理轨迹中的证据结构，在“虚构共识”场景下表现脆弱——即多个智能体因共享相关偏差而趋同于相同的错误推理路径。我们提出了AgentAuditor，它用在显式表示各智能体推理轨迹中一致与分歧的“推理树”上的路径搜索，取代了传统的投票机制。AgentAuditor通过在关键分歧点比较不同推理分支来解决冲突，将全局裁决转化为高效、局部化的验证过程。此外，我们还提出了反共识偏好优化（ACPO），该方法在多数派失败的案例上训练裁决模型，并奖励基于证据的少数派选择，而非流行的错误答案。AgentAuditor与具体的MAS设置无关，我们在5种主流设置中均发现，其相比多数投票可提升最高达5%的绝对准确率，相比使用LLM作为裁判的方法可提升最高达3%。"
  },
  {
    "date": "2026-02-10",
    "title": "Trustworthy Agentic AI Requires Deterministic Architectural Boundaries",
    "authors": "Manish Bhattarai, Minh Vu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09947v1",
    "source": "arXiv",
    "abstract": "Current agentic AI architectures are fundamentally incompatible with the security and epistemological requirements of high-stakes scientific workflows. The problem is not inadequate alignment or insufficient guardrails, it is architectural: autoregressive language models process all tokens uniformly, making deterministic command--data separation unattainable through training alone. We argue that deterministic, architectural enforcement, not probabilistic learned behavior, is a necessary condition for trustworthy AI-assisted science. We introduce the Trinity Defense Architecture, which enforces security through three mechanisms: action governance via a finite action calculus with reference-monitor enforcement, information-flow control via mandatory access labels preventing cross-scope leakage, and privilege separation isolating perception from execution. We show that without unforgeable provenance and deterministic mediation, the ``Lethal Trifecta'' (untrusted inputs, privileged data access, external action capability) turns authorization security into an exploit-discovery problem: training-based defenses may reduce empirical attack rates but cannot provide deterministic guarantees. The ML community must recognize that alignment is insufficient for authorization security, and that architectural mediation is required before agentic AI can be safely deployed in consequential scientific domains.",
    "title_zh": "可信的代理型人工智能需要确定性的架构边界",
    "abstract_zh": "当前的代理型人工智能架构在根本上与高风险科学工作流所要求的安全性和认识论标准不相容。问题并非对齐不足或防护机制不够，而在于架构本身：自回归语言模型对所有标记（token）一视同仁地处理，仅靠训练无法实现确定性的命令-数据分离。我们认为，可信的人工智能辅助科学必须依赖确定性的架构强制机制，而非概率性的学习行为。为此，我们提出“三位一体防御架构”（Trinity Defense Architecture），通过三种机制实现安全防护：通过有限动作演算与参考监视器强制执行的动作治理、通过强制访问标签实现的信息流控制以防止跨作用域泄露，以及通过权限分离将感知与执行隔离。我们证明，若缺乏不可伪造的溯源机制和确定性的中介控制，“致命三重奏”（不可信输入、特权数据访问、外部行动能力）将授权安全问题转化为漏洞发现难题：基于训练的防御措施或许能降低实际攻击频率，却无法提供确定性保障。机器学习领域必须认识到，对齐不足以保障授权安全，只有在引入架构级中介机制之后，代理型人工智能才可能在具有重大影响的科学领域中安全部署。"
  },
  {
    "date": "2026-02-10",
    "title": "LLMs Encode Their Failures: Predicting Success from Pre-Generation Activations",
    "authors": "William Lugoloobi, Thomas Foster, William Bankes, Chris Russell",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09924v1",
    "source": "arXiv",
    "abstract": "Running LLMs with extended reasoning on every problem is expensive, but determining which inputs actually require additional compute remains challenging. We investigate whether their own likelihood of success is recoverable from their internal representations before generation, and if this signal can guide more efficient inference. We train linear probes on pre-generation activations to predict policy-specific success on math and coding tasks, substantially outperforming surface features such as question length and TF-IDF. Using E2H-AMC, which provides both human and model performance on identical problems, we show that models encode a model-specific notion of difficulty that is distinct from human difficulty, and that this distinction increases with extended reasoning. Leveraging these probes, we demonstrate that routing queries across a pool of models can exceed the best-performing model whilst reducing inference cost by up to 70\\% on MATH, showing that internal representations enable practical efficiency gains even when they diverge from human intuitions about difficulty. Our code is available at: https://github.com/KabakaWilliam/llms_know_difficulty",
    "title_zh": "大语言模型编码其失败：从生成前激活预测成功",
    "abstract_zh": "在每个问题上使用扩展推理运行大型语言模型（LLM）成本高昂，但确定哪些输入确实需要额外计算仍具挑战性。我们研究了在生成之前，是否可以从模型的内部表征中恢复其成功可能性，并探讨这一信号能否用于引导更高效的推理过程。我们通过在生成前的激活值上训练线性探测器，来预测在数学和编程任务上的策略特定成功率，结果显著优于诸如问题长度和TF-IDF等表面特征。利用E2H-AMC数据集（该数据集提供了相同问题上人类与模型的表现），我们发现模型编码了一种与人类难度感知不同的、模型特有的难度概念，且这种差异在采用扩展推理时愈发明显。基于这些探测器，我们证明了通过在多个模型间路由查询，可以在MATH数据集上将推理成本降低高达70%的同时，超越表现最佳的单一模型。这表明，即使模型的内部表征与人类对难度的直觉相悖，也能实现实际的效率提升。我们的代码已公开：https://github.com/KabakaWilliam/llms_know_difficulty"
  },
  {
    "date": "2026-02-10",
    "title": "Design and Evaluation of an Assisted Programming Interface for Behavior Trees in Robotics",
    "authors": "Jonathan Styrud, Matteo Iovino, Rebecca Stower, Mart Kartašev, Mikael Norrlöf, Mårten Björkman, Christian Smith",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09772v1",
    "source": "arXiv",
    "abstract": "The possibility to create reactive robot programs faster without the need for extensively trained programmers is becoming increasingly important. So far, it has not been explored how various techniques for creating Behavior Tree (BT) program representations could be combined with complete graphical user interfaces (GUIs) to allow a human user to validate and edit trees suggested by automated methods. In this paper, we introduce BEhavior TRee GUI (BETR-GUI) for creating BTs with the help of an AI assistant that combines methods using large language models, planning, genetic programming, and Bayesian optimization with a drag-and-drop editor. A user study with 60 participants shows that by combining different assistive methods, BETR-GUI enables users to perform better at solving the robot programming tasks. The results also show that humans using the full variant of BETR-GUI perform better than the AI assistant running on its own.",
    "title_zh": "面向机器人行为树的辅助编程界面的设计与评估",
    "abstract_zh": "快速创建反应式机器人程序而无需依赖大量受过专业训练的程序员，其重要性正日益凸显。迄今为止，尚未有研究探讨如何将多种用于创建行为树（BT）程序表示的技术与完整的图形用户界面（GUI）相结合，以使人类用户能够验证和编辑由自动化方法生成的行为树。本文提出了BEhavior TRee GUI（BETR-GUI），一种借助人工智能助手创建行为树的工具。该助手融合了大型语言模型、规划、遗传编程以及贝叶斯优化等多种方法，并配备拖拽式编辑器。一项包含60名参与者的用户研究显示，通过结合多种辅助方法，BETR-GUI显著提升了用户完成机器人编程任务的表现。研究结果还表明，使用完整版BETR-GUI的人类用户，其表现优于仅依赖AI助手独立运行的情况。"
  },
  {
    "date": "2026-2-10",
    "title": "DaVinci: Performance-Driven Analog Routing via Multi-modality Guidance Prediction",
    "authors": "Peng Xu, Rong Sun, Su Zheng, Song Chen, Qi Xu, Bei Yu",
    "publish": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
    "url": "https://doi.org/10.1109/tcad.2026.3663518",
    "source": "IEEE",
    "abstract": "Analog routing automation remains challenging because variability in parasitics and mismatch are highly design- and topology-dependent, limiting the transferability of heuristics across different circuits. Existing methods typically imitate manual routing heuristics or use only one type of routing feature, missing valuable information from both the layout and netlist. In this work, we introduce DaVinci, a unified framework that employs multi‑modal guidance prediction with neural networks to generate routing directives explicitly guided by performance objectives. By combining detailed layout cost maps with netlist connectivity using a heterogeneous message‑passing architecture and multi‑scale feature extraction, DaVinci incorporates performance gradients directly into the routing process. This integration enables the model to capture both local and global circuit interactions, aligning routing decisions with post-layout performance targets. Experimental results demonstrate that our method significantly outperforms conventional approaches, delivering enhanced routing quality and design efficiency.",
    "title_zh": "达芬奇：基于多模态引导预测的性能驱动模拟布线",
    "abstract_zh": "模拟布线自动化仍然具有挑战性，因为寄生参数的变异性和不匹配程度高度依赖于设计和拓扑结构，这限制了启发式方法在不同电路之间的可迁移性。现有方法通常模仿人工布线的启发式规则，或仅使用单一类型的布线特征，未能充分挖掘来自版图和网表的宝贵信息。本文提出 DaVinci，一个统一的框架，采用基于神经网络的多模态引导预测，生成明确由性能目标指导的布线指令。通过结合详细的版图代价图与网表连接关系，利用异构消息传递架构和多尺度特征提取，DaVinci 将性能梯度直接融入布线过程。这种集成使模型能够捕捉电路的局部与全局交互关系，使布线决策与后版图性能目标保持一致。实验结果表明，我们的方法显著优于传统方法，在提升布线质量与设计效率方面表现突出。"
  },
  {
    "date": "2026-2-10",
    "title": "A Large-Scale Empirical Evaluation of LLMs for Automated Self-Admitted Technical Debt Repayment",
    "authors": "Mohammad Sadegh Sheikhaei, Yuan Tian, Shaowei Wang, Bowen Xu",
    "publish": "ACM Transactions on Software Engineering and Methodology",
    "url": "https://doi.org/10.1145/3796704",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "大规模实证评估大语言模型在自动自报技术债修复中的应用",
    "abstract_zh": "None"
  },
  {
    "date": "2026-2-10",
    "title": "[Front Cover]",
    "authors": "N/A",
    "publish": "2025 26th International Conference on Electronic Packaging Technology (ICEPT)",
    "url": "https://doi.org/10.1109/icept67137.2025.11387740",
    "source": "IEEE",
    "abstract": null,
    "title_zh": "[封面]",
    "abstract_zh": "None"
  },
  {
    "date": "2026-2-10",
    "title": "Promptware Engineering: Software Engineering for Prompt-Enabled Systems",
    "authors": "Zhenpeng Chen, Chong Wang, Weisong Sun, Xuanzhe Liu, Jie M. Zhang, Yang Liu",
    "publish": "ACM Transactions on Software Engineering and Methodology",
    "url": "https://doi.org/10.1145/3796535",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "提示工程：面向提示驱动系统的软件工程",
    "abstract_zh": "None"
  },
  {
    "date": "2026-2-10",
    "title": "Prompt Engineering for Advancing Software Engineering: Using Generative Language Models in the Development of Domain-Specific Languages",
    "authors": "Ahmad F. Subahi",
    "publish": "IEEE Access",
    "url": "https://doi.org/10.1109/access.2026.3663331",
    "source": "IEEE",
    "abstract": "In this research, a comprehensive framework employing large language models (LLMs) and prompt engineering techniques is developed with the aim of optimizing the creation of domain-specific languages (DSLs). Domain experts utilize DSLs to create precise, high-level specifications that can be customized to their application domains. However, challenges regarding usability, semantic correctness, and syntax design are often encountered in the development process. With recent advances in LLaMA, Claude, DeepSeek, Gemini, and ChatGPT, it is now possible to automate and refine several DSL development stages through carefully written prompts. The proposed framework provides structured prompt patterns that are customized to each successive stage of the DSL life cycle: domain knowledge acquisition, grammar definition, syntax construction, and validation. A case study involving a hospital monitoring system is used to explore the effectiveness of the models and the prompting strategies. This study also classifies the prompting techniques and assesses five LLMs in terms of readability, expressiveness, accuracy, syntax clarity, and completeness. The findings suggest that prompt engineering significantly contributes to the development of DSLs, with the five LLMs excelling in different evaluation measurements. For example, DeepSeek and LLaMA display superior syntactic precision, while Claude and ChatGPT perform better in terms of expressiveness and readability. The need to align the choice of LLM with stakeholder expectations, whether they be technical or domain specific, is highlighted by expert assessments. This research contributes to the body of knowledge relating to LLM-assisted DSL development and provides a basis for further study in adaptive prompting and multimodal integration.",
    "title_zh": "用于推进软件工程的提示工程：在领域特定语言开发中使用生成式语言模型",
    "abstract_zh": "本研究提出了一种综合框架，利用大语言模型（LLMs）和提示工程（prompt engineering）技术，旨在优化领域特定语言（DSL）的创建过程。领域专家使用DSL来编写精确且高层次的规范，这些规范可根据其应用领域进行定制。然而，在DSL开发过程中，常常面临可用性、语义正确性以及语法设计等方面的挑战。随着LLaMA、Claude、DeepSeek、Gemini和ChatGPT等模型的最新进展，通过精心设计的提示，现已能够自动化并优化DSL开发的多个阶段。所提出的框架为DSL生命周期的各个阶段——领域知识获取、语法定义、语法构建和验证——提供了结构化的提示模式。通过一个医院监控系统的案例研究，验证了这些模型及提示策略的有效性。此外，本研究对提示技术进行了分类，并从可读性、表达能力、准确性、语法清晰度和完整性五个维度评估了五种LLM的表现。研究结果表明，提示工程在DSL开发中起到了显著作用，五种LLM在不同评估指标上各具优势：例如，DeepSeek和LLaMA在语法精确性方面表现突出，而Claude和ChatGPT在表达能力和可读性方面更为优异。专家评估进一步强调了根据利益相关者（无论是技术背景还是领域专长）的期望来选择合适的LLM的重要性。本研究为LLM辅助DSL开发领域积累了知识，并为后续关于自适应提示和多模态融合的研究奠定了基础。"
  },
  {
    "date": "2026-2-10",
    "title": "CID4IoT: IoT-Oriented Command Injection Vulnerability Detection Based on Critical Code Extraction and LLM-Analysis_supp1-3662325.pdf",
    "authors": "Zhenji Zhou",
    "publish": "N/A",
    "url": "https://doi.org/10.1109/jiot.2026.3662325/mm1",
    "source": "IEEE",
    "abstract": "The Internet of Things (IoT) devices have brought invaluable convenience to our daily lives. However, they also introduce significant security challenges. Common vulnerabilities in numerous IoT devices predominantly reside in their web services. Unfortunately, existing vulnerability detection methods either incur heavy execution overhead or produce excessive false positives/negatives. This significantly hinders the efficient analysis of vulnerabilities in web services. This paper proposes CID4IoT, a novel static automated vulnerability detection approach designed to effectively detect command injection vulnerabilities in web services provided by IoT devices. Inspired by the concept of taint analysis, CID4IoT first analyzes vulnerability reports to identify taint source functions and sink functions. Then, CID4IoT extracts pseudocode between taint source functions and sink functions to form critical code snippets. Subsequently, it utilizes LLM to analyze these critical code snippets for vulnerability detection.We implemented a prototype of CID4IoT and evaluated it on real firmware devices. CID4IoT discovered 54 previously unknown vulnerabilities, of which 39 are confirmed by CVE. Compared with state-of-the-art tools KARONTE and SaTC, it identified significantly more vulnerabilities in the test set while achieving notable improvements in analysis efficiency. The results demonstrate that CID4IoT is effective in detecting flaws in IoT devices",
    "title_zh": "CID4IoT：基于关键代码提取与大语言模型分析的面向物联网的命令注入漏洞检测（补充材料1-3662325.pdf）",
    "abstract_zh": "物联网（IoT）设备为我们的日常生活带来了无与伦比的便利，但同时也引入了重大的安全挑战。众多物联网设备中的常见漏洞主要集中在其Web服务上。然而，现有的漏洞检测方法要么执行开销过大，要么产生过多的误报或漏报，这严重阻碍了对Web服务漏洞的高效分析。本文提出了一种名为CID4IoT的新颖静态自动化漏洞检测方法，旨在有效检测物联网设备Web服务中的命令注入漏洞。受污点分析思想的启发，CID4IoT首先分析漏洞报告，识别出污点源函数和污点汇函数；随后，提取污点源函数与污点汇函数之间的伪代码，形成关键代码片段；最后，利用大语言模型（LLM）对这些关键代码片段进行分析，以实现漏洞检测。我们实现了一个CID4IoT的原型系统，并在真实固件设备上进行了评估。实验结果表明，CID4IoT共发现了54个此前未知的漏洞，其中39个已获得CVE认证。与最先进的工具KARONTE和SaTC相比，CID4IoT在测试集上识别出显著更多的漏洞，同时在分析效率方面也取得了显著提升。结果表明，CID4IoT在检测物联网设备缺陷方面具有良好的有效性。"
  },
  {
    "date": "2026-2-10",
    "title": "Introduction to the Special Issue on Advances in Physical Design Automation",
    "authors": "Stephan Held, Gracieli Posser, Iris Hui-Ru Jiang, David Chinnery",
    "publish": "ACM Transactions on Design Automation of Electronic Systems",
    "url": "https://doi.org/10.1145/3770739",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "物理设计自动化进展专题介绍",
    "abstract_zh": "None"
  },
  {
    "date": "2026-2-10",
    "title": "SASRO: Hardware-Software Co-Design of Real-time Self-adaptive Deep Stereo Towards Robotic Applications",
    "authors": "Yuanfan Xu, Xinting Yang, Yunfei Xiang, Xiaolong Shan, Dawei Zhao, Wenbo Ding, Jincheng Yu, Yu Wang",
    "publish": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
    "url": "https://doi.org/10.1109/tcad.2026.3663250",
    "source": "IEEE",
    "abstract": "With the development of AI, robotics is gradually becoming one of the most popular research topics. Current robotic applications, such as Unmanned Aerial Vehicles (UAVs), robotic manipulators, and humanoid robots, all require stereo vision to perceive the accurate depth map of the environment in real time for subsequent decision-making. In the field of autonomous driving, learning-based stereo depth estimation methods achieve significantly higher accuracy compared to traditional methods. However, for the diverse environments in robots, self-adaptive deep stereo is considered as a promising approach to improve the poor generalization of learning-based methods. Although existing neural network accelerators are available for real-time stereo depth estimation, they all present limitations when it comes to self-adaptive scenarios. Some accelerators focus solely on optimizing the computational flow of forward or backward propagation, failing to support the computation of the self-supervised loss function and exploit the specific workload characteristics commonly found in self-adaptive algorithms. On the other hand, accelerator designers typically rely on domain knowledge from hardware rather than robotic applications to optimize compute-system metrics such as accuracy or latency, leading to suboptimal outcomes for application-level performance. To overcome these limitations, we propose a hardware-software co-design approach of real-time Self-Adaptive deep Stereo towards RObotic applications named SASRO. It includes a heterogeneous computing architecture consisting of an off-the-shelf inference accelerator and our designed Self-Adaptive Processing Unit (SAPU). Based on this architecture, a quantization-friendly network architecture is proposed. We believe that this design can fully leverage its efficiency in self-adaptive deep stereo, achieving a 20% improvement in accuracy and a 1.95× reduction in latency. Different from traditional design methods towards a single workload, SASRO additionally contains an automatic design framework which optimizes the end-to-end robotic metrics instead of isolated compute-system metrics. Taking robot navigation as an example, SASRO achieves a 1.3∼2.2× increase in navigation speed compared to the low-latency and high-accuracy designs.",
    "title_zh": "SASRO：面向机器人应用的实时自适应深度立体视觉的软硬件协同设计",
    "abstract_zh": "随着人工智能的发展，机器人技术正逐渐成为最受关注的研究领域之一。当前的机器人应用，如无人机（UAV）、机械臂以及人形机器人，均需要通过立体视觉实时获取环境的精确深度图，以支持后续的决策过程。在自动驾驶领域，基于学习的立体深度估计方法相比传统方法显著提升了精度。然而，面对机器人所处的多样化环境，自适应深度立体方法被认为是一种极具前景的解决方案，可有效改善基于学习方法泛化能力差的问题。尽管现有的神经网络加速器已可用于实时立体深度估计，但在自适应场景下仍存在诸多局限。部分加速器仅专注于优化前向或反向传播的计算流程，无法支持自监督损失函数的计算，也未能充分利用自适应算法中常见的特定工作负载特征。另一方面，加速器设计者通常依赖硬件领域的专业知识来优化计算系统指标（如精度或延迟），而非从机器人应用的实际需求出发，导致最终在应用层面的性能表现不尽如人意。\n\n为克服上述挑战，我们提出了一种面向机器人应用的实时自适应深度立体硬件-软件协同设计方法，命名为SASRO（Self-Adaptive Stereo for Robotic Applications）。SASRO采用异构计算架构，由现成的推理加速器与我们自主设计的自适应处理单元（SAPU）构成。基于该架构，我们进一步提出了一种适合量化部署的网络结构。我们认为，该设计能够充分发挥在自适应深度立体任务中的效率优势，实现精度提升20%，延迟降低1.95倍。与传统针对单一工作负载的设计方法不同，SASRO还集成了一套自动设计框架，其优化目标是端到端的机器人应用性能指标，而非孤立的计算系统指标。以机器人导航为例，SASRO相较于低延迟与高精度的现有设计，导航速度提升了1.3至2.2倍。"
  },
  {
    "date": "2026-2-10",
    "title": "Can Large Language Models Improve SE Active Learning via Warm-Starts?",
    "authors": "Lohith Senthilkumar, Tim Menzies",
    "publish": "ACM Transactions on Software Engineering and Methodology",
    "url": "https://doi.org/10.1145/3796511",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "大型语言模型能否通过预热启动提升软件工程中的主动学习？",
    "abstract_zh": "None"
  },
  {
    "date": "2026-2-10",
    "title": "Formal Reasoning Meets LLMs: Toward AI for Mathematics and Verification",
    "authors": "Kaiyu Yang, Gabriel Poesia, Jingxuan He, Wenda Li, Kristin Lauter, Swarat Chaudhuri, Dawn Song",
    "publish": "Communications of the ACM",
    "url": "https://doi.org/10.1145/3750038",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "形式推理与大语言模型的融合：迈向数学与验证领域的AI",
    "abstract_zh": "None"
  },
  {
    "date": "2026-2-10",
    "title": "A Systematic Literature Review of Parameter-Efficient Fine-Tuning for Large Code Models",
    "authors": "Saima Afrin, MD Zahidul Haque, Antonio Mastropaolo",
    "publish": "ACM Transactions on Software Engineering and Methodology",
    "url": "https://doi.org/10.1145/3796522",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "大型代码模型参数高效微调的系统性文献综述",
    "abstract_zh": "None"
  },
  {
    "date": "2026-2-10",
    "title": "GeoSynth: Constraint-Aware Automation for Scalable Superconducting Quantum Chip Design",
    "authors": "Xiaohan Yu, Bo Zhao, Yimin Gao, Zhengyu Chen, Kang An, Zheng Shan",
    "publish": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
    "url": "https://doi.org/10.1109/tcad.2026.3663259",
    "source": "IEEE",
    "abstract": "As superconducting quantum processors scale beyond thousands of qubits, manual layout methods pose the primary bottleneck to industrial deployment. Traditional quantum electronic design automation (EDA) tools emphasize component-level modeling but lack full-chip integration for scalable systems. This work formulates scalable superconducting quantum chip design as a geometric constraint satisfaction problem (CSP) and introduces GeoSynth, a constraint-aware automation framework to accelerate scalable quantum chip design. By leveraging global symmetry and local similarity of quantum chip, GeoSynth enables hierarchical constraint decomposition through four novel algorithms: adaptive launch pad allocation (GeoSynth-ALA), systematic component correspondence (GeoSynth-SCC), scalable template-based routing synthesis (GeoSynth-TRS), and global constraint propagation (GeoSynth-GCP). Implemented in EDA-Q, GeoSynth delivers an end-to-end automation flow for a 156-qubit flip-chip processor in 2,882 seconds. This represents a 2× speedup over a prior method that addressed only the routing subproblem for 64 qubits, and is crowned by the first fabrication validation of an automatically generated large-scale layout, achieving a T1 coherence time of 69.24 μs. Beyond fabrication validation, GeoSynth demonstrates robust scalability by successfully generating layouts for diverse configurations spanning 64 to 620 qubits across multiple architectural variants, confirming its capability to handle industrially relevant scales. GeoSynth’s efficient, constraint-driven automation offers a scalable solution for quantum hardware industrialization.",
    "title_zh": "GeoSynth：面向可扩展超导量子芯片设计的约束感知自动化方法",
    "abstract_zh": "随着超导量子处理器的规模突破数千个量子比特，传统的人工布局方法已成为工业部署的主要瓶颈。传统的量子电子设计自动化（EDA）工具侧重于器件级建模，但缺乏面向可扩展系统的全芯片集成能力。本文将可扩展的超导量子芯片设计建模为一个几何约束满足问题（CSP），并提出GeoSynth——一种具有约束感知能力的自动化框架，以加速大规模量子芯片的设计。通过利用量子芯片的全局对称性与局部相似性，GeoSynth引入四种新颖算法，实现分层约束分解：自适应发射平台分配（GeoSynth-ALA）、系统性组件对应（GeoSynth-SCC）、可扩展的模板化布线综合（GeoSynth-TRS）以及全局约束传播（GeoSynth-GCP）。该框架在EDA-Q平台中实现，可在2,882秒内完成一个156量子比特翻转芯片处理器的端到端自动化设计。相比此前仅解决64量子比特布线子问题的方法，效率提升2倍；同时，这也是首个经实际制造验证的全自动生成大规模布局，实现了69.24微秒的T1相干时间。除制造验证外，GeoSynth还展现出强大的可扩展性，成功生成了涵盖64至620量子比特、多种架构变体的多样化布局，证实其具备处理工业级规模设计的能力。GeoSynth所提出的高效、约束驱动的自动化方案，为量子硬件的工业化提供了可扩展的解决方案。"
  }
]