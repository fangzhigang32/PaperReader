[
  {
    "date": "2026-01-15",
    "title": "OctoBench: Benchmarking Scaffold-Aware Instruction Following in Repository-Grounded Agentic Coding",
    "authors": "Deming Ding, Shichun Liu, Enhui Yang, Jiahang Lin, Ziying Chen, Shihan Dou, Honglin Guo, Weiyu Cheng, Pengyu Zhao, Chengjun Xiao, Qunhong Zeng, Qi Zhang, Xuanjing Huang, Qidi Xu, Tao Gui",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10343v1",
    "source": "arXiv",
    "abstract": "Modern coding scaffolds turn LLMs into capable software agents, but their ability to follow scaffold-specified instructions remains under-examined, especially when constraints are heterogeneous and persist across interactions. To fill this gap, we introduce OctoBench, which benchmarks scaffold-aware instruction following in repository-grounded agentic coding. OctoBench includes 34 environments and 217 tasks instantiated under three scaffold types, and is paired with 7,098 objective checklist items. To disentangle solving the task from following the rules, we provide an automated observation-and-scoring toolkit that captures full trajectories and performs fine-grained checks. Experiments on eight representative models reveal a systematic gap between task-solving and scaffold-aware compliance, underscoring the need for training and evaluation that explicitly targets heterogeneous instruction following. We release the benchmark to support reproducible benchmarking and to accelerate the development of more scaffold-aware coding agents.",
    "title_zh": "OctoBench：面向代码库的代理式编程中支架感知指令遵循的基准测试",
    "abstract_zh": "现代代码 scaffold 将大语言模型（LLM）转变为具备能力的软件代理，但其遵循 scaffold 所指定指令的能力仍缺乏深入研究，尤其是在约束条件异质且跨交互持续存在的情况下。为填补这一空白，我们提出了 OctoBench，这是一个用于评估基于代码仓库的智能体在代码编写中对 scaffold 的感知与指令遵循能力的基准测试。OctoBench 包含 34 个环境和 217 项任务，涵盖三种 scaffold 类型，并配有 7,098 项客观检查清单。为了将任务求解与规则遵循相分离，我们提供了一个自动化观测与评分工具包，能够记录完整的行为轨迹并执行细粒度检查。对八种代表性模型的实验表明，任务求解能力与 scaffold 意识性合规之间存在系统性差距，凸显了需要专门针对异质化指令遵循进行训练与评估。我们已公开发布该基准，以支持可复现的基准测试，并推动更具备 scaffold 意识的代码代理的开发。"
  },
  {
    "date": "2026-01-15",
    "title": "Evolving with AI: A Longitudinal Analysis of Developer Logs",
    "authors": "Agnia Sergeyuk, Eric Huang, Dariia Karaeva, Anastasiia Serova, Yaroslav Golubev, Iftekhar Ahmed",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10258v1",
    "source": "arXiv",
    "abstract": "AI-powered coding assistants are rapidly becoming fixtures in professional IDEs, yet their sustained influence on everyday development remains poorly understood. Prior research has focused on short-term use or self-reported perceptions, leaving open questions about how sustained AI use reshapes actual daily coding practices in the long term. We address this gap with a mixed-method study of AI adoption in IDEs, combining longitudinal two-year fine-grained telemetry from 800 developers with a survey of 62 professionals. We analyze five dimensions of workflow change: productivity, code quality, code editing, code reuse, and context switching. Telemetry reveals that AI users produce substantially more code but also delete significantly more. Meanwhile, survey respondents report productivity gains and perceive minimal changes in other dimensions. Our results offer empirical insights into the silent restructuring of software workflows and provide implications for designing future AI-augmented tooling.",
    "title_zh": "与人工智能共同演进：开发者日志的纵向分析",
    "abstract_zh": "基于人工智能的编程助手正迅速成为专业集成开发环境（IDE）中的标配，但它们对日常开发工作的持续影响仍缺乏深入理解。以往研究多集中于短期使用情况或自我报告的感知，未能揭示长期持续使用AI工具如何重塑实际的日常编码实践。为此，我们开展了一项混合方法研究，探讨AI在IDE中的采纳情况，结合来自800名开发者为期两年的细粒度使用数据（遥测数据）与62名专业人士的问卷调查。我们从五个维度分析工作流程的变化：生产效率、代码质量、代码编辑、代码复用以及上下文切换。遥测数据显示，使用AI的开发者虽然产出的代码量显著增加，但同时删除的代码也明显增多；而问卷受访者则普遍报告了生产效率的提升，并认为其他方面变化不大。本研究为软件开发工作流的隐性重构提供了实证洞察，同时也为未来AI增强型开发工具的设计提供了重要启示。"
  },
  {
    "date": "2026-01-15",
    "title": "Agentic Pipelines in Embedded Software Engineering: Emerging Practices and Challenges",
    "authors": "Simin Sun, Miroslaw Staron",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10220v1",
    "source": "arXiv",
    "abstract": "A new transformation is underway in software engineering, driven by the rapid adoption of generative AI in development workflows. Similar to how version control systems once automated manual coordination, AI tools are now beginning to automate many aspects of programming. For embedded software engineering organizations, however, this marks their first experience integrating AI into safety-critical and resource-constrained environments. The strict demands for determinism, reliability, and traceability pose unique challenges for adopting generative technologies. In this paper, we present findings from a qualitative study with ten senior experts from four companies who are evaluating generative AI-augmented development for embedded software. Through semi-structured focus group interviews and structured brainstorming sessions, we identified eleven emerging practices and fourteen challenges related to the orchestration, responsible governance, and sustainable adoption of generative AI tools. Our results show how embedded software engineering teams are rethinking workflows, roles, and toolchains to enable a sustainable transition toward agentic pipelines and generative AI-augmented development.",
    "title_zh": "嵌入式软件工程中的代理化流程：新兴实践与挑战",
    "abstract_zh": "软件工程领域正经历一场新的变革，其驱动力是生成式人工智能在开发工作流中的快速应用。正如版本控制系统曾自动化了以往的手动协作流程一样，如今人工智能工具也开始自动化编程的诸多方面。然而，对于嵌入式软件工程组织而言，这却是首次将人工智能引入安全关键且资源受限的环境。对确定性、可靠性和可追溯性的严格要求，为生成式技术的采用带来了独特的挑战。本文基于对来自四家公司的十位资深专家进行的定性研究，呈现了相关发现。通过半结构化焦点小组访谈和结构化头脑风暴会议，我们识别出十一项新兴实践以及十四项与生成式人工智能工具的编排、负责任治理及可持续采纳相关的挑战。研究结果表明，嵌入式软件工程团队正在重新思考工作流程、角色分工和工具链，以实现向智能代理流水线和生成式人工智能增强型开发的可持续转型。"
  },
  {
    "date": "2026-01-15",
    "title": "Be Your Own Red Teamer: Safety Alignment via Self-Play and Reflective Experience Replay",
    "authors": "Hao Wang, Yanting Wang, Hao Li, Rui Li, Lei Sha",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10589v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) have achieved remarkable capabilities but remain vulnerable to adversarial ``jailbreak'' attacks designed to bypass safety guardrails. Current safety alignment methods depend heavily on static external red teaming, utilizing fixed defense prompts or pre-collected adversarial datasets. This leads to a rigid defense that overfits known patterns and fails to generalize to novel, sophisticated threats. To address this critical limitation, we propose empowering the model to be its own red teamer, capable of achieving autonomous and evolving adversarial attacks. Specifically, we introduce Safety Self- Play (SSP), a system that utilizes a single LLM to act concurrently as both the Attacker (generating jailbreaks) and the Defender (refusing harmful requests) within a unified Reinforcement Learning (RL) loop, dynamically evolving attack strategies to uncover vulnerabilities while simultaneously strengthening defense mechanisms. To ensure the Defender effectively addresses critical safety issues during the self-play, we introduce an advanced Reflective Experience Replay Mechanism, which uses an experience pool accumulated throughout the process. The mechanism employs a Upper Confidence Bound (UCB) sampling strategy to focus on failure cases with low rewards, helping the model learn from past hard mistakes while balancing exploration and exploitation. Extensive experiments demonstrate that our SSP approach autonomously evolves robust defense capabilities, significantly outperforming baselines trained on static adversarial datasets and establishing a new benchmark for proactive safety alignment.",
    "title_zh": "做自己的红队队员：通过自对弈与反思性经验回放实现安全对齐",
    "abstract_zh": "大型语言模型（LLMs）已展现出卓越的能力，但仍容易受到旨在绕过安全防护机制的对抗性“越狱”攻击。当前的安全对齐方法严重依赖静态的外部红队测试，使用固定的防御提示或预先收集的对抗数据集。这种防御方式僵化，容易过度拟合已知攻击模式，难以应对新型且复杂的威胁。为解决这一关键局限，我们提出让模型成为其自身的红队成员，具备自主演化对抗攻击的能力。具体而言，我们引入了安全自对弈（Safety Self-Play, SSP）系统，该系统利用单一语言模型在统一的强化学习（RL）循环中同时扮演攻击者（生成越狱指令）和防御者（拒绝有害请求）双重角色，动态演化攻击策略以发现漏洞，同时持续强化防御机制。为确保防御者在自对弈过程中有效应对关键安全问题，我们设计了一种先进的反思式经验回放机制，该机制利用整个过程积累的经验池，并采用上置信界（UCB）采样策略，聚焦于奖励较低的失败案例，使模型能够从过往的困难错误中学习，同时平衡探索与利用。大量实验表明，我们的SSP方法能自主演化出强大的防御能力，显著优于基于静态对抗数据集训练的基线模型，为前瞻性安全对齐树立了新的基准。"
  },
  {
    "date": "2026-01-15",
    "title": "A Generalizable Framework for Building Executable Domain-Specific LLMs under Data Scarcity: Demonstration on Semiconductor TCAD Simulation",
    "authors": "Di Wang, Zhenhua Wu, Yu Liu, Kai Chang, Shaohua Wu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10128v1",
    "source": "arXiv",
    "abstract": "Scientific and engineering verticals often suffer from data scarcity and strict executability requirements: models must generate not only fluent text, but also syntactically valid, tool-compilable scripts. We present a schema-first alignment framework for building compact, executable domain-specific LLMs in low-resource settings. The framework integrates three core components: (i) large-scale synthetic QA data generation from expert documentation to instill foundational domain knowledge; (ii) a code-centric IR->DPO workflow that converts verified tool decks into interpretable intermediate representations (IR), performs equivalence-preserving diversification, and constructs preference pairs to directly optimize instruction compliance and code executability; and (iii) a controlled evaluation of Retrieval-Augmented Generation (RAG), showing that while RAG benefits general LLMs, it can marginally degrade the performance of already domain-aligned models. We demonstrate the framework by instantiating TcadGPT for semiconductor Technology Computer-Aided Design (TCAD). Using 1.5M synthetic QA pairs and an IR-driven DPO dataset, TcadGPT attains 85.6% semantic accuracy and an 80.0% syntax pass rate on SDE executability tests, substantially outperforming state-of-the-art general LLMs such as GPT-4o. To probe portability beyond TCAD, we apply the same recipe to the open-source FEM solver Elmer, observing consistent improvements in script-level success rates over general-purpose baselines. All datasets, benchmarks, and code (including P1, P2, and IR->DPO) are released for reproducibility. Together, these results suggest that the proposed framework provides a robust and reproducible path toward executable LLMs in specialized, data-scarce professional domains.",
    "title_zh": "一种在数据稀缺条件下构建可执行领域特定大模型的通用框架：以半导体TCAD仿真为例的演示",
    "abstract_zh": "科学与工程领域常常面临数据稀缺和严格可执行性要求的挑战：模型不仅需要生成流畅的文本，还必须生成语法正确、可被工具编译执行的代码。我们提出了一种以模式为导向的对齐框架，用于在低资源环境下构建紧凑且可执行的领域专用大语言模型（LLM）。该框架整合了三个核心组件：(i) 从专家文档中大规模生成合成问答数据，以注入基础领域的知识；(ii) 一种以代码为中心的“检索→DPO”工作流，将经过验证的工具脚本集转换为可解释的中间表示（IR），进行保持语义等价性的多样化处理，并构建偏好对，直接优化指令遵循能力与代码可执行性；(iii) 对检索增强生成（RAG）的受控评估，结果表明虽然RAG对通用大模型有益，但可能轻微降低已具备领域对齐能力模型的性能。我们通过构建半导体技术计算机辅助设计（TCAD）领域的TcadGPT实例验证了该框架的有效性。利用150万条合成问答对及基于IR驱动的DPO数据集，TcadGPT在SDE可执行性测试中达到了85.6%的语义准确率和80.0%的语法通过率，显著优于当前最先进的通用大模型如GPT-4o。为进一步检验其可迁移性，我们将相同方法应用于开源有限元分析求解器Elmer，同样观察到在脚本级成功率上相对于通用基线模型的一致提升。所有数据集、基准测试和代码（包括P1、P2以及IR→DPO流程）均已公开发布，以确保研究的可复现性。综上所述，这些结果表明，所提出的框架为在专业性高、数据稀缺的领域中实现可执行的大语言模型提供了一条稳健且可复现的技术路径。"
  },
  {
    "date": "2026-01-15",
    "title": "Continuous-Depth Transformers with Learned Control Dynamics",
    "authors": "Peter Jemley",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10007v1",
    "source": "arXiv",
    "abstract": "We present a hybrid transformer architecture that replaces discrete middle layers with a continuous-depth Neural Ordinary Differential Equation (ODE) block, enabling inference-time control over generation attributes via a learned steering signal. Unlike standard transformers that process representations through fixed discrete layers, our approach treats depth as a continuous variable governed by a learned vector field $F_θ(H, τ, u)$, where $u$ is a low-dimensional control signal injected via explicit concatenation. We validate the architecture through four experiments: (1) gradient flow stability with zero exploding/vanishing gradient events, (2) semantic steering achieving 98\\%/88\\% accuracy for positive/negative sentiment control, (3) continuous interpolation validated by a negligible 0.068\\% trajectory divergence between fixed and adaptive solvers, and (4) efficiency benchmarking demonstrating latency parity with standard discrete baselines. Additionally, we show that adaptive ODE solvers reveal geometric structure in the learned dynamics: the control signal partitions the vector field into distinct dynamical regimes with different curvature characteristics. The adjoint method enables $O(1)$ memory training regardless of integration depth. Our results demonstrate that continuous-depth dynamics with learned control signals provide a viable, efficient mechanism for steerable language generation.",
    "title_zh": "具有学习控制动力学的连续深度Transformer",
    "abstract_zh": "我们提出了一种混合Transformer架构，该架构用连续深度的神经常微分方程（ODE）模块替代了传统的离散中间层，从而在推理阶段通过学习得到的控制信号实现对生成属性的灵活调控。与标准Transformer通过固定离散层处理表示不同，我们的方法将深度视为一个由学习得到的向量场 $F_θ(H, τ, u)$ 所支配的连续变量，其中控制信号 $u$ 通过显式拼接方式注入模型。我们通过四项实验验证了该架构的有效性：（1）梯度流稳定性，未出现任何梯度爆炸或消失事件；（2）语义控制能力，在正/负情感控制任务中分别达到98%和88%的准确率；（3）连续插值验证显示，固定求解器与自适应求解器之间的轨迹偏差极小，仅为0.068%；（4）效率基准测试表明，其延迟与标准离散基线相当。此外，我们发现自适应ODE求解器揭示了所学动态中的几何结构：控制信号将向量场划分为具有不同曲率特征的独立动力学区域。伴随方法实现了与积分深度无关的 $O(1)$ 内存训练。结果表明，结合学习控制信号的连续深度动态为可调控的语言生成提供了一种可行且高效的机制。"
  },
  {
    "date": "2026-01-15",
    "title": "Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems",
    "authors": "Amir Khurshid, Abhishek Sehgal",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10681v1",
    "source": "arXiv",
    "abstract": "Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.",
    "title_zh": "面向企业检索增强系统的结构与多样性感知的上下文气泡构建",
    "abstract_zh": "大型语言模型（LLM）的上下文通常通过检索增强生成（RAG）构建，该方法涉及对候选段落进行排序并选取前k个最相关的片段。然而，这种做法会导致文档结构中信息图的碎片化、过度检索、内容重复，以及查询上下文不足，尤其在第二、第三层级的语义层面覆盖不全。本文提出一种结构感知且多样性受限的“上下文气泡”构建框架，能够在严格的token预算约束下，整合连贯且可引用的文本片段集合。该方法通过组织多粒度的文本片段（如章节、表格行等），并利用任务相关的结构先验来引导检索过程，从而保留并充分利用文档的内在结构。从高相关性的锚点片段出发，通过受控选择构建上下文气泡，平衡查询相关性、边际覆盖率与冗余惩罚。该方法显式地对多样性和预算进行约束，生成紧凑而信息丰富的上下文集合，区别于传统的top-k检索方式。此外，系统会输出完整的检索过程记录，追踪每条记录的打分与选择决策，从而实现可审计性与确定性调优。在企业级文档上的实验表明，上下文气泡方法显著减少了冗余上下文，更有效地覆盖了次要语义层面，同时在有限的上下文窗口内提升了答案质量与引用忠实度。消融实验进一步证明，结构先验和多样性约束的选择机制均不可或缺；移除任一组件都会导致覆盖范围下降，并引发冗余或不完整上下文的问题。"
  },
  {
    "date": "2026-01-15",
    "title": "Development of Ontological Knowledge Bases by Leveraging Large Language Models",
    "authors": "Le Ngoc Luyen, Marie-Hélène Abel, Philippe Gouspillou",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10436v1",
    "source": "arXiv",
    "abstract": "Ontological Knowledge Bases (OKBs) play a vital role in structuring domain-specific knowledge and serve as a foundation for effective knowledge management systems. However, their traditional manual development poses significant challenges related to scalability, consistency, and adaptability. Recent advancements in Generative AI, particularly Large Language Models (LLMs), offer promising solutions for automating and enhancing OKB development. This paper introduces a structured, iterative methodology leveraging LLMs to optimize knowledge acquisition, automate ontology artifact generation, and enable continuous refinement cycles. We demonstrate this approach through a detailed case study focused on developing a user context profile ontology within the vehicle sales domain. Key contributions include significantly accelerated ontology construction processes, improved ontological consistency, effective bias mitigation, and enhanced transparency in the ontology engineering process. Our findings highlight the transformative potential of integrating LLMs into ontology development, notably improving scalability, integration capabilities, and overall efficiency in knowledge management systems.",
    "title_zh": "利用大型语言模型开发本体知识库",
    "abstract_zh": "本体知识库（OKBs）在结构化特定领域知识方面发挥着至关重要的作用，是高效知识管理系统的基础。然而，其传统的手工构建方式在可扩展性、一致性以及适应性方面面临显著挑战。近年来，生成式人工智能（Generative AI），特别是大型语言模型（LLMs）的进展，为自动化和提升OKB开发提供了极具前景的解决方案。本文提出了一种结构化、迭代的方法，利用LLMs优化知识获取过程，实现本体构件的自动化生成，并支持持续的迭代完善。我们通过一个详细的案例研究，展示了在汽车销售领域构建用户情境画像本体的实践过程。主要贡献包括：显著加速了本体构建流程，提升了本体的一致性，有效缓解了偏见问题，并增强了本体工程过程的透明度。研究结果凸显了将LLMs融入本体开发中的变革潜力，尤其在提升可扩展性、系统集成能力以及整体知识管理效率方面表现突出。"
  },
  {
    "date": "2026-01-15",
    "title": "Repository Intelligence Graph: Deterministic Architectural Map for LLM Code Assistants",
    "authors": "Tsvi Cherny-Shahar, Amiram Yehudai",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10112v1",
    "source": "arXiv",
    "abstract": "Repository aware coding agents often struggle to recover build and test structure, especially in multilingual projects where cross language dependencies are encoded across heterogeneous build systems and tooling. We introduce the Repository Intelligence Graph (RIG), a deterministic, evidence backed architectural map that represents buildable components, aggregators, runners, tests, external packages, and package managers, connected by explicit dependency and coverage edges that trace back to concrete build and test definitions. We also present SPADE, a deterministic extractor that constructs RIG from build and test artifacts (currently with an automatic CMake plugin based on the CMake File API and CTest metadata), and exposes RIG as an LLM friendly JSON view that agents can treat as the authoritative description of repository structure. We evaluate three commercial agents (Claude Code, Cursor, Codex) on eight repositories spanning low to high build oriented complexity, including the real world MetaFFI project. Each agent answers thirty structured questions per repository with and without RIG in context, and we measure accuracy, wall clock completion time, and efficiency (seconds per correct answer). Across repositories and agents, providing RIG improves mean accuracy by 12.2\\% and reduces completion time by 53.9\\%, yielding a mean 57.8\\% reduction in seconds per correct answer. Gains are larger in multilingual repositories, which improve by 17.7\\% in accuracy and 69.5\\% in efficiency on average, compared to 6.6\\% and 46.1\\% in single language repositories. Qualitative analysis suggests that RIG shifts failures from structural misunderstandings toward reasoning mistakes over a correct structure, while rare regressions highlight that graph based reasoning quality remains a key factor.",
    "title_zh": "仓库智能图：用于大语言模型代码助手的确定性架构地图",
    "abstract_zh": "仓库感知型编码代理在恢复构建与测试结构方面常常面临挑战，尤其是在多语言项目中，跨语言依赖关系往往分散在异构的构建系统和工具链之间。为此，我们提出了**仓库智能图（Repository Intelligence Graph, RIG）**，这是一种确定性、基于证据的架构地图，能够表示可构建组件、聚合器、运行器、测试、外部包以及包管理器，并通过显式的依赖关系和覆盖关系边将它们连接起来，这些边均源自具体的构建与测试定义。\n\n我们还推出了 **SPADE**——一种确定性的提取工具，能够从构建与测试产物中生成 RIG（目前支持基于 CMake File API 和 CTest 元数据的自动 CMake 插件），并将 RIG 以 LLM 友好的 JSON 格式暴露出来，使编码代理可以将其视为仓库结构的权威描述。\n\n我们在涵盖低到高构建复杂度的八个仓库上评估了三种商业级编码代理（Claude Code、Cursor、Codex），其中包括真实世界中的 MetaFFI 项目。每个代理在每项仓库中分别在有无 RIG 上下文的情况下回答 30 个结构化问题，并测量准确率、实际耗时以及效率（每正确答案所用秒数）。在所有仓库和代理中，提供 RIG 后，平均准确率提升 12.2%，完成时间减少 53.9%，平均每正确答案耗时降低 57.8%。\n\n在多语言仓库中，性能提升更为显著：准确率平均提高 17.7%，效率提升 69.5%；而在单语言仓库中，对应指标分别为 6.6% 和 46.1%。定性分析表明，RIG 将原本因结构误解导致的失败，更多地转变为对正确结构下的推理错误；而少数回归情况也提示，基于图的推理质量仍是影响整体表现的关键因素。"
  },
  {
    "date": "2026-01-15",
    "title": "State of AI: An Empirical 100 Trillion Token Study with OpenRouter",
    "authors": "Malika Aubakirova, Alex Atallah, Chris Clark, Justin Summerville, Anjney Midha",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10088v1",
    "source": "arXiv",
    "abstract": "The past year has marked a turning point in the evolution and real-world use of large language models (LLMs). With the release of the first widely adopted reasoning model, o1, on December 5th, 2024, the field shifted from single-pass pattern generation to multi-step deliberation inference, accelerating deployment, experimentation, and new classes of applications. As this shift unfolded at a rapid pace, our empirical understanding of how these models have actually been used in practice has lagged behind. In this work, we leverage the OpenRouter platform, which is an AI inference provider across a wide variety of LLMs, to analyze over 100 trillion tokens of real-world LLM interactions across tasks, geographies, and time. In our empirical study, we observe substantial adoption of open-weight models, the outsized popularity of creative roleplay (beyond just the productivity tasks many assume dominate) and coding assistance categories, plus the rise of agentic inference. Furthermore, our retention analysis identifies foundational cohorts: early users whose engagement persists far longer than later cohorts. We term this phenomenon the Cinderella \"Glass Slipper\" effect. These findings underscore that the way developers and end-users engage with LLMs \"in the wild\" is complex and multifaceted. We discuss implications for model builders, AI developers, and infrastructure providers, and outline how a data-driven understanding of usage can inform better design and deployment of LLM systems.",
    "title_zh": "人工智能现状：基于OpenRouter的100万亿Token实证研究",
    "abstract_zh": "过去一年标志着大型语言模型（LLMs）在演进及其现实应用中的一个转折点。2024年12月5日，首个被广泛采用的推理模型o1发布，推动该领域从单步模式生成转向多步骤深思熟虑的推理方式，从而加速了部署、实验以及新型应用场景的发展。然而，随着这一转变以极快的速度推进，我们对这些模型在实际中如何被使用的实证理解却明显滞后。在本研究中，我们利用OpenRouter平台——一个覆盖多种大型语言模型的AI推理服务提供商——分析了超过100万亿个真实世界中LLM交互的token数据，涵盖任务类型、地理区域和时间维度。我们的实证研究发现，开源权重模型得到了广泛采纳，创意角色扮演（远超人们普遍认为主导场景的生产力任务）和代码辅助类别表现出异常高的受欢迎程度，同时代理式推理（agentic inference）正在兴起。此外，通过留存率分析，我们识别出一些基础用户群体：早期使用者的参与度远高于后续加入者。我们将这一现象称为“灰姑娘”式的“玻璃鞋效应”。这些发现表明，开发者与终端用户在真实环境中使用LLM的方式极为复杂且多面。本文还探讨了对模型构建者、AI开发者及基础设施提供商的启示，并阐述了如何通过数据驱动的使用洞察来优化LLM系统的设计与部署。"
  },
  {
    "date": "2026-01-15",
    "title": "In-Context Source and Channel Coding",
    "authors": "Ziqiong Wang, Tianqi Ren, Rongpeng Li, Zhifeng Zhao, Honggang Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10267v1",
    "source": "arXiv",
    "abstract": "Separate Source-Channel Coding (SSCC) remains attractive for text transmission due to its modularity and compatibility with mature entropy coders and powerful channel codes. However, SSCC often suffers from a pronounced cliff effect in low Signal-to-Noise Ratio (SNR) regimes, where residual bit errors after channel decoding can catastrophically break lossless source decoding, especially for Arithmetic Coding (AC) driven by Large Language Models (LLMs). This paper proposes a receiver-side In-Context Decoding (ICD) framework that enhances SSCC robustness without modifying the transmitter. ICD leverages an Error Correction Code Transformer (ECCT) to obtain bit-wise reliability for the decoded information bits. Based on the context-consistent bitstream, ICD constructs a confidence-ranked candidate pool via reliability-guided bit flipping, samples a compact yet diverse subset of candidates, and applies an LLM-based arithmetic decoder to obtain both reconstructions and sequence-level log-likelihoods. A reliability-likelihood fusion rule then selects the final output. We further provide theoretical guarantees on the stability and convergence of the proposed sampling procedure. Extensive experiments over Additive White Gaussian Noise (AWGN) and Rayleigh fading channels demonstrate consistent gains compared with conventional SSCC baselines and representative Joint Source-Channel Coding (JSCC) schemes.",
    "title_zh": "上下文相关的源与信道编码",
    "abstract_zh": "分离源信道编码（SSCC）因其模块化设计以及与成熟的熵编码器和强大信道码的良好兼容性，仍被广泛应用于文本传输。然而，SSCC在低信噪比（SNR）环境下常表现出显著的“悬崖效应”，即信道解码后残留的比特错误会灾难性地破坏无损源解码过程，尤其对于由大语言模型（LLMs）驱动的算术编码（AC）而言更为严重。本文提出一种接收端的上下文内解码（In-Context Decoding, ICD）框架，可在不修改发送端的前提下提升SSCC的鲁棒性。ICD利用纠错码Transformer（Error Correction Code Transformer, ECCT）获取解码信息比特的逐位可靠性。基于上下文一致的比特流，ICD通过可靠性引导的比特翻转构建置信度排序的候选集，从中采样出一个紧凑且多样化的子集，并采用基于LLM的算术解码器获得重建结果及序列级对数似然值。随后，通过可靠性-似然融合规则选择最终输出。此外，本文还为所提出的采样过程提供了稳定性与收敛性的理论保证。在加性高斯白噪声（AWGN）和瑞利衰落信道上的大量实验表明，该方法相较于传统的SSCC基线以及典型的联合源信道编码（JSCC）方案均实现了持续的性能提升。"
  },
  {
    "date": "2026-01-15",
    "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
    "authors": "Changle Qu, Sunhao Dai, Hengyi Cai, Jun Xu, Shuaiqiang Wang, Dawei Yin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10712v1",
    "source": "arXiv",
    "abstract": "Tool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by interleaving reasoning steps with external tool interactions. However, existing reinforcement learning methods typically rely on outcome- or trajectory-level rewards, assigning uniform advantages to all steps within a trajectory. This coarse-grained credit assignment fails to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios. To address this, we propose MatchTIR, a framework that introduces fine-grained supervision via bipartite matching-based turn-level reward assignment and dual-level advantage estimation. Specifically, we formulate credit assignment as a bipartite matching problem between predicted and ground-truth traces, utilizing two assignment strategies to derive dense turn-level rewards. Furthermore, to balance local step precision with global task success, we introduce a dual-level advantage estimation scheme that integrates turn-level and trajectory-level signals, assigning distinct advantage values to individual interaction turns. Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model surpasses the majority of 8B competitors, particularly in long-horizon and multi-turn tasks. Our codes are available at https://github.com/quchangle1/MatchTIR.",
    "title_zh": "MatchTIR：通过二分匹配实现工具集成推理的细粒度监督",
    "abstract_zh": "工具集成推理（Tool-Integrated Reasoning, TIR）通过在推理步骤中穿插外部工具调用，赋予大型语言模型（LLMs）处理复杂任务的能力。然而，现有的强化学习方法通常依赖于结果级或轨迹级奖励，对轨迹内所有步骤分配相同的奖励优势。这种粗粒度的信用分配机制难以区分有效的工具调用与冗余或错误的调用，尤其在长时程、多轮交互场景中表现不佳。为解决这一问题，我们提出 MatchTIR 框架，通过基于二分图匹配的逐轮奖励分配和双层优势估计，引入细粒度监督。具体而言，我们将信用分配建模为预测轨迹与真实轨迹之间的二分图匹配问题，采用两种不同的分配策略以生成密集的逐轮奖励。此外，为了平衡局部步骤精度与全局任务成功率，我们设计了一种双层优势估计机制，融合逐轮与轨迹级信号，为每个交互轮次分配差异化的优势值。在三个基准数据集上的大量实验表明，MatchTIR 具有显著优势。值得注意的是，我们的 4B 模型在长时程和多轮任务中超越了多数 8B 级别的竞争模型。代码已开源，地址为：https://github.com/quchangle1/MatchTIR。"
  },
  {
    "date": "2026-01-15",
    "title": "Grounding Agent Memory in Contextual Intent",
    "authors": "Ruozhen Yang, Yucheng Jiang, Yueqi Jiang, Priyanka Kargupta, Yunyi Zhang, Jiawei Han",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10702v1",
    "source": "arXiv",
    "abstract": "Deploying large language models in long-horizon, goal-oriented interactions remains challenging because similar entities and facts recur under different latent goals and constraints, causing memory systems to retrieve context-mismatched evidence. We propose STITCH (Structured Intent Tracking in Contextual History), an agentic memory system that indexes each trajectory step with a structured retrieval cue, contextual intent, and retrieves history by matching the current step's intent. Contextual intent provides compact signals that disambiguate repeated mentions and reduce interference: (1) the current latent goal defining a thematic segment, (2) the action type, and (3) the salient entity types anchoring which attributes matter. During inference, STITCH filters and prioritizes memory snippets by intent compatibility, suppressing semantically similar but context-incompatible history. For evaluation, we introduce CAME-Bench, a benchmark for context-aware retrieval in realistic, dynamic, goal-oriented trajectories. Across CAME-Bench and LongMemEval, STITCH achieves state-of-the-art performance, outperforming the strongest baseline by 35.6%, with the largest gains as trajectory length increases. Our analysis shows that intent indexing substantially reduces retrieval noise, supporting intent-aware memory for robust long-horizon reasoning.",
    "title_zh": "在上下文意图中对齐代理记忆",
    "abstract_zh": "在长时程、目标导向的交互中部署大型语言模型仍然面临挑战，因为相似的实体和事实会在不同的潜在目标与约束条件下反复出现，导致记忆系统检索到与上下文不匹配的证据。我们提出了STITCH（上下文历史中的结构化意图追踪），一种代理型记忆系统，该系统通过结构化的检索线索、上下文意图对每个轨迹步骤进行索引，并根据当前步骤的意图来检索历史信息。上下文意图提供了紧凑的信号，能够消除重复提及的歧义并减少干扰：（1）定义主题段落的当前潜在目标，（2）动作类型，以及（3）锚定关键属性的显著实体类型。在推理阶段，STITCH通过意图兼容性对记忆片段进行过滤和优先排序，抑制语义相似但上下文不兼容的历史信息。为评估该方法，我们引入了CAME-Bench，这是一个用于真实、动态、目标导向轨迹中上下文感知检索的基准测试。在CAME-Bench和LongMemEval两个数据集上，STITCH均达到了最先进的性能，相比最强基线提升了35.6%，且随着轨迹长度增加，提升幅度更为显著。我们的分析表明，意图索引显著降低了检索噪声，验证了意图感知记忆在实现稳健长时程推理方面的有效性。"
  },
  {
    "date": "2026-01-15",
    "title": "DR-Arena: an Automated Evaluation Framework for Deep Research Agents",
    "authors": "Yiwen Gao, Ruochen Zhao, Yang Deng, Wenxuan Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10504v1",
    "source": "arXiv",
    "abstract": "As Large Language Models (LLMs) increasingly operate as Deep Research (DR) Agents capable of autonomous investigation and information synthesis, reliable evaluation of their task performance has become a critical bottleneck. Current benchmarks predominantly rely on static datasets, which suffer from several limitations: limited task generality, temporal misalignment, and data contamination. To address these, we introduce DR-Arena, a fully automated evaluation framework that pushes DR agents to their capability limits through dynamic investigation. DR-Arena constructs real-time Information Trees from fresh web trends to ensure the evaluation rubric is synchronized with the live world state, and employs an automated Examiner to generate structured tasks testing two orthogonal capabilities: Deep reasoning and Wide coverage. DR-Arena further adopts Adaptive Evolvement Loop, a state-machine controller that dynamically escalates task complexity based on real-time performance, demanding deeper deduction or wider aggregation until a decisive capability boundary emerges. Experiments with six advanced DR agents demonstrate that DR-Arena achieves a Spearman correlation of 0.94 with the LMSYS Search Arena leaderboard. This represents the state-of-the-art alignment with human preferences without any manual efforts, validating DR-Arena as a reliable alternative for costly human adjudication.",
    "title_zh": "DR-Arena：一种用于深度研究代理的自动化评估框架",
    "abstract_zh": "随着大型语言模型（LLMs）越来越多地作为深度研究（DR）代理，具备自主探究与信息整合能力，其任务表现的可靠评估已成为一个关键瓶颈。当前的评测基准主要依赖静态数据集，存在诸多局限：任务泛化能力有限、时间上存在错位，以及数据污染问题。为解决这些问题，我们提出了 DR-Arena——一个完全自动化的评估框架，通过动态调查将 DR 代理的能力推向极限。DR-Arena 基于实时网络趋势构建信息树，确保评估标准与现实世界状态同步；同时采用自动化“评审员”生成结构化任务，用以测试两种正交能力：深度推理与广泛覆盖。此外，DR-Arena 引入了自适应演进循环（Adaptive Evolvement Loop），这是一种基于状态机的控制器，能够根据实时性能动态提升任务复杂度，持续要求更深层次的推断或更广范围的信息聚合，直至明确的能力边界显现。在六种先进 DR 代理上的实验表明，DR-Arena 与 LMSYS Search Arena 排行榜之间达到了 0.94 的斯皮尔曼等级相关系数。这一结果在无需任何人工干预的情况下实现了与人类偏好高度一致的评估效果，验证了 DR-Arena 作为昂贵人工评判的可靠替代方案的可行性。"
  },
  {
    "date": "2026-01-15",
    "title": "Model See, Model Do? Exposure-Aware Evaluation of Bug-vs-Fix Preference in Code LLMs",
    "authors": "Ali Al-Kaswan, Claudio Spiess, Prem Devanbu, Arie van Deursen, Maliheh Izadi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10496v1",
    "source": "arXiv",
    "abstract": "Large language models are increasingly used for code generation and debugging, but their outputs can still contain bugs, that originate from training data. Distinguishing whether an LLM prefers correct code, or a familiar incorrect version might be influenced by what it's been exposed to during training. We introduce an exposure-aware evaluation framework that quantifies how prior exposure to buggy versus fixed code influences a model's preference. Using the ManySStuBs4J benchmark, we apply Data Portraits for membership testing on the Stack-V2 corpus to estimate whether each buggy and fixed variant was seen during training. We then stratify examples by exposure and compare model preference using code completion as well as multiple likelihood-based scoring metrics We find that most examples (67%) have neither variant in the training data, and when only one is present, fixes are more frequently present than bugs. In model generations, models reproduce buggy lines far more often than fixes, with bug-exposed examples amplifying this tendency and fix-exposed examples showing only marginal improvement. In likelihood scoring, minimum and maximum token-probability metrics consistently prefer the fixed code across all conditions, indicating a stable bias toward correct fixes. In contrast, metrics like the Gini coefficient reverse preference when only the buggy variant was seen. Our results indicate that exposure can skew bug-fix evaluations and highlight the risk that LLMs may propagate memorised errors in practice.",
    "title_zh": "模型看，模型做？代码大模型中针对缺陷与修复偏好性的暴露感知评估",
    "abstract_zh": "大型语言模型在代码生成与调试中的应用日益广泛，但其输出仍可能包含源自训练数据的错误。区分模型是倾向于正确代码，还是更偏好于熟悉但错误的版本，可能受到其训练过程中所接触内容的影响。为此，我们提出了一种暴露感知的评估框架，用于量化模型在训练中对含错代码与修复后代码的先前接触程度如何影响其偏好。基于ManySStuBs4J基准测试，我们利用数据画像（Data Portraits）对Stack-V2语料库进行成员身份检测，以估算每个含错版本和修复版本是否曾在训练中出现过。随后，我们根据暴露情况对样本进行分层，并通过代码补全任务以及多种基于似然性的评分指标来比较模型偏好。\n\n研究发现，大多数示例（67%）在训练数据中均未包含任何版本；当仅有一个版本出现时，修复版本比错误版本更常被看到。在模型生成结果中，模型复现错误代码行的频率远高于修复代码，且在曾暴露于错误代码的样本中，这种倾向进一步加剧；而暴露于修复代码的样本仅表现出微弱的改进。在似然性评分方面，最小与最大词元概率等指标在所有条件下均一致偏好修复代码，表明模型存在一种稳定的向正确修复倾斜的偏见。相比之下，如Gini系数等指标在仅见过错误版本的情况下会反转偏好。\n\n我们的结果表明，训练数据的暴露情况可能扭曲对代码修复效果的评估，并凸显出大模型在实际应用中可能传播记忆中的错误的风险。"
  },
  {
    "date": "2026-01-15",
    "title": "NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models",
    "authors": "Ziming Dai, Dabiao Ma, Jinle Tong, Mengyuan Han, Jian Yang, Haojun Fei",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10457v1",
    "source": "arXiv",
    "abstract": "Although the Gradient Boosted Decision Trees (GBDTs) dominate industrial tabular applications, upgrading legacy models in high-concurrency production environments still faces prohibitive retraining costs and systemic risks. To address this problem, we present NSR-Boost, a neuro-symbolic residual boosting framework designed specifically for industrial scenarios. Its core advantage lies in being \"non-intrusive\". It treats the legacy model as a frozen model and performs targeted repairs on \"hard regions\" where predictions fail. The framework comprises three key stages: first, finding hard regions through residuals, then generating interpretable experts by generating symbolic code structures using Large Language Model (LLM) and fine-tuning parameters using Bayesian optimization, and finally dynamically integrating experts with legacy model output through a lightweight aggregator. We report on the successful deployment of NSR-Boost within the core financial risk control system at Qfin Holdings. This framework not only significantly outperforms state-of-the-art (SOTA) baselines across six public datasets and one private dataset, more importantly, shows excellent performance gains on real-world online data. In conclusion, it effectively captures long-tail risks missed by traditional models and offers a safe, low-cost evolutionary paradigm for industry.",
    "title_zh": "NSR-Boost：一种用于工业遗留模型的神经符号残差提升框架",
    "abstract_zh": "尽管梯度提升决策树（GBDTs）在工业领域的表格数据应用中占据主导地位，但在高并发生产环境中对遗留模型进行升级，仍面临高昂的重新训练成本和系统性风险。为解决这一问题，我们提出了NSR-Boost——一种专为工业场景设计的神经符号残差提升框架。其核心优势在于“非侵入性”：该框架将原有遗留模型视为冻结模型，在预测失败的“困难区域”进行针对性修复。整个框架包含三个关键阶段：首先通过残差识别困难区域；其次利用大语言模型（LLM）生成符号化代码结构以构建可解释的专家模型，并通过贝叶斯优化微调参数；最后通过轻量级聚合器动态融合专家模型与遗留模型的输出。我们报告了NSR-Boost在Qfin Holdings核心金融风控系统中的成功部署。实验结果表明，该框架不仅在六个公开数据集和一个私有数据集上显著优于当前最先进的基线方法，更重要的是，在真实线上数据上展现出卓越的性能提升。综上所述，NSR-Boost能够有效捕捉传统模型遗漏的长尾风险，为工业界提供了一种安全、低成本的模型演进范式。"
  },
  {
    "date": "2026-01-15",
    "title": "LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models",
    "authors": "Tiesunlong Shen, Rui Mao, Jin Wang, Heming Sun, Jian Zhang, Xuejie Zhang, Erik Cambria",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10416v1",
    "source": "arXiv",
    "abstract": "Aligning Large Language Models (LLMs) with human preferences is critical, yet traditional fine-tuning methods are computationally expensive and inflexible. While test-time alignment offers a promising alternative, existing approaches often rely on distorted trajectory-level signals or inefficient sampling, fundamentally capping performance and failing to preserve the generative diversity of the base model. This paper introduces LLMdoctor, a novel framework for efficient test-time alignment that operates via a patient-doctor paradigm. It integrates token-level reward acquisition with token-level flow-guided preference optimization (TFPO) to steer a large, frozen patient LLM with a smaller, specialized doctor model. Unlike conventional methods that rely on trajectory-level rewards, LLMdoctor first extracts fine-grained, token-level preference signals from the patient model's behavioral variations. These signals then guide the training of the doctor model via TFPO, which establishes flow consistency across all subtrajectories, enabling precise token-by-token alignment while inherently preserving generation diversity. Extensive experiments demonstrate that LLMdoctor significantly outperforms existing test-time alignment methods and even surpasses the performance of full fine-tuning approaches like DPO.",
    "title_zh": "LLMdoctor：基于令牌级流程引导的偏好优化，实现大语言模型高效测试时对齐",
    "abstract_zh": "将大型语言模型（LLMs）与人类偏好对齐至关重要，但传统的微调方法计算成本高昂且灵活性不足。尽管测试时对齐提供了一种有前景的替代方案，但现有方法通常依赖于扭曲的轨迹级信号或低效的采样策略，从根本上限制了性能，并未能保持基础模型的生成多样性。本文提出 LLMdoctor，一种新型的高效测试时对齐框架，采用“患者-医生”范式。该框架结合了逐标记级别的奖励获取与逐标记级别的流引导偏好优化（TFPO），通过一个较小且专门化的“医生”模型来引导一个大型冻结的“患者”语言模型。与依赖轨迹级奖励的传统方法不同，LLMdoctor首先从患者模型的行为差异中提取细粒度的逐标记偏好信号。这些信号随后通过TFPO指导“医生”模型的训练，建立所有子轨迹间的流一致性，从而实现精确到每个标记的对齐，同时天然地保留了生成多样性。大量实验表明，LLMdoctor 显著优于现有的测试时对齐方法，甚至在性能上超越了如 DPO 等完整的微调方法。"
  },
  {
    "date": "2026-01-15",
    "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
    "authors": "Xinyu Zhu, Yuzhu Cai, Zexi Liu, Bingyang Zheng, Cheng Wang, Rui Ye, Jiaao Chen, Hanrui Wang, Wei-Chen Wang, Yuzhi Zhang, Linfeng Zhang, Weinan E, Di Jin, Siheng Chen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10402v1",
    "source": "arXiv",
    "abstract": "The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.",
    "title_zh": "面向超长时域智能科学：机器学习工程中的认知积累",
    "abstract_zh": "当前，人工智能向代理型科学迈进的过程中，正受到超长时程自主性这一挑战的制约——即在跨越数天甚至数周的实验周期中，持续保持战略连贯性与迭代修正能力。尽管大型语言模型（LLMs）在短时程推理方面表现出色，但在现实科研所面临的高维、延迟反馈环境中，它们极易被执行细节所淹没，难以将稀疏的反馈信息整合为连贯的长期指导。在此，我们提出 ML-Master 2.0，一个能够掌握超长时程机器学习工程（MLE）的自主智能体，而 MLE 是科学发现的一个典型微观范例。通过将上下文管理重新定义为认知积累的过程，我们的方法引入了分层认知缓存（HCC），这是一种受计算机系统启发的多层级架构，能够实现经验随时间的结构化分化。通过动态地将瞬时执行轨迹提炼为稳定的知识与跨任务智慧，HCC 使智能体得以将即时执行与长期实验策略解耦，从而有效突破静态上下文窗口的扩展瓶颈。在 OpenAI 的 MLE-Bench 基准测试中，ML-Master 2.0 在 24 小时预算下实现了 56.44% 的领先奖牌率。研究结果表明，超长时程自主性为具备超越人类先例复杂性的自主探索能力的人工智能提供了一个可扩展的蓝图。"
  },
  {
    "date": "2026-01-15",
    "title": "Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale",
    "authors": "Yi Liu, Weizhe Wang, Ruitao Feng, Yao Zhang, Guangquan Xu, Gelei Deng, Yuekang Li, Leo Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10338v1",
    "source": "arXiv",
    "abstract": "The rise of AI agent frameworks has introduced agent skills, modular packages containing instructions and executable code that dynamically extend agent capabilities. While this architecture enables powerful customization, skills execute with implicit trust and minimal vetting, creating a significant yet uncharacterized attack surface. We conduct the first large-scale empirical security analysis of this emerging ecosystem, collecting 42,447 skills from two major marketplaces and systematically analyzing 31,132 using SkillScan, a multi-stage detection framework integrating static analysis with LLM-based semantic classification. Our findings reveal pervasive security risks: 26.1% of skills contain at least one vulnerability, spanning 14 distinct patterns across four categories: prompt injection, data exfiltration, privilege escalation, and supply chain risks. Data exfiltration (13.3%) and privilege escalation (11.8%) are most prevalent, while 5.2% of skills exhibit high-severity patterns strongly suggesting malicious intent. We find that skills bundling executable scripts are 2.12x more likely to contain vulnerabilities than instruction-only skills (OR=2.12, p<0.001). Our contributions include: (1) a grounded vulnerability taxonomy derived from 8,126 vulnerable skills, (2) a validated detection methodology achieving 86.7% precision and 82.5% recall, and (3) an open dataset and detection toolkit to support future research. These results demonstrate an urgent need for capability-based permission systems and mandatory security vetting before this attack vector is further exploited.",
    "title_zh": "野外环境中的代理技能：一项大规模安全漏洞的实证研究",
    "abstract_zh": "人工智能代理框架的兴起引入了“代理技能”这一概念，即包含指令与可执行代码的模块化包，能够动态扩展代理的功能。尽管这种架构支持强大的定制能力，但这些技能在执行时往往基于隐式信任且缺乏充分审查，从而形成了一个显著 yet 未被充分描述的安全攻击面。我们首次对这一新兴生态系统进行了大规模实证安全分析，从两个主要市场收集了42,447个技能，并利用SkillScan——一种结合静态分析与基于大语言模型（LLM）语义分类的多阶段检测框架——系统性地分析了其中31,132个技能。研究发现，普遍存在严重的安全风险：26.1%的技能至少包含一种漏洞，涵盖四大类共14种不同的漏洞模式，包括提示注入、数据外泄、权限提升以及供应链风险。其中，数据外泄（13.3%）和权限提升（11.8%）最为普遍；另有5.2%的技能表现出高危漏洞模式，强烈暗示其具有恶意意图。研究还发现，包含可执行脚本的技能比仅含指令的技能更容易存在漏洞，前者出现漏洞的可能性高出2.12倍（OR=2.12，p<0.001）。本研究的主要贡献包括：(1) 基于8,126个存在漏洞的技能构建了一个扎实的漏洞分类体系；(2) 验证了一种检测方法，其精度达86.7%，召回率达82.5%；(3) 开放提供一个数据集和检测工具包，以支持未来的研究工作。这些结果表明，亟需建立基于能力的权限管理系统，并在该攻击面被进一步滥用之前实施强制性的安全审查机制。"
  },
  {
    "date": "2026-01-15",
    "title": "ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback",
    "authors": "Yutao Mou, Zhangchi Xue, Lijun Li, Peiyang Liu, Shikun Zhang, Wei Ye, Jing Shao",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10156v1",
    "source": "arXiv",
    "abstract": "While LLM-based agents can interact with environments via invoking external tools, their expanded capabilities also amplify security risks. Monitoring step-level tool invocation behaviors in real time and proactively intervening before unsafe execution is critical for agent deployment, yet remains under-explored. In this work, we first construct TS-Bench, a novel benchmark for step-level tool invocation safety detection in LLM agents. We then develop a guardrail model, TS-Guard, using multi-task reinforcement learning. The model proactively detects unsafe tool invocation actions before execution by reasoning over the interaction history. It assesses request harmfulness and action-attack correlations, producing interpretable and generalizable safety judgments and feedback. Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.",
    "title_zh": "ToolSafe：通过主动的步骤级防护机制与反馈提升基于大模型代理的工具调用安全性",
    "abstract_zh": "尽管基于大语言模型（LLM）的智能体可以通过调用外部工具与环境交互，其能力的扩展也带来了更高的安全风险。实时监控每一步的工具调用行为，并在不安全执行发生前主动干预，对于智能体的部署至关重要，但这一问题目前仍缺乏深入研究。本文首先构建了TS-Bench——一个面向LLM智能体步骤级工具调用安全检测的新基准。随后，我们提出一种基于多任务强化学习的防护模型TS-Guard，该模型通过分析交互历史，提前推理并识别潜在不安全的工具调用行为。它能够评估请求的危害性及动作与攻击之间的关联性，生成可解释且具备泛化能力的安全判断与反馈。此外，我们还引入了TS-Flow——一种基于防护反馈驱动的推理框架，使ReAct风格智能体在面对提示注入攻击时，有害工具调用平均减少65%，同时良性任务完成率提升约10%。"
  },
  {
    "date": "2026-01-15",
    "title": "Long-Chain Reasoning Distillation via Adaptive Prefix Alignment",
    "authors": "Zhenghao Liu, Zhuoyang Wu, Xinze Li, Yukun Yan, Shuo Wang, Zulong Chen, Yu Gu, Ge Yu, Maosong Sun",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10064v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities, particularly in solving complex mathematical problems. Recent studies show that distilling long reasoning trajectories can effectively enhance the reasoning performance of small-scale student models. However, teacher-generated reasoning trajectories are often excessively long and structurally complex, making them difficult for student models to learn. This mismatch leads to a gap between the provided supervision signal and the learning capacity of the student model. To address this challenge, we propose Prefix-ALIGNment distillation (P-ALIGN), a framework that fully exploits teacher CoTs for distillation through adaptive prefix alignment. Specifically, P-ALIGN adaptively truncates teacher-generated reasoning trajectories by determining whether the remaining suffix is concise and sufficient to guide the student model. Then, P-ALIGN leverages the teacher-generated prefix to supervise the student model, encouraging effective prefix alignment. Experiments on multiple mathematical reasoning benchmarks demonstrate that P-ALIGN outperforms all baselines by over 3%. Further analysis indicates that the prefixes constructed by P-ALIGN provide more effective supervision signals, while avoiding the negative impact of redundant and uncertain reasoning components. All code is available at https://github.com/NEUIR/P-ALIGN.",
    "title_zh": "基于自适应前缀对齐的长链推理蒸馏",
    "abstract_zh": "大型语言模型（LLMs）在解决复杂数学问题方面展现了卓越的推理能力。近期研究表明，通过提炼长篇推理路径，可以有效提升小规模学生模型的推理性能。然而，教师模型生成的推理路径通常过于冗长且结构复杂，导致学生模型难以学习。这种不匹配使得提供的监督信号与学生模型的学习能力之间存在差距。为应对这一挑战，我们提出了前缀对齐蒸馏框架（Prefix-ALIGNment distillation, P-ALIGN），该框架通过自适应前缀对齐，充分挖掘教师模型的思维链（CoT）进行知识蒸馏。具体而言，P-ALIGN通过判断剩余后缀是否简洁且足以引导学生模型，自适应地截断教师生成的推理路径；随后，利用教师生成的前缀来监督学生模型，促进有效的前缀对齐。在多个数学推理基准上的实验表明，P-ALIGN相较于所有基线方法性能提升超过3%。进一步分析显示，P-ALIGN构建的前缀提供了更有效的监督信号，同时避免了冗余和不确定推理成分带来的负面影响。全部代码已公开于 https://github.com/NEUIR/P-ALIGN。"
  },
  {
    "date": "2026-01-15",
    "title": "Translating database mathematical schemes into relational database software applications with MatBase",
    "authors": "Christian Mancas, Diana Christina Mancas",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10604v1",
    "source": "arXiv",
    "abstract": "We present a pseudocode algorithm for translating our (Elementary) Mathematical Data Model schemes into relational ones and associated sets of non-relational constraints, used by MatBase, our intelligent database management system prototype. We prove that this algorithm is very fast, solid, complete, and optimal. We apply it to a mathematical scheme modeling the genealogical trees subuniverse. We also provide examples of SQL and VBA code for enforcing some of its non-relational constraints, as well as guidelines to develop code for enforcing such constraints.",
    "title_zh": "将数据库数学方案转换为关系数据库软件应用的MatBase",
    "abstract_zh": "我们提出了一种伪代码算法，用于将我们的（初等）数学数据模型方案转换为关系模型及其相关的非关系约束集，该算法由我们的智能数据库管理系统原型MatBase所使用。我们证明了该算法具有极高的速度、稳定性、完备性和最优性。我们将该算法应用于一个建模家谱树子宇宙的数学方案。此外，我们还提供了用于强制执行部分非关系约束的SQL和VBA代码示例，并给出了开发此类约束强制执行代码的指导原则。"
  },
  {
    "date": "2026-01-15",
    "title": "Discrete Feynman-Kac Correctors",
    "authors": "Mohsin Hasan, Viktor Ohanesian, Artem Gazizov, Yoshua Bengio, Alán Aspuru-Guzik, Roberto Bondesan, Marta Skreta, Kirill Neklyudov",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10403v1",
    "source": "arXiv",
    "abstract": "Discrete diffusion models have recently emerged as a promising alternative to the autoregressive approach for generating discrete sequences. Sample generation via gradual denoising or demasking processes allows them to capture hierarchical non-sequential interdependencies in the data. These custom processes, however, do not assume a flexible control over the distribution of generated samples. We propose Discrete Feynman-Kac Correctors, a framework that allows for controlling the generated distribution of discrete masked diffusion models at inference time. We derive Sequential Monte Carlo (SMC) algorithms that, given a trained discrete diffusion model, control the temperature of the sampled distribution (i.e. perform annealing), sample from the product of marginals of several diffusion processes (e.g. differently conditioned processes), and sample from the product of the marginal with an external reward function, producing likely samples from the target distribution that also have high reward. Notably, our framework does not require any training of additional models or fine-tuning of the original model. We illustrate the utility of our framework in several applications including: efficient sampling from the annealed Boltzmann distribution of the Ising model, improving the performance of language models for code generation and amortized learning, as well as reward-tilted protein sequence generation.",
    "title_zh": "离散费曼-卡茨校正项",
    "abstract_zh": "离散扩散模型最近作为生成离散序列的一种有前景的替代方法，逐渐受到关注，其相较于自回归方法具有优势。通过逐步去噪或去掩码过程进行采样，这类模型能够捕捉数据中分层的非顺序依赖关系。然而，这些定制化的过程通常无法灵活控制生成样本的分布。为此，我们提出了**离散费曼-卡茨校正器（Discrete Feynman-Kac Correctors）**框架，该框架可在推理阶段对离散掩码扩散模型的生成分布进行调控。我们推导出一系列序贯蒙特卡洛（SMC）算法，能够在给定已训练的离散扩散模型的基础上，实现以下功能：调节采样分布的温度（即执行退火操作）、从多个扩散过程的边缘分布乘积中采样（例如不同条件下的过程），以及从边缘分布与外部奖励函数的乘积中采样，从而生成既符合目标分布又具有高奖励值的高概率样本。值得注意的是，我们的框架无需额外训练模型，也无需对原始模型进行微调。我们在多个应用场景中展示了该框架的有效性，包括：高效地从伊辛模型的退火玻尔兹曼分布中采样、提升语言模型在代码生成和近似学习中的性能，以及实现奖励导向的蛋白质序列生成。"
  },
  {
    "date": "2026-01-15",
    "title": "Developer Interaction Patterns with Proactive AI: A Five-Day Field Study",
    "authors": "Nadine Kuo, Agnia Sergeyuk, Valerie Chen, Maliheh Izadi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10253v1",
    "source": "arXiv",
    "abstract": "Current in-IDE AI coding tools typically rely on time-consuming manual prompting and context management, whereas proactive alternatives that anticipate developer needs without explicit invocation remain underexplored. Understanding when humans are receptive to such proactive AI assistance during their daily work remains an open question in human-AI interaction research. We address this gap through a field study of proactive AI assistance in professional developer workflows. We present a five-day in-the-wild study with 15 developers who interacted with a proactive feature of an AI assistant integrated into a production-grade IDE that offers code quality suggestions based on in-IDE developer activity. We examined 229 AI interventions across 5,732 interaction points to understand how proactive suggestions are received across workflow stages, how developers experience them, and their perceived impact. Our findings reveal systematic patterns in human receptivity to proactive suggestions: interventions at workflow boundaries (e.g., post-commit) achieved 52% engagement rates, while mid-task interventions (e.g., on declined edit) were dismissed 62% of the time. Notably, well-timed proactive suggestions required significantly less interpretation time than reactive suggestions (45.4s versus 101.4s, W = 109.00, r = 0.533, p = 0.0016), indicating enhanced cognitive alignment. This study provides actionable implications for designing proactive coding assistants, including how to time interventions, align them with developer context, and strike a balance between AI agency and user control in production IDEs.",
    "title_zh": "开发者与主动型AI的交互模式：一项为期五天的实地研究",
    "abstract_zh": "当前IDE中的AI编程工具通常依赖耗时的手动提示和上下文管理，而无需显式调用即可主动预判开发者需求的替代方案仍处于探索阶段。在人机交互研究中，一个尚未解决的关键问题是：人类在日常工作中何时对这种主动型AI辅助持开放态度。为填补这一空白，我们通过一项针对专业开发者工作流程中主动AI辅助的实地研究来探讨该问题。本研究开展了一项为期五天的野外实验，共招募15名开发者，他们使用集成于生产级IDE中的AI助手的主动功能，该功能基于开发者的IDE内活动提供代码质量建议。我们分析了5,732个交互点中的229次AI干预，以理解主动建议在不同工作流程阶段的接受程度、开发者的体验感受及其感知影响。研究发现，人类对主动建议的接受度呈现出系统性模式：在工作流边界（如提交后）进行的干预获得了52%的参与率，而在任务进行中（如拒绝编辑时）的干预则有62%被忽略。值得注意的是，时机恰当的主动建议所需的理解时间显著少于被动响应建议（分别为45.4秒 vs. 101.4秒，W = 109.00，r = 0.533，p = 0.0016），表明其在认知上与开发者更契合。本研究为设计主动式编程助手提供了可操作的启示，包括如何把握干预时机、如何与开发者上下文保持一致，以及如何在生产级IDE中平衡AI自主性与用户控制权。"
  },
  {
    "date": "2026-01-15",
    "title": "PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary",
    "authors": "Jiarui Yao, Ruida Wang, Tong Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10201v1",
    "source": "arXiv",
    "abstract": "Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show the effectiveness of PRL could be verified and generalized.",
    "title_zh": "PRL：过程奖励学习提升大模型推理能力并拓展推理边界",
    "abstract_zh": "近期，提升大型语言模型（LLMs）的推理能力已成为一个持续关注的话题。然而，大多数相关工作仅基于轨迹层面的结果奖励，缺乏对推理过程中细粒度的监督信号。此外，一些现有的训练框架试图将过程信号整合以优化LLMs，但往往依赖于繁琐的额外步骤，如蒙特卡洛树搜索（MCTS）、训练独立的奖励模型等，严重影响了训练效率。更关键的是，这些过程信号的设计直觉缺乏严格的理论支撑，使得优化机制的理解仍不清晰。\n\n本文提出了一种**过程奖励学习**（Process Reward Learning, PRL），其核心思想是将熵正则化的强化学习目标分解为中间步骤，并为模型在每个步骤上分配严格定义的过程奖励。从理论动机出发，我们推导出PRL的数学形式，发现其本质上等价于最大化奖励目标加上策略模型与参考模型之间的KL散度惩罚项。然而，PRL能够将原本仅在最终结果上使用的奖励转化为过程层面的监督信号，从而更有效地引导强化学习中的探索过程。\n\n实验结果表明，PRL不仅显著提升了LLM推理能力的平均表现（以平均@n指标衡量），还通过改善通过率@n（pass@n）指标，拓展了模型的推理边界。大量实验证明，PRL的有效性不仅在多种任务中得到验证，且具备良好的泛化能力。"
  },
  {
    "date": "2026-01-15",
    "title": "Mark My Works Autograder for Programming Courses",
    "authors": "Yiding Qiu, Seyed Mahdi Azimi, Artem Lensky",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10093v1",
    "source": "arXiv",
    "abstract": "Large programming courses struggle to provide timely, detailed feedback on student code. We developed Mark My Works, a local autograding system that combines traditional unit testing with LLM-generated explanations. The system uses role-based prompts to analyze submissions, critique code quality, and generate pedagogical feedback while maintaining transparency in its reasoning process. We piloted the system in a 191-student engineering course, comparing AI-generated assessments with human grading on 79 submissions. While AI scores showed no linear correlation with human scores (r = -0.177, p = 0.124), both systems exhibited similar left-skewed distributions, suggesting they recognize comparable quality hierarchies despite different scoring philosophies. The AI system demonstrated more conservative scoring (mean: 59.95 vs 80.53 human) but generated significantly more detailed technical feedback.",
    "title_zh": "Mark My Works 自动评分系统（用于编程课程）",
    "abstract_zh": "大规模编程课程难以对学生代码提供及时且详尽的反馈。为此，我们开发了“Mark My Works”——一个本地自动评分系统，该系统结合了传统的单元测试与由大语言模型（LLM）生成的解释说明。系统采用基于角色的提示策略，对提交的作业进行分析，评估代码质量，并生成具有教学意义的反馈，同时保持推理过程的透明性。我们在一门包含191名学生的工程学课程中对该系统进行了试点，将AI生成的评分结果与人工评分在79份作业上进行了对比。尽管AI评分与人工评分之间未表现出线性相关性（r = -0.177，p = 0.124），但两种评分体系均呈现出相似的左偏分布，表明它们虽在评分理念上存在差异，却能识别出相近的代码质量层级。此外，AI系统表现出更为保守的评分倾向（平均分：59.95 vs 人类评分者平均分80.53），但生成的技术反馈显著更详细。"
  },
  {
    "date": "2026-01-15",
    "title": "The Conversational Exam: A Scalable Assessment Design for the AI Era",
    "authors": "Lorena A. Barba, Laura Stegner",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10691v1",
    "source": "arXiv",
    "abstract": "Traditional assessment methods collapse when students use generative AI to complete work without genuine engagement, creating an illusion of competence where they believe they're learning but aren't. This paper presents the conversational exam -- a scalable oral examination format that restores assessment validity by having students code live while explaining their reasoning. Drawing on human-computer interaction principles, we examined 58 students in small groups across just two days, demonstrating that oral exams can scale to typical class sizes. The format combines authentic practice (students work with documentation and supervised AI access) with inherent validity (real-time performance cannot be faked). We provide detailed implementation guidance to help instructors adapt this approach, offering a practical path forward when many educators feel paralyzed between banning AI entirely or accepting that valid assessment is impossible.",
    "title_zh": "对话式考试：面向人工智能时代的一种可扩展评估设计",
    "abstract_zh": "当学生使用生成式人工智能完成任务却缺乏真正参与时，传统的评估方法便失效了，这制造出一种虚假的掌握感——学生误以为自己在学习，实则并未真正掌握。本文提出了一种对话式考试（conversational exam）——一种可扩展的口头考核形式，通过让学生在实时编码的同时解释其思维过程，恢复评估的有效性。基于人机交互原则，我们在短短两天内对58名学生进行了小组测试，证明这种口头考试模式能够适应常规班级规模。该形式融合了真实实践（学生可查阅文档并获得受监督的AI支持）与内在有效性（实时表现无法伪装）。我们提供了详细的实施指南，帮助教师采用这一方法，为许多在“全面禁止AI”与“认为有效评估已不可能”之间举棋不定的教育者提供了一条切实可行的出路。"
  },
  {
    "date": "2026-01-15",
    "title": "GeoSteer: Faithful Chain-of-Thought Steering via Latent Manifold Gradients",
    "authors": "Kentaro Kazama, Daiki Shirafuji, Tatsuhiko Saito",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10229v1",
    "source": "arXiv",
    "abstract": "Recent advances in Large Language Models (LLMs) have improved multi-step reasoning. Most approaches rely on Chain-of-Thought (CoT) rationales. Previous studies have shown that LLMs often generate logically inconsistent reasoning steps even when their final answers are correct. These inconsistencies reduce the reliability of step-level reasoning. We propose GeoSteer, a manifold-based framework that improves the quality of intermediate reasoning. The method consists of: (1) constructing a CoT dataset with segment-level scores, (2) training a Variational Autoencoder (VAE) model and a quality estimation model to learn a low-dimensional manifold of high-quality CoT trajectories, and (3) steering hidden states of target LLMs toward higher-quality regions in the latent space. This update in a latent space behaves like a natural-gradient adjustment in the original hidden-state space. It ensures geometrically coherent steering. We evaluate GeoSteer on the GSM8k dataset using the Qwen3 series. We measure via answer accuracy and overall reasoning performance. GeoSteer improved the exact match accuracy by up to 2.6 points. It also enhanced the pairwise win rate by 5.3 points. These results indicate that GeoSteer provides an effective and controllable mechanism for improving the quality of intermediate reasoning in LLMs.",
    "title_zh": "GeoSteer：通过潜在流形梯度实现忠实的思维链引导",
    "abstract_zh": "大型语言模型（LLMs）的最新进展显著提升了多步推理能力。目前大多数方法依赖于思维链（Chain-of-Thought, CoT）推理过程。然而，先前的研究表明，即使最终答案正确，LLMs 仍常常生成逻辑不一致的中间推理步骤，这种不一致性降低了逐步推理的可靠性。为此，我们提出了 GeoSteer——一种基于流形的框架，用于提升中间推理的质量。该方法包含三个核心步骤：(1) 构建带有段级评分的 CoT 数据集；(2) 训练变分自编码器（VAE）模型和质量评估模型，以学习高质量 CoT 轨迹的低维流形结构；(3) 将目标 LLM 的隐藏状态引导至潜在空间中质量更高的区域。这一潜在空间中的更新行为类似于在原始隐藏状态空间中的自然梯度调整，从而确保了几何上的一致性引导。我们在 GSM8k 数据集上使用 Qwen3 系列模型对 GeoSteer 进行了评估，通过答案准确率和整体推理性能进行衡量。实验结果表明，GeoSteer 最多可将精确匹配准确率提升 2.6 个百分点，同时将成对胜率提升 5.3 个百分点。这些结果表明，GeoSteer 为提升 LLM 中间推理质量提供了一种有效且可控的新机制。"
  },
  {
    "date": "2026-01-15",
    "title": "Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text",
    "authors": "Zhihao Xu, Rumei Li, Jiahuan Li, Rongxiang Weng, Jingang Wang, Xunliang Cai, Xiting Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10355v1",
    "source": "arXiv",
    "abstract": "Enabling Large Language Models (LLMs) to effectively utilize tools in multi-turn interactions is essential for building capable autonomous agents. However, acquiring diverse and realistic multi-turn tool-use data remains a significant challenge. In this work, we propose a novel text-based paradigm. We observe that textual corpora naturally contain rich, multi-step problem-solving experiences, which can serve as an untapped, scalable, and authentic data source for multi-turn tool-use tasks. Based on this insight, we introduce GEM, a data synthesis pipeline that enables the generation and extraction of multi-turn tool-use trajectories from text corpora through a four-stage process: relevance filtering, workflow & tool extraction, trajectory grounding, and complexity refinement. To reduce the computational cost, we further train a specialized Trajectory Synthesizer via supervised fine-tuning. This model distills the complex generation pipeline into an efficient, end-to-end trajectory generator. Experiments demonstrate that our GEM-32B achieve a 16.5% improvement on the BFCL V3 Multi-turn benchmark. Our models partially surpass the performance of models trained on τ - bench (Airline and Retail) in-domain data, highlighting the superior generalization capability derived from our text-based synthesis paradigm. Notably, our Trajectory Synthesizer matches the quality of the full pipeline while significantly reducing inference latency and costs.",
    "title_zh": "解锁隐性经验：从文本中合成工具使用轨迹",
    "abstract_zh": "使大型语言模型（LLMs）在多轮交互中有效利用工具，对于构建具备能力的自主智能体至关重要。然而，获取多样化且真实的多轮工具使用数据仍面临重大挑战。本文提出了一种新颖的基于文本的范式。我们观察到，文本语料库天然包含丰富的多步骤问题求解经验，可作为未被充分开发、可扩展且真实可靠的多轮工具使用任务数据来源。基于这一洞察，我们提出了GEM——一种数据合成流程，通过四个阶段实现从文本语料库中生成并提取多轮工具使用轨迹：相关性过滤、工作流与工具提取、轨迹定位以及复杂度优化。为降低计算成本，我们进一步通过监督微调训练了一个专用的轨迹生成器（Trajectory Synthesizer）。该模型将复杂的生成流程提炼为一个高效、端到端的轨迹生成器。实验表明，我们的GEM-32B在BFCL V3多轮基准测试中实现了16.5%的性能提升。我们的模型在部分任务上甚至超越了在τ-bench（航空和零售领域）内部数据上训练的模型表现，凸显了基于文本合成范式所带来卓越的泛化能力。值得注意的是，我们的轨迹生成器在保持与完整流程相当质量的同时，显著降低了推理延迟和成本。"
  },
  {
    "date": "2026-01-15",
    "title": "Autonomous Quantum Simulation through Large Language Model Agents",
    "authors": "Weitang Li, Jiajun Ren, Lixue Cheng, Cunxi Gong",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10194v1",
    "source": "arXiv",
    "abstract": "We demonstrate that large language model (LLM) agents can autonomously perform tensor network simulations of quantum many-body systems, achieving approximately 90% success rate across representative benchmark tasks. Tensor network methods are powerful tools for quantum simulation, but their effective use requires expertise typically acquired through years of graduate training. By combining in-context learning with curated documentation and multi-agent decomposition, we create autonomous AI agents that can be trained in specialized computational domains within minutes. We benchmark three configurations (baseline, single-agent with in-context learning, and multi-agent with in-context learning) on problems spanning quantum phase transitions, open quantum system dynamics, and photochemical reactions. Systematic evaluation using DeepSeek-V3.2, Gemini 2.5 Pro, and Claude Opus 4.5 demonstrates that both in-context learning and multi-agent architecture are essential. Analysis of failure modes reveals characteristic patterns across models, with the multi-agent configuration substantially reducing implementation errors and hallucinations compared to simpler architectures.",
    "title_zh": "通过大型语言模型代理实现自主量子模拟",
    "abstract_zh": "我们证明，大型语言模型（LLM）代理能够自主执行量子多体系统的张量网络模拟，在代表性基准任务中实现了约90%的成功率。张量网络方法是量子模拟的强大工具，但其有效应用通常需要经过多年研究生训练才能掌握的专业知识。通过结合上下文学习、精心整理的文档以及多智能体分解策略，我们构建了可在数分钟内完成特定计算领域训练的自主AI代理。我们在涵盖量子相变、开放量子系统动力学及光化学反应的问题上，对三种配置（基线模型、仅使用上下文学习的单智能体，以及结合上下文学习的多智能体）进行了基准测试。利用DeepSeek-V3.2、Gemini 2.5 Pro和Claude Opus 4.5进行的系统评估表明，上下文学习与多智能体架构均至关重要。对失败模式的分析揭示了各模型中存在的典型问题，相比简单架构，多智能体配置显著降低了实现错误和幻觉现象的发生。"
  },
  {
    "date": "2026-01-15",
    "title": "PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution",
    "authors": "Minghao Yan, Bo Peng, Benjamin Coleman, Ziqi Chen, Zhouhang Xie, Zhankui He, Noveen Sachdeva, Isabella Ye, Weili Wang, Chi Wang, Ed H. Chi, Wang-Cheng Kang, Derek Zhiyuan Cheng, Beidou Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2601.10657v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) have emerged as powerful operators for evolutionary search, yet the design of efficient search scaffolds remains ad hoc. While promising, current LLM-in-the-loop systems lack a systematic approach to managing the evolutionary process. We identify three distinct failure modes: Context Pollution, where experiment history biases future candidate generation; Mode Collapse, where agents stagnate in local minima due to poor exploration-exploitation balance; and Weak Collaboration, where rigid crossover strategies fail to leverage parallel search trajectories effectively. We introduce Progress-Aware Consistent Evolution (PACEvolve), a framework designed to robustly govern the agent's context and search dynamics, to address these challenges. PACEvolve combines hierarchical context management (HCM) with pruning to address context pollution; momentum-based backtracking (MBB) to escape local minima; and a self-adaptive sampling policy that unifies backtracking and crossover for dynamic search coordination (CE), allowing agents to balance internal refinement with cross-trajectory collaboration. We demonstrate that PACEvolve provides a systematic path to consistent, long-horizon self-improvement, achieving state-of-the-art results on LLM-SR and KernelBench, while discovering solutions surpassing the record on Modded NanoGPT.",
    "title_zh": "PACEvolve：实现长时程进展感知的一致性演化",
    "abstract_zh": "大规模语言模型（LLMs）已成为进化搜索的强大执行者，但高效的搜索框架设计仍缺乏系统性。尽管前景广阔，当前基于“LLM在环”（LLM-in-the-loop）的系统在管理进化过程方面仍缺乏系统的策略。我们识别出三种典型的失败模式：**上下文污染**（Context Pollution），即实验历史会干扰后续候选解的生成；**模式坍缩**（Mode Collapse），由于探索与利用之间的平衡不佳，智能体陷入局部最优而停滞不前；以及**协作薄弱**（Weak Collaboration），即僵化的交叉策略无法有效利用并行搜索轨迹的优势。\n\n为此，我们提出了**进度感知的一致进化框架**（Progress-Aware Consistent Evolution, PACEvolve），旨在稳健地调控智能体的上下文状态与搜索动态，以应对上述挑战。PACEvolve通过以下三项核心机制实现优化：\n\n- **分层上下文管理**（Hierarchical Context Management, HCM）结合剪枝策略，有效缓解上下文污染问题；\n- **基于动量的回溯机制**（Momentum-based Backtracking, MBB），帮助智能体跳出局部极小值；\n- 一种**自适应采样策略**，将回溯与交叉操作统一整合，实现动态搜索协调（Consistent Evolution, CE），使智能体能够在内部精炼与跨轨迹协作之间取得良好平衡。\n\n实验表明，PACEvolve为实现长期、一致的自我改进提供了系统化路径，在LLM-SR和KernelBench基准上均达到当前最优性能，并成功发现了超越Modded NanoGPT现有记录的新解。"
  }
]