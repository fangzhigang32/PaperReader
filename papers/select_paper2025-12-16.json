[
  {
    "date": "2025-12-16",
    "title": "Lost in the Pages: WebAssembly Code Recovery through SEV-SNP's Exposed Address Space",
    "authors": "Markus Berthilsson, Christian Gehrmann",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14376v1",
    "source": "arXiv",
    "abstract": "WebAssembly (Wasm) has risen as a widely used technology to distribute computing workloads on different platforms. The platform independence offered through Wasm makes it an attractive solution for many different applications that can run on disparate infrastructures. In addition, Trusted Execution Environments (TEEs) are offered in many computing infrastructures, which allows also running security sensitive Wasm workloads independent of the specific platforms offered. However, recent work has shown that Wasm binaries are more sensitive to code confidentiality attacks than native binaries. The previous result was obtained for Intel SGX only. In this paper, we take this one step further, introducing a new Wasm code-confidentiality attack that exploits exposed address-space information in TEEs. Our attack enables the extraction of crucial execution features which, when combined with additional side channels, allows us to with high reliability obtain more than 70% of the code in most cases. This is a considerably larger amount than was previously obtained by single stepping Intel SGX where only upwards to 50% of the code could be obtained.",
    "title_zh": "迷失于页面之间：通过SEV-SNP暴露的地址空间恢复WebAssembly代码",
    "abstract_zh": "WebAssembly（Wasm）已成为一种广泛使用的技术，用于在不同平台上分发计算任务。由于Wasm具备平台无关性，使其成为许多可在异构基础设施上运行的应用程序的吸引力解决方案。此外，许多计算基础设施提供了可信执行环境（TEEs），使得安全敏感的Wasm工作负载能够独立于特定平台运行。然而，近期研究显示，与原生二进制文件相比，Wasm二进制文件更容易受到代码机密性攻击。此前的研究仅针对Intel SGX展开。本文进一步推进这一研究，提出了一种新的Wasm代码机密性攻击，该攻击利用了TEEs中暴露的地址空间信息。我们的攻击能够提取关键的执行特征，结合其他旁道信道后，可高可靠地恢复出大部分代码，大多数情况下可获取超过70%的代码内容。这一成果显著高于以往通过单步调试Intel SGX所获得的最高50%代码量。"
  },
  {
    "date": "2025-12-16",
    "title": "TEMP: A Memory Efficient Physical-aware Tensor Partition-Mapping Framework on Wafer-scale Chips",
    "authors": "Huizheng Wang, Taiquan Wei, Zichuan Wang, Dingcheng Jiang, Qize Yang, Jiaxin Liu, Jingxiang Hou, Chao Li, Jinyi Deng, Yang Hu, Shouyi Yin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14256v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) demand significant memory and computation resources. Wafer-scale chips (WSCs) provide high computation power and die-to-die (D2D) bandwidth but face a unique trade-off between on-chip memory and compute resources due to limited wafer area. Therefore, tensor parallelism strategies for wafer should leverage communication advantages while maintaining memory efficiency to maximize WSC performance. However, existing approaches fail to address these challenges. To address these challenges, we propose the tensor stream partition paradigm (TSPP), which reveals an opportunity to leverage WSCs' abundant communication bandwidth to alleviate stringent on-chip memory constraints. However, the 2D mesh topology of WSCs lacks long-distance and flexible interconnects, leading to three challenges: 1) severe tail latency, 2) prohibitive D2D traffic contention, and 3) intractable search time for optimal design. We present TEMP, a framework for LLM training on WSCs that leverages topology-aware tensor-stream partition, traffic-conscious mapping, and dual-level wafer solving to overcome hardware constraints and parallelism challenges. These integrated approaches optimize memory efficiency and throughput, unlocking TSPP's full potential on WSCs. Evaluations show TEMP achieves 1.7x average throughput improvement over state-of-the-art LLM training systems across various models.",
    "title_zh": "TEMP：一种面向晶圆级芯片的内存高效物理感知张量分块映射框架",
    "abstract_zh": "大型语言模型（LLMs）需要大量的内存和计算资源。晶圆级芯片（WSCs）提供了高算力和芯片间（D2D）带宽，但由于晶圆面积有限，其片上内存与计算资源之间存在独特的权衡关系。因此，针对晶圆级芯片的张量并行策略应在利用通信优势的同时保持内存效率，以最大化WSC的性能表现。然而，现有方法未能有效应对这些挑战。为解决这些问题，我们提出了张量流划分范式（TSPP），该范式揭示了利用WSC丰富的通信带宽来缓解严格片上内存限制的潜力。然而，WSC所采用的二维网格拓扑缺乏长距离且灵活的互连结构，由此带来了三大挑战：1）严重的尾延迟；2）难以承受的D2D流量竞争；3）最优设计搜索时间过长。为此，我们提出TEMP框架——一种面向WSC上LLM训练的解决方案，通过拓扑感知的张量流划分、流量敏感的映射策略以及双层晶圆求解机制，有效克服硬件限制与并行性难题。这些集成方法显著优化了内存效率与吞吐量，充分释放了TSPP在WSC上的全部潜力。实验结果表明，TEMP在多种模型上相较当前最先进的LLM训练系统平均实现了1.7倍的吞吐量提升。"
  },
  {
    "date": "2025-12-16",
    "title": "Estimating problem difficulty without ground truth using Large Language Model comparisons",
    "authors": "Marthe Ballon, Andres Algaba, Brecht Verbeken, Vincent Ginis",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14220v1",
    "source": "arXiv",
    "abstract": "Recent advances in the finetuning of large language models (LLMs) have significantly improved their performance on established benchmarks, emphasizing the need for increasingly difficult, synthetic data. A key step in this data generation pipeline is a method for estimating problem difficulty. Current approaches, such as human calibration or performance-based scoring, fail to generalize to out-of-distribution problems, i.e. problems currently unsolvable by humans and LLMs, because they are not scalable, time-consuming, and ground truth dependent. Therefore, we propose a new method for estimating problem difficulty, LLM compare, that addresses these limitations. An LLM performs pairwise difficulty comparisons, and then Bradley-Terry scores are computed based on the outcomes. To validate our method, we first propose a conceptual framework that positions existing approaches on three orthogonal planes--construction, scale and dependence--identifying which quadrants a measure needs to occupy to score out-of-distribution problems. LLM compare naturally occupies all desirable quadrants as the first measure that is continuous and dynamic, model-agnostic and independent of ground truth information. As a second validation, we show that LLM compare demonstrates strong alignment with human annotations: Pearson $r \\geq 0.80$ for $n=1876$. Thirdly, we show that LLM compare is robust to hallucinations, with less than $6\\%$ degradation in Pearson correlation for $10\\%$ noise injection. Our work represents a significant step towards replacing time-consuming human annotations and synthetic data generation, and will be an important driver for curriculum design, model evaluation, and AI-assisted research ideation.",
    "title_zh": "使用大型语言模型比较来估计问题难度（无需真实答案）",
    "abstract_zh": "近年来，大型语言模型（LLMs）的微调技术取得了显著进展，使其在现有基准测试中的表现大幅提升，这进一步凸显了对日益复杂、合成数据的需求。该数据生成流程中的关键步骤是估计问题难度的方法。当前的方法，如人工校准或基于性能的评分，难以推广到分布外（out-of-distribution）的问题——即目前人类和LLMs均无法解决的问题，因为这些方法不具备可扩展性、耗时且依赖真实标签。因此，我们提出一种新的问题难度估计方法——LLM compare，以克服上述局限。该方法让LLM执行成对难度比较，并基于比较结果计算Bradley-Terry得分。为验证该方法的有效性，我们首先构建了一个概念框架，将现有方法置于三个正交维度——构造方式、规模和依赖性——上进行定位，从而明确衡量分布外问题难度所需占据的理想象限。LLM compare自然地占据了所有理想象限，成为首个连续、动态、模型无关且不依赖真实标签信息的度量方法。作为第二重验证，我们证明LLM compare与人类标注具有高度一致性：在n=1876的情况下，皮尔逊相关系数$r \\geq 0.80$。第三，我们展示了LLM compare对幻觉现象具有鲁棒性，在注入10%噪声的情况下，皮尔逊相关系数下降不足6%。本研究标志着向替代耗时的人工标注和合成数据生成迈出了重要一步，将在课程设计、模型评估以及人工智能辅助研究构思等领域发挥重要作用。"
  },
  {
    "date": "2025-12-16",
    "title": "Improved diffusive approximation of Markov jump processes close to equilibrium",
    "authors": "David Roberts, Trevor McCourt, Geremia Massarelli, Jeremy Rothschild, Nahuel Freitas",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14088v1",
    "source": "arXiv",
    "abstract": "Diffusive approximations of Markov jump processes often fail to accurately capture large fluctuations. This is confounding, as the rare events triggered by these large fluctuations, such as the failure of electronic memories, are often the object of interest. In this paper we present an improved diffusive approximation, extending a method previously limited to equilibrium systems. Using new tools from stochastic thermodynamics, we prove its validity to linear order in departures from equilibrium and demonstrate its superior accuracy over the Kramers-Moyal expansion in predicting both steady-state and transient properties, including the error rate of a non-equilibrium electronic memory.",
    "title_zh": "接近平衡态的马尔可夫跳跃过程的改进扩散近似",
    "abstract_zh": "扩散近似在处理马尔可夫跳跃过程时，通常无法准确捕捉大波动。这令人困惑，因为由这些大波动引发的罕见事件（如电子存储器的失效）往往是研究的重点。本文提出了一种改进的扩散近似方法，该方法扩展了此前仅适用于平衡系统的方法。借助随机热力学领域的新工具，我们证明了该方法在偏离平衡程度线性范围内具有有效性，并且在预测稳态与瞬态性质方面，包括非平衡电子存储器的误码率，其精度显著优于克兰默斯-莫伊尔展开。"
  },
  {
    "date": "2025-12-16",
    "title": "Relevant HAL Interface Requirements for Embedded Systems",
    "authors": "Manuel Bentele, Andreas Podelski, Axel Sikora, Bernd Westphal",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14514v1",
    "source": "arXiv",
    "abstract": "Embedded applications often use a Hardware Abstraction Layer (HAL) to access hardware. Improper use of the HAL can lead to incorrect hardware operations, resulting in system failure and potentially serious damage to the hardware. The question is how one can obtain prioritize, among a possibly large set of HAL interface requirements, those that are indisputably relevant for preventing this kind of system failure. In this paper, we introduce a formal notion of relevance. This allows us to leverage a formal method, i.e., software model checking, to produce a mathematical proof that a requirement is indisputably relevant. We propose an approach to extract provably relevant requirements from issue reports on system failures. We present a case study to demonstrate that the approach is feasible in principle. The case study uses three examples of issue reports on embedded applications that use the SPI bus via the spidev HAL. The overall contribution of this paper is to pave the way for the study of approaches to a new kind of prioritization aimed at preventing a specific kind of system failure.",
    "title_zh": "嵌入式系统相关的HAL接口要求",
    "abstract_zh": "嵌入式应用通常使用硬件抽象层（HAL）来访问硬件。HAL的不当使用可能导致硬件操作错误，进而引发系统故障，甚至对硬件造成严重损害。问题在于：如何从可能数量庞大的HAL接口需求中，识别出那些无可争议地与防止此类系统故障相关的需求？本文引入了一个形式化的“相关性”概念，使得我们能够借助形式化方法——即软件模型检查——生成数学证明，以确认某项需求确实具有不可争议的相关性。我们提出一种方法，从系统故障的缺陷报告中提取出可被证明相关的具体需求。通过一个案例研究，我们展示了该方法在原则上是可行的。该案例研究选取了三个关于使用SPI总线并通过spidev HAL实现的嵌入式应用的缺陷报告作为实例。本文的整体贡献在于为研究一类新型优先级排序方法铺平道路，其目标是预防特定类型的系统故障。"
  },
  {
    "date": "2025-12-16",
    "title": "C-ing Clearly: Enhanced Binary Code Explanations using C code",
    "authors": "Teodor Poncu, Ioana Pintilie, Marius Dragoi, Dragos Tantaru, Florin Brad",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14500v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) typically excel at coding tasks involving high-level programming languages, as opposed to lower-level programming languages, such as assembly. We propose a synthetic data generation method named C-ing Clearly, which leverages the corresponding C code to enhance an LLM's understanding of assembly. By fine-tuning on data generated through our method, we demonstrate improved LLM performance for binary code summarization and vulnerability detection. Our approach demonstrates consistent gains across different LLM families and model sizes.",
    "title_zh": "清晰地理解C语言：使用C代码增强二进制代码的解释",
    "abstract_zh": "大型语言模型（LLMs）通常在处理高级编程语言的编码任务方面表现优异，而对低级编程语言（如汇编语言）则相对薄弱。我们提出了一种名为“C-ing Clearly”的合成数据生成方法，该方法利用对应的C语言代码来增强大语言模型对汇编语言的理解能力。通过在我们方法生成的数据上进行微调，我们在二进制代码摘要和漏洞检测任务中均实现了大语言模型性能的提升。该方法在不同大语言模型家族及不同模型规模下均表现出一致的性能增益。"
  },
  {
    "date": "2025-12-16",
    "title": "From Obfuscated to Obvious: A Comprehensive JavaScript Deobfuscation Tool for Security Analysis",
    "authors": "Dongchao Zhou, Lingyun Ying, Huajun Chai, Dongbin Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14070v1",
    "source": "arXiv",
    "abstract": "JavaScript's widespread adoption has made it an attractive target for malicious attackers who employ sophisticated obfuscation techniques to conceal harmful code. Current deobfuscation tools suffer from critical limitations that severely restrict their practical effectiveness. Existing tools struggle with diverse input formats, address only specific obfuscation types, and produce cryptic output that impedes human analysis. To address these challenges, we present JSIMPLIFIER, a comprehensive deobfuscation tool using a multi-stage pipeline with preprocessing, abstract syntax tree-based static analysis, dynamic execution tracing, and Large Language Model (LLM)-enhanced identifier renaming. We also introduce multi-dimensional evaluation metrics that integrate control/data flow analysis, code simplification assessment, entropy measures and LLM-based readability assessments. We construct and release the largest real-world obfuscated JavaScript dataset with 44,421 samples (23,212 wild malicious + 21,209 benign samples). Evaluation shows JSIMPLIFIER outperforms existing tools with 100% processing capability across 20 obfuscation techniques, 100% correctness on evaluation subsets, 88.2% code complexity reduction, and over 4-fold readability improvement validated by multiple LLMs. Our results advance benchmarks for JavaScript deobfuscation research and practical security applications.",
    "title_zh": "从混淆到清晰：用于安全分析的全面JavaScript去混淆工具",
    "abstract_zh": "JavaScript 的广泛采用使其成为恶意攻击者青睐的目标，攻击者常使用复杂的混淆技术隐藏恶意代码。当前的去混淆工具存在严重局限性，极大限制了其实际应用效果：现有工具难以应对多样化的输入格式，仅针对特定类型的混淆，且生成的输出晦涩难懂，不利于人工分析。为解决这些挑战，我们提出 JSIMPLIFIER——一种基于多阶段流水线的综合性去混淆工具，包含预处理、基于抽象语法树的静态分析、动态执行追踪以及由大语言模型（LLM）增强的标识符重命名等环节。同时，我们引入多维度评估指标，融合控制流/数据流分析、代码简化程度评估、熵值度量以及基于 LLM 的可读性评价。我们构建并发布了目前最大的真实世界混淆 JavaScript 数据集，包含 44,421 个样本（其中 23,212 个为真实恶意样本，21,209 个为良性样本）。实验结果表明，JSIMPLIFIER 在 20 种混淆技术上实现了 100% 的处理能力，在评估子集上达到 100% 正确率，代码复杂度降低 88.2%，并通过多个 LLM 验证，可读性提升超过四倍。我们的研究成果显著推动了 JavaScript 去混淆研究的基准发展，并为实际安全应用提供了有力支持。"
  },
  {
    "date": "2025-12-16",
    "title": "RePo: Language Models with Context Re-Positioning",
    "authors": "Huayang Li, Tianyu Zhao, Richard Sproat",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14391v1",
    "source": "arXiv",
    "abstract": "In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, $f_φ$, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo.",
    "title_zh": "RePo：具有上下文重定位的语言模型",
    "abstract_zh": "上下文学习是现代大语言模型（LLM）的核心机制；然而，当前主流架构通过赋予线性或恒定的位置索引，施加了僵化且固定的上下文结构。基于认知负荷理论（CLT），我们认为这种缺乏信息量的结构增加了外在认知负荷，消耗了本应用于深度推理和注意力分配的有限工作记忆容量。为解决这一问题，我们提出RePo——一种新颖的机制，通过上下文重定位来降低外在认知负荷。与传统方法不同，RePo利用一个可微模块 $f_φ$ 为标记分配位置，以捕捉上下文依赖关系，而非依赖预定义的整数范围。通过在 OLMo-2 1B 基础模型上持续预训练，我们证明 RePo 在处理噪声上下文、结构化数据以及更长上下文长度的任务中显著提升性能，同时在一般短上下文任务上仍保持竞争力。详细分析表明，RePo 能够有效将更高注意力分配给远距离但相关的信息，在密集且非线性的空间中分配位置，并成功捕捉输入上下文的内在结构。我们的代码已开源，地址为 https://github.com/SakanaAI/repo。"
  },
  {
    "date": "2025-12-16",
    "title": "From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition",
    "authors": "Yiqing Zhou, Yu Lei, Shuzheng Si, Qingyan Sun, Wei Wang, Yifei Wu, Hao Wen, Gang Chen, Fanchao Qi, Maosong Sun",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14244v1",
    "source": "arXiv",
    "abstract": "Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.",
    "title_zh": "从上下文到基本话语单元：通过基本话语单元分解实现忠实且结构化的上下文压缩",
    "abstract_zh": "管理长篇上下文仍是大型语言模型（LLMs）面临的关键瓶颈，尤其在长文档问答和自主代理等应用场景中，长输入不仅带来高昂的计算成本，还引入了噪声。现有的压缩技术通常通过离散的标记删除破坏局部连贯性，或依赖隐式潜在编码，而后者存在位置偏差问题，并且与闭源API不兼容。为解决这些局限性，我们提出了一种基于EDU（基本话语单元）的上下文压缩器（EDU-based Context Compressor），这是一种全新的显式压缩框架，旨在同时保留全局结构与细粒度细节。我们的方法将上下文压缩重构为“结构化-再选择”的两阶段过程：首先，通过LingoEDU将线性文本转化为由EDUs构成的结构关系树，且EDUs严格锚定于原始文本索引，从而杜绝幻觉；其次，一个轻量级排序模块筛选出与查询相关的子树并进行线性化处理。为严格评估模型的结构理解能力，我们发布了StructBench——一个包含248个多样化文档的手动标注数据集。实验结果表明，该方法在结构预测准确率上达到当前最优水平，显著优于前沿LLM，同时大幅降低计算开销。此外，这种具备结构感知能力的压缩方式，在从长上下文任务到复杂深度搜索等多种下游任务中均显著提升了性能。"
  },
  {
    "date": "2025-12-16",
    "title": "Model-First Reasoning LLM Agents: Reducing Hallucinations through Explicit Problem Modeling",
    "authors": "Annu Rana, Gaurav Kumar",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14474v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) often struggle with complex multi-step planning tasks, showing high rates of constraint violations and inconsistent solutions. Existing strategies such as Chain-of-Thought and ReAct rely on implicit state tracking and lack an explicit problem representation. Inspired by classical AI planning, we propose Model-First Reasoning (MFR), a two-phase paradigm in which the LLM first constructs an explicit model of the problem, defining entities, state variables, actions, and constraints, before generating a solution plan. Across multiple planning domains, including medical scheduling, route planning, resource allocation, logic puzzles, and procedural synthesis, MFR reduces constraint violations and improves solution quality compared to Chain-of-Thought and ReAct. Ablation studies show that the explicit modeling phase is critical for these gains. Our results suggest that many LLM planning failures stem from representational deficiencies rather than reasoning limitations, highlighting explicit modeling as a key component for robust and interpretable AI agents. All prompts, evaluation procedures, and task datasets are documented to facilitate reproducibility.",
    "title_zh": "模型优先推理的大语言模型智能体：通过显式问题建模减少幻觉",
    "abstract_zh": "大型语言模型（LLMs）在处理复杂的多步骤规划任务时常常表现不佳，表现出较高的约束违反率和解决方案不一致的问题。现有的策略如思维链（Chain-of-Thought）和ReAct依赖于隐式的状态追踪，缺乏显式的問題表征。受经典人工智能规划思想的启发，我们提出了“模型优先推理”（Model-First Reasoning, MFR），这是一种两阶段范式：首先，大语言模型构建问题的显式模型，明确定义实体、状态变量、动作及约束；随后再生成解决方案计划。在多个规划领域——包括医疗排班、路径规划、资源分配、逻辑谜题以及过程合成——MFR相较于思维链和ReAct显著降低了约束违反率，并提升了方案质量。消融实验表明，显式建模阶段对于这些性能提升至关重要。我们的研究结果表明，许多LLM规划失败的根本原因在于表征能力的不足，而非推理能力的局限，凸显了显式建模在构建稳健且可解释的人工智能代理中的关键作用。所有提示模板、评估流程及任务数据集均已详细记录，以确保结果的可复现性。"
  },
  {
    "date": "2025-12-16",
    "title": "PortAgent: LLM-driven Vehicle Dispatching Agent for Port Terminals",
    "authors": "Jia Hu, Junqi Li, Weimeng Lin, Peng Jia, Yuxiong Ji, Jintao Lai",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14417v1",
    "source": "arXiv",
    "abstract": "Vehicle Dispatching Systems (VDSs) are critical to the operational efficiency of Automated Container Terminals (ACTs). However, their widespread commercialization is hindered due to their low transferability across diverse terminals. This transferability challenge stems from three limitations: high reliance on port operational specialists, a high demand for terminal-specific data, and time-consuming manual deployment processes. Leveraging the emergence of Large Language Models (LLMs), this paper proposes PortAgent, an LLM-driven vehicle dispatching agent that fully automates the VDS transferring workflow. It bears three features: (1) no need for port operations specialists; (2) low need of data; and (3) fast deployment. Specifically, specialist dependency is eliminated by the Virtual Expert Team (VET). The VET collaborates with four virtual experts, including a Knowledge Retriever, Modeler, Coder, and Debugger, to emulate a human expert team for the VDS transferring workflow. These experts specialize in the domain of terminal VDS via a few-shot example learning approach. Through this approach, the experts are able to learn VDS-domain knowledge from a few VDS examples. These examples are retrieved via a Retrieval-Augmented Generation (RAG) mechanism, mitigating the high demand for terminal-specific data. Furthermore, an automatic VDS design workflow is established among these experts to avoid extra manual interventions. In this workflow, a self-correction loop inspired by the LLM Reflexion framework is created",
    "title_zh": "港口代理：基于大语言模型的港口码头车辆调度代理",
    "abstract_zh": "车辆调度系统（VDSs）对于自动化集装箱码头（ACTs）的运营效率至关重要。然而，由于其在不同码头之间迁移能力较差，导致这些系统难以实现广泛商业化。这种可迁移性难题主要源于三个局限：对港口运营专家的高度依赖、对码头特定数据的高需求，以及耗时的手动部署过程。本文借助大型语言模型（LLMs）的兴起，提出了一种名为PortAgent的基于LLM的车辆调度代理，能够完全自动化VDS的迁移流程。该系统具备三大优势：（1）无需依赖港口运营专家；（2）对数据需求低；（3）部署速度快。具体而言，通过“虚拟专家团队”（VET）机制消除了对专业人员的依赖。VET由四位虚拟专家协同工作，包括知识检索员、建模师、程序员和调试员，共同模拟人类专家团队完成VDS迁移流程。这四位专家通过少量示例学习的方法，在码头VDS领域具备专长。借助这一方法，专家们仅需少量VDS示例即可学习到相关领域的知识。这些示例通过增强型检索生成（RAG）机制获取，从而有效缓解了对码头特定数据的高需求。此外，系统还建立了一个自动化的VDS设计流程，避免了额外的人工干预。在此流程中，借鉴LLM反思框架（Reflexion），构建了一个自我纠错循环，进一步提升了系统的自主性和可靠性。"
  },
  {
    "date": "2025-12-16",
    "title": "TorchTraceAP: A New Benchmark Dataset for Detecting Performance Anti-Patterns in Computer Vision Models",
    "authors": "Hanning Chen, Keyu Man, Kevin Zhu, Chenguang Zhu, Haonan Li, Tongbo Luo, Xizhou Feng, Wei Sun, Sreen Tallam, Mohsen Imani, Partha Kanuparthy",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14141v1",
    "source": "arXiv",
    "abstract": "Identifying and addressing performance anti-patterns in machine learning (ML) models is critical for efficient training and inference, but it typically demands deep expertise spanning system infrastructure, ML models and kernel development. While large tech companies rely on dedicated ML infrastructure engineers to analyze torch traces and benchmarks, such resource-intensive workflows are largely inaccessible to computer vision researchers in general. Among the challenges, pinpointing problematic trace segments within lengthy execution traces remains the most time-consuming task, and is difficult to automate with current ML models, including LLMs. In this work, we present the first benchmark dataset specifically designed to evaluate and improve ML models' ability to detect anti patterns in traces. Our dataset contains over 600 PyTorch traces from diverse computer vision models classification, detection, segmentation, and generation collected across multiple hardware platforms. We also propose a novel iterative approach: a lightweight ML model first detects trace segments with anti patterns, followed by a large language model (LLM) for fine grained classification and targeted feedback. Experimental results demonstrate that our method significantly outperforms unsupervised clustering and rule based statistical techniques for detecting anti pattern regions. Our method also effectively compensates LLM's limited context length and reasoning inefficiencies.",
    "title_zh": "TorchTraceAP：一种用于检测计算机视觉模型性能反模式的新基准数据集",
    "abstract_zh": "在机器学习（ML）模型中识别并解决性能反模式，对于实现高效训练和推理至关重要，但这一过程通常需要涵盖系统基础设施、机器学习模型以及内核开发等多方面的深厚专业知识。尽管大型科技公司依赖专门的机器学习基础设施工程师来分析PyTorch追踪数据和基准测试结果，但这类资源密集型的工作流程对大多数计算机视觉研究人员而言仍难以企及。其中，从冗长的执行追踪中精确定位存在问题的追踪片段，是最耗时的挑战之一，且目前的机器学习模型（包括大语言模型LLMs）也难以有效自动化完成该任务。\n\n在本研究中，我们首次提出一个专门用于评估和提升机器学习模型在追踪数据中检测反模式能力的基准数据集。该数据集包含超过600个来自多种计算机视觉模型（包括分类、检测、分割和生成任务）的PyTorch追踪记录，覆盖多个硬件平台。同时，我们提出一种新颖的迭代式方法：首先使用一个轻量级的机器学习模型识别出存在反模式的追踪片段，随后由大型语言模型（LLM）进行细粒度分类，并提供针对性反馈。实验结果表明，我们的方法在检测反模式区域方面显著优于无监督聚类和基于规则的统计技术。此外，该方法还能有效弥补LLM在上下文长度有限和推理效率低下方面的不足。"
  },
  {
    "date": "2025-12-16",
    "title": "Grammar Search for Multi-Agent Systems",
    "authors": "Mayank Singh, Vikas Yadav, Shiva Krishna Reddy Malay, Shravan Nayak, Sai Rajeswar, Sathwik Tejaswi Madhusudhan, Eduardo Blanco",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14079v1",
    "source": "arXiv",
    "abstract": "Automatic search for Multi-Agent Systems has recently emerged as a key focus in agentic AI research. Several prior approaches have relied on LLM-based free-form search over the code space. In this work, we propose a more structured framework that explores the same space through a fixed set of simple, composable components. We show that, despite lacking the generative flexibility of LLMs during the candidate generation stage, our method outperforms prior approaches on four out of five benchmarks across two domains: mathematics and question answering. Furthermore, our method offers additional advantages, including a more cost-efficient search process and the generation of modular, interpretable multi-agent systems with simpler logic.",
    "title_zh": "多智能体系统中的语法搜索",
    "abstract_zh": "自动搜索多智能体系统最近已成为代理型人工智能研究中的一个关键焦点。此前的多种方法依赖于基于大语言模型（LLM）在代码空间中的自由形式搜索。在本工作中，我们提出了一种更为结构化的框架，通过一组固定且可组合的简单组件来探索相同的空间。尽管在候选生成阶段缺乏LLM所具有的生成灵活性，我们的方法在两个领域——数学和问答——的五个基准测试中，有四个表现优于先前的方法。此外，该方法还具有其他优势，包括更高效的搜索过程，以及生成模块化、可解释且逻辑更简单的多智能体系统。"
  },
  {
    "date": "2025-12-16",
    "title": "ReflCtrl: Controlling LLM Reflection via Representation Engineering",
    "authors": "Ge Yan, Chung-En Sun, Tsui-Wei, Weng",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.13979v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) with Chain-of-Thought (CoT) reasoning have achieved strong performance across diverse tasks, including mathematics, coding, and general reasoning. A distinctive ability of these reasoning models is self-reflection: the ability to review and revise previous reasoning steps. While self-reflection enhances reasoning performance, it also increases inference cost. In this work, we study self-reflection through the lens of representation engineering. We segment the model's reasoning into steps, identify the steps corresponding to reflection, and extract a reflection direction in the latent space that governs this behavior. Using this direction, we propose a stepwise steering method that can control reflection frequency. We call our framework ReflCtrl. Our experiments show that (1) in many cases reflections are redundant, especially in stronger models (in our experiments, we can save up to 33.6 percent of reasoning tokens while preserving performance), and (2) the model's reflection behavior is highly correlated with an internal uncertainty signal, implying self-reflection may be controlled by the model's uncertainty.",
    "title_zh": "ReflCtrl：通过表示工程控制大语言模型的反思",
    "abstract_zh": "具有思维链（Chain-of-Thought, CoT）推理能力的大语言模型在数学、编程和通用推理等多种任务中表现出色。这类推理模型的一个显著能力是自我反思：即能够回顾并修正先前的推理步骤。尽管自我反思能提升推理性能，但也会增加推理成本。在本研究中，我们从表示工程的角度探讨自我反思机制。我们将模型的推理过程分解为多个步骤，识别出与反思相关的步骤，并在隐空间中提取出控制这一行为的“反思方向”。基于该方向，我们提出一种分步引导方法，可灵活调控反思频率。我们将其框架命名为ReflCtrl。实验结果表明：（1）在许多情况下，反思是冗余的，尤其是在更强的模型中（在我们的实验中，可在保持性能不变的前提下最多节省33.6%的推理token）；（2）模型的反思行为与内部的不确定性信号高度相关，暗示自我反思可能由模型自身的不确定性所驱动。"
  },
  {
    "date": "2025-12-16",
    "title": "Step-Tagging: Toward controlling the generation of Language Reasoning Models through step monitoring",
    "authors": "Yannis Belkhiter, Seshu Tirupathi, Giulio Zizzo, John D. Kelleher",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14332v1",
    "source": "arXiv",
    "abstract": "The field of Language Reasoning Models (LRMs) has been very active over the past few years with advances in training and inference techniques enabling LRMs to reason longer, and more accurately. However, a growing body of studies show that LRMs are still inefficient, over-generating verification and reflection steps. To address this challenge, we introduce the Step-Tagging framework, a lightweight sentence-classifier enabling real-time annotation of the type of reasoning steps that an LRM is generating. To monitor reasoning behaviors, we introduced ReasonType: a novel taxonomy of reasoning steps. Building on this framework, we demonstrated that online monitoring of the count of specific steps can produce effective interpretable early stopping criteria of LRM inferences. We evaluate the Step-tagging framework on three open-source reasoning models across standard benchmark datasets: MATH500, GSM8K, AIME and non-mathematical tasks (GPQA and MMLU-Pro). We achieve 20 to 50\\% token reduction while maintaining comparable accuracy to standard generation, with largest gains observed on more computation-heavy tasks. This work offers a novel way to increase control over the generation of LRMs, and a new tool to study behaviors of LRMs.",
    "title_zh": "步骤标记：通过步骤监控实现对语言推理模型生成过程的控制",
    "abstract_zh": "近年来，语言推理模型（Language Reasoning Models, LRMs）领域发展迅速，得益于训练与推理技术的进步，LRMs 的推理能力在长度和准确性方面均显著提升。然而，越来越多的研究表明，LRMs 仍存在效率低下、过度生成验证与反思步骤的问题。为应对这一挑战，我们提出了“步骤标记”（Step-Tagging）框架，这是一种轻量级的句子分类器，可实时标注 LRM 生成的推理步骤类型。为了监控推理行为，我们引入了“ReasonType”——一种全新的推理步骤分类体系。基于该框架，我们证明了对特定推理步骤数量进行在线监测，能够有效生成可解释的早期停止准则，从而优化 LRM 的推理过程。我们在三个开源推理模型上，针对标准基准数据集（MATH500、GSM8K、AIME）以及非数学类任务（GPQA 和 MMLU-Pro）评估了该框架。实验结果表明，在保持与标准生成相当准确率的前提下，token 消耗减少了 20% 至 50%，且在计算开销较大的任务中取得了最显著的收益。本研究为增强对 LRM 生成过程的控制提供了新思路，并提供了一种研究 LRM 行为的新工具。"
  },
  {
    "date": "2025-12-16",
    "title": "Dynamic Context Selection for Retrieval-Augmented Generation: Mitigating Distractors and Positional Bias",
    "authors": "Malika Iratni, Mohand Boughanem, Taoufiq Dkaki",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14313v1",
    "source": "arXiv",
    "abstract": "Retrieval Augmented Generation (RAG) enhances language model performance by incorporating external knowledge retrieved from large corpora, which makes it highly suitable for tasks such as open domain question answering. Standard RAG systems typically rely on a fixed top k retrieval strategy, which can either miss relevant information or introduce semantically irrelevant passages, known as distractors, that degrade output quality. Additionally, the positioning of retrieved passages within the input context can influence the model attention and generation outcomes. Context placed in the middle tends to be overlooked, which is an issue known as the \"lost in the middle\" phenomenon. In this work, we systematically analyze the impact of distractors on generation quality, and quantify their effects under varying conditions. We also investigate how the position of relevant passages within the context window affects their influence on generation. Building on these insights, we propose a context-size classifier that dynamically predicts the optimal number of documents to retrieve based on query-specific informational needs. We integrate this approach into a full RAG pipeline, and demonstrate improved performance over fixed k baselines.",
    "title_zh": "用于检索增强生成的动态上下文选择：缓解干扰项和位置偏差",
    "abstract_zh": "检索增强生成（Retrieval Augmented Generation, RAG）通过从大规模语料库中检索外部知识来提升语言模型的性能，因此在开放域问答等任务中表现出色。然而，传统的RAG系统通常采用固定的前k个文档检索策略，这可能导致遗漏相关信息，或引入语义无关的干扰段落（称为“干扰项”），从而降低生成结果的质量。此外，检索到的段落在输入上下文中的位置也会影响模型的关注度和生成效果。位于中间位置的内容往往被忽视，这一现象被称为“中间丢失”问题。\n\n在本研究中，我们系统地分析了干扰项对生成质量的影响，并量化了其在不同条件下的作用程度。同时，我们探究了相关段落在上下文窗口中所处位置对其生成影响的作用机制。基于这些发现，我们提出了一种上下文规模分类器，能够根据查询的具体信息需求动态预测最优的文档检索数量。我们将该方法集成到完整的RAG流程中，并在实验中证明其性能优于固定k值的基线方法。"
  },
  {
    "date": "2025-12-16",
    "title": "PentestEval: Benchmarking LLM-based Penetration Testing with Modular and Stage-Level Design",
    "authors": "Ruozhao Yang, Mingfei Cheng, Gelei Deng, Tianwei Zhang, Junjie Wang, Xiaofei Xie",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14233v1",
    "source": "arXiv",
    "abstract": "Penetration testing is essential for assessing and strengthening system security against real-world threats, yet traditional workflows remain highly manual, expertise-intensive, and difficult to scale. Although recent advances in Large Language Models (LLMs) offer promising opportunities for automation, existing applications rely on simplistic prompting without task decomposition or domain adaptation, resulting in unreliable black-box behavior and limited insight into model capabilities across penetration testing stages. To address this gap, we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision. PentestEval integrates expert-annotated ground truth with a fully automated evaluation pipeline across 346 tasks covering all stages in 12 realistic vulnerable scenarios. Our stage-level evaluation of 9 widely used LLMs reveals generally weak performance and distinct limitations across the stages of penetration-testing workflow. End-to-end pipelines reach only 31% success rate, and existing LLM-powered systems such as PentestGPT, PentestAgent, and VulnBot exhibit similar limitations, with autonomous agents failing almost entirely. These findings highlight that autonomous penetration testing demands stronger structured reasoning, where modularization enhances each individual stage and improves overall performance. PentestEval provides the foundational benchmark needed for future research on fine-grained, stage-level evaluation, paving the way toward more reliable LLM-based automation.",
    "title_zh": "PentestEval：基于模块化与阶段级设计的LLM渗透测试基准评估",
    "abstract_zh": "渗透测试对于评估和增强系统安全以应对现实世界威胁至关重要，但传统的测试流程仍高度依赖人工操作，需要大量专业知识且难以扩展。尽管大型语言模型（LLMs）的最新进展为自动化带来了潜在机遇，但现有应用大多仅采用简单的提示策略，缺乏任务分解与领域适应，导致模型行为不可靠、呈现“黑箱”特性，并在渗透测试各阶段中难以提供深入洞察。为填补这一空白，我们提出PentestEval——首个针对渗透测试六个分解阶段的综合性基准评测体系，包括：信息收集、弱点搜集与筛选、攻击决策、漏洞利用生成与修订。PentestEval结合专家标注的真实答案，构建了覆盖12个真实漏洞场景下全部阶段的346项任务，并实现了全流程自动化评估。对9种广泛使用的LLM进行分阶段评估发现，其整体表现普遍较弱，且在渗透测试工作流的不同阶段存在显著局限性。端到端的自动化流程成功率仅为31%，而现有的LLM驱动系统如PentestGPT、PentestAgent和VulnBot也表现出类似缺陷，自主代理几乎完全失效。这些结果表明，实现自主渗透测试需要更强的结构化推理能力，通过模块化设计优化每个独立阶段，才能有效提升整体性能。PentestEval为未来研究提供了细粒度、分阶段评估的基础基准，推动更可靠、更高效的LLM自动化渗透测试发展。"
  },
  {
    "date": "2025-12-16",
    "title": "Fast and Accurate Causal Parallel Decoding using Jacobi Forcing",
    "authors": "Lanxiang Hu, Siqi Kou, Yichao Fu, Samyam Rajbhandari, Tajana Rosing, Yuxiong He, Zhijie Deng, Hao Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14681v1",
    "source": "arXiv",
    "abstract": "Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance. Based on Jacobi Forcing Models' trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing.",
    "title_zh": "使用雅可比强制的快速准确因果并行解码",
    "abstract_zh": "多标记生成已成为加速基于Transformer的大规模模型推理的一种有前景的范式。近期研究主要探索扩散型大语言模型（dLLMs）以实现并行解码，从而降低推理延迟。为了达到自回归（AR）模型的生成质量，许多方法将AR模型改造为dLLMs，以支持并行解码。然而，由于预训练与后训练之间的分布不匹配，这些方法相较于AR模型所能带来的加速有限。具体而言，后训练阶段使用的掩码数据分布与预训练阶段所见的真实世界数据分布存在显著差异；同时，dLLMs依赖双向注意力机制，这与预训练过程中学习到的因果先验相冲突，阻碍了精确KV缓存复用的整合。\n\n为解决这一问题，我们提出了**雅可比强制（Jacobi Forcing）**——一种渐进式知识蒸馏范式。在此范式下，模型在自身生成的并行解码轨迹上进行训练，逐步将AR模型转化为高效的并行解码器，同时保留其预训练时的因果推理特性。基于该范式的模型——雅可比强制模型（Jacobi Forcing Model），在代码和数学基准测试中实现了3.8倍的墙钟时间加速，且性能损失极小。\n\n进一步地，基于雅可比强制模型的轨迹特性，我们引入了**多块解码与拒绝回收机制**，使每轮迭代中的token接受数量最高提升4.5倍，壁钟时间加速接近4.0倍，有效通过增加计算开销来换取更低的推理延迟。\n\n我们的代码已开源，地址为：https://github.com/hao-ai-lab/JacobiForcing。"
  },
  {
    "date": "2025-12-16",
    "title": "Professional Software Developers Don't Vibe, They Control: AI Agent Use for Coding in 2025",
    "authors": "Ruanqianqian Huang, Avery Reyna, Sorin Lerner, Haijun Xia, Brian Hempel",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14012v1",
    "source": "arXiv",
    "abstract": "The rise of AI agents is transforming how software can be built. The promise of agents is that developers might write code quicker, delegate multiple tasks to different agents, and even write a full piece of software purely out of natural language. In reality, what roles agents play in professional software development remains in question. This paper investigates how experienced developers use agents in building software, including their motivations, strategies, task suitability, and sentiments. Through field observations (N=13) and qualitative surveys (N=99), we find that while experienced developers value agents as a productivity boost, they retain their agency in software design and implementation out of insistence on fundamental software quality attributes, employing strategies for controlling agent behavior leveraging their expertise. In addition, experienced developers feel overall positive about incorporating agents into software development given their confidence in complementing the agents' limitations. Our results shed light on the value of software development best practices in effective use of agents, suggest the kinds of tasks for which agents may be suitable, and point towards future opportunities for better agentic interfaces and agentic use guidelines.",
    "title_zh": "专业的软件开发人员不靠感觉，而是掌控：2025年AI代理在编程中的应用",
    "abstract_zh": "AI代理的兴起正在改变软件构建的方式。代理的潜力在于，开发者可能能够更快地编写代码，将多个任务分配给不同的代理，甚至仅通过自然语言就能完成整套软件的开发。然而，在现实中，代理在专业软件开发中究竟扮演何种角色仍存疑问。本文通过实地观察（N=13）和定性调查（N=99），研究了经验丰富的开发者如何在软件开发中使用代理，包括他们的动机、策略、任务适用性以及情感态度。研究发现，尽管经验丰富的开发者普遍认为代理能显著提升生产力，但他们仍坚持在软件设计与实现中保持自主性，以确保基本的软件质量属性；他们利用自身专业知识，采取策略来控制代理的行为。此外，由于对代理局限性的充分认知，开发者总体上对将代理融入软件开发持积极态度。本研究揭示了软件开发最佳实践在有效使用代理中的价值，指出了代理可能适用的任务类型，并为未来更优的代理界面及代理使用指南提供了方向。"
  },
  {
    "date": "2025-12-16",
    "title": "Aligning Security Compliance and DevOps: A Longitudinal Study",
    "authors": "Fabiola Moyón, Florian Angermeir, Daniel Mendez, Tony Gorschek, Markus Voggenreiter, Pierre-Louis Bonvin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14453v1",
    "source": "arXiv",
    "abstract": "Companies adopt agile methodologies and DevOps to facilitate efficient development and deployment of software-intensive products. This, in turn, introduces challenges in relation to security standard compliance traditionally following a more linear workflow. This is especially a challenge for the engineering of products and services associated with critical infrastructures. To support companies in their transition towards DevOps, this paper presents an adaptation of DevOps according to security regulations and standards. We report on our longitudinal study at Siemens AG, consisting of several individual sub-studies in the inception, validation, and initial adoption of our framework based on RefA as well as the implications for practice. RefA is a prescriptive model of a security compliant DevOps lifecycle based on the IEC 62443-4-1 standard. The overall framework is aimed at professionals, not only security experts, being able to use it on implementing DevOps processes while remaining compliant with security norms. We demonstrate how RefA facilitates the transfer of security compliance knowledge to product development teams. This knowledge transfer supports the agility aim of ensuring that cross-functional teams have all the skills needed to deliver the compliant products.",
    "title_zh": "安全合规与DevOps的协同：一项纵向研究",
    "abstract_zh": "企业采用敏捷方法和DevOps，以促进软件密集型产品的高效开发与部署。然而，这一转变也带来了与传统线性工作流程相比的安全标准合规性挑战，尤其在涉及关键基础设施的产品和服务工程中更为突出。为支持企业向DevOps的转型，本文提出了一种基于安全法规与标准的DevOps适应性改进方案。我们报告了在西门子集团开展的纵向研究，该研究包括多个独立的子研究，涵盖框架（基于RefA）的初步设计、验证及初期应用阶段，并探讨了其对实践的启示。RefA是一种基于IEC 62443-4-1标准的、具有指导性的安全合规DevOps生命周期模型。该整体框架旨在面向广大专业人员，而不仅限于安全专家，使其能够在实施DevOps流程的同时，仍能保持对安全规范的合规性。本文展示了RefA如何促进安全合规知识向产品开发团队的有效传递，从而支持敏捷目标——确保跨职能团队具备交付合规产品所需的所有技能。"
  },
  {
    "date": "2025-12-16",
    "title": "Seismology modeling agent: A smart assistant for geophysical researchers",
    "authors": "Yukun Ren, Siwei Yu, Kai Chen, Jianwei Ma",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14429v1",
    "source": "arXiv",
    "abstract": "To address the steep learning curve and reliance on complex manual file editing and command-line operations in the traditional workflow of the mainstream open-source seismic wave simulation software SPECFEM, this paper proposes an intelligent, interactive workflow powered by Large Language Models (LLMs). We introduce the first Model Context Protocol (MCP) server suite for SPECFEM (supporting 2D, 3D Cartesian, and 3D Globe versions), which decomposes the entire simulation process into discrete, agent-executable tools spanning from parameter generation and mesh partitioning to solver execution and visualization. This approach enables a paradigm shift from file-driven to intent-driven conversational interactions. The framework supports both fully automated execution and human-in-the-loop collaboration, allowing researchers to guide simulation strategies in real time and retain scientific decision-making authority while significantly reducing tedious low-level operations. Validated through multiple case studies, the workflow operates seamlessly in both autonomous and interactive modes, yielding high-fidelity results consistent with standard baselines. As the first application of MCP technology to computational seismology, this study significantly lowers the entry barrier, enhances reproducibility, and offers a promising avenue for advancing computational geophysics toward AI-assisted and automated scientific research. The complete source code is available at https://github.com/RenYukun1563/specfem-mcp.",
    "title_zh": "地震学建模助手：为地球物理研究人员提供的智能助手",
    "abstract_zh": "为应对主流开源地震波模拟软件SPECFEM传统工作流中陡峭的学习曲线以及对复杂的手动文件编辑和命令行操作的高度依赖，本文提出了一种由大型语言模型（LLMs）驱动的智能、交互式工作流。我们首次推出了面向SPECFEM的模型上下文协议（MCP）服务器套件，支持2D、3D笛卡尔坐标系及3D球面版本，将整个模拟过程分解为一系列可由智能体执行的离散工具，涵盖参数生成、网格划分、求解器执行到可视化等环节。该方法实现了从“文件驱动”向“意图驱动”的对话式交互范式转变。该框架既支持完全自动化执行，也支持人机协同模式，使研究人员能够实时引导模拟策略，在显著减少繁琐底层操作的同时，仍保有对科学决策的主导权。通过多个案例研究验证，该工作流在自主与交互两种模式下均能无缝运行，结果精度高且与标准基准一致。作为MCP技术首次应用于计算地震学的研究，本工作显著降低了入门门槛，提升了研究可重复性，并为计算地球物理学迈向AI辅助与自动化科研提供了极具前景的发展路径。完整源代码已公开，获取地址为：https://github.com/RenYukun1563/specfem-mcp。"
  },
  {
    "date": "2025-12-16",
    "title": "Gödel's Poetry",
    "authors": "Kelly J. Davis",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14252v1",
    "source": "arXiv",
    "abstract": "Formal, automated theorem proving has long been viewed as a challenge to artificial intelligence. We introduce here a new approach to computer theorem proving, one that employs specialized language models for Lean4 proof generation combined with recursive decomposition of difficult theorems into simpler entailing propositions. These models are coordinated through a multi-agent architecture that orchestrates autoformalization (if required), proof generation, decomposition of difficult theorems into simpler entailing propositions, and recursive proof (and/or decomposition) of these propositions. Without decomposition, we achieve a 90.4% pass rate on miniF2F. With decomposition, this is significantly improved. A key technical contribution lies in our extension of the Kimina Lean Server with abstract syntax tree (AST) parsing capabilities to facilitate automated, recursive proof decomposition. The system is made available on PyPI as goedels-poetry (at https://pypi.org/project/goedels-poetry ), and the open-source implementation KellyJDavis/goedels-poetry (at https://github.com/KellyJDavis/goedels-poetry ) facilitates both adaptation to alternative language models and extension with custom functionality.",
    "title_zh": "哥德尔的诗",
    "abstract_zh": "形式化、自动化的定理证明长期以来被视为人工智能领域的一项挑战。本文提出了一种计算机定理证明的新方法，该方法结合了针对Lean4证明生成的专用语言模型，并通过递归分解复杂定理为更简单的蕴含命题来实现突破。这些模型通过多智能体架构进行协调，该架构能够统筹自动形式化（如需）、证明生成、将复杂定理分解为较简单蕴含命题，以及对这些命题进行递归证明（和/或进一步分解）。在不使用分解的情况下，系统在miniF2F基准测试中达到了90.4%的通过率；而引入分解后，性能得到显著提升。本研究的关键技术贡献在于我们扩展了Kimina Lean Server，增加了抽象语法树（AST）解析功能，从而支持自动化、递归式的证明分解。该系统已发布于PyPI，项目名为goedels-poetry（访问地址：https://pypi.org/project/goedels-poetry），其开源实现KellyJDavis/goedels-poetry（访问地址：https://github.com/KellyJDavis/goedels-poetry）不仅便于适配其他语言模型，也支持自定义功能的扩展。"
  },
  {
    "date": "2025-12-16",
    "title": "A Deep Dive into Function Inlining and its Security Implications for ML-based Binary Analysis",
    "authors": "Omar Abusabha, Jiyong Uhm, Tamer Abuhmed, Hyungjoon Koo",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14045v1",
    "source": "arXiv",
    "abstract": "A function inlining optimization is a widely used transformation in modern compilers, which replaces a call site with the callee's body in need. While this transformation improves performance, it significantly impacts static features such as machine instructions and control flow graphs, which are crucial to binary analysis. Yet, despite its broad impact, the security impact of function inlining remains underexplored to date. In this paper, we present the first comprehensive study of function inlining through the lens of machine learning-based binary analysis. To this end, we dissect the inlining decision pipeline within the LLVM's cost model and explore the combinations of the compiler options that aggressively promote the function inlining ratio beyond standard optimization levels, which we term extreme inlining. We focus on five ML-assisted binary analysis tasks for security, using 20 unique models to systematically evaluate their robustness under extreme inlining scenarios. Our extensive experiments reveal several significant findings: i) function inlining, though a benign transformation in intent, can (in)directly affect ML model behaviors, being potentially exploited by evading discriminative or generative ML models; ii) ML models relying on static features can be highly sensitive to inlining; iii) subtle compiler settings can be leveraged to deliberately craft evasive binary variants; and iv) inlining ratios vary substantially across applications and build configurations, undermining assumptions of consistency in training and evaluation of ML models.",
    "title_zh": "深入探讨函数内联及其对基于机器学习的二进制分析的安全性影响",
    "abstract_zh": "函数内联优化是现代编译器中广泛采用的一种变换技术，它将调用点替换为被调用函数的函数体。尽管这种变换能够提升程序性能，但其对静态特征（如机器指令和控制流图）产生了显著影响，而这些特征对于二进制分析至关重要。然而，尽管该技术影响广泛，其在安全方面的潜在影响至今仍缺乏深入研究。本文首次从基于机器学习的二进制分析视角，系统地研究了函数内联的影响。为此，我们剖析了LLVM成本模型中的内联决策流程，并探索了那些激进地提高函数内联比例、超越标准优化级别的编译器选项组合，我们称之为“极端内联”。我们聚焦于五类面向安全的机器学习辅助二进制分析任务，使用20个不同的模型，系统评估它们在极端内联场景下的鲁棒性。我们的大量实验揭示了若干重要发现：i）尽管函数内联在意图上是无害的，但它可能直接或间接地影响机器学习模型的行为，从而被用于规避判别型或生成型机器学习模型；ii）依赖静态特征的机器学习模型对内联极为敏感；iii）细微的编译器设置可被刻意利用，以构造出逃避检测的二进制变体；iv）不同应用程序和构建配置下的内联比例存在显著差异，这破坏了机器学习模型训练与评估中关于一致性的基本假设。"
  },
  {
    "date": "2025-12-16",
    "title": "Intention Chain-of-Thought Prompting with Dynamic Routing for Code Generation",
    "authors": "Shen Li, Li Huang, Shaoxiong Zhan, Weifeng Sun, Tao Yin, Zhongxin Liu, Meng Yan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14048v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) exhibit strong generative capabilities and have shown great potential in code generation. Existing chain-of-thought (CoT) prompting methods enhance model reasoning by eliciting intermediate steps, but suffer from two major limitations: First, their uniform application tends to induce overthinking on simple tasks. Second, they lack intention abstraction in code generation, such as explicitly modeling core algorithmic design and efficiency, leading models to focus on surface-level structures while neglecting the global problem objective. Inspired by the cognitive economy principle of engaging structured reasoning only when necessary to conserve cognitive resources, we propose RoutingGen, a novel difficulty-aware routing framework that dynamically adapts prompting strategies for code generation. For simple tasks, it adopts few-shot prompting; for more complex ones, it invokes a structured reasoning strategy, termed Intention Chain-of-Thought (ICoT), which we introduce to guide the model in capturing task intention, such as the core algorithmic logic and its time complexity. Experiments across three models and six standard code generation benchmarks show that RoutingGen achieves state-of-the-art performance in most settings, while reducing total token usage by 46.37% on average across settings. Furthermore, ICoT outperforms six existing prompting baselines on challenging benchmarks.",
    "title_zh": "意图链式思维提示与动态路由在代码生成中的应用",
    "abstract_zh": "大型语言模型（LLMs）展现出强大的生成能力，在代码生成方面具有巨大潜力。现有的思维链（Chain-of-Thought, CoT）提示方法通过激发中间推理步骤来增强模型的推理能力，但存在两大主要局限：首先，其统一的应用方式容易导致在简单任务上产生过度思考；其次，代码生成过程中缺乏对意图的抽象建模，例如未能显式刻画核心算法设计与效率，致使模型过于关注表面结构而忽视全局问题目标。受认知经济性原则启发——即仅在必要时才启用结构化推理以节省认知资源，我们提出 RoutingGen，一种新型的难度感知路由框架，可动态适配代码生成的提示策略。对于简单任务，该框架采用少样本提示；而对于更复杂的任务，则调用一种结构化推理策略，称为意图思维链（Intention Chain-of-Thought, ICoT），以引导模型捕捉任务意图，如核心算法逻辑及其时间复杂度。在三个模型和六个标准代码生成基准上的实验表明，RoutingGen 在大多数场景下达到了当前最优性能，同时在各设置下平均减少了 46.37% 的总 token 使用量。此外，ICoT 在挑战性基准测试中也优于六种现有的提示基线方法。"
  },
  {
    "date": "2025-12-16",
    "title": "PerfCoder: Large Language Models for Interpretable Code Performance Optimization",
    "authors": "Jiuding Yang, Shengyao Lu, Hongxuan Liu, Shayan Shirahmad Gale Bagi, Zahra Fazel, Tomasz Czajkowski, Di Niu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14018v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) have achieved remarkable progress in automatic code generation, yet their ability to produce high-performance code remains limited--a critical requirement in real-world software systems. We argue that current LLMs struggle not only due to data scarcity but, more importantly, because they lack supervision that guides interpretable and effective performance improvements. In this work, we introduce PerfCoder, a family of LLMs specifically designed to generate performance-enhanced code from source code via interpretable, customized optimizations. PerfCoder is fine-tuned on a curated collection of real-world optimization trajectories with human-readable annotations, and preference-aligned by reinforcement fine-tuning using runtime measurements, enabling it to propose input-specific improvement strategies and apply them directly without relying on iterative refinement. On the PIE code performance benchmark, PerfCoder surpasses all existing models in both runtime speedup and effective optimization rate, demonstrating that performance optimization cannot be achieved by scale alone but requires optimization stratetgy awareness. In addition, PerfCoder can generate interpretable feedback about the source code, which, when provided as input to a larger LLM in a planner-and-optimizer cooperative workflow, can further improve outcomes. Specifically, we elevate the performance of 32B models and GPT-5 to new levels on code optimization, substantially surpassing their original performance.",
    "title_zh": "PerfCoder：用于可解释代码性能优化的大型语言模型",
    "abstract_zh": "大型语言模型（LLMs）在自动代码生成方面取得了显著进展，但其生成高性能代码的能力仍然有限——而这正是现实世界软件系统中的关键需求。我们认为，当前LLMs的局限性不仅源于数据稀缺，更关键的是缺乏能够引导可解释且高效的性能优化的监督机制。在本研究中，我们提出了PerfCoder，一个专为通过可解释、定制化优化从源代码生成性能增强代码而设计的LLM系列。PerfCoder在经过精心筛选的真实世界优化轨迹数据集上进行微调，并通过基于运行时测量结果的强化学习微调实现偏好对齐，使其能够提出针对具体输入的优化策略并直接应用，无需依赖迭代 refinements（精炼）。在PIE代码性能基准测试中，PerfCoder在运行时加速和有效优化率两个方面均超越了所有现有模型，证明了仅靠规模无法实现性能优化，必须具备对优化策略的认知能力。此外，PerfCoder还能生成关于源代码的可解释反馈；当这些反馈作为输入提供给更大的LLM，在“规划-优化”协作工作流中使用时，可进一步提升优化效果。具体而言，我们成功将32B模型和GPT-5在代码优化方面的性能提升至全新水平，显著超越了它们原有的表现。"
  },
  {
    "date": "2025-12-16",
    "title": "HAL -- An Open-Source Framework for Gate-Level Netlist Analysis",
    "authors": "Julian Speith, Jörn Langheinrich, Marc Fyrbiak, Max Hoffmann, Sebastian Wallat, Simon Klix, Nils Albartus, René Walendy, Steffen Becker, Christof Paar",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14139v1",
    "source": "arXiv",
    "abstract": "HAL is an open-source framework for gate-level netlist analysis, an integral step in hardware reverse engineering. It provides analysts with an interactive GUI, an extensible plugin system, and APIs in both C++ and Python for rapid prototyping and automation. In addition, HAL ships with plugins for word-level modularization, cryptographic analysis, simulation, and graph-based exploration. Since its release in 2019, HAL has become widely adopted in academia, industry, government, and teaching. It underpins at least 23 academic publications, is taught in hands-on trainings, conference tutorials, and university classes, and has collected over 680 stars and 86 forks on GitHub. By enabling accessible and reproducible hardware reverse engineering research, HAL has significantly advanced the field and the understanding of real-world capabilities and threats.",
    "title_zh": "HAL——用于门级网表分析的开源框架",
    "abstract_zh": "HAL 是一个用于门级网表分析的开源框架，是硬件逆向工程中不可或缺的一步。它为分析人员提供了交互式图形界面、可扩展的插件系统，以及 C++ 和 Python 两种语言的 API，支持快速原型设计与自动化操作。此外，HAL 还自带多个插件，涵盖字级模块化、密码学分析、仿真和基于图的探索等功能。自 2019 年发布以来，HAL 已在学术界、工业界、政府部门及教学领域得到广泛应用。它已支撑至少 23 篇学术论文，被纳入实践培训、会议教程和大学课程，并在 GitHub 上获得了超过 680 颗星标和 86 次分叉。通过推动可访问且可复现的硬件逆向工程研究，HAL 显著促进了该领域的发展，加深了人们对现实世界中硬件能力与威胁的理解。"
  },
  {
    "date": "2025-12-16",
    "title": "Teralizer: Semantics-Based Test Generalization from Conventional Unit Tests to Property-Based Tests",
    "authors": "Johann Glock, Clemens Bauer, Martin Pinzger",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.14475v1",
    "source": "arXiv",
    "abstract": "Conventional unit tests validate single input-output pairs, leaving most inputs of an execution path untested. Property-based testing addresses this shortcoming by generating multiple inputs satisfying properties but requires significant manual effort to define properties and their constraints. We propose a semantics-based approach that automatically transforms unit tests into property-based tests by extracting specifications from implementations via single-path symbolic analysis. We demonstrate this approach through Teralizer, a prototype for Java that transforms JUnit tests into property-based jqwik tests. Unlike prior work that generalizes from input-output examples, Teralizer derives specifications from program semantics. We evaluated Teralizer on three progressively challenging datasets. On EvoSuite-generated tests for EqBench and Apache Commons utilities, Teralizer improved mutation scores by 1-4 percentage points. Generalization of mature developer-written tests from Apache Commons utilities showed only 0.05-0.07 percentage points improvement. Analysis of 632 real-world Java projects from RepoReapers highlights applicability barriers: only 1.7% of projects completed the generalization pipeline, with failures primarily due to type support limitations in symbolic analysis and static analysis limitations in our prototype. Based on the results, we provide a roadmap for future work, identifying research and engineering challenges that need to be tackled to advance the field of test generalization. Artifacts available at: https://doi.org/10.5281/zenodo.17950381",
    "title_zh": "Teralizer：基于语义的测试泛化——从传统单元测试到基于性质的测试",
    "abstract_zh": "传统单元测试仅验证单一的输入-输出对，导致执行路径中的大部分输入未被测试。属性测试通过生成满足特定性质的多个输入来弥补这一缺陷，但需要大量手动工作来定义性质及其约束条件。我们提出一种基于语义的方法，通过单路径符号分析从实现中提取规范，自动将单元测试转换为属性测试。我们通过 Teralizer——一个针对 Java 的原型系统——展示了该方法的有效性，该系统可将 JUnit 测试自动转化为基于 jqwik 的属性测试。与以往从输入-输出示例进行泛化的工作不同，Teralizer 是从程序语义中推导出规范。我们在三个逐步增加难度的数据集上评估了 Teralizer：在 EvoSuite 生成的 EqBench 和 Apache Commons 工具类测试用例上，Teralizer 将突变覆盖率提升了 1 至 4 个百分点；而在成熟开发者编写的 Apache Commons 工具类测试用例上，提升幅度仅为 0.05 至 0.07 个百分点。对 RepoReapers 中 632 个真实世界 Java 项目的分析揭示了应用上的障碍：仅有 1.7% 的项目成功完成了泛化流程，失败主要归因于符号分析中的类型支持限制以及我们原型系统在静态分析方面的局限性。基于这些结果，我们提出了未来工作的路线图，指出了推动测试泛化领域发展所需解决的研究与工程挑战。相关工具和数据可在以下链接获取：https://doi.org/10.5281/zenodo.17950381"
  },
  {
    "date": "2025-12-16",
    "title": "Integration of Machine Learning with Quantum Computing: Compilation of Quantum Circuit for Efficacy on Noisy Intermediate-Scale Quantum Devices",
    "authors": "M. Praba, Ahmad Abdelhafiz Ali Samhan, V. Samuthira Pandi, A. Sowmiya, Navyatha Ravi, G. Karthikeyan",
    "publish": "2025 5th Asian Conference on Innovation in Technology (ASIANCON)",
    "url": "https://doi.org/10.1109/asiancon66527.2025.11281256",
    "source": "IEEE",
    "abstract": "Hybridizing machine learning (ML) with quantum computing has become an attractive strategy to increase computational power on Noisy Intermediate Scale Quantum (NISQ) devices. Although these can be potentially quantum advantage devices, their qubit numbers and noise are bounded and there are challenges when it comes to practical quantum circuit realization. This work concentrates on the creation of optimized quantum circuits via ML-based strategies, to enhance the performance of the circuits, to reduce the error rate and to accommodate the particular noise profile of the hardware of NISQ devices. The resulting suboptimal mappings with little or no consideration of noise and hardware topology in traditional quantum compilers can ruin the fidelity of quantum algorithm. Leveraging machine learning, and specifically reinforcement and supervised learning methods, our method learns to optimize compilation strategy for specific quantum hardware architecture. The paper proposes a framework that uses machine learning models to predict qubit mappings, gate decompositions, and transpilation paths that minimize gate depth and error accumulation. We also investigate transfer learning methods for propagating the learned compilation strategies to novel NISQ machines. Such methods are empirically validated on both simulated and real quantum hardware backends, providing evidence of circuit depth, execution time and error reductions. The results show that ML-assisted compilation can achieve better success probabilities for benchmark quantum algorithms like VQE and QAOA than conventional compilers. Furthermore, the work examines the interpretability of ML models providing hardware-aware optimization recommendations and understanding the noise patterns and methods of error correction. Our work highlights the key role of ML for transferring between theoretical quantum algorithms and their practical NISQ implementations. This integration boosts the immediate applicability of existing quantum devices and provides a basis for adaptive and scalable likelihood-based compilation for the era of fault-tolerant quantum computation. Our results motivate further investigation of hybrid classical-quantum workflows, in which ML iteratively improves compilation in accordance with changing hardware noise and capabilities. By improving quantum circuit compilation onNISQ computers, this work is taking steps toward the realization of quantum advantage in application scenarios such as optimization, cryptography, and quantum simulation without needing to wait for fully fault-tolerant large-scale quantum computers.",
    "title_zh": "机器学习与量子计算的融合：面向噪声中等规模量子设备的量子线路编译优化",
    "abstract_zh": "将机器学习（ML）与量子计算相结合，已成为提升噪声中等规模量子（NISQ）设备计算能力的一种极具吸引力的策略。尽管这些设备在理论上可能具备量子优势，但其量子比特数量和噪声水平均受到限制，在实际量子电路实现方面仍面临诸多挑战。本文聚焦于通过基于机器学习的策略优化量子电路设计，以提升电路性能、降低错误率，并适配NISQ设备特有的噪声特性。传统量子编译器生成的次优映射往往忽略噪声和硬件拓扑结构，严重损害了量子算法的保真度。本文利用机器学习技术，特别是强化学习与监督学习方法，使系统能够针对特定量子硬件架构学习并优化编译策略。论文提出了一种框架，使用机器学习模型预测最优的量子比特映射、门分解方式以及线路转换路径，从而最小化门深度和误差累积。此外，我们还研究了迁移学习方法，以将已学习的编译策略推广至新型NISQ设备。该方法在模拟环境和真实量子硬件后端上均进行了实证验证，结果表明其在电路深度、执行时间和错误率方面均有显著改善。实验结果显示，基于机器学习的编译方法在VQE和QAOA等基准量子算法上的成功概率，优于传统编译器。同时，本研究还探讨了机器学习模型的可解释性，提供了面向硬件的优化建议，并深入理解噪声模式及纠错机制。我们的工作凸显了机器学习在连接理论量子算法与其在NISQ设备上的实际实现中的关键作用。这种融合不仅提升了现有量子设备的即时可用性，也为容错量子计算时代下自适应、可扩展的概率化编译奠定了基础。研究结果激励我们进一步探索混合经典-量子工作流，其中机器学习能够根据硬件噪声和性能的变化持续迭代优化编译过程。通过改进NISQ计算机上的量子电路编译，本工作正朝着在优化、密码学和量子模拟等应用场景中实现量子优势迈出重要一步，而无需等待完全容错的大规模量子计算机的到来。"
  },
  {
    "date": "2025-12-16",
    "title": "Stratus Cloud: Security Expert’ An Automated Web Application Securtiy Assessment Tool",
    "authors": "Kanishka Jha, Shreya Limkar, Shubhangi Kharche, Sujit Londhe, Jailingam Santhanakumar, Ankit Sharma, Shreelakshmi Nair",
    "publish": "2025 5th Asian Conference on Innovation in Technology (ASIANCON)",
    "url": "https://doi.org/10.1109/asiancon66527.2025.11281235",
    "source": "IEEE",
    "abstract": "Stratus Cloud: Security Expert is a cloud-deployable, lightweight CLI tool that scans web application vulnerabilities automatically. It detects SQL Injection, XSS, insecure HTTP methods, and misconfigured directories through multi-threaded payload analysis. In contrast to AI-powered scanners, it keeps false positives below 5% and attains 92%+ accuracy. With subdomain enumeration, SSL analysis, WHOIS lookups, and port scanning features, Stratus offers a scalable, actionable, and accessible solution for security audits. Comparison with OWASP ZAP, Nikto, Arachni, and Nmap indicates that Stratus beats in terms of automation, scan time, and modular control. This work reports its architecture, comparative outcomes, performance assessment, and considers future enhancements.",
    "title_zh": "Stratus Cloud：安全专家——一款自动化的Web应用安全评估工具",
    "abstract_zh": "Stratus Cloud：安全专家是一款可部署于云端的轻量级命令行工具，能够自动扫描Web应用程序漏洞。它通过多线程载荷分析，检测SQL注入、跨站脚本（XSS）、不安全的HTTP方法以及配置错误的目录。与基于人工智能的扫描工具相比，Stratus的误报率低于5%，准确率超过92%。该工具还具备子域名枚举、SSL分析、WHOIS查询和端口扫描等功能，为安全审计提供了一种可扩展、可操作且易于访问的解决方案。与OWASP ZAP、Nikto、Arachni及Nmap等工具的对比表明，Stratus在自动化程度、扫描速度和模块化控制方面表现更优。本文报告了其架构设计、对比结果、性能评估，并探讨了未来的改进方向。"
  },
  {
    "date": "2025-12-16",
    "title": "Energy-Efficient Acceleration of Hash-Based Post-Quantum Cryptographic Schemes on Embedded Spatial Architectures",
    "authors": "Yanze Wu, Md Tanvir Arafin",
    "publish": "2025 34th International Conference on Parallel Architectures and Compilation Techniques (PACT)",
    "url": "https://doi.org/10.1109/pact65351.2025.00033",
    "source": "IEEE",
    "abstract": "This work introduces AXIOS, a novel spatial architecture for accelerating hash-based post-quantum cryptography (PQC) primitives. AXIOS demonstrates that structural regularities in hash-based algorithms can be efficiently mapped to spatial accelerators that support FPGA-based programming for granular control and coarse-grained reconfigurable arrays (CGRA) for repeated tasks. AXIOS selects the key generation task of the eXtended Markle Signature Scheme (XMSS), which embodies critical implementation challenges in modern hash-based (PQC) algorithms. The AXIOS implementation on AMD’s VCK190 platform demonstrates an $8.54 \\times$ improvement in runtime and a $71.65 \\times$ improvement in energy efficiency compared to a benchmark implementation on Intel’s Core $\\mathbf{i 9 - 1 4 9 0 0 K}$. AXIOS also breaks the current record of XMSS acceleration in terms of execution time on an embedded SoC or an FPGA platform. To our knowledge, this is the first efficient hardware implementation of compute-intensive hash-based PQC schemes in an embedded spatial architecture. Albeit complex, this FPGA+CGRA-based design is a promising step to support compute-intensive PQC applications at the edge. This work’s code and experimental artifacts are publicly available at https://github.com/SPIRE-GMU/AXIOS.",
    "title_zh": "基于嵌入式空间架构的哈希类后量子密码方案能效优化加速",
    "abstract_zh": "本文介绍了AXIOS，一种用于加速基于哈希的后量子密码学（PQC）原语的新颖空间架构。AXIOS证明了哈希算法中的结构规律可以高效地映射到支持FPGA编程的硬件加速器上，实现细粒度控制；同时也能利用粗粒度可重构阵列（CGRA）来处理重复性任务。AXIOS选取了扩展梅克尔签名方案（XMSS）中的密钥生成任务作为研究对象，该任务体现了现代基于哈希的PQC算法中关键的实现挑战。在AMD VCK190平台上的AXIOS实现相比Intel Core i9-14900K上的基准实现，运行时间提升了8.54倍，能效提高了71.65倍。此外，AXIOS在嵌入式SoC或FPGA平台上创造了XMSS加速执行时间的新纪录。据我们所知，这是首个在嵌入式空间架构中实现的高效计算密集型基于哈希的PQC方案硬件设计。尽管该FPGA+CGRA架构设计较为复杂，但其为在边缘端支持计算密集型PQC应用提供了极具前景的解决方案。本工作的代码及实验数据已公开发布于 https://github.com/SPIRE-GMU/AXIOS。"
  },
  {
    "date": "2025-12-16",
    "title": "NeuroHLS: A Flexible Framework for Accelerating SNNs on FPGAs",
    "authors": "Renan Carlos Gomes de Farias, Fernando Pedrazzi Pozzer, Mateus Beck Rutzig",
    "publish": "2025 XV Symposium on Computing Systems Engineering (SBESC)",
    "url": "https://doi.org/10.1109/sbesc68008.2025.11288819",
    "source": "IEEE",
    "abstract": "Field-Programmable Gate Arrays (FPGAs) have emerged as a promising platform for accelerating AI applications in IoT devices. In this context, Spiking Neural Networks (SNNs) have demonstrated superior energy efficiency compared to traditional artificial neural networks when implemented on FPGAs. Although various frameworks have been proposed for mapping SNNs onto FPGAs, none of them have leveraged High-Level Synthesis (HLS), which facilitates faster development cycles and greater design transparency. This work introduces NeuroHLS, an HLS-based framework for implementing SNNs on FPGAs. NeuroHLS offers both resource- and latency-optimized implementation strategies, and supports fine-grained parameter customization at the layer level. To validate the framework, a 784128-10 SNN was implemented and evaluated on the N-MNIST dataset. Performance was benchmarked against existing state-of-the-art frameworks. Experimental results demonstrate that both implementation strategies offered by NeuroHLS significantly outperform existing approaches in terms of inference latency and energy efficiency. In particular, one configuration achieved an inference latency of 0.8us and an average energy consumption of 2.16uJ per inference.",
    "title_zh": "NeuroHLS：一种用于在FPGA上加速SNN的灵活框架",
    "abstract_zh": "现场可编程门阵列（FPGAs）已成为加速物联网设备中人工智能应用的有前景平台。在此背景下，脉冲神经网络（SNNs）在FPGA上的实现展现出相较于传统人工神经网络更优异的能效表现。尽管已有多种框架被提出用于将SNN映射到FPGAs上，但尚未有框架充分利用高层次综合（HLS）技术，而HLS能够显著缩短开发周期并提高设计透明度。本文提出了一种基于HLS的SNN实现框架——NeuroHLS。该框架提供了资源优化与延迟优化两种实现策略，并支持在层级别进行细粒度的参数定制。为验证该框架的有效性，我们在N-MNIST数据集上实现并评估了一个784-128-10结构的SNN。性能与现有最先进的框架进行了对比。实验结果表明，NeuroHLS所提供的两种实现策略在推理延迟和能效方面均显著优于现有方法。特别是，其中一种配置实现了0.8微秒的推理延迟以及每推理平均2.16微焦耳的能量消耗。"
  },
  {
    "date": "2025-12-16",
    "title": "Accelerating High-Speed Interconnect Development with a Closed-Loop Design and Validation Framework",
    "authors": "Chiew Yee Ho, Jimmy Hsu, Brian Ho, Thonas Su, Ryan Chang, Vick Chuang, Colin Chen",
    "publish": "2025 20th International Microsystems, Packaging, Assembly and Circuits Technology Conference (IMPACT)",
    "url": "https://doi.org/10.1109/impact67645.2025.11281705",
    "source": "IEEE",
    "abstract": "As the datacenter industry rapidly grows and high-speed interconnect signal data rate increases to PCIe Gen7 with Pulse Amplitude Modulation (PAM4) encoding, the board design is crucial to ensure the quality of the high-speed signal data transmission. Conventionally, in platform design and qualification stage, System Design Verification (DV), Signal Integrity (SI) and Electrical Validation (EV) tests are conducted to ensure the quality of the high-speed signal data transmission on the specific platform design. However, due to the cost and time constraints, board manufacturers may decide to omit some of the testing which may result in late found issues after platform post-production launch, such as PCIe link stability or data integrity issues that could potentially cause an outage in the datacenter fleet services. In this paper, a closed loop design and validation methodology for datacenter platform design which includes feedback loop and correlation of test results and issues found in DV, SI and EV tests is proposed and detailed case studies with including design, SI and EV analysis and correlation will be provided.",
    "title_zh": "通过闭环设计与验证框架加速高速互连开发",
    "abstract_zh": "随着数据中心行业迅速发展，高速互连信号的数据速率已提升至PCIe Gen7，并采用脉冲幅度调制（PAM4）编码技术，因此PCB板的设计对于保障高速信号传输质量至关重要。传统上，在平台设计与认证阶段，通常会进行系统设计验证（DV）、信号完整性（SI）和电气验证（EV）测试，以确保特定平台设计下的高速信号传输质量。然而，由于成本和时间的限制，板卡制造商可能会选择省略部分测试，从而导致在平台投产后才发现问题，例如PCIe链路稳定性或数据完整性问题，这些问题可能引发数据中心集群服务中断。本文提出了一种针对数据中心平台设计的闭环设计与验证方法，该方法包含DV、SI和EV测试结果及发现的问题之间的反馈机制与关联分析，并通过详细的案例研究，展示设计、SI分析与EV验证之间的关联性与协同作用。"
  },
  {
    "date": "2025-12-16",
    "title": "Agentic Auto-Scheduling: An Experimental Study of LLM-Guided Loop Optimization",
    "authors": "Massinissa Merouani, Islem Kara Bernou, Riyadh Baghdadi",
    "publish": "2025 34th International Conference on Parallel Architectures and Compilation Techniques (PACT)",
    "url": "https://doi.org/10.1109/pact65351.2025.00027",
    "source": "IEEE",
    "abstract": "Automatic code optimization remains a difficult challenge, particularly for complex loop nests on modern hardware. This paper investigates a novel approach to code optimization where Large Language Models (LLMs) guide the process through a closed-loop interaction with a compiler. We present Compilot, an experimental framework that leverages off-theshelf LLMs, without any task-specific fine-tuning, as interactive optimization agents. ComPilot establishes a feedback loop where an LLM proposes transformations for a given loop nest to a compiler. The compiler attempts the transformations, reporting back legality status and measured speedup or slowdown. The LLM utilizes this concrete feedback to iteratively refine its optimization strategy. Our extensive evaluation across the PolyBench benchmark suite demonstrates the effectiveness of this zero-shot approach. ComPilot achieves geometric mean speedups of $2.66 x$ (single run) and $3.54 x$ (best-of- 5 runs) over the original code. Furthermore, ComPilot demonstrates competitive performance against the state-of-the-art Pluto polyhedral optimizer, outperforming it in many cases. This experimental study demonstrates that general-purpose LLMs can effectively guide the code optimization process when grounded by compiler feedback, opening promising research directions for agentic AI in code optimization.",
    "title_zh": "代理式自动调度：基于大模型引导的循环优化实验研究",
    "abstract_zh": "自动代码优化仍然是一个极具挑战性的问题，尤其是在现代硬件上的复杂循环嵌套场景中。本文提出了一种新颖的代码优化方法，即利用大型语言模型（LLMs）通过与编译器的闭环交互来引导优化过程。我们提出了ComPilot——一个实验性框架，该框架直接使用现成的大型语言模型，无需任何针对特定任务的微调，作为交互式的优化代理。ComPilot建立了一个反馈循环：LLM为给定的循环嵌套提出优化变换方案，编译器尝试执行这些变换，并返回合法性状态以及实测的加速或减速结果；随后，LLM根据这些具体的反馈信息，迭代地改进其优化策略。我们在PolyBench基准测试套件上进行了广泛的评估，结果表明这种零样本（zero-shot）方法具有显著有效性。ComPilot在单次运行下实现了原始代码2.66倍的几何平均加速，在五次运行中的最佳结果达到3.54倍。此外，ComPilot在多数情况下表现优于当前最先进的Pluto多面体优化器，展现出竞争力。本实验研究证明，当结合编译器反馈进行“接地”（grounded）时，通用型大型语言模型能够有效指导代码优化过程，为智能体式人工智能在代码优化领域的应用开辟了充满前景的研究方向。"
  },
  {
    "date": "2025-12-16",
    "title": "Novel Two-Stage GNN Approach For VLSI Floorplanning With ISPD-Inflated Data",
    "authors": "Harshada Hemant Vaidya, Vaidehi Deshmukh",
    "publish": "2025 5th Asian Conference on Innovation in Technology (ASIANCON)",
    "url": "https://doi.org/10.1109/asiancon66527.2025.11281078",
    "source": "IEEE",
    "abstract": "Physical design Floorplanning is a critical phase in VLSI, where joint optimization of design objectives such as Half-Perimeter Wirelength (HPWL), aspect ratio, displacement, and overflow is taken care of. This paper presents optimization of a critical phase of physical design of VLSI floorplanning, using key metrices such as Loss, Half-Perimeter Wire Length (HPWL), Aspect ratio, Displacement and Overflow using Two - stage Graph Neural Network (GNN) Model. In this approach the model learns placement trends directly by analyzing ISPD 2005 Aptec1 Inflated benchmark dataset. The results from Stage 1 and Stage 2 are evaluated to highlight the performance and convergence of the algorithm. The discussion on current limitations such as congestion handling and outline future integration paths with legalization and detailed placement tools is also mentioned.",
    "title_zh": "基于ISPD扩充数据的VLSI布局规划新型两阶段图神经网络方法",
    "abstract_zh": "物理设计中的布局规划（Floorplanning）是VLSI设计中一个关键阶段，需要对多个设计目标进行联合优化，例如半周长布线长度（HPWL）、长宽比、位移量以及溢出量等。本文提出了一种基于两阶段图神经网络（GNN）模型的优化方法，用于改进VLSI布局规划这一关键阶段。该方法通过分析ISPD 2005 Aptec1膨胀基准数据集，直接学习布局趋势，并利用关键指标如损失（Loss）、半周长布线长度（HPWL）、长宽比、位移量和溢出量进行优化。通过对第一阶段和第二阶段结果的评估，展示了算法在性能与收敛性方面的表现。同时，文章还讨论了当前方法的局限性，特别是对拥塞问题的处理能力，并展望了未来与合法化（legalization）及详细布局（detailed placement）工具集成的发展路径。"
  },
  {
    "date": "2025-12-16",
    "title": "AutoPen: Towards Autonomous Penetration Testing Using LLM-Powered Agents",
    "authors": "Jiahao Mei, Shuangwu Chen, Yuanyi Ma, Huizi Song",
    "publish": "Proceedings of the 9th International Conference on Computer Science and Application Engineering",
    "url": "https://doi.org/10.1145/3772886.3772899",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "AutoPen：基于大语言模型驱动的智能体实现自主渗透测试",
    "abstract_zh": "None"
  },
  {
    "date": "2025-12-16",
    "title": "Development of Cloud-Based Structured Query Language Injection (SQLi) Detection using Deep Learning and FastAPI",
    "authors": "Carlo Jude P. Abuda, Ariel Roy L. Reyes",
    "publish": "2025 Eight International Conference on Vocational Education and Electrical Engineering (ICVEE)",
    "url": "https://doi.org/10.1109/icvee66651.2025.11281431",
    "source": "IEEE",
    "abstract": "Structured Query Language Injection (SQLi) attacks continue to pose a serious threat to web applications, especially those hosted in cloud-based environments. Traditional detection methods, such as signature-based and rule-based systems, often fail to detect obfuscated or evolving injection patterns. Despite the increasing use of deep learning techniques for SQLi detection, which are particularly effective in recognizing complex, obfuscated, and evolving SQL injection patterns. However, most studies remain limited to offline experiments without real-time system deployment. This research addresses that gap by developing and operationalizing a deep learning-based SQLi detection model integrated with FastAPI. The study aimed to design a classification model capable of distinguishing malicious from safe SQL queries using numerically encoded inputs. Following the Evolutionary Prototyping Model, the system was developed through data preprocessing, model training using TensorFlow-Keras, and deployment as an API. A total of 30,919 SQL queries were used, and the model achieved 97% training accuracy and 94.5% validation accuracy, supported by strong precision, recall, and F1-scores above 93%. Findings confirm the model’s high reliability and real-time responsiveness, with an average detection response under 50 milliseconds. The research demonstrates that integrating deep learning with lightweight API frameworks can result in a scalable and accurate solution suitable for web security systems. These results suggest practical implications for deploying intelligent cybersecurity tools in real-world cloud applications.",
    "title_zh": "基于深度学习和FastAPI的云平台结构化查询语言注入（SQLi）检测系统开发",
    "abstract_zh": "结构化查询语言注入（SQLi）攻击持续对Web应用程序构成严重威胁，尤其是在基于云的环境中。传统的检测方法，如基于签名和规则的系统，往往无法识别经过混淆或不断演变的注入模式。尽管深度学习技术在SQLi检测中的应用日益增多，其在识别复杂、混淆及不断演化的SQL注入模式方面表现出显著优势，但大多数研究仍局限于离线实验，缺乏实时系统的部署。本研究通过开发并实际部署一个基于深度学习的SQLi检测模型，并将其与FastAPI集成，填补了这一空白。研究旨在设计一种分类模型，能够利用数值编码输入区分恶意与安全的SQL查询。根据进化原型模型，系统通过数据预处理、使用TensorFlow-Keras进行模型训练，并以API形式部署。共使用了30,919条SQL查询数据，模型在训练阶段达到97%的准确率，在验证阶段达到94.5%的准确率，同时各项指标——精确率、召回率和F1分数均超过93%，表现优异。研究结果证实该模型具有高度可靠性与实时响应能力，平均检测响应时间低于50毫秒。研究表明，将深度学习与轻量级API框架相结合，可构建出可扩展且高精度的解决方案，适用于Web安全系统。这些成果为在真实世界云应用中部署智能网络安全工具提供了切实可行的参考。"
  },
  {
    "date": "2025-12-16",
    "title": "Closed-Loop IIoT Control for Sustainable and Intelligent PCB Manufacturing: A Data-Driven Approach Integrating Domain Expertise (IMPACT 2025)",
    "authors": "Mickey Huang",
    "publish": "2025 20th International Microsystems, Packaging, Assembly and Circuits Technology Conference (IMPACT)",
    "url": "https://doi.org/10.1109/impact67645.2025.11281761",
    "source": "IEEE",
    "abstract": "The increasing demand for environmentally responsible, highly adaptive manufacturing processes, increased technical complexity, higher pressure on labor costs, energy costs, productivity and regulations has driven the electronics industry toward digitalization. [1] [2] [3] [4] [5] [6] [7]",
    "title_zh": "面向可持续与智能PCB制造的闭环工业物联网控制：一种融合领域专业知识的数据驱动方法（IMPACT 2025）",
    "abstract_zh": "对环境友好、高度灵活的制造工艺日益增长的需求，技术复杂性的提升，劳动力成本、能源成本、生产效率以及法规方面的压力不断增加，推动电子行业走向数字化。[1] [2] [3] [4] [5] [6] [7]"
  },
  {
    "date": "2025-12-16",
    "title": "Knowledge Graph Construction Technology for Vehicle-Grid Collaborative Planning Based on Context-Aware Prompt Engineering",
    "authors": "Ming Gao, Lixiao Wang, Longming Xu, Ling Yuan, Yuanfa Cen, Hui Wu, Xiaorou Huo",
    "publish": "2025 10th International Seminar on Computer Technology, Mechanical and Electrical Engineering (ISCME)",
    "url": "https://doi.org/10.1109/iscme66795.2025.11281458",
    "source": "IEEE",
    "abstract": "To address limitations in knowledge organization and reasoning in vehicle-grid collaborative planning, this paper proposes a knowledge graph construction method based on context-aware prompt engineering. Leveraging large language models (LLMs) and an optimized prompt strategy, the method enables effective knowledge extraction and structured storage within the vehicle-grid collaboration domain. The method supports structured knowledge storage, querying, and analysis, offering intelligent support for charging infrastructure deployment, power supply optimization, and safety management [1].",
    "title_zh": "基于上下文感知提示工程的车网协同规划知识图谱构建技术",
    "abstract_zh": "为解决车辆-电网协同规划中知识组织与推理能力的局限性，本文提出一种基于上下文感知提示工程的知识图谱构建方法。该方法利用大语言模型（LLMs）和优化的提示策略，实现了在车辆-电网协同领域内高效的知识提取与结构化存储。该方法支持知识的结构化存储、查询与分析，为充电基础设施布局、供电优化及安全管理提供智能化支持[1]。"
  },
  {
    "date": "2025-12-16",
    "title": "FPGA based TMDS Encoding and Decoding Protocol for Communication Interface Applications",
    "authors": "Ganesh Racha, Srinivasa Rao Perumalla, Yedukondalu Kamatham, Karthik Gupta Telukunta, Murtaza Ahmed Farooqui, Nagireddy Manaswini",
    "publish": "2025 5th Asian Conference on Innovation in Technology (ASIANCON)",
    "url": "https://doi.org/10.1109/asiancon66527.2025.11281283",
    "source": "IEEE",
    "abstract": "The present portable communication devices need high speed data transmission to support different interfaces and display technologies. These communication devices transmit data between transmitter and receiver blocks using a communication interface protocol. The interface is used for data transferring of image/audio/video processing applications. This work deals with the design of Transition Minimized Differential Signaling (TMDS) technology protocol based encoder and decoder using Verilog HDL and FPGA device targeting Spartan-7 FPGA and AMD Vivado EDA tools. The TMDS protocol supports video transfer interfaces for providing better flexibility and efficiency. This work proposes TMDS encoder and decoder algorithm to support best interfacing and high speed communication.",
    "title_zh": "基于FPGA的TMDS编码与解码协议在通信接口应用中的实现",
    "abstract_zh": "当前的便携式通信设备需要高速数据传输来支持多种接口和显示技术。这些通信设备通过通信接口协议在发送端和接收端之间传输数据，该接口用于图像/音频/视频处理应用中的数据传输。本文研究了基于Verilog HDL和FPGA器件（针对Spartan-7 FPGA及AMD Vivado EDA工具）设计的过渡最小化差分信号（TMDS）技术协议编码器与解码器。TMDS协议支持视频传输接口，能够提供更高的灵活性和效率。本文提出了一种TMDS编码器与解码器算法，以实现最佳接口兼容性和高速通信。"
  },
  {
    "date": "2025-12-16",
    "title": "Design and Comparative Analysis of Arithmetic Logic Unit Using Full Adder in SCL 180nm CMOS Technology",
    "authors": "Dinesh Kumar Choudhary, D. S. Ajnar, Rupali Mahant, R. C. Gurjar, Rajesh Khatri, Anand Yadav",
    "publish": "2025 IEEE International Conference on Advances in Computing Research On Science Engineering and Technology (ACROSET)",
    "url": "https://doi.org/10.1109/acroset66531.2025.11280653",
    "source": "IEEE",
    "abstract": "This paper discusses the design and comparative analysis of an ALU (Arithmetic logic unit) using different types of full adders in SCL 180nm CMOS technology. This ALU is a two-bit unit that performs two basic operations binary addition and the AND operation. In this ALU an opcode is used for performing two basic logical operations, meaning 0 is used for addition and 1 is used for the AND operation. This ALU is perform quickly logical operations. It makes for computational work. In this design the main aim is to minimize the number of transistors, low power consumption, minimum delay and high performance. The design and comparative analysis of ALU are along with simulation results and this proposed design in SCL 180nm technology using Cadence Virtuoso EDA tools.",
    "title_zh": "基于SCL 180nm CMOS工艺的全加器实现算术逻辑单元的设计与对比分析",
    "abstract_zh": "本文讨论了在SCL 180nm CMOS工艺下，采用不同类型的全加器设计并进行比较分析的算术逻辑单元（ALU）。该ALU是一个两位单元，能够执行两种基本操作：二进制加法和逻辑与（AND）运算。在该ALU中，通过一个操作码（opcode）来实现两种基本逻辑操作：当操作码为0时执行加法，当操作码为1时执行AND运算。该ALU能够快速完成逻辑运算，从而提升计算效率。本设计的主要目标是减少晶体管数量、降低功耗、最小化延迟，并实现高性能。文中还对ALU的设计与比较分析进行了仿真验证，所提出的方案基于SCL 180nm工艺，使用Cadence Virtuoso EDA工具完成。"
  },
  {
    "date": "2025-12-16",
    "title": "Generating Two-Level, GPU-Aware Mappings for Distributed Tensor Computations",
    "authors": "Botao Wu, Martin Kong",
    "publish": "2025 34th International Conference on Parallel Architectures and Compilation Techniques (PACT)",
    "url": "https://doi.org/10.1109/pact65351.2025.00043",
    "source": "IEEE",
    "abstract": "We introduce a two-level scheme to generate GPUaware MPI/NCCL code for distributed tensor computations. Our generator takes the specification of a linearized Directed Acyclic Graph (DAG) of tensor operators and produces a global mapping solution that considers MPI communication (inter and intranode) and the local computation. The core of our generator is a new bit-vector representation that compactly models mappings as well as communication directions along the grid. We incorporate the 2-level mapping decisions into a non-linear formulation which is optimized in an iterative fashion with the Z3 SMT solver. The new mapper supports both NVIDIA NCCL, MVAPICH-gdr, allowing for better portability. We demonstrate the efficiency of our mapping generator on a set of matrix- and tensor- DAGs, on two multi-GPU clusters with NVLink or PCIe intra-node interconnect, and compare against the COSMA library and CTF framework, achieving speedups ranging from 2.6× (over COSMA) to 18× (over CTF).",
    "title_zh": "面向分布式张量计算的两级、GPU感知映射生成",
    "abstract_zh": "我们提出了一种两级方案，用于为分布式张量计算生成GPU感知的MPI/NCCL代码。我们的代码生成器接收张量算子线性化有向无环图（DAG）的规范描述，并生成一个全局映射方案，该方案综合考虑了MPI通信（节点间及节点内）以及本地计算。生成器的核心是一种新的位向量表示方法，能够紧凑地建模映射关系以及网格中的通信方向。我们将两级映射决策融入非线性优化模型中，并通过Z3 SMT求解器以迭代方式优化求解。新设计的映射器同时支持NVIDIA NCCL和MVAPICH-gdr，从而提升了代码的可移植性。我们在多个矩阵与张量DAG上验证了该映射生成器的效率，在配备NVLink或PCIe节点内互连的两套多GPU集群上进行了测试，并与COSMA库和CTF框架进行了对比，取得了2.6倍（相对于COSMA）至18倍（相对于CTF）的加速效果。"
  },
  {
    "date": "2025-12-16",
    "title": "Leveraging AI Development Tools to Accelerate Learning, UI Implementation, and Educational Support in Mobile and Web Application Development",
    "authors": "Perry Weinthal, David Jaramillo, Alexandro Galvez-Vega, Alonzo Velez, Joshua Thaw",
    "publish": "2025 IEEE Digital Education and MOOCS Conference (DEMOcon)",
    "url": "https://doi.org/10.1109/democon65705.2025.11282753",
    "source": "IEEE",
    "abstract": "In recent years, tools powered by artificial intelligence have made their way into the hands of student developers, instructors, and teaching assistants. This paper shares our firsthand experience integrating these tools into courses focused on mobile and web application development. Our goal was to see how they could help students learn faster, build cleaner interfaces, and get better support while working through real-world projects.We used AI assistants to help students pick up new languages and frameworks more quickly. These tools let them ask questions in plain language and get working code or explanations right away—especially helpful when they were stuck or trying something new. They also sped up the process of turning user interface designs into actual code and helped with debugging, documentation, and version control.From a teaching standpoint, we leaned into a more hands-on, project-driven approach. This cut down on the risk of copy-paste dishonesty and made it easier to see who really understood the work. TAs also used the same tools—alongside platforms like Slack and Discord—to guide students with quick, clear answers without giving too much away.Overall, we found that these tools didn’t just boost productivity—they helped make the learning process more focused and more fair. When used carefully, they can support a deeper kind of engagement with the material and help instructors meet students where they are.",
    "title_zh": "利用人工智能开发工具加速移动与Web应用开发中的学习、用户界面实现及教育支持",
    "abstract_zh": "近年来，由人工智能驱动的工具已进入学生开发者、教师及助教的手中。本文分享了我们在移动应用与网页开发课程中整合这些工具的亲身经验。我们的目标是探索这些工具如何帮助学生更快地学习、构建更清晰的界面，并在完成真实项目时获得更好的支持。\n\n我们利用AI助手帮助学生更快掌握新的编程语言和框架。这些工具允许学生用自然语言提问，并立即获得可运行的代码或即时解释——尤其在他们遇到瓶颈或尝试新事物时尤为有用。此外，这些工具还加速了从用户界面设计到实际代码的转化过程，同时在调试、文档编写和版本控制方面提供了有力支持。\n\n从教学角度来看，我们采用了更加注重实践、以项目为导向的教学方式。这不仅降低了复制粘贴作弊的风险，也使我们更容易判断学生是否真正理解了所学内容。助教们同样使用这些工具，结合Slack、Discord等平台，能够快速、清晰地为学生提供指导，既解答问题又不直接给出完整答案。\n\n总体而言，我们发现这些工具不仅提升了工作效率，更使学习过程更加专注且更具公平性。只要合理使用，它们能够促进学生对知识的深度参与，帮助教师更好地因材施教，真正实现“以学生为中心”的教学理念。"
  },
  {
    "date": "2025-12-16",
    "title": "Optimising Embedded Neural Network Inference in Smart Traps for Fruit Pest Detection via Quantization-Aware Training and FPGA Acceleration",
    "authors": "Lucas C. Freitas, Isadora V. Dias, Victor R. S. Santos, Paulo R. Ferreira, Julio C. B. De Mattos, Lisane B. De Brisolara",
    "publish": "2025 XV Symposium on Computing Systems Engineering (SBESC)",
    "url": "https://doi.org/10.1109/sbesc68008.2025.11288842",
    "source": "IEEE",
    "abstract": "Deploying deep learning models in embedded agricultural systems requires balancing predictive performance with strict hardware constraints such as memory, power, and latency. This work uses quantization techniques, specifically post-training quantization (PTQ) and quantization-aware training (QAT), to optimize convolutional neural networks for real-time pest detection in smart traps. Using the Brevitas framework for quantization and FINN for hardware generation targeting FPGAs, we evaluate a range of weight and activation bit-width configurations. Our results show that QAT significantly outperforms PTQ, particularly in aggressive low-bit scenarios, achieving high accuracy while drastically reducing hardware resource utilization. Among the proposed solutions, one achieves 87.47% accuracy while using less than 10% of the LUTs required by its full-precision counterpart. Comparative analysis with standard models such as ResNet18 and MobileNet further validates the effectiveness of our approach. This study highlights the practicality of QAT-driven quantization for edge Artificial Intelligence applications in agriculture. It paves the way for future work, including power, latency, and throughput profiling, to support large-scale deployment.",
    "title_zh": "通过量化感知训练与FPGA加速优化智能诱捕器中嵌入式神经网络的推理以检测果虫",
    "abstract_zh": "在嵌入式农业系统中部署深度学习模型，需要在预测性能与严格的硬件约束（如内存、功耗和延迟）之间取得平衡。本文采用量化技术，特别是训练后量化（PTQ）和量化感知训练（QAT），对卷积神经网络进行优化，以实现在智能捕虫器中的实时害虫检测。我们使用Brevitas框架进行量化，并借助FINN工具链为FPGA生成硬件，评估了多种权重和激活位宽配置。实验结果表明，QAT显著优于PTQ，尤其在极端低比特场景下表现突出，能够在大幅降低硬件资源占用的同时保持高精度。在所提出的方案中，某一配置实现了87.47%的准确率，且所用查找表（LUT）资源不足全精度模型的10%。与ResNet18、MobileNet等标准模型的对比分析进一步验证了本方法的有效性。本研究凸显了基于QAT的量化技术在农业领域边缘人工智能应用中的实际可行性，为未来工作奠定了基础，包括对功耗、延迟和吞吐量的深入分析，以支持大规模部署。"
  },
  {
    "date": "2025-12-16",
    "title": "AI-Powered Offline Voice Assistant for Rural Communities",
    "authors": "Umesh R, Sudharasan Dev K, Praveen Kumar M",
    "publish": "2025 7th International Conference on Innovative Data Communication Technologies and Application (ICIDCA)",
    "url": "https://doi.org/10.1109/icidca66325.2025.11280419",
    "source": "IEEE",
    "abstract": "This paper discusses the design and evaluation of an offline, AI-enabled voice assistant that employs Whisper-based speech recognition; lightweight, large language models (LLMs) using Ollama; and pyttsx3-based text to speech (TTS). The offline nature of the voice assistant was designed, in part, to tackle privacy concerns associated with technology that requires a connection to the internet, while still enabling the conversational interactions defined at the beginning of this section. Three LLMs—Gemma 2B, Phi-3, and Mistral—were tested, using a varied set of multilingual and domain-specific contextual prompts (including agriculture, health, and general knowledge, etc.) for their ability to generate appropriate responses. Functional characteristics of the assistant include session memory and a visual modular architecture that enable components and included features to be easily upgraded and interchanged. The Whisper API, in combination with a TTS engine, provided accurate recognition instance-to-instance while TTS produced plausible responses that maintained a natural sense of pacing and development throughout. Performance testing demonstrated between 75% and 90% transcription accuracy in quiet and moderate noise environments, respectively, and an average response time of 2.5 seconds, with significantly lower latency for repeat requests. The fact that the assistant is developed to run on any device with at least 2GB of shared RAM means it will also run on low-resourced or rural technologies. Relative to cloud-hosted alternatives, the proposed voice assistant also has the benefit of privacy, offline reliability, and independence from cloud-based data services and the dangers associated with the widespread, unchecked, use of those services. While the experimental results demonstrate that Mistral model performed the most balanced in terms of response accuracy and clarity (followed by Gemma and Phi-3).",
    "title_zh": "面向农村社区的AI赋能离线语音助手",
    "abstract_zh": "本文探讨了一款离线运行、基于人工智能的语音助手的设计与评估，该助手采用Whisper语音识别技术；通过Ollama实现的轻量级大语言模型（LLMs）；以及基于pyttsx3的文本转语音（TTS）系统。该语音助手的离线特性部分旨在解决依赖互联网连接的技术所引发的隐私问题，同时仍能实现本节开头定义的对话式交互功能。研究测试了三种大语言模型——Gemma 2B、Phi-3 和 Mistral，并使用涵盖多语言及特定领域（如农业、健康和通用知识等）的多样化上下文提示，评估其生成恰当回应的能力。该助手具备会话记忆功能，以及可视化的模块化架构，使得各组件及其功能可轻松升级与替换。Whisper API 与 TTS 引擎相结合，在实例间实现了高精度的语音识别，而 TTS 生成的回应则具有自然流畅的节奏感与发展性。性能测试表明，在安静和中等噪声环境下，语音转录准确率分别达到75%至90%，平均响应时间为2.5秒，重复请求时延迟显著降低。由于该助手可在至少配备2GB共享内存的任意设备上运行，因此也适用于资源有限或偏远地区的设备。相较于云端托管的替代方案，该语音助手在隐私保护、离线可靠性，以及摆脱对云数据服务的依赖方面具有明显优势，避免了这些服务被广泛且不受控制使用所带来的潜在风险。实验结果表明，Mistral 模型在响应准确性与清晰度方面表现最为均衡（其次为Gemma和Phi-3）。"
  },
  {
    "date": "2025-12-16",
    "title": "Counterfactual Explanations for Enhanced Interpretability in Cross-Site Scripting (XSS) Detection",
    "authors": "Alphin Kayalathu Mathew, Ashwin M, Vysakh Kani Kolil, Devi Rajeev",
    "publish": "2025 7th International Conference on Innovative Data Communication Technologies and Application (ICIDCA)",
    "url": "https://doi.org/10.1109/icidca66325.2025.11280340",
    "source": "IEEE",
    "abstract": "Machine learning (ML) models are vital for detecting Cross-Site Scripting (XSS) attacks in web security, but are often hindered by their opaque decision-making processes. To address this, we propose a novel interpretability framework combining Local Interpretable Model-agnostic Explanations (LIME) and Diverse Counterfactual Explanations (DiCE) to enhance the transparency of an XSS detection model. Our approach utilizes lexical and structural URL features to train four ML models, Random Forest (RF), Support Vector Machine (SVM), Multi-Layer Perceptron (MLP), and Logistic Regression, with RF achieving superior performance (99.28% accuracy, 99.11% precision, 98.29% recall, and a 98.70% F1-score) after hyperparameter optimization via 5-fold cross-validation and Grid Search. LIME identifies critical features like html_attr_background, html_tag_svg, and url_special_characters as key indicators of XSS attacks, while DiCE generates actionable counterfactuals, demonstrating how minimal feature adjustments can shift predictions from malicious to benign. This framework enhances trust and provides actionable insights for security practitioners, improving the deployment of ML in security critical contexts.",
    "title_zh": "用于增强跨站脚本（XSS）检测可解释性的反事实解释",
    "abstract_zh": "机器学习（ML）模型在Web安全中对跨站脚本（XSS）攻击的检测至关重要，但其决策过程往往缺乏透明性，限制了实际应用。为解决这一问题，我们提出了一种新颖的可解释性框架，结合局部可解释模型无关解释（LIME）与多样化反事实解释（DiCE），以提升XSS检测模型的透明度。我们的方法利用URL的词法和结构特征，训练了四种机器学习模型：随机森林（RF）、支持向量机（SVM）、多层感知机（MLP）和逻辑回归。经过5折交叉验证与网格搜索进行超参数优化后，随机森林表现最优，准确率达到99.28%，精确率为99.11%，召回率为98.29%，F1得分为98.70%。LIME识别出html_attr_background、html_tag_svg和url_special_characters等关键特征，作为XSS攻击的重要指示因子；而DiCE则生成具有行动指导意义的反事实案例，展示了仅需对少数特征进行微小调整，即可使模型预测从恶意转变为良性。该框架不仅增强了用户对模型的信任，还为安全从业人员提供了可操作的洞察，显著提升了机器学习在安全关键场景中的部署效果。"
  },
  {
    "date": "2025-12-16",
    "title": "Cloud-Edge Collaborative Learning: Real-Time IoT Data Processing with GoLang Microservices for Agriculture",
    "authors": "Ashish Gawande",
    "publish": "2025 IEEE International Conference on Internet of Things and Intelligence Systems (IoTaIS)",
    "url": "https://doi.org/10.1109/iotais67227.2025.11282031",
    "source": "IEEE",
    "abstract": "Agriculture is entering a new digital era, with IoT devices generating a large amount of data from soil sensors, drones, livestock trackers, and greenhouse controllers. Traditional cloud-only systems send all this data to remote servers, which causes delays, excessive network usage, and a dependence on constant internet connectivity. This paper introduces a GoLang‑based cloud–edge collaborative learning framework. In this design, edge devices handle tasks such as local inference, anomaly detection, and actuator control that require rapid processing. At the same time, the cloud collects anomaly data, retrains AI models, and sends updated TinyML models back to the edge. This paper provides clear technical guidance, including GoLang microservice code, JSON formats, Kafka topic, MQTT message structures, and Kubernetes deployment examples. A basic evaluation reveals that the system can respond in under 250 ms, reduce bandwidth usage by approximately 80%, and scale effectively to support large farms.",
    "title_zh": "云边协同学习：基于Go语言微服务的实时物联网数据处理在农业中的应用",
    "abstract_zh": "农业正进入一个全新的数字化时代，物联网（IoT）设备从土壤传感器、无人机、牲畜追踪器到温室控制器源源不断地产生大量数据。传统的仅依赖云端的系统将所有数据传输至远程服务器，导致延迟高、网络资源消耗过大，并且对持续的互联网连接高度依赖。本文提出了一种基于Go语言的云-边协同学习框架。在该设计中，边缘设备负责处理需要快速响应的任务，如本地推理、异常检测和执行器控制；同时，云端则收集异常数据，重新训练AI模型，并将更新后的TinyML模型下发至边缘端。本文提供了清晰的技术指导，包括Go语言微服务代码、JSON数据格式、Kafka主题配置、MQTT消息结构以及Kubernetes部署示例。初步评估表明，该系统响应时间可低于250毫秒，带宽使用量减少约80%，并能有效扩展以支持大型农场的应用需求。"
  }
]