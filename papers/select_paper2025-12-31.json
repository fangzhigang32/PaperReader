[
  {
    "date": "2025-12-31",
    "title": "Vibe Coding, Interface Flattening",
    "authors": "Hongrui Jin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24939v1",
    "source": "arXiv",
    "abstract": "Large language models are reshaping programming by enabling 'vibe coding': the development of softwares through natural-language interaction with model-driven toolchains. This article argues that vibe coding is best understood as interface flattening, a reconfiguration in which previously distinct modalities (GUI, CLI, and API) appear to converge into a single conversational surface, even as the underlying chain of translation from intention to machinic effect lengthens and thickens. Drawing on Friedrich Kittler's materialist media theory and Alexander Galloway's account of interfaces as sites of protocol control, the paper situates programming as a historically localised interface arrangement rather than an essential relation to computation. Through a materialist reconstruction of the contemporary vibe-coding stack, it shows how remote compute infrastructures, latency and connectivity, structured outputs, function/tool calling, and interoperability standards such as the Model Context Protocol relocate control and meaning-making power to model and protocol providers. The apparent democratisation of technical capability therefore depends on new dependencies and new literacies. By foregrounding the tension between experiential flattening and infrastructural thickening, I demonstrate how LLM-mediated development redistributes symbolic labour/power, obscures responsibility, and privatises competencies previously dispersed across programming communities, contributing a critical lens on the political economy of AI-mediated human-computer interaction.",
    "title_zh": "氛围编码，界面扁平化",
    "abstract_zh": "大型语言模型正在重塑编程，催生出一种名为“氛围编码”（vibe coding）的新范式：通过与以模型驱动的工具链进行自然语言交互来开发软件。本文认为，“氛围编码”最恰当的理解方式是“界面扁平化”——即原本各自独立的交互模式（图形用户界面GUI、命令行界面CLI、应用程序接口API）看似融合为单一的对话式界面，而与此同时，从意图到机器执行效果之间的翻译链条却变得愈发冗长且复杂。本文借鉴弗里德里希·基特勒（Friedrich Kittler）的物质主义媒介理论，以及亚历山大·加洛韦（Alexander Galloway）关于界面作为协议控制场域的观点，将编程重新定位为一种历史性的、特定于情境的界面安排，而非与计算之间某种本质性的关联。通过对当代“氛围编码”技术栈的物质性重构，文章揭示了远程计算基础设施、延迟与连接性、结构化输出、函数/工具调用，以及诸如模型上下文协议（Model Context Protocol）等互操作标准如何将控制权和意义建构能力转移至模型与协议提供者手中。因此，表面上的技术能力民主化，实则依赖于新的依赖关系与新的读写能力。通过凸显体验上的界面扁平化与底层基础设施日益复杂化之间的张力，本文展示了由大语言模型中介的开发过程如何重新分配象征性劳动与权力，模糊责任归属，并将原本分散在编程社区中的专业能力私有化，从而为理解人工智能中介的人机交互的政治经济提供了批判性视角。"
  },
  {
    "date": "2025-12-31",
    "title": "Adaptive Dependency-aware Prompt Optimization Framework for Multi-Step LLM Pipeline",
    "authors": "Minjun Zhao, Xinyu Zhang, Shuai Zhang, Deyang Li, Ruifeng Shi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24933v1",
    "source": "arXiv",
    "abstract": "Multi-step LLM pipelines invoke large language models multiple times in a structured sequence and can effectively solve complex tasks, but their performance heavily depends on the prompts used at each step. Jointly optimizing these prompts is difficult due to missing step-level supervision and inter-step dependencies. Existing end-to-end prompt optimization methods struggle under these conditions and often yield suboptimal or unstable updates. We propose ADOPT, an Adaptive Dependency-aware Prompt Optimization framework for multi-step LLM pipelines. ADOPT explicitly models the dependency between each LLM step and the final task outcome, enabling precise text-gradient estimation analogous to computing analytical derivatives. It decouples textual gradient estimation from gradient updates, reducing multi-prompt optimization to flexible single-prompt optimization steps, and employs a Shapley-based mechanism to adaptively allocate optimization resources. Experiments on real-world datasets and diverse pipeline structures show that ADOPT is effective and robust, consistently outperforming state-of-the-art prompt optimization baselines.",
    "title_zh": "多步大语言模型流水线的自适应依赖感知提示优化框架",
    "abstract_zh": "多步大型语言模型（LLM）流水线通过结构化序列多次调用大语言模型，能够有效解决复杂任务，但其性能高度依赖于每一步所使用的提示词。由于缺乏步骤级的监督信号以及步骤间的相互依赖关系，联合优化这些提示词极具挑战性。现有的端到端提示优化方法在该条件下表现不佳，常导致次优或不稳定的更新结果。为此，我们提出了ADOPT——一种面向多步LLM流水线的自适应依赖感知提示优化框架。ADOPT显式建模了每个LLM步骤与最终任务结果之间的依赖关系，从而实现类似解析导数的精确文本梯度估计。它将文本梯度估计与梯度更新解耦，将多提示优化简化为灵活的单提示优化步骤，并采用基于Shapley值的机制自适应地分配优化资源。在真实数据集和多种流水线结构上的实验表明，ADOPT具有良好的有效性与鲁棒性，始终优于当前最先进的提示优化基线方法。"
  },
  {
    "date": "2025-12-31",
    "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem",
    "authors": "Weixun Wang, XiaoXiao Xu, Wanhe An, Fangwen Dai, Wei Gao, Yancheng He, Ju Huang, Qiang Ji, Hanqi Jin, Xiaoyang Li, Yang Li, Zhongwen Li, Shirong Lin, Jiashun Liu, Zenan Liu, Tao Luo, Dilxat Muhtar, Yuanbin Qu, Jiaqiang Shi, Qinghui Sun, Yingshui Tan, Hao Tang, Runze Wang, Yi Wang, Zhaoguo Wang, Yanan Wu, Shaopan Xiong, Binchen Xu, Xander Xu, Yuchi Xu, Qipeng Zhang, Xixia Zhang, Haizhou Zhao, Jie Zhao, Shuaibing Zhao, Baihui Zheng, Jianhui Zheng, Suhang Zheng, Yanni Zhu, Mengze Cai, Kerui Cao, Xitong Chen, Yue Dai, Lifan Du, Tao Feng, Tao He, Jin Hu, Yijie Hu, Ziyu Jiang, Cheng Li, Xiang Li, Jing Liang, Chonghuan Liu, ZhenDong Liu, Haodong Mi, Yanhu Mo, Junjia Ni, Shixin Pei, Jingyu Shen, XiaoShuai Song, Cecilia Wang, Chaofan Wang, Kangyu Wang, Pei Wang, Tao Wang, Wei Wang, Ke Xiao, Mingyu Xu, Tiange Xu, Nan Ya, Siran Yang, Jianan Ye, Yaxing Zang, Duo Zhang, Junbo Zhang, Boren Zheng, Wanxi Deng, Ling Pan, Lin Qu, Wenbo Su, Jiamang Wang, Wei Wang, Hu Wei, Minggang Wu, Cheng Yu, Bing Zhao, Zhicheng Zheng, Bo Zheng",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24873v1",
    "source": "arXiv",
    "abstract": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.",
    "title_zh": "随流而行：摇滚乐中的自主创作，构建开放自主学习生态中的ROME模型",
    "abstract_zh": "代理式构建（Agentic crafting）要求大语言模型（LLMs）在真实环境中通过多轮交互，采取行动、观察结果，并迭代优化生成物。尽管这一能力至关重要，但开源社区目前仍缺乏一个系统化、端到端的生态体系来简化代理开发流程。为此，我们提出了**代理学习生态系统**（Agentic Learning Ecosystem, ALE），这是一个基础性基础设施，旨在优化代理型LLM的生产管线。\n\nALE由三个核心组件构成：  \n- **ROLL**：一种后训练框架，用于权重优化；  \n- **ROCK**：一个沙盒环境管理器，用于生成智能体行为轨迹；  \n- **iFlow CLI**：一个高效的上下文工程代理框架。\n\n我们发布了**ROME**（ROME is Obviously an Agentic Model），一个基于ALE构建的开源智能体，其在超过一百万条轨迹上进行训练。我们的方法包含一套数据合成协议，用于生成复杂行为；同时提出了一种新颖的策略优化算法——**基于交互的策略对齐**（Interaction-based Policy Alignment, IPA），该算法将奖励分配给语义层面的交互片段，而非单个词元，从而显著提升长周期训练的稳定性。\n\n在实证评估中，我们在结构化环境中测试了ROME，并引入了**Terminal Bench Pro**基准测试，该基准具备更高的规模和更强的污染控制能力。实验结果表明，ROME在SWE-bench Verified、Terminal Bench等多个基准上均表现出色，充分验证了ALE基础设施的有效性与实用性。"
  },
  {
    "date": "2025-12-31",
    "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization",
    "authors": "Yuchen Shi, Yuzheng Cai, Siqi Cai, Zihan Xu, Lichao Chen, Yulei Qin, Zhijian Zhou, Xiang Fei, Chaofan Qiu, Xiaoyu Tan, Gang Li, Zongyi Li, Haojia Lin, Guocan Cai, Yong Mao, Yunsheng Wu, Ke Li, Xing Sun",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24615v1",
    "source": "arXiv",
    "abstract": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose \\textbf{Youtu-Agent}, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a \\textbf{Workflow} mode for standard tasks and a \\textbf{Meta-Agent} mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an \\textbf{Agent Practice} module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an \\textbf{Agent RL} module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.",
    "title_zh": "Youtu-Agent：通过自动化生成与混合策略优化提升智能体生产力",
    "abstract_zh": "现有的大型语言模型（LLM）智能体框架面临两大核心挑战：配置成本高与能力静态化。构建一个高质量的智能体通常需要大量手动投入进行工具集成和提示工程，而部署后的智能体在面对动态环境时又难以适应，往往依赖昂贵的微调来实现改进。为解决这些问题，我们提出 \\textbf{Youtu-Agent}——一个面向 LLM 智能体自动化生成与持续演化的模块化框架。Youtu-Agent 采用结构化的配置体系，将执行环境、工具集与上下文管理解耦，支持灵活复用与自动化合成。我们引入两种生成范式：一种是针对标准任务的 \\textbf{Workflow} 模式，另一种是应对复杂、非标准化需求的 \\textbf{Meta-Agent} 模式，能够自动完成工具代码、提示词及配置文件的生成。此外，Youtu-Agent 构建了混合式策略优化系统：(1) \\textbf{Agent Practice} 模块，使智能体可通过上下文内优化积累经验并提升性能，无需更新参数；(2) \\textbf{Agent RL} 模块，可与分布式训练框架无缝集成，实现对任意 Youtu-Agent 的端到端、大规模稳定强化学习。实验结果表明，Youtu-Agent 在 WebWalkerQA（71.47%）和 GAIA（72.8%）基准上均达到当前最优水平，仅使用开源权重模型即可实现卓越表现。我们的自动化生成流水线工具合成成功率超过 81%，而 Practice 模块在 AIME 2024/2025 上分别带来 +2.7% 和 +5.4% 的性能提升。此外，Agent RL 训练在 7B 规模大模型上实现了 40% 的加速，同时保持性能稳步增长，在数学推理与通用/多跳问答任务中，编码/推理与搜索能力分别提升最高达 35% 和 21%。"
  },
  {
    "date": "2025-12-31",
    "title": "Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization",
    "authors": "Dong Qiu, Duo Xu, Limengxi Yue",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24609v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) perform well in language tasks but often lack collaborative awareness and struggle to optimize global performance in multi-agent settings. We present a reinforcement learning-augmented LLM agent framework that formulates cooperation as a decentralized partially observable Markov decision process (Dec-POMDP) and adopts centralized training with decentralized execution (CTDE). We introduce Group Relative Policy Optimization (GRPO) to jointly optimize agent policies with access to global signals during training, together with a simplified joint reward that balances task quality, speed, and coordination cost. On collaborative writing and coding benchmarks, our framework delivers a 3x increase in task processing speed over single-agent baselines, 98.7% structural/style consistency in writing, and a 74.6% test pass rate in coding. The approach consistently outperforms strong multi-agent LLM baselines and provides a practical path toward reliable collaboration in complex workflows.",
    "title_zh": "强化学习增强的大型语言模型代理在协作决策与性能优化中的应用",
    "abstract_zh": "大型语言模型（LLMs）在语言任务中表现优异，但在多智能体场景下往往缺乏协作意识，难以优化全局性能。我们提出了一种增强强化学习的LLM智能体框架，将合作建模为一个去中心化的部分可观测马尔可夫决策过程（Dec-POMDP），并采用集中式训练、去中心化执行（CTDE）策略。我们引入了群体相对策略优化（GRPO），在训练过程中通过访问全局信号联合优化各智能体的策略，并设计了一种简化的联合奖励机制，以平衡任务质量、执行速度与协调成本。在协同写作和编程基准测试中，该框架相较于单智能体基线，任务处理速度提升了3倍，写作的结构与风格一致性达到98.7%，编程测试通过率达到74.6%。该方法持续优于现有的强大多智能体LLM基线，为复杂工作流中的可靠协作提供了一条切实可行的路径。"
  },
  {
    "date": "2025-12-31",
    "title": "SynRAG: A Large Language Model Framework for Executable Query Generation in Heterogeneous SIEM System",
    "authors": "Md Hasan Saju, Austin Page, Akramul Azim, Jeff Gardiner, Farzaneh Abazari, Frank Eargle",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24571v1",
    "source": "arXiv",
    "abstract": "Security Information and Event Management (SIEM) systems are essential for large enterprises to monitor their IT infrastructure by ingesting and analyzing millions of logs and events daily. Security Operations Center (SOC) analysts are tasked with monitoring and analyzing this vast data to identify potential threats and take preventive actions to protect enterprise assets. However, the diversity among SIEM platforms, such as Palo Alto Networks Qradar, Google SecOps, Splunk, Microsoft Sentinel and the Elastic Stack, poses significant challenges. As these systems differ in attributes, architecture, and query languages, making it difficult for analysts to effectively monitor multiple platforms without undergoing extensive training or forcing enterprises to expand their workforce. To address this issue, we introduce SynRAG, a unified framework that automatically generates threat detection or incident investigation queries for multiple SIEM platforms from a platform-agnostic specification. SynRAG can generate platformspecific queries from a single high-level specification written by analysts. Without SynRAG, analysts would need to manually write separate queries for each SIEM platform, since query languages vary significantly across systems. This framework enables seamless threat detection and incident investigation across heterogeneous SIEM environments, reducing the need for specialized training and manual query translation. We evaluate SynRAG against state-of-the-art language models, including GPT, Llama, DeepSeek, Gemma, and Claude, using Qradar and SecOps as representative SIEM systems. Our results demonstrate that SynRAG generates significantly better queries for crossSIEM threat detection and incident investigation compared to the state-of-the-art base models.",
    "title_zh": "SynRAG：一种用于异构SIEM系统中可执行查询生成的大语言模型框架",
    "abstract_zh": "安全信息与事件管理（SIEM）系统对于大型企业监控其IT基础设施至关重要，能够每日接收并分析数百万条日志和事件。安全运营中心（SOC）分析师负责监控和分析这些海量数据，以识别潜在威胁，并采取预防措施保护企业资产。然而，不同SIEM平台之间的差异——如Palo Alto Networks Qradar、Google SecOps、Splunk、Microsoft Sentinel以及Elastic Stack——带来了显著挑战。由于这些系统在属性、架构和查询语言方面各不相同，分析师若要有效监控多个平台，往往需要接受大量培训，或迫使企业扩大人员规模。为解决这一问题，我们提出了SynRAG，一个统一的框架，能够从平台无关的规范中自动生成适用于多种SIEM平台的威胁检测或事件调查查询。SynRAG可基于分析师编写的一条高层次规范，自动转换生成针对特定平台的查询语句。若无SynRAG，分析师必须手动为每个SIEM平台分别编写查询，因为各系统的查询语言存在显著差异。该框架实现了异构SIEM环境下的无缝威胁检测与事件调查，减少了对专业培训的需求以及手动查询转换的工作量。我们在代表性SIEM系统Qradar和SecOps上，将SynRAG与当前最先进的语言模型（包括GPT、Llama、DeepSeek、Gemma和Claude）进行了对比评估。结果表明，SynRAG在跨SIEM平台的威胁检测与事件调查查询生成方面，显著优于现有的基础模型。"
  },
  {
    "date": "2025-12-31",
    "title": "Safe in the Future, Dangerous in the Past: Dissecting Temporal and Linguistic Vulnerabilities in LLMs",
    "authors": "Muhammad Abdullahi Said, Muhammad Sammani Sani",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24556v1",
    "source": "arXiv",
    "abstract": "As Large Language Models (LLMs) integrate into critical global infrastructure, the assumption that safety alignment transfers zero-shot from English to other languages remains a dangerous blind spot. This study presents a systematic audit of three state of the art models (GPT-5.1, Gemini 3 Pro, and Claude 4.5 Opus) using HausaSafety, a novel adversarial dataset grounded in West African threat scenarios (e.g., Yahoo-Yahoo fraud, Dane gun manufacturing). Employing a 2 x 4 factorial design across 1,440 evaluations, we tested the non-linear interaction between language (English vs. Hausa) and temporal framing. Our results challenge the prevailing multilingual safety gap narrative. Instead of a simple degradation in low-resource settings, we identified a mechanism of Complex Interference where safety is determined by the intersection of variables. While models exhibited a Reverse Linguistic with Claude 4.5 Opus proving significantly safer in Hausa (45.0%) than in English (36.7%) due to uncertainty-driven refusal they suffered catastrophic failures in temporal reasoning. We report a profound Temporal Asymmetry, where past-tense framing bypassed defenses (15.6% safe) while future-tense scenarios triggered hyper-conservative refusals (57.2% safe). The magnitude of this volatility is illustrated by a 9.2x disparity between the safest and most vulnerable configurations, proving that safety is not a fixed property but a context-dependent state. We conclude that current models rely on superficial heuristics rather than robust semantic understanding, creating Safety Pockets that leave Global South users exposed to localized harms. We propose Invariant Alignment as a necessary paradigm shift to ensure safety stability across linguistic and temporal shifts.",
    "title_zh": "未来安全，过去危险：剖析大语言模型中的时间与语言漏洞",
    "abstract_zh": "随着大型语言模型（LLMs）日益融入全球关键基础设施，人们普遍假设其安全对齐能力可“零样本”地从英语无缝迁移至其他语言，这一假设实则存在危险盲区。本研究通过一项系统性审计，对三款前沿模型（GPT-5.1、Gemini 3 Pro 和 Claude 4.5 Opus）进行了评估，所用数据集为全新构建的对抗性测试集——HausaSafety，其基于西非典型威胁场景（如“Yahoo-Yahoo”诈骗、Dane枪支制造）。研究采用2×4因子设计，在1,440次评估中检验了语言（英语 vs. 哈萨语）与时间框架之间的非线性交互作用。结果挑战了当前关于多语言安全差距的主流叙事：并非简单的低资源语言下安全性能退化，而是揭示出一种“复杂干扰”机制——安全表现由多重变量的交集共同决定。\n\n尽管各模型在哈萨语中的表现存在差异，但Claude 4.5 Opus展现出“反向语言效应”：在哈萨语中安全率高达45.0%，显著优于其在英语中的36.7%。这种提升源于模型在面对不确定性时更倾向于拒绝响应，然而其在时间推理方面却遭遇灾难性失败。我们发现一种深刻的“时间不对称性”：过去时态情境下，防御机制几乎失效（仅15.6%安全），而未来时态情境则触发过度保守的拒绝行为（安全率达57.2%）。该波动幅度之大，最安全配置与最脆弱配置之间相差达9.2倍，充分证明：安全性并非固定属性，而是一种高度依赖上下文的状态。\n\n我们得出结论：当前模型依赖的是表面启发式规则，而非深层语义理解，从而形成“安全盲区”，使全球南方用户暴露于特定地域性风险之中。为此，我们提出“不变对齐”（Invariant Alignment）作为必要范式转型，以确保模型在语言和时间维度变化下仍能保持安全稳定性。"
  },
  {
    "date": "2025-12-31",
    "title": "FPGA Co-Design for Efficient N:M Sparse and Quantized Model Inference",
    "authors": "Fen-Yu Hsieh, Yun-Chang Teng, Ding-Yong Hong, Jan-Jan Wu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24713v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of language processing tasks. However, this success comes at the cost of substantial computation and memory requirements, which significantly impedes their deployment in resource-constrained environments. To address this challenge, this work introduces an automation framework that leverages weight pruning and low-bit quantization, and presents a hardware-software co-design method that generates accelerators on the Field-Programmable Gate Array (FPGA) platform. In particular, we implement a unified pipeline that applies N:M structured pruning and 4-bit integer quantization to reduce the memory footprint, followed by optimized dequantization and matrix multiplication to enhance LLM inference on several hardware platforms, including CPUs, NVIDIA GPUs with Dense and 2:4 Sparse Tensor Cores, and a custom systolic-array-based FPGA accelerator. Utilizing 2:4 sparsity combined with quantization on $4096 \\times 4096$ matrices, our approach achieves a reduction of up to $4\\times$ in weight storage and a $1.71\\times$ speedup in matrix multiplication, yielding a $1.29\\times$ end-to-end latency reduction compared to dense GPU baselines. Scaling analysis on the LLaMA-7B model further shows that structured sparsity enhances the throughput per token by $1.36\\times$. These results demonstrate the synergy of fine-grained N:M sparsity and quantization for enabling efficient and deployable LLM inference, while the proposed FPGA accelerator offers a flexible architectural path for supporting a broader class of sparsity patterns beyond the fixed 2:4 hardware constraints.",
    "title_zh": "面向高效N:M稀疏量化模型推理的FPGA协同设计",
    "abstract_zh": "大规模语言模型（LLMs）在众多自然语言处理任务中展现了卓越的性能。然而，这种成功伴随着巨大的计算与内存开销，严重制约了其在资源受限环境中的部署。为应对这一挑战，本文提出了一种自动化框架，结合权重剪枝与低比特量化技术，并设计了一种软硬件协同的方法，在现场可编程门阵列（FPGA）平台上生成专用加速器。具体而言，我们实现了一个统一的处理流水线，采用N:M结构化剪枝与4比特整数量化技术以降低模型内存占用，随后通过优化的反量化和矩阵乘法操作，显著提升了在多种硬件平台上的LLM推理效率，包括CPU、配备密集型及2:4稀疏张量核心的NVIDIA GPU，以及自研的基于阵列结构的FPGA加速器。在$4096 \\times 4096$矩阵上结合2:4稀疏性与量化技术，我们的方法实现了权重存储量最高降低4倍，矩阵乘法速度提升1.71倍，相较于密集型GPU基线，端到端延迟降低1.29倍。对LLaMA-7B模型的扩展性分析进一步表明，结构化稀疏性使每token的吞吐量提升了1.36倍。这些结果充分展示了细粒度N:M稀疏性与量化技术协同带来的高效性与可部署性优势；同时，所提出的FPGA加速器提供了一种灵活的架构路径，能够支持超出固定2:4硬件约束的更广泛稀疏模式。"
  },
  {
    "date": "2025-12-31",
    "title": "CellSecInspector: Safeguarding Cellular Networks via Automated Security Analysis on Specifications",
    "authors": "Ke Xie, Xingyi Zhao, Yiwen Hu, Munshi Saifuzzaman, Wen Li, Shuhan Yuan, Tian Xie, Guan-Hua Tu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24682v1",
    "source": "arXiv",
    "abstract": "The complexity, interdependence, and rapid evolution of 3GPP specifications present fundamental challenges for ensuring the security of modern cellular networks. Manual reviews and existing automated approaches, which often depend on rule-based parsing or small sets of manually crafted security requirements, fail to capture deep semantic dependencies, cross-sentence/clause relationships, and evolving specification behaviors. In this work, we present CellSecInspector, an automated framework for security analysis of 3GPP specifications. CellSecInspector extracts structured state-condition-action (SCA) representations, models mobile network procedures with comprehensive function chains, systematically validates them against 9 foundational security properties under 4 adversarial scenarios, and automatically generates test cases. This end-to-end pipeline enables the automated discovery of vulnerabilities without relying on manually predefined security requirements or rules. Applying CellSecInspector to the well-studied 5G and 4G NAS and RRC specifications, it discovers 43 vulnerabilities, 8 of which are previously unreported. Our findings show that CellSecInspector is a scalable, adaptive, and effective solution to assess 3GPP specifications for safeguarding operational and next-generation cellular networks.",
    "title_zh": "CellSecInspector：通过规范的自动化安全分析保障蜂窝网络安全",
    "abstract_zh": "3GPP规范的复杂性、相互依赖性以及快速演进特性，给保障现代蜂窝网络的安全带来了根本性挑战。传统的手动审查方法以及现有的自动化手段，通常依赖于基于规则的解析或少量手工编写的安全部分要求，难以捕捉深层次的语义依赖关系、跨句子/条款的关联性以及不断变化的规范行为。在本研究中，我们提出了CellSecInspector——一种针对3GPP规范的安全分析自动化框架。CellSecInspector能够提取结构化的状态-条件-动作（SCA）表示，以全面的功能链建模移动网络流程，并在四种攻击场景下，系统性地对9项基础安全属性进行验证，同时自动生成测试用例。该端到端的分析流程无需依赖人工预定义的安全需求或规则，即可实现漏洞的自动发现。我们将CellSecInspector应用于广泛研究的5G和4G NAS与RRC规范，共发现了43个漏洞，其中8个为此前未报告的新漏洞。研究结果表明，CellSecInspector是一种可扩展、自适应且高效的方法，能够有效评估3GPP规范，从而保护现有及下一代蜂窝网络的安全运行。"
  },
  {
    "date": "2025-12-31",
    "title": "Characterizing Bugs and Quality Attributes in Quantum Software: A Large-Scale Empirical Study",
    "authors": "Mir Mohammad Yousuf, Shabir Ahmad Sofi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24656v1",
    "source": "arXiv",
    "abstract": "Quantum Software Engineering (QSE) is essential for ensuring the reliability and maintainability of hybrid quantum-classical systems, yet empirical evidence on how bugs emerge and affect quality in real-world quantum projects remains limited. This study presents the first ecosystem-scale longitudinal analysis of software defects across 123 open source quantum repositories from 2012 to 2024, spanning eight functional categories, including full-stack libraries, simulators, annealing, algorithms, compilers, assembly, cryptography, and experimental computing. Using a mixed method approach combining repository mining, static code analysis, issue metadata extraction, and a validated rule-based classification framework, we analyze 32,296 verified bug reports. Results show that full-stack libraries and compilers are the most defect-prone categories due to circuit, gate, and transpilation-related issues, while simulators are mainly affected by measurement and noise modeling errors. Classical bugs primarily impact usability and interoperability, whereas quantum-specific bugs disproportionately degrade performance, maintainability, and reliability. Longitudinal analysis indicates ecosystem maturation, with defect densities peaking between 2017 and 2021 and declining thereafter. High-severity defects cluster in cryptography, experimental computing, and compiler toolchains. Repositories employing automated testing detect more defects and resolve issues faster. A negative binomial regression further shows that automated testing is associated with an approximate 60 percent reduction in expected defect incidence. Overall, this work provides the first large-scale data-driven characterization of quantum software defects and offers empirical guidance for improving testing, documentation, and maintainability practices in QSE.",
    "title_zh": "量子软件中缺陷与质量属性的特征分析：一项大规模实证研究",
    "abstract_zh": "量子软件工程（QSE）对于确保混合量子-经典系统在可靠性与可维护性方面至关重要，然而关于真实世界量子项目中缺陷如何产生及其对质量影响的实证研究仍然有限。本研究首次对2012年至2024年间共123个开源量子软件仓库的生态系统级纵向缺陷数据进行了分析，涵盖八大功能类别：全栈库、模拟器、退火算法、量子算法、编译器、汇编工具、密码学以及实验计算。研究采用混合方法，结合代码仓库挖掘、静态代码分析、问题元数据提取以及经过验证的基于规则的分类框架，分析了32,296份经核实的缺陷报告。结果表明，全栈库和编译器是缺陷最集中的类别，主要由于电路、门操作及编译转换相关问题；而模拟器类项目则主要受测量误差和噪声建模错误的影响。经典软件缺陷主要影响系统的可用性和互操作性，而量子特有缺陷则显著降低性能、可维护性与可靠性。纵向分析显示，整个生态体系呈现成熟化趋势，缺陷密度在2017至2021年间达到峰值后逐渐下降。高严重性缺陷集中于密码学、实验计算以及编译器工具链领域。采用自动化测试的项目能够发现更多缺陷，并以更快速度修复问题。负二项回归分析进一步表明，自动化测试可使预期缺陷发生率降低约60%。总体而言，本研究首次提供了大规模数据驱动的量子软件缺陷特征刻画，为提升QSE领域的测试实践、文档编写与可维护性管理提供了实证指导。"
  },
  {
    "date": "2025-12-31",
    "title": "DynaFix: Iterative Automated Program Repair Driven by Execution-Level Dynamic Information",
    "authors": "Zhili Huang, Ling Xu, Chao Liu, Weifeng Sun, Xu Zhang, Yan Lei, Meng Yan, Hongyu Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24635v1",
    "source": "arXiv",
    "abstract": "Automated Program Repair (APR) aims to automatically generate correct patches for buggy programs. Recent approaches leveraging large language models (LLMs) have shown promise but face limitations. Most rely solely on static analysis, ignoring runtime behaviors. Some attempt to incorporate dynamic signals, but these are often restricted to training or fine-tuning, or injected only once into the repair prompt, without iterative use. This fails to fully capture program execution. Current iterative repair frameworks typically rely on coarse-grained feedback, such as pass/fail results or exception types, and do not leverage fine-grained execution-level information effectively. As a result, models struggle to simulate human stepwise debugging, limiting their effectiveness in multi-step reasoning and complex bug repair. To address these challenges, we propose DynaFix, an execution-level dynamic information-driven APR method that iteratively leverages runtime information to refine the repair process. In each repair round, DynaFix captures execution-level dynamic information such as variable states, control-flow paths, and call stacks, transforming them into structured prompts to guide LLMs in generating candidate patches. If a patch fails validation, DynaFix re-executes the modified program to collect new execution information for the next attempt. This iterative loop incrementally improves patches based on updated feedback, similar to the stepwise debugging practices of human developers. We evaluate DynaFix on the Defects4J v1.2 and v2.0 benchmarks. DynaFix repairs 186 single-function bugs, a 10% improvement over state-of-the-art baselines, including 38 bugs previously unrepaired. It achieves correct patches within at most 35 attempts, reducing the patch search space by 70% compared with existing methods, thereby demonstrating both effectiveness and efficiency in repairing complex bugs.",
    "title_zh": "DynaFix：基于执行级动态信息的迭代式自动化程序修复",
    "abstract_zh": "自动化程序修复（APR）旨在自动为存在缺陷的程序生成正确补丁。近年来，基于大型语言模型（LLM）的方法展现出巨大潜力，但仍面临诸多局限。大多数方法仅依赖静态分析，忽略了程序运行时的行为特征；少数尝试引入动态信号的方法，其使用也往往局限于训练或微调阶段，或仅在修复提示中一次性注入动态信息，缺乏迭代利用。这导致无法充分捕捉程序的实际执行过程。当前的迭代式修复框架通常依赖于粗粒度反馈，如通过/失败结果或异常类型，未能有效利用细粒度的执行层面信息。因此，模型难以模拟人类开发者逐步调试的过程，限制了其在多步推理和复杂缺陷修复中的表现。\n\n为解决上述问题，我们提出DynaFix——一种基于执行级动态信息驱动的自动化程序修复方法，通过迭代地利用运行时信息来不断优化修复过程。在每一轮修复中，DynaFix会捕获变量状态、控制流路径、调用栈等执行层面的动态信息，并将其转化为结构化提示，引导LLM生成候选补丁。若补丁验证失败，DynaFix将重新执行修改后的程序，收集新的执行信息以支持下一次尝试。这一迭代循环使得补丁能够根据更新后的反馈逐步改进，类似于人类开发者的分步调试实践。\n\n我们在Defects4J v1.2和v2.0基准数据集上对DynaFix进行了评估。实验结果表明，DynaFix成功修复了186个单函数缺陷，较现有最先进基线方法提升10%，其中包括38个此前未被修复的缺陷。所有正确补丁均在最多35次尝试内生成，相比现有方法，补丁搜索空间减少了70%，充分体现了该方法在修复复杂缺陷方面的有效性与高效性。"
  },
  {
    "date": "2025-12-31",
    "title": "On the Effectiveness of Training Data Optimization for LLM-based Code Generation: An Empirical Study",
    "authors": "Shiqi Kuang, Zhao Tian, Tao Xiao, Dong Wang, Junjie Chen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24570v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) have achieved remarkable progress in code generation, largely driven by the availability of high-quality code datasets for effective training. To further improve data quality, numerous training data optimization techniques have been proposed; however, their overall effectiveness has not been systematically evaluated. To bridge this gap, we conduct the first large-scale empirical study, examining five widely-used training data optimization techniques and their pairwise combinations for LLM-based code generation across three benchmarks and four LLMs. Our results show that data synthesis is the most effective technique for improving functional correctness and reducing code smells, although it performs relatively worse on code maintainability compared to data refactoring, cleaning, and selection. Regarding combinations, we find that most combinations do not further improve functional correctness but can effectively enhance code quality (code smells and maintainability). Among all combinations, data synthesis combined with data refactoring achieves the strongest overall performance. Furthermore, our fine-grained analysis reinforces these findings and provides deeper insights into how individual techniques and their combinations influence code generation effectiveness. Overall, this work represents a first step toward a systematic understanding of training data optimization and combination strategies, offering practical guidance for future research and deployment in LLM-based code generation.",
    "title_zh": "基于大语言模型的代码生成中训练数据优化的有效性：一项实证研究",
    "abstract_zh": "大型语言模型（LLMs）在代码生成方面取得了显著进展，这主要得益于高质量代码数据集的可用性，为有效训练提供了支持。为了进一步提升数据质量，研究者们提出了多种训练数据优化技术；然而，这些技术的整体有效性尚未得到系统评估。为填补这一空白，我们开展了首次大规模实证研究，考察了五种广泛使用的训练数据优化技术及其两两组合在三个基准测试和四种LLM上的表现，以评估其对基于LLM的代码生成的影响。研究结果表明，数据合成是提升代码功能正确性并减少代码异味最有效的技术，尽管在代码可维护性方面相较于数据重构、清洗和选择等技术表现稍逊。关于组合策略，我们发现大多数组合虽未能进一步提升功能正确性，但能有效改善代码质量（包括代码异味和可维护性）。在所有组合中，数据合成与数据重构相结合展现出最强的综合性能。此外，我们的细粒度分析进一步验证了上述结论，并深入揭示了各项技术及其组合如何影响代码生成效果。总体而言，本研究为系统理解训练数据优化及其组合策略迈出了第一步，为未来在基于LLM的代码生成领域的研究与实际应用提供了切实可行的指导。"
  },
  {
    "date": "2025-12-31",
    "title": "MCPAgentBench: A Real-world Task Benchmark for Evaluating LLM Agent MCP Tool Use",
    "authors": "Wenrui Liu, Zixiang Liu, Elsie Dai, Wenhan Yu, Lei Yu, Tong Yang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24565v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) are increasingly serving as autonomous agents, and their utilization of external tools via the Model Context Protocol (MCP) is considered a future trend. Current MCP evaluation sets suffer from issues such as reliance on external MCP services and a lack of difficulty awareness. To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. We construct a dataset containing authentic tasks and simulated MCP tools. The evaluation employs a dynamic sandbox environment that presents agents with candidate tool lists containing distractors, thereby testing their tool selection and discrimination abilities. Furthermore, we introduce comprehensive metrics to measure both task completion rates and execution efficiency. Experiments conducted on various latest mainstream Large Language Models reveal significant performance differences in handling complex, multi-step tool invocations. All code is open-source at Github.",
    "title_zh": "MCPAgentBench：评估大语言模型代理MCP工具使用的现实任务基准",
    "abstract_zh": "大型语言模型（LLMs）正越来越多地作为自主代理使用，而通过模型上下文协议（MCP）调用外部工具被认为是未来的发展趋势。然而，当前的MCP评估数据集存在依赖外部MCP服务、缺乏难度感知等问题。为解决这些局限性，我们提出了MCPAgentBench——一个基于真实世界MCP定义构建的基准测试框架，用于评估智能体的工具使用能力。我们构建了一个包含真实任务和模拟MCP工具的数据集。评估过程采用动态沙盒环境，向智能体提供包含干扰项的候选工具列表，从而检验其工具选择与辨别能力。此外，我们引入了全面的评估指标，以衡量任务完成率和执行效率。在多个主流最新大型语言模型上的实验表明，它们在处理复杂、多步骤工具调用时表现出显著的性能差异。所有代码均已开源，发布于GitHub。"
  },
  {
    "date": "2025-12-31",
    "title": "How Do Agentic AI Systems Deal With Software Energy Concerns? A Pull Request-Based Study",
    "authors": "Tanjum Motin Mitul, Md. Masud Mazumder, Md Nahidul Islam Opu, Shaiful Chowdhury",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24636v1",
    "source": "arXiv",
    "abstract": "As Software Engineering enters its new era (SE 3.0), AI coding agents increasingly automate software development workflows. However, it remains unclear how exactly these agents recognize and address software energy concerns-an issue growing in importance due to large-scale data centers, energy-hungry language models, and battery-constrained devices. In this paper, we examined the energy awareness of agent-authored pull requests (PRs) using a publicly available dataset. We identified 216 energy-explicit PRs and conducted a thematic analysis, deriving a taxonomy of energy-aware work. Our further analysis of the applied optimization techniques shows that most align with established research recommendations. Although building and running these agents is highly energy intensive, encouragingly, the results indicate that they exhibit energy awareness when generating software artifacts. However, optimization-related PRs are accepted less frequently than others, largely due to their negative impact on maintainability.",
    "title_zh": "代理型人工智能系统如何应对软件能耗问题？一项基于拉取请求的研究",
    "abstract_zh": "随着软件工程进入新纪元（SE 3.0），人工智能编码代理正日益自动化软件开发流程。然而，这些代理如何识别并解决软件能耗问题仍不明确——这一问题因大规模数据中心、耗能巨大的语言模型以及电池容量受限的设备而愈发重要。本文利用一个公开可用的数据集，研究了由代理编写的拉取请求（PR）在能源意识方面的表现。我们识别出216个明确涉及能源问题的PR，并进行了主题分析，归纳出一套能源意识工作的分类体系。对所采用优化技术的进一步分析表明，大多数优化方法与现有研究建议相符。尽管构建和运行这些代理本身具有极高的能耗，但令人鼓舞的是，结果表明它们在生成软件制品时表现出一定的能源意识。然而，与优化相关的PR被接受的频率低于其他类型，这主要是因为其对可维护性产生了负面影响。"
  },
  {
    "date": "2025-12-31",
    "title": "Chat-Driven Optimal Management for Virtual Network Services",
    "authors": "Yuya Miyaoka, Masaki Inoue, Kengo Urata, Shigeaki Harada",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24614v1",
    "source": "arXiv",
    "abstract": "This paper proposes a chat-driven network management framework that integrates natural language processing (NLP) with optimization-based virtual network allocation, enabling intuitive and reliable reconfiguration of virtual network services. Conventional intent-based networking (IBN) methods depend on statistical language models to interpret user intent but cannot guarantee the feasibility of generated configurations. To overcome this, we develop a two-stage framework consisting of an Interpreter, which extracts intent from natural language prompts using NLP, and an Optimizer, which computes feasible virtual machine (VM) placement and routing via an integer linear programming. In particular, the Interpreter translates user chats into update directions, i.e., whether to increase, decrease, or maintain parameters such as CPU demand and latency bounds, thereby enabling iterative refinement of the network configuration. In this paper, two intent extractors, which are a Sentence-BERT model with support vector machine (SVM) classifiers and a large language model (LLM), are introduced. Experiments in single-user and multi-user settings show that the framework dynamically updates VM placement and routing while preserving feasibility. The LLM-based extractor achieves higher accuracy with fewer labeled samples, whereas the Sentence-BERT with SVM classifiers provides significantly lower latency suitable for real-time operation. These results underscore the effectiveness of combining NLP-driven intent extraction with optimization-based allocation for safe, interpretable, and user-friendly virtual network management.",
    "title_zh": "聊天驱动的虚拟网络服务最优管理",
    "abstract_zh": "本文提出了一种由聊天驱动的网络管理框架，将自然语言处理（NLP）与基于优化的虚拟网络分配相结合，实现了对虚拟网络服务的直观且可靠的重构。传统的意图驱动网络（IBN）方法依赖于统计语言模型来解析用户意图，但无法保证生成配置的可行性。为克服这一局限，我们设计了一个两阶段框架：第一阶段为“解释器”（Interpreter），利用自然语言处理技术从自然语言提示中提取用户意图；第二阶段为“优化器”（Optimizer），通过整数线性规划计算出可行的虚拟机（VM）部署与路由方案。特别地，解释器将用户聊天内容转化为更新方向，即明确参数（如CPU需求、延迟约束等）应增加、减少或保持不变，从而支持网络配置的迭代优化。本文引入了两种意图提取器：一种是基于Sentence-BERT模型结合支持向量机（SVM）分类器的方法，另一种是大型语言模型（LLM）。在单用户和多用户场景下的实验结果表明，该框架能够动态调整虚拟机部署与路由，同时确保配置的可行性。基于LLM的提取器在较少标注样本下即可实现更高的准确率，而Sentence-BERT结合SVM的方法则表现出显著更低的延迟，适用于实时操作。这些结果充分证明了将NLP驱动的意图提取与基于优化的资源分配相结合，在实现安全、可解释且用户友好的虚拟网络管理方面的有效性。"
  },
  {
    "date": "2025-12-31",
    "title": "A Tale of 1001 LoC: Potential Runtime Error-Guided Specification Synthesis for Verifying Large-Scale Programs",
    "authors": "Zhongyi Wang, Tengjie Lin, Mingshuai Chen, Haokun Li, Mingqi Yang, Xiao Yi, Shengchao Qin, Yixing Luo, Xiaofeng Li, Bin Gu, Liqiang Lu, Jianwei Yin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24594v1",
    "source": "arXiv",
    "abstract": "Fully automated verification of large-scale software and hardware systems is arguably the holy grail of formal methods. Large language models (LLMs) have recently demonstrated their potential for enhancing the degree of automation in formal verification by, e.g., generating formal specifications as essential to deductive verification, yet exhibit poor scalability due to long-context reasoning limitations and, more importantly, the difficulty of inferring complex, interprocedural specifications. This paper presents Preguss -- a modular, fine-grained framework for automating the generation and refinement of formal specifications. Preguss synergizes between static analysis and deductive verification by steering two components in a divide-and-conquer fashion: (i) potential runtime error-guided construction and prioritization of verification units, and (ii) LLM-aided synthesis of interprocedural specifications at the unit level. We show that Preguss substantially outperforms state-of-the-art LLM-based approaches and, in particular, it enables highly automated RTE-freeness verification for real-world programs with over a thousand LoC, with a reduction of 80.6%~88.9% human verification effort.",
    "title_zh": "一千行代码的故事：基于潜在运行时错误引导的规范合成以验证大规模程序",
    "abstract_zh": "对大规模软硬件系统进行完全自动化验证，可以说是形式化方法领域的“圣杯”。近年来，大型语言模型（LLMs）在提升形式化验证自动化程度方面展现出巨大潜力，例如生成演绎验证所必需的形式化规范。然而，由于长上下文推理能力的局限性，以及难以推断复杂、跨过程的规范，这些模型在可扩展性方面表现不佳。本文提出了一种名为 Preguss 的模块化、细粒度框架，用于自动化生成和精化形式化规范。Preguss 通过静态分析与演绎验证之间的协同作用，以分而治之的方式引导两个核心组件：(i) 基于潜在运行时错误的验证单元构建与优先级排序；(ii) 在单元级别上利用 LLM 辅助合成跨过程规范。实验表明，Preguss 显著优于现有的基于 LLM 的先进方法，并特别实现了对超过一千行代码的真实程序的高度自动化运行时错误无害性验证，使人工验证工作量减少了 80.6% 至 88.9%。"
  },
  {
    "date": "2025-12-31",
    "title": "Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time",
    "authors": "Zhenyu Zhang, Xiaoxia Wu, Zhongzhu Zhou, Qingyang Wu, Yineng Zhang, Pragaash Ponnusamy, Harikaran Subbaraj, Jue Wang, Shuaiwen Leon Song, Ben Athiwaratkun",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24574v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) often rely on long chain-of-thought (CoT) reasoning to solve complex tasks. While effective, these trajectories are frequently inefficient, leading to high latency from excessive token generation, or unstable reasoning that alternates between underthinking (shallow, inconsistent steps) and overthinking (repetitive, verbose reasoning). In this work, we study the structure of reasoning trajectories and uncover specialized attention heads that correlate with distinct cognitive behaviors such as verification and backtracking. By lightly intervening on these heads at inference time, we can steer the model away from inefficient modes. Building on this insight, we propose CREST, a training-free method for Cognitive REasoning Steering at Test-time. CREST has two components: (1) an offline calibration step that identifies cognitive heads and derives head-specific steering vectors, and (2) an inference-time procedure that rotates hidden representations to suppress components along those vectors. CREST adaptively suppresses unproductive reasoning behaviors, yielding both higher accuracy and lower computational cost. Across diverse reasoning benchmarks and models, CREST improves accuracy by up to 17.5% while reducing token usage by 37.6%, offering a simple and effective pathway to faster, more reliable LLM reasoning.",
    "title_zh": "理解与引导推理模型在测试时的认知行为",
    "abstract_zh": "大型语言模型（LLMs）通常依赖于长链式思维（CoT）推理来解决复杂任务。尽管这种方法有效，但其推理轨迹往往效率低下，导致因过度生成 token 而产生高延迟，或出现不稳定的推理过程——在思考不足（浅层、不一致的步骤）与过度思考（重复、冗长的推理）之间来回摇摆。在本研究中，我们分析了推理轨迹的结构，发现了一些与特定认知行为（如验证和回溯）相关联的注意力头。通过在推理阶段轻量级地干预这些注意力头，我们可以引导模型避开低效的推理模式。基于这一发现，我们提出了 CREST：一种无需训练的测试时认知推理引导方法。CREST 包含两个部分：(1) 离线校准步骤，用于识别认知相关注意力头并推导出针对每个头的引导向量；(2) 推理阶段的处理流程，通过旋转隐藏表示来抑制沿这些向量方向的分量。CREST 能自适应地抑制无益的推理行为，从而在提升准确率的同时降低计算开销。在多种推理基准和模型上，CREST 最多可将准确率提升 17.5%，同时减少 37.6% 的 token 使用量，为实现更快、更可靠的 LLM 推理提供了一条简单而高效的新路径。"
  },
  {
    "date": "2025-12-31",
    "title": "Recursive Language Models",
    "authors": "Alex L. Zhang, Tim Kraska, Omar Khattab",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24601v1",
    "source": "arXiv",
    "abstract": "We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.",
    "title_zh": "递归语言模型",
    "abstract_zh": "我们从推理时扩展的角度，研究让大型语言模型（LLMs）处理任意长提示词的可行性。我们提出了递归语言模型（Recursive Language Models, RLMs），这是一种通用的推理策略，将长提示词视为外部环境的一部分，使LLM能够以编程方式检查、分解并递归调用自身来处理提示词的片段。我们发现，RLMs能够成功处理超出模型上下文窗口两个数量级的输入；即使对于较短的提示词，其在四项不同类型的长上下文任务中，也显著优于基础LLM和常见的长上下文框架，同时每次查询的成本相当（或更低）。"
  },
  {
    "date": "2025-12-31",
    "title": "MultiRisk: Multiple Risk Control via Iterative Score Thresholding",
    "authors": "Sunay Joshi, Yan Sun, Hamed Hassani, Edgar Dobriban",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24587v1",
    "source": "arXiv",
    "abstract": "As generative AI systems are increasingly deployed in real-world applications, regulating multiple dimensions of model behavior has become essential. We focus on test-time filtering: a lightweight mechanism for behavior control that compares performance scores to estimated thresholds, and modifies outputs when these bounds are violated. We formalize the problem of enforcing multiple risk constraints with user-defined priorities, and introduce two efficient dynamic programming algorithms that leverage this sequential structure. The first, MULTIRISK-BASE, provides a direct finite-sample procedure for selecting thresholds, while the second, MULTIRISK, leverages data exchangeability to guarantee simultaneous control of the risks. Under mild assumptions, we show that MULTIRISK achieves nearly tight control of all constraint risks. The analysis requires an intricate iterative argument, upper bounding the risks by introducing several forms of intermediate symmetrized risk functions, and carefully lower bounding the risks by recursively counting jumps in symmetrized risk functions between appropriate risk levels. We evaluate our framework on a three-constraint Large Language Model alignment task using the PKU-SafeRLHF dataset, where the goal is to maximize helpfulness subject to multiple safety constraints, and where scores are generated by a Large Language Model judge and a perplexity filter. Our experimental results show that our algorithm can control each individual risk at close to the target level.",
    "title_zh": "MultiRisk：通过迭代评分阈值实现多重风险控制",
    "abstract_zh": "随着生成式AI系统在现实应用中日益普及，对模型行为的多维度调控变得至关重要。本文聚焦于测试时过滤（test-time filtering）——一种轻量级的行为控制机制，该机制通过将性能得分与预估阈值进行比较，在违反这些边界时调整输出。我们形式化了在用户定义优先级下同时满足多个风险约束的问题，并提出了两种高效的动态规划算法，以利用这一序列结构。第一种算法MULTIRISK-BASE提供了一种直接的有限样本阈值选择方法；第二种算法MULTIRISK则借助数据可交换性，确保对各项风险实现同步控制。在较弱假设条件下，我们证明MULTIRISK能够近乎紧致地控制所有约束风险。分析过程涉及复杂的迭代论证：通过引入多种中间对称化风险函数来上界风险，再通过递归计数对称化风险函数在适当风险水平之间的“跳跃”来精确下界风险。我们在使用PKU-SafeRLHF数据集的三重约束大语言模型对齐任务中评估了本框架，目标是在满足多项安全约束的前提下最大化有用性，其中评分由大语言模型评判器和困惑度过滤器生成。实验结果表明，我们的算法能够将每项个体风险控制在接近目标水平。"
  },
  {
    "date": "2025-12-31",
    "title": "Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search",
    "authors": "Rohit Dwivedula, Divyanshu Saxena, Sujay Yadalam, Daehyeok Kim, Aditya Akella",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.25065v1",
    "source": "arXiv",
    "abstract": "Resource-management tasks in modern operating and distributed systems continue to rely primarily on hand-designed heuristics for tasks such as scheduling, caching, or active queue management. Designing performant heuristics is an expensive, time-consuming process that we are forced to continuously go through due to the constant flux of hardware, workloads and environments. We propose a new alternative: synthesizing instance-optimal heuristics -- specialized for the exact workloads and hardware where they will be deployed -- using code-generating large language models (LLMs). To make this synthesis tractable, Vulcan separates policy and mechanism through LLM-friendly, task-agnostic interfaces. With these interfaces, users specify the inputs and objectives of their desired policy, while Vulcan searches for performant policies via evolutionary search over LLM-generated code. This interface is expressive enough to capture a wide range of system policies, yet sufficiently constrained to allow even small, inexpensive LLMs to generate correct and executable code. We use Vulcan to synthesize performant heuristics for cache eviction and memory tiering, and find that these heuristics outperform all human-designed state-of-the-art algorithms by upto 69% and 7.9% in performance for each of these tasks respectively.",
    "title_zh": "Vulcan：通过大语言模型驱动搜索实现实例最优的系统启发式方法",
    "abstract_zh": "现代操作系统和分布式系统中的资源管理任务，仍然主要依赖于人工设计的启发式算法来完成调度、缓存或主动队列管理等任务。设计高性能的启发式算法是一项成本高昂且耗时的过程，由于硬件、工作负载和运行环境持续变化，我们必须不断重复这一过程。我们提出了一种新方法：利用生成代码的大语言模型（LLM）合成针对特定部署环境和工作负载的实例最优启发式算法。为了使这种合成过程可行，Vulcan 通过 LLM 友好的、与任务无关的接口，将策略与机制分离。借助这些接口，用户只需描述其期望策略的输入和目标，而 Vulcan 则通过在 LLM 生成的代码上进行进化搜索，自动寻找性能优异的策略。该接口具有足够的表达能力，能够涵盖广泛的系统策略，同时又足够受限，使得小型且低成本的 LLM 也能生成正确且可执行的代码。我们使用 Vulcan 为缓存淘汰和内存分层任务合成了高性能的启发式算法，结果表明，这些算法在各自任务上的性能分别优于所有由人类设计的最先进算法，提升幅度最高达 69% 和 7.9%。"
  },
  {
    "date": "2025-12-31",
    "title": "Feature Slice Matching for Precise Bug Detection",
    "authors": "Ke Ma, Jianjun Huang, Wei You, Bin Liang, Jingzheng Wu, Yanjun Wu, Yuanjun Gong",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24858v1",
    "source": "arXiv",
    "abstract": "Measuring the function similarity to detect bugs is effective, but the statements unrelated to the bugs can impede the performance due to the noise interference. Suppressing the noise interference in existing works does not manage the tough job, i.e., eliminating the noise in the targets. In this paper, we propose MATUS to mitigate the target noise for precise bug detection based on similarity measurement. Feature slices are extracted from both the buggy query and the targets to represent the semantic feature of (potential) bug logics. In particular, MATUS guides the target slicing with the prior knowledge from the buggy code, in an end-to-end way to pinpoint the slicing criterion in the targets. All feature slices are embedded and compared based on the vector similarity. Buggy candidates are audited to confirm unknown bugs in the targets. Experiments show that MATUS holds advantages in bug detection for real-world projects with acceptable efficiency. In total, MATUS has spotted 31 unknown bugs in the Linux kernel. All of them have been confirmed by the kernel developers, and 11 have been assigned CVEs.",
    "title_zh": "特征切片匹配用于精确的缺陷检测",
    "abstract_zh": "基于函数相似性检测漏洞是有效的，但与漏洞无关的语句会因噪声干扰而影响性能。现有方法对噪声干扰的抑制并未真正解决核心问题，即消除目标代码中的噪声。本文提出MATUS，通过相似性度量来缓解目标代码中的噪声，实现更精确的漏洞检测。MATUS从存在缺陷的查询代码和目标代码中提取特征切片，以表征（潜在）漏洞逻辑的语义特征。特别地，MATUS以端到端的方式，利用有缺陷代码中的先验知识指导目标代码的切片过程，从而精准定位目标代码中的切片标准。所有特征切片均被嵌入并基于向量相似性进行比较。随后对可疑候选漏洞进行人工审核，以确认目标代码中隐藏的未知漏洞。实验结果表明，MATUS在真实世界项目中具有显著的漏洞检测优势，且效率可接受。总计，MATUS在Linux内核中发现了31个未知漏洞，全部经内核开发者确认，其中11个已分配CVE编号。"
  },
  {
    "date": "2025-12-31",
    "title": "Advances in Agentic AI: Back to the Future",
    "authors": "Sergio Alvarez-Telena, Marta Diez-Fernandez",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24856v1",
    "source": "arXiv",
    "abstract": "In light of the recent convergence between Agentic AI and our field of Algorithmization, this paper seeks to restore conceptual clarity and provide a structured analytical framework for an increasingly fragmented discourse. First, (a) it examines the contemporary landscape and proposes precise definitions for the key notions involved, ranging from intelligence to Agentic AI. Second, (b) it reviews our prior body of work to contextualize the evolution of methodologies and technological advances developed over the past decade, highlighting their interdependencies and cumulative trajectory. Third, (c) by distinguishing Machine and Learning efforts within the field of Machine Learning (d) it introduces the first Machine in Machine Learning (M1) as the underlying platform enabling today's LLM-based Agentic AI, conceptualized as an extension of B2C information-retrieval user experiences now being repurposed for B2B transformation. Building on this distinction, (e) the white paper develops the notion of the second Machine in Machine Learning (M2) as the architectural prerequisite for holistic, production-grade B2B transformation, characterizing it as Strategies-based Agentic AI and grounding its definition in the structural barriers-to-entry that such systems must overcome to be operationally viable. Further, (f) it offers conceptual and technical insight into what appears to be the first fully realized implementation of an M2. Finally, drawing on the demonstrated accuracy of the two previous decades of professional and academic experience in developing the foundational architectures of Algorithmization, (g) it outlines a forward-looking research and transformation agenda for the coming two decades.",
    "title_zh": "代理型人工智能的进展：重返未来",
    "abstract_zh": "鉴于代理型人工智能（Agentic AI）与我们所处的算法化领域之间近期的融合趋势，本文旨在恢复概念上的清晰性，并为日益碎片化的讨论提供一个结构化的分析框架。首先，（a）本文审视了当前的发展格局，提出了涉及智能、代理型人工智能等核心概念的精确定义。其次，（b）回顾了我们过去的研究成果，以梳理过去十年中方法论与技术进步的演进脉络，突出其相互依赖关系及累积发展的轨迹。第三，（c）通过区分机器学习领域中的“机器”与“学习”两个维度，（d）本文首次提出“机器学习中的第一台机器”（M1），将其作为当今基于大语言模型（LLM）的代理型人工智能的底层平台，这一平台可被理解为对原有面向消费者（B2C）信息检索用户体验的延伸，并正被重新定位用于企业级（B2B）转型。在此区分的基础上，（e）本白皮书进一步提出“机器学习中的第二台机器”（M2）的概念，将其视为实现全面、可投入生产的B2B转型的架构前提，将M2界定为基于策略的代理型人工智能，并从系统必须克服的结构性进入壁垒出发，确立其定义基础。此外，（f）本文还深入探讨了目前看来首个真正实现的M2系统的概念与技术特征。最后，基于过去二十年在算法化基础架构开发方面积累的专业与学术经验所展现出的高度准确性，（g）本文勾勒出未来二十年的研究与转型蓝图，为该领域的持续发展指明方向。"
  },
  {
    "date": "2025-12-31",
    "title": "MUSIC: MUlti-Step Instruction Contrast for Multi-Turn Reward Models",
    "authors": "Wenzhe Li, Shujian Zhang, Wenxuan Zhou, John Lambert, Chi Jin, Andrew Hard, Rajiv Mathews, Lun Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24693v1",
    "source": "arXiv",
    "abstract": "Evaluating the quality of multi-turn conversations is crucial for developing capable Large Language Models (LLMs), yet remains a significant challenge, often requiring costly human evaluation. Multi-turn reward models (RMs) offer a scalable alternative and can provide valuable signals for guiding LLM training. While recent work has advanced multi-turn \\textit{training} techniques, effective automated \\textit{evaluation} specifically for multi-turn interactions lags behind. We observe that standard preference datasets, typically contrasting responses based only on the final conversational turn, provide insufficient signal to capture the nuances of multi-turn interactions. Instead, we find that incorporating contrasts spanning \\textit{multiple} turns is critical for building robust multi-turn RMs. Motivated by this finding, we propose \\textbf{MU}lti-\\textbf{S}tep \\textbf{I}nstruction \\textbf{C}ontrast (MUSIC), an unsupervised data augmentation strategy that synthesizes contrastive conversation pairs exhibiting differences across multiple turns. Leveraging MUSIC on the Skywork preference dataset, we train a multi-turn RM based on the Gemma-2-9B-Instruct model. Empirical results demonstrate that our MUSIC-augmented RM outperforms baseline methods, achieving higher alignment with judgments from advanced proprietary LLM judges on multi-turn conversations, crucially, without compromising performance on standard single-turn RM benchmarks.",
    "title_zh": "音乐：用于多轮奖励模型的多步指令对比",
    "abstract_zh": "评估多轮对话的质量对于开发高性能大语言模型（LLMs）至关重要，但这一任务仍面临巨大挑战，通常需要昂贵的人工评估。多轮奖励模型（RMs）提供了一种可扩展的替代方案，能够为LLM训练提供有价值的指导信号。尽管近期研究在多轮训练技术方面取得了进展，但针对多轮交互的有效自动化评估方法却相对滞后。我们观察到，传统的偏好数据集通常仅基于对话的最后一轮进行响应对比，所提供的信号不足以捕捉多轮交互中的细微差别。相反，我们发现引入跨越多个对话轮次的对比信息对于构建稳健的多轮奖励模型至关重要。基于这一发现，我们提出了**MUSIC**——一种无监督的数据增强策略，即**多步指令对比**（Multi-Step Instruction Contrast），该策略能合成在多个对话轮次中存在差异的对比性对话对。我们在Skywork偏好数据集上应用MUSIC，基于Gemma-2-9B-Instruct模型训练了一个多轮奖励模型。实验结果表明，经过MUSIC增强的奖励模型在多轮对话上的表现优于基线方法，与先进专有LLM评判者的一致性更高，同时并未牺牲在标准单轮奖励模型基准测试中的性能。"
  },
  {
    "date": "2025-12-31",
    "title": "How Do Agentic AI Systems Address Performance Optimizations? A BERTopic-Based Analysis of Pull Requests",
    "authors": "Md Nahidul Islam Opu, Shahidul Islam, Muhammad Asaduzzaman, Shaiful Chowdhury",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24630v1",
    "source": "arXiv",
    "abstract": "LLM-based software engineering is influencing modern software development. In addition to correctness, prior studies have also examined the performance of software artifacts generated by AI agents. However, it is unclear how exactly the agentic AI systems address performance concerns in practice. In this paper, we present an empirical study of performance-related pull requests generated by AI agents. Using LLM-assisted detection and BERTopic-based topic modeling, we identified 52 performance-related topics grouped into 10 higher-level categories. Our results show that AI agents apply performance optimizations across diverse layers of the software stack and that the type of optimization significantly affects pull request acceptance rates and review times. We also found that performance optimization by AI agents primarily occurs during the development phase, with less focus on the maintenance phase. Our findings provide empirical evidence that can support the evaluation and improvement of agentic AI systems with respect to their performance optimization behaviors and review outcomes.",
    "title_zh": "代理型AI系统如何应对性能优化？基于BERTopic的拉取请求分析",
    "abstract_zh": "基于大语言模型（LLM）的软件工程正在深刻影响现代软件开发。除了关注代码正确性外，以往的研究也探讨了由AI代理生成的软件制品在性能方面的表现。然而，目前尚不明确这些智能代理在实际中如何应对性能相关问题。本文通过一项实证研究，分析了由AI代理提交的与性能相关的代码合并请求（pull requests）。我们采用LLM辅助检测与BERTopic主题建模方法，识别出52个与性能相关的具体主题，并将其归纳为10个更高层次的类别。研究结果表明，AI代理在软件栈的多个层级上实施了性能优化，且优化类型显著影响了代码合并请求的接受率和评审时长。此外，我们发现AI代理的性能优化主要集中在开发阶段，而在维护阶段的关注度相对较低。本研究的发现为评估和改进智能代理系统在性能优化行为及其评审结果方面的表现提供了实证依据。"
  },
  {
    "date": "2025-12-31",
    "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models",
    "authors": "Junru Lu, Jiarui Qin, Lingfeng Qiao, Yinghui Li, Xinyi Dai, Bo Ke, Jianfeng He, Ruizhi Qiao, Di Yin, Xing Sun, Yunsheng Wu, Yinsong Liu, Shuangyin Liu, Mingkong Tang, Haodong Lin, Jiayi Kuang, Fanxu Meng, Xiaojuan Tang, Yunjia Xi, Junjie Huang, Haotong Yang, Zhenyi Shen, Yangning Li, Qianwen Zhang, Yifei Yu, Siyu An, Junnan Dong, Qiufeng Wang, Jie Wang, Keyu Chen, Wei Wen, Taian Guo, Zhifeng Shen, Daohai Yu, Jiahao Li, Ke Li, Zongyi Li, Xiaoyu Tan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24618v1",
    "source": "arXiv",
    "abstract": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.",
    "title_zh": "优图-LLM：解锁轻量级大语言模型的原生智能体潜力",
    "abstract_zh": "我们推出了 Youtu-LLM，这是一个轻量级但功能强大的语言模型，兼具高计算效率与原生的智能体（agentic）能力。与依赖知识蒸馏的典型小型模型不同，Youtu-LLM（19.6亿参数）从零开始进行预训练，系统性地培养其推理与规划能力。其核心技术突破如下：\n\n（1）紧凑架构支持长上下文：基于一种密集型多潜空间注意力（MLA）架构，并采用面向STEM领域的新型词汇表，Youtu-LLM 支持高达 128k 的上下文窗口。该设计在极小的内存开销下实现了强大的长上下文推理与状态追踪能力，特别适用于长周期智能体任务和复杂推理场景。\n\n（2）科学合理的“常识-STEM-智能体”渐进式训练课程：我们构建了一个约 11T 标记的超大规模语料库，并实施了多阶段训练策略。通过逐步将预训练数据分布从通用常识过渡到复杂的 STEM 问题及智能体任务，确保模型获得深层次的认知能力，而非仅停留在表面的对齐表现。\n\n（3）可扩展的智能体中期训练机制：针对智能体能力的中期训练，我们采用多样化的数据构造方法，在数学、编程和工具使用等多个领域合成丰富且多样的行为轨迹。高质量的数据使模型能够有效内化规划与反思等关键行为模式。\n\n大量实验证明，Youtu-LLM 在参数量低于 20 亿的语言模型中达到了新的性能标杆。在通用基准测试中，其表现可媲美更大规模模型；而在智能体专用任务上，更是显著超越现有最先进基线，充分证明轻量级模型同样具备强大的内在智能体能力。"
  },
  {
    "date": "2025-12-31",
    "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
    "authors": "Xingwei Qu, Shaowen Wang, Zihao Huang, Kai Hua, Fan Yin, Rui-Jie Zhu, Jundong Zhou, Qiyang Min, Zihao Wang, Yizhi Li, Tianyu Zhang, He Xing, Zheng Zhang, Yuxuan Song, Tianyu Zheng, Zhiyuan Zeng, Chenghua Lin, Ge Zhang, Wenhao Huang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24617v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose $\\textbf{Dynamic Large Concept Models (DLCM)}$, a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first $\\textbf{compression-aware scaling law}$, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a $\\textbf{decoupled $μ$P parametrization}$ that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting ($R=4$, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a $\\textbf{+2.69$\\%$ average improvement}$ across 12 zero-shot benchmarks under matched inference FLOPs.",
    "title_zh": "动态大概念模型：自适应语义空间中的潜在推理",
    "abstract_zh": "大型语言模型（LLMs）对所有标记（tokens）采用统一的计算方式，尽管语言本身具有高度非均匀的信息密度。这种“标记均匀”的计算模式在局部可预测的文本片段上浪费了大量算力，同时却未能为语义关键的转换部分分配足够的计算资源。为此，我们提出**动态大概念模型（Dynamic Large Concept Models, DLCM）**，这是一种分层语言建模框架，能够从潜在表示中学习语义边界，并将计算从标记层面转移到一个压缩后的概念空间，在该空间中推理更加高效。DLCM 以端到端的方式发现变长的概念，无需依赖预定义的语言单元。分层压缩从根本上改变了模型的扩展规律。我们首次提出了**压缩感知扩展定律（compression-aware scaling law）**，该定律将标记级容量、概念级推理能力与压缩比解耦，从而在固定浮点运算量（FLOPs）条件下实现有原则的算力分配。为了稳定训练这一异构架构，我们进一步开发了**解耦的μP参数化方法**，支持在不同宽度和压缩率之间实现零样本超参数迁移。在实际设置下（压缩比 $R=4$，即每个概念平均对应四个标记），DLCM 将约三分之一的推理算力重新分配至更高容量的推理主干网络，在匹配推理 FLOPs 的前提下，于12个零样本基准测试中实现了**平均提升+2.69%** 的性能表现。"
  },
  {
    "date": "2025-12-31",
    "title": "Localized Calibrated Uncertainty in Code Language Models",
    "authors": "David Gros, Prem Devanbu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24560v1",
    "source": "arXiv",
    "abstract": "Large Language models (LLMs) can generate complicated source code from natural language prompts. However, LLMs can generate output that deviates from what the user wants, requiring supervision and editing. To support this process, we offer techniques to localize where generations might be misaligned from user intent. We first create a dataset of \"Minimal Intent Aligning Patches\" of repaired LLM generated programs. Each program uses test cases to verify correctness. After creating a dataset of programs, we measure how well various techniques can assign a well-calibrated probability to indicate which parts of code will be edited in a minimal patch (i.e., give a probability that corresponds with empirical odds it is edited). We compare white-box probing (where we propose a technique for efficient arbitrary-span querying), against black-box reflective and self-consistency based approaches. We find probes with a small supervisor model can achieve low calibration error and Brier Skill Score of approx 0.2 estimating edited lines on code generated by models many orders of magnitude larger. We discuss the generalizability of the techniques, and the connections to AI oversight and control, finding a probe trained only on code shows some signs of generalizing to natural language errors if new probability scaling is allowed.",
    "title_zh": "代码语言模型中的局部校准不确定性",
    "abstract_zh": "大型语言模型（LLMs）能够根据自然语言提示生成复杂的源代码。然而，LLMs 生成的输出有时会偏离用户的真实意图，因此需要人工监督和修改。为了支持这一过程，我们提出了一种技术，用于定位生成代码中可能与用户意图不一致的部分。我们首先构建了一个名为“最小意图对齐补丁”（Minimal Intent Aligning Patches）的数据集，其中每个修复后的程序都通过测试用例验证其正确性。在建立该数据集后，我们评估了多种技术在为代码中可能被最小补丁修改的部分分配合理概率方面的表现（即：给出的概率应与实际被修改的可能性相匹配）。我们比较了白盒探测方法（提出一种高效的任意跨度查询技术）与黑盒反思式及自一致性方法。结果表明，仅使用一个小型监督模型作为探测器，即可在由远超其规模的大型模型生成的代码上实现约 0.2 的低校准误差和 Brier 技能得分，准确估计出将被修改的代码行。我们还讨论了这些技术的泛化能力，以及其与人工智能监督与控制的关系；发现仅在代码上训练的探测器，在允许新的概率缩放机制的情况下，表现出向自然语言错误泛化的初步迹象。"
  },
  {
    "date": "2025-12-31",
    "title": "Many Minds from One Model: Bayesian Transformers for Population Intelligence",
    "authors": "Diji Yang, Yi Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.25063v1",
    "source": "arXiv",
    "abstract": "Despite their scale and success, modern transformers are almost universally trained as single-minded systems: optimization produces one deterministic set of parameters, representing a single functional hypothesis about the data. Motivated by the idea that intelligence emerge from many minds, we propose Population Bayesian Transformers (B-Trans), which transform a standard Large Language Model into a Bayesian Transformer model to supports sampling diverse yet coherent model instances from a single set of pre-trained weights. B-Trans introduces a Bayesian-motivated posterior proxy by treating the bias-like offsets in normalization layers as stochastic variables with a Gaussian variational approximation, inducing a distribution over model behavior without the cost of training full Bayesian neural networks. Sampling from this proxy yields a set of model instances with diverse behaviors while maintaining general competence. To preserve coherence within each generation, we freeze the sampled noise at the sequence level, enforcing temporal consistency across tokens. B-Trans allows for population-level decision-making, where aggregating predictions across sampled individuals significantly enhances exploration. Experiments across zero-shot generation, Reinforcement Learning with Verifiable Rewards (RLVR), and RL without explicit labels demonstrate that B-Trans effectively leverage the wisdom of crowds, yielding superior semantic diversity while achieving better task performance compared to deterministic baselines.",
    "title_zh": "一模多思：用于群体智能的贝叶斯Transformer",
    "abstract_zh": "尽管规模庞大且成果显著，现代Transformer模型几乎都是以单一目标为导向进行训练的：优化过程产生一组确定性的参数，代表了对数据的一种单一功能假设。受“智能源于多元思维”这一理念的启发，我们提出了**种群贝叶斯Transformer（B-Trans）**，将标准的大语言模型转化为一个贝叶斯Transformer模型，能够在单个预训练权重集合的基础上，采样出多样且连贯的模型实例。B-Trans通过将归一化层中的偏置型偏移量视为具有高斯变分近似的随机变量，引入了一种基于贝叶斯思想的后验代理，从而在不付出训练完整贝叶斯神经网络成本的前提下，为模型行为引入分布特性。从该代理中采样可得到一系列行为各异但整体仍具备通用能力的模型实例。为了确保生成过程内部的一致性，我们在序列级别冻结所采样的噪声，以保证跨标记的时间一致性。B-Trans支持群体层面的决策机制，通过对多个采样个体的预测进行聚合，显著增强了探索能力。在零样本生成、可验证奖励强化学习（RLVR）以及无显式标签的强化学习等任务上的实验表明，B-Trans能够有效利用“群体智慧”，在实现更优任务表现的同时，显著提升语义多样性，优于传统的确定性基线模型。"
  },
  {
    "date": "2025-12-31",
    "title": "Modeling Language as a Sequence of Thoughts",
    "authors": "Nasim Borazjanizadeh, James McClelland",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.25026v1",
    "source": "arXiv",
    "abstract": "Transformer language models can generate strikingly natural text by modeling language as a sequence of tokens. Yet, by relying primarily on surface-level co-occurrence statistics, they fail to form globally consistent latent representations of entities and events, lack of which contributes to brittleness in relational direction (e.g., reversal curse), contextualization errors, and data inefficiency. On the other hand, cognitive science shows that human comprehension involves converting the input linguistic stream into compact, event-like representations that persist in memory while verbatim form is short-lived. Motivated by this view, we introduce Thought Gestalt (TG) model, a recurrent Transformer that models language at two levels of abstraction - tokens and sentence-level \"thought\" states. TG generates the tokens of one sentence at a time while cross-attending to a memory of prior sentence representations. In TG, token and sentence representations are generated using the same set of model parameters and trained with a single objective, the next-token cross-entropy: by retaining the computation graph of sentence representations written to memory, gradients from future token losses flow backward through cross-attention to optimize the parameters generating earlier sentence vectors. In scaling experiments, TG consistently improves efficiency over matched GPT-2 runs, among other baselines, with scaling fits indicating GPT-2 requires ~5-8% more data and ~33-42% more parameters to match TG's loss. TG also reduces errors on relational direction generalization on a father-son reversal curse probe.",
    "title_zh": "将语言建模为一系列思想",
    "abstract_zh": "Transformer语言模型通过将语言建模为一系列标记（token）的序列，能够生成极为自然的文本。然而，由于主要依赖于表层的共现统计信息，它们难以形成对实体和事件的全局一致的潜在表示，这种缺失导致了关系方向上的脆弱性（例如“反转诅咒”问题）、上下文理解错误以及数据利用效率低下。相比之下，认知科学表明，人类的理解过程涉及将输入的语言流转化为紧凑的、类事件的表征，这些表征在记忆中持续存在，而原始语言形式则很快消逝。\n\n受此观点启发，我们提出了Thought Gestalt（TG）模型——一种具有递归结构的Transformer，它在两个抽象层次上对语言进行建模：标记级别和句子级别的“思想”状态。TG逐句生成标记，并通过跨注意力机制访问先前句子表征的记忆。在TG中，标记与句子的表征均由同一组模型参数生成，并通过单一目标函数——下一个标记的交叉熵损失进行训练。通过保留写入记忆的句子表征的计算图，未来标记损失所产生的梯度可反向穿过跨注意力机制，从而优化早期句子向量生成的参数。\n\n在扩展性实验中，TG在多个基准测试中持续优于对应规模的GPT-2模型，其缩放拟合结果表明，GPT-2需要约5%-8%更多的数据，以及约33%-42%更多的参数，才能达到TG的损失水平。此外，TG在“父亲-儿子反转诅咒”探测任务中显著降低了关系方向泛化错误。"
  },
  {
    "date": "2025-12-31",
    "title": "PhysTalk: Language-driven Real-time Physics in 3D Gaussian Scenes",
    "authors": "Luca Collorone, Mert Kiray, Indro Spinelli, Fabio Galasso, Benjamin Busam",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.24986v1",
    "source": "arXiv",
    "abstract": "Realistic visual simulations are omnipresent, yet their creation requires computing time, rendering, and expert animation knowledge. Open-vocabulary visual effects generation from text inputs emerges as a promising solution that can unlock immense creative potential. However, current pipelines lack both physical realism and effective language interfaces, requiring slow offline optimization. In contrast, PhysTalk takes a 3D Gaussian Splatting (3DGS) scene as input and translates arbitrary user prompts into real time, physics based, interactive 4D animations. A large language model (LLM) generates executable code that directly modifies 3DGS parameters through lightweight proxies and particle dynamics. Notably, PhysTalk is the first framework to couple 3DGS directly with a physics simulator without relying on time consuming mesh extraction. While remaining open vocabulary, this design enables interactive 3D Gaussian animation via collision aware, physics based manipulation of arbitrary, multi material objects. Finally, PhysTalk is train-free and computationally lightweight: this makes 4D animation broadly accessible and shifts these workflows from a \"render and wait\" paradigm toward an interactive dialogue with a modern, physics-informed pipeline.",
    "title_zh": "PhysTalk：基于语言的3D高斯场景中实时物理模拟",
    "abstract_zh": "现实中的视觉模拟无处不在，但其创建过程需要大量的计算时间、渲染以及专业的动画知识。从文本输入生成开放词汇的视觉特效，正成为一种极具前景的解决方案，能够释放巨大的创意潜力。然而，当前的流程在物理真实性和有效的语言交互界面方面均存在不足，且依赖缓慢的离线优化。相比之下，PhysTalk以3D高斯溅射（3DGS）场景为输入，将任意用户提示实时转化为基于物理的、可交互的4D动画。通过大型语言模型（LLM）生成可执行代码，该代码通过轻量级代理和粒子动力学直接修改3DGS参数。值得注意的是，PhysTalk是首个无需依赖耗时的网格提取，即可将3DGS与物理模拟器直接耦合的框架。在保持开放词汇特性的前提下，这一设计实现了对任意多材质物体的、具备碰撞感知能力的、基于物理的交互式3D高斯动画。最后，PhysTalk无需训练且计算开销小，使4D动画变得广泛可及，并将工作流程从传统的“渲染后等待”模式转变为与现代、具备物理先验知识的系统之间的互动对话。"
  },
  {
    "date": "2025-12-31",
    "title": "AIware in the Foundation Model Era",
    "authors": "Zhen Ming Jiang, Ahmed E. Hassan, Thomas Zimmermann, Mark Harman",
    "publish": "IEEE Software",
    "url": "https://doi.org/10.1109/ms.2025.3624446",
    "source": "IEEE",
    "abstract": "Foundation models (FMs) and other AI advances have shifted the center of gravity in software. Natural language has become a credible interface for intent, planning, and even execution. The result is software for all—and by all: a world where more people can shape software, and where expert engineers amplify their impact by partnering with AI. This is not a story of replacement; it is a story of symbiosis among humans, intelligent agents, and proven software engineering (SE) methods that we now need to update for a new era.",
    "title_zh": "大模型时代的人工智能基础设施",
    "abstract_zh": "基础模型（FMs）及其他人工智能进展已改变了软件领域的重心。自然语言已成为表达意图、规划乃至执行的可信接口。其结果是，软件将属于所有人，也由所有人共同创造：一个更多人能够参与塑造软件的世界，而专业工程师则通过与人工智能协作，进一步放大自身影响力。这并非关于替代的故事，而是人类、智能代理与经过验证的软件工程方法之间协同共生的新篇章——我们如今需要为这一新时代重新定义和更新这些方法。"
  },
  {
    "date": "2025-12-31",
    "title": "PAMI-GPT: A Conversational Custom-GPT Model for Pattern Mining",
    "authors": "Madhavi PALLA, Arjun Chakravarthi POGAKU, Uday Kiran RAGE",
    "publish": "2025 IEEE 18th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC)",
    "url": "https://doi.org/10.1109/mcsoc67473.2025.00024",
    "source": "IEEE",
    "abstract": "Large Language Models (LLMs) play a crucial role in natural language conversations. While they perform well on general tasks that do not require Domain-Specific Knowledge (DSK), conventional LLMs often generate incorrect responses for tasks that demand such expertise. To address this challenge, researchers have proposed fine-tuning approaches such as Parameter-Efficient Fine-Tuning (PEFT) and instruction tuning. Although effective, these methods require substantial computational resources. To overcome this limitation, OpenAI introduced the custom-GPT framework, which enables the integration of DSK without the high computational cost of traditional finetuning. Building on this framework, we present Pattern MiningGPT (PAMI-GPT), a domain-specialized LLM that combines a structured Domain Knowledge Base (DKB) of pattern mining with the GPT-5 architecture. Pattern mining, a fundamental technique in big data analytics that extracts various types of interesting patterns from large-scale data, has long remained restricted to experts due to its implementation complexity. PAMIGPT addresses this barrier by providing precise algorithm definitions, valid parameter configurations, and reproducible code. Experimental evaluations under zero-shot, one-shot, and few-shot prompting demonstrate superior accuracy, reliability, and domain consistency compared to leading LLMs. A case study further highlights its capability to generate executable pattern mining workflows, making advanced techniques more accessible to non-specialists.",
    "title_zh": "PAMI-GPT：一种用于模式挖掘的对话式定制GPT模型",
    "abstract_zh": "大型语言模型（LLMs）在自然语言对话中发挥着至关重要的作用。尽管它们在不需要领域特定知识（DSK）的通用任务上表现良好，但传统LLM在需要专业知识的任务中常常产生错误的回答。为应对这一挑战，研究人员提出了参数高效微调（PEFT）和指令微调等改进方法。虽然这些方法效果显著，但通常需要大量的计算资源。为了克服这一局限，OpenAI推出了自定义GPT（custom-GPT）框架，能够在不付出传统微调高昂计算成本的情况下集成领域特定知识。基于该框架，我们提出Pattern MiningGPT（PAMI-GPT），这是一种结合了结构化模式挖掘领域知识库（DKB）与GPT-5架构的领域专用大模型。模式挖掘是大数据分析中的基础技术，能够从大规模数据中提取各类有趣模式，但由于其实现复杂性，长期以来仅限于专家使用。PAMI-GPT通过提供精确的算法定义、有效的参数配置以及可复现的代码，有效打破了这一壁垒。在零样本、单样本及少样本提示下的实验评估表明，PAMI-GPT在准确性、可靠性和领域一致性方面均优于当前领先的LLM。案例研究进一步展示了其生成可执行模式挖掘工作流的能力，使高级技术对非专业人员更加友好和易用。"
  },
  {
    "date": "2025-12-31",
    "title": "Modern Llvm-Based Compiler Autotuning for Wcet Optimization",
    "authors": "Gabriele Magnani, Davide Baroffio, Federico Reghenzani, Giovanni Agosta, William Fornaciari",
    "publish": "2025 IEEE Real-Time Systems Symposium (RTSS)",
    "url": "https://doi.org/10.1109/rtss66672.2025.00043",
    "source": "IEEE",
    "abstract": "The problem of compiler optimization selection and ordering, known in the literature as compiler autotuning, has been tackled many times for average-case execution time reduction. Optimizing the WCET is becoming a prominent problem for modern hard real-time systems, where the difficulties in accurate WCET estimation hinder the full exploitation of computing platform capabilities. In this article, we propose a novel methodology and a tool based on LLVM for iterative WCET-driven compiler autotuning, which is the first strategy to operate at function-level granularity and to consider not only the selection of optimization passes, but also their ordering. Our findings show that standard optimization levels <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathrm{O} 0, \\mathrm{O} 1, \\mathrm{O} 2$</tex>, and O 3 are suboptimal when targeting the WCET, and that a per-function selection and ordering of the transformations is necessary. Experimental results show that our approach outperforms the standard optimizations and opens up new directions for future research.",
    "title_zh": "基于现代LLVM的编译器自动调优在WCET优化中的应用",
    "abstract_zh": "编译器优化选择与排序问题，即文献中所称的编译器自动调优（compiler autotuning），以往多次被用于平均执行时间的减少。然而，在现代硬实时系统中，最坏情况执行时间（WCET）的优化正成为一个日益突出的问题，因为准确估计WCET存在困难，从而限制了计算平台能力的充分发挥。本文提出了一种基于LLVM的新方法和工具，实现迭代式的WCET驱动型编译器自动调优。这是首个在函数级粒度上操作，并不仅考虑优化过程的选择，还同时考虑其执行顺序的策略。我们的研究结果表明，传统的优化级别O0、O1、O2和O3在针对WCET优化时均非最优，必须对每个函数进行独立的优化变换选择与排序。实验结果表明，本方法优于标准优化方案，并为未来的研究开辟了新的方向。"
  },
  {
    "date": "2026-1-1",
    "title": "Unifying Knowledge in Agentic LLMs: Concepts, Methods, and Recent Advancements",
    "authors": "Lihui Liu, Kai Shu",
    "publish": "ACM SIGKDD Explorations Newsletter",
    "url": "https://doi.org/10.1145/3787470.3787478",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "代理型大语言模型中的知识统一：概念、方法与最新进展",
    "abstract_zh": "None"
  },
  {
    "date": "2025-12-31",
    "title": "Syntax Tree Parsing for Automation Control System Optimization in Robotics Learning Platform",
    "authors": "Nadira Khalkhmurzayeva, Lolakhon Saidova, Muhammadbobur Turg'unboyev, Zilola Sattorov, Dadaxon Abdullayev, Gulistan Kanaatova, Bobur Ismoilov",
    "publish": "2025 Second International Conference on Intelligent Technologies for Sustainable Electric and Communications Systems (iTech SECOM)",
    "url": "https://doi.org/10.1109/itechsecom64750.2025.11307231",
    "source": "IEEE",
    "abstract": "The increasing integration of robotics in digital era has created a demand for intuitive programming environments that balance simplicity with computational efficiency. Automation Control Language (ACL), often employed in robotics training platforms, provides students with a structured method for coding robot tasks but suffers from complexity in optimization and error-prone manual interpretation, which hinders learning outcomes. The problem lies in the lack of automated tools that can analyze and refine ACL scripts while ensuring pedagogical clarity and maintaining execution efficiency. To address this gap, the paper propose a Syntax Tree Parsing for Automation Control Language Optimization (STP-ACLO) framework, which employs abstract syntax tree (AST) techniques to systematically parse ACL instructions, identify redundancies, and restructure code for optimized performance. STP-ACLO enhances the readability of student programs, provides automated feedback, and reduces execution overhead by minimizing redundant loops and control flow inefficiencies. Experimental evaluation conducted on robotics digital era simulators demonstrated that STP-ACLO achieved up to 28% reduction in code execution time and improved program comprehension scores among students by 35 %, compared to traditional ACL-based programming approaches. The findings suggest that syntax tree parsing not only improves code efficiency but also strengthens digital eraal outcomes by fostering better conceptual understanding of control structures. In conclusion, the STP-ACLO framework offers a robust and scalable method for optimizing ACL in robotics digital era, bridging the gap between performance and pedagogy while laying the groundwork for future AI-assisted code generation tools in STEM learning environments.",
    "title_zh": "机器人学习平台中自动化控制系统优化的语法树解析",
    "abstract_zh": "数字时代机器人技术的日益融合，催生了对直观编程环境的需求——这类环境需在简洁性与计算效率之间取得平衡。自动化控制语言（ACL）常被用于机器人教学平台，为学生提供一种结构化的机器人任务编程方法，但其在优化方面存在复杂性，且人工解读容易出错，从而影响学习效果。问题的核心在于缺乏能够自动分析并优化ACL脚本的工具，同时确保教学清晰性并维持执行效率。\n\n为填补这一空白，本文提出一种面向自动化控制语言优化的语法树解析框架（STP-ACLO）。该框架采用抽象语法树（AST）技术，系统化地解析ACL指令，识别冗余代码，并重构程序以实现性能优化。STP-ACLO不仅提升了学生程序的可读性，还提供了自动化反馈，通过减少冗余循环和控制流低效，显著降低了执行开销。\n\n在数字时代机器人仿真器上的实验评估表明，与传统的基于ACL的编程方法相比，STP-ACLO实现了最高达28%的代码执行时间缩减，同时学生对程序的理解能力评分提升了35%。研究结果表明，语法树解析不仅能提升代码效率，还能通过增强学生对控制结构概念的理解，有效促进数字时代的教育成果。\n\n综上所述，STP-ACLO框架为数字时代机器人领域中ACL的优化提供了一种强大且可扩展的方法，成功弥合了性能与教学之间的鸿沟，同时也为未来在STEM教育环境中实现人工智能辅助代码生成工具奠定了基础。"
  },
  {
    "date": "2025-12-31",
    "title": "Performance Analysis of Multiplier Architecture and Their Application in Convolution Operations on FPGA",
    "authors": "Reshma S, Subhitha GS, Jayasanthi M",
    "publish": "2025 Second International Conference on Intelligent Technologies for Sustainable Electric and Communications Systems (iTech SECOM)",
    "url": "https://doi.org/10.1109/itechsecom64750.2025.11307488",
    "source": "IEEE",
    "abstract": "Convolution is a fundamental operation in digital signal processing and machine learning, and its efficiency strongly depends on the underlying multiplier architecture. This works conducts a detailed evaluation of Braun, Booth and Wallace tree multipliers on FPGA platforms, focusing on their role within convolution operations. The 32×32-bit designs were developed in Verilog HDL and synthesized using Xilinx Vivado, with performance measured in terms of delay, logic resources, and power efficiency. The Booth design exhibited the lowest power consumption (61.418 W) and smallest area (1026 LUTs), but introduced a longer delay (33.969 ns). The Braun structure provided a moderate balance with 21.428 ns delay, 81.587 W power, and 1073 LUTs. In contrast, the Wallace tree was the fastest (21.488 ns), though it consumed the most resources (115.416 W, 1479 LUTs). Within a 3-tap FIR convolution module, the dedicated DSP multiplier proved the most efficient (37.828 W, 15.594 ns, 68 LUTs). Substituting it with a Booth multiplier raised both power (62.979 W) and delay (18.903 ns), while leaving the area nearly unchanged (69 LUTs). Overall, DSP slices remain the most efficient choice for FPGA convolution, though Booth multipliers offer a compact alternative, Braun designs provide modularity, and Wallace trees trade power and area for speed.",
    "title_zh": "基于FPGA的乘法器架构性能分析及其在卷积运算中的应用",
    "abstract_zh": "卷积是数字信号处理和机器学习中的基本操作，其效率高度依赖于底层乘法器架构。本文在FPGA平台上对Braun、Booth和Wallace树乘法器进行了详细评估，重点分析它们在卷积运算中的表现。32×32位的乘法器设计采用Verilog HDL编写，并使用Xilinx Vivado进行综合，性能指标包括延迟、逻辑资源占用和功耗效率。Booth乘法器表现出最低的功耗（61.418 W）和最小的面积（1026个LUT），但引入了较长的延迟（33.969 ns）。Braun结构则提供了较为均衡的表现：延迟为21.428 ns，功耗81.587 W，占用1073个LUT。相比之下，Wallace树乘法器速度最快（21.488 ns），但功耗和资源开销最大（115.416 W，1479个LUT）。在3抽头FIR卷积模块中，专用DSP乘法器最为高效（功耗37.828 W，延迟15.594 ns，仅需68个LUT）。若用Booth乘法器替代，则功耗上升至62.979 W，延迟增至18.903 ns，而面积几乎不变（69个LUT）。总体而言，DSP Slice仍是FPGA卷积实现中最优的选择；Booth乘法器提供紧凑的替代方案，Braun结构具有良好的可扩展性，而Wallace树则以牺牲功耗和面积为代价换取更高的速度。"
  },
  {
    "date": "2025-12-31",
    "title": "From Strategy to Structure: Guiding Code Quality with GPST in Game-Based Programming Environments",
    "authors": "Chien-Wei Hu, Yi-Hsuan Liao, Hewijin Christine Jiau",
    "publish": "IEEE Access",
    "url": "https://doi.org/10.1109/access.2025.3649779",
    "source": "IEEE",
    "abstract": "Enhancing programming skills is essential for developers to keep pace with technological advancements and to maintain effective participation in software development practices. Game-based programming platforms have been widely adopted to promote learner engagement and skill acquisition. However, without structured guidance, programmers may adopt ineffective strategies, leading to stagnation and wasted effort. This paper investigates the programming skills developed through ELOP, a competitive game-based training platform that has accumulated longitudinal programming data from hundreds of users. A mixed-methods analysis reveals that while ELOP fosters iterative strategy refinement, key skills such as writing well-documented code and refactoring maintainable programs remain difficult for many learners to master. To address these challenges, we present <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">GPST</i> (Game-based Programming Skill Trainer), an extended platform that augments ELOP with instructional features including automated code smell detection, comment quality guidance, and targeted training materials. GPST aims to support learners in developing clean, readable, and maintainable code while preserving the motivational benefits of game-based learning. Preliminary evaluation results from a small-scale pilot study (n=5) demonstrate the feasibility of GPST and suggest positive learning outcomes, while indicating directions for larger future deployments.",
    "title_zh": "从策略到结构：在基于游戏的编程环境中利用GPST引导代码质量",
    "abstract_zh": "提升编程技能对开发者而言至关重要，有助于跟上技术发展的步伐，并持续有效地参与软件开发实践。基于游戏的编程平台已被广泛采用，以增强学习者的参与度并促进技能习得。然而，在缺乏系统指导的情况下，程序员可能会采取低效的学习策略，导致进步停滞和资源浪费。本文研究了ELOP——一个积累了数百名用户纵向编程数据的竞技型游戏化训练平台——所培养的编程技能。混合方法分析表明，尽管ELOP能够促进学习者不断优化编程策略，但诸如编写良好注释的代码以及重构可维护程序等关键技能，仍对许多学习者构成挑战。为应对这些难题，我们提出了<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">GPST</i>（基于游戏的编程技能训练平台），该平台在ELOP基础上扩展了教学功能，包括自动代码异味检测、注释质量指导以及针对性的培训材料。GPST旨在帮助学习者在保持游戏化学习激励优势的同时，发展出清晰、易读且可维护的代码能力。初步的小规模试点研究（n=5）结果表明，GPST具有可行性，并显示出积极的学习成效，同时指明了未来更大规模部署的方向。"
  },
  {
    "date": "2025-12-31",
    "title": "Comparative Evaluation of ChatGPT, Gemini, and DeepSeek in Educational Problem Solving",
    "authors": "Daniel M. Muepu, Yutaka Watanobe, Amin Md Faizul Ibn, Mia Md. Shahajada",
    "publish": "2025 IEEE 18th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC)",
    "url": "https://doi.org/10.1109/mcsoc67473.2025.00089",
    "source": "IEEE",
    "abstract": "This study compares the performance of three large language models; ChatGPT, Gemini, and DeepSeek, on a set of programming-related educational problems from the Aizu Online Judge (AOJ) platform. The evaluation focuses on problem-solving accuracy and code characteristics, with additional comparisons to human Java submissions to contextualize model performance. Metrics include CPU time, memory usage, and code size, enabling a detailed analysis of solution quality and efficiency. Results indicate that ChatGPT consistently achieves the most efficient solutions while maintaining high accuracy, often matching the fastest human submissions. Gemini and DeepSeek also demonstrate strong accuracy but tend to produce less optimized code in computationally demanding cases. These findings contribute to understanding how current LLMs can address structured problem-solving tasks within educational environments.",
    "title_zh": "ChatGPT、Gemini与DeepSeek在教育问题解决中的对比评估",
    "abstract_zh": "本研究对比了三种大型语言模型——ChatGPT、Gemini 和 DeepSeek——在会津在线评测系统（AOJ）平台上的编程类教育题目中的表现。评估重点包括解题准确率和代码特性，并与人类编写的 Java 提交代码进行对比，以更全面地理解模型性能。评估指标涵盖 CPU 运行时间、内存使用量和代码规模，从而实现对解决方案质量与效率的深入分析。结果显示，ChatGPT 在保持高准确率的同时，始终能够生成最高效的解决方案，其表现常可媲美人类最快的提交代码。Gemini 和 DeepSeek 也展现出较强的准确性，但在计算密集型任务中，其生成的代码往往优化程度较低。这些发现有助于深入理解当前大型语言模型在教育环境中处理结构化问题求解任务的能力。"
  },
  {
    "date": "2025-12-31",
    "title": "Performance Optimization of Machine Inference Applications on Edge Devices with a RISC-V Custom Instruction",
    "authors": "Daichi Higashi, Hitoshi Oi",
    "publish": "2025 IEEE 18th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC)",
    "url": "https://doi.org/10.1109/mcsoc67473.2025.00082",
    "source": "IEEE",
    "abstract": "In this paper, we present a case study of optimizing the executions of machine learning applications on edge devices. Workload profiling of two typical applications, Micro Speech and Person Detection, has revealed that more than 70% of the clock cycles are spent in convolution operations which are actually multiply-and-accumulate operations of two arrays. We have added a custom instruction, which is a packed-SIMD multiply-and-accumulation into the basic RISC-V ISA. The results of cycle-accurate simulations show that the relative speed of the Person Detection is accelerated to 240% of the execution without the custom instruction. On the other hand, the performance of the Micro Speech is degraded by 13%. Analysis of this performance degradation needs further investigation but one possibility is the pipeline flushes caused by array boundary check and the overhead of additional instructions for packing the operands into registers for the custom instruction. Compared to a sample RISC-V implementation, PicoRV32, the hardware overhead of the function unit for the custom instruction is 3%.",
    "title_zh": "基于RISC-V自定义指令的边缘设备上机器推理应用的性能优化",
    "abstract_zh": "本文提出了一项针对边缘设备上机器学习应用执行优化的案例研究。对两种典型应用——Micro Speech 和 Person Detection 的工作负载分析表明，超过70%的时钟周期都消耗在卷积操作上，而这些操作本质上是两个数组之间的乘加运算。为此，我们在基础的 RISC-V 指令集架构中添加了一条自定义指令，即打包的 SIMD 乘加操作。基于周期精确的仿真结果表明，Person Detection 应用的执行速度相比未使用自定义指令的情况提升了 240%。另一方面，Micro Speech 的性能则下降了 13%。对于这一性能下降现象，尚需进一步分析，但一种可能的原因是：由于数组边界检查导致的流水线冲刷，以及为将操作数打包至寄存器以适配自定义指令所引入的额外指令开销。与一个典型的 RISC-V 实现（PicoRV32）相比，该自定义指令功能单元的硬件开销仅为 3%。"
  },
  {
    "date": "2025-12-31",
    "title": "ASIC Implementation of 5-Stage Pipelined RISC-Vprocessor with 4-Stage Pipelined Multiply Unit using Openlane",
    "authors": "Pankaj Nair V. M., P. Jayakrishnan",
    "publish": "2025 Innovations in Power and Advanced Computing Technologies (i-PACT)",
    "url": "https://doi.org/10.1109/i-pact65952.2025.11307852",
    "source": "IEEE",
    "abstract": "The paper presents the ASIC implementation of a 32-bit, in-order, 5-stage pipelined RISC-V processor compliant with the RV32IM Instruction Set Architecture (ISA). The processor adopts a hybrid control logic scheme combining onehot and binary encoding to optimize control complexity and area efficiency. A notable enhancement is the introduction of arithmetic pipelining within the multiply unit, which improves the throughput of multiplication operations. The design was described in Verilog HDL and functionally verified using ModelSim to ensure correctness prior to synthesis. Physical implementation was carried out using the OpenLane open-source EDA toolchain, targeting the SkyWater 130nm process design kit (SKY130 PDK). The ASIC flow included synthesis, floorplanning, placement, clock tree synthesis, and routing. Post-layout analysis reports a maximum operating frequency of 100 MHz, a total power consumption of 36.1 mW, and a core area of <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$272,563 \\mu^{2}$</tex>, with <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathbf{5 5 \\%}$</tex> utilization. The work demonstrates an efficient, lightweight RISC-V processor implementation and highlights a complete open-source ASIC design flow from RTL to GDSII.",
    "title_zh": "使用OpenLane实现带有4级流水线乘法单元的5级流水线RISC-V处理器的ASIC设计",
    "abstract_zh": "本文介绍了基于RV32IM指令集架构（ISA）的32位、有序、五级流水线RISC-V处理器的ASIC实现。该处理器采用了一种结合onehot编码与二进制编码的混合控制逻辑方案，以优化控制逻辑的复杂度和面积效率。一个显著的改进是在乘法单元中引入了算术流水线，从而提升了乘法操作的吞吐率。设计使用Verilog HDL描述，并通过ModelSim进行功能验证，确保在综合前的正确性。物理实现采用开源EDA工具链OpenLane，目标工艺为SkyWater 130nm工艺设计套件（SKY130 PDK）。ASIC流程包括综合、布局规划、放置、时钟树综合及布线。后版图分析显示，最大工作频率达到100 MHz，总功耗为36.1 mW，核心面积小于$272,563 \\mu^2$，利用率高达$\\mathbf{55\\%}$。本工作展示了高效、轻量级的RISC-V处理器实现，并凸显了从RTL到GDSII的完整开源ASIC设计流程。"
  },
  {
    "date": "2025-12-31",
    "title": "AI-Driven Stylistic Analysis of PLC Programming Content for Industrial Automation Education",
    "authors": "Bekpashsha Ahmedova, Dilrabo Quvvatova, Dadaxon Abdullayev, Muhayyo Azimova, Shavkatjon Mirzayev, Muhabbat Qodirova, Khalima Akhmedova",
    "publish": "2025 Second International Conference on Intelligent Technologies for Sustainable Electric and Communications Systems (iTech SECOM)",
    "url": "https://doi.org/10.1109/itechsecom64750.2025.11307199",
    "source": "IEEE",
    "abstract": "Artificial intelligence (AI) is increasingly being deployed in industrial automation education to enhance the comprehension and delivery of programming content for programmable logic controllers (PLCs), which remain foundational to modern factory systems. However, the rapid growth of PLC-based instructional materials often varies widely in style, complexity, and pedagogy, creating inconsistencies that hinder student engagement, knowledge retention, and adaptability across diverse learning environments. Traditional curriculum designs rarely account for stylistic alignment between instructional text and learner needs, limiting the effectiveness of teaching PLC logic and programming structures. To address this problem, the paper propose an AI-Driven Stylistic Profiling and Adaptation Model (AISPAM), which combines natural language processing (NLP), transformer-based text representation, and unsupervised clustering to analyze and adapt PLC programming content according to stylistic markers such as technical density, instructional clarity, cognitive load, and domain-specific readability. AISPAM classifies content into pedagogical clusters, aligning instructional material style with targeted learner categories ranging from novice technicians to advanced engineering students. Experimental validation on a curated dataset of PLC programming manuals, learning modules, and industrial training documents demonstrated that AISPAM achieved over 87 % accuracy in stylistic clustering, while adaptive transformation improved student comprehension scores by 22 % in test scenarios. These findings confirm that AI-based stylistic analysis can effectively bridge the gap between technical rigor and adaptive pedagogy, providing a robust framework for developing dynamic, learner-centered curricula in automation education. The work highlights future potential for integrating stylistic adaptability into smart industrial training platforms and intelligent tutoring systems for lifelong learning.",
    "title_zh": "基于人工智能的PLC编程内容风格分析在工业自动化教育中的应用",
    "abstract_zh": "人工智能（AI）正越来越多地应用于工业自动化教育领域，以提升可编程逻辑控制器（PLC）编程内容的理解与教学效果。PLC作为现代工厂系统的基础，其编程教学至关重要。然而，基于PLC的教学材料近年来迅速增长，其风格、复杂程度和教学方法差异显著，导致教学内容不一致，影响学生的学习兴趣、知识掌握以及在不同学习环境中的适应能力。传统的课程设计往往忽视了教学文本风格与学习者需求之间的匹配性，从而限制了PLC逻辑与编程结构教学的有效性。\n\n为解决这一问题，本文提出一种人工智能驱动的风格分析与自适应模型（AISPAM），该模型融合自然语言处理（NLP）、基于Transformer的文本表征技术以及无监督聚类算法，能够根据技术密度、教学清晰度、认知负荷及领域特定可读性等风格特征，对PLC编程内容进行分析与动态调整。AISPAM将教学内容划分为多个教学风格集群，使其与从初学者技术员到高级工程专业学生的不同学习者群体实现精准匹配。\n\n在对精选的PLC编程手册、学习模块及工业培训文档数据集进行实验验证后发现，AISPAM在风格聚类任务中达到了超过87%的准确率；同时，经过自适应转换的内容使学生在测试场景中的理解成绩提升了22%。研究结果表明，基于AI的风格分析能够有效弥合技术严谨性与个性化教学之间的鸿沟，为构建动态化、以学习者为中心的自动化教育课程体系提供了坚实框架。\n\n本研究还揭示了未来将风格自适应能力集成至智能工业培训平台与智能辅导系统中的巨大潜力，有助于支持终身学习背景下的高效、个性化技能培养。"
  },
  {
    "date": "2025-12-31",
    "title": "HLS to FPGAs: Extending Software Regions Via Transformations and Offloading Functions to the CPU",
    "authors": "Tiago Santos, João Bispo, João M. P. Cardoso, James C. Hoe",
    "publish": "2025 IEEE 18th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC)",
    "url": "https://doi.org/10.1109/mcsoc67473.2025.00015",
    "source": "IEEE",
    "abstract": "On a CPU-FPGA system, C/C++ applications are typically accelerated by offloading specific code regions onto the FPGA using High-level Synthesis (HLS). Although modern FPGAs can implement increasingly large and complex designs, the size and variety of potential offloading code regions remain constrained by the limitations of HLS tools (e.g., no support for dynamic memory allocation and system calls). This paper proposes automated C/C++ source-to-source transformations that tackle these limitations in two steps. Firstly, transformations reduce the entropy of an input C/C++ application by converting it into a subset of C, e.g., by flattening arrays and structs. Secondly, additional transformations make a selected code region synthesizable, e.g., by moving dynamic memory allocations out of the region, converting them to static memory, and offloading non-synthesizable C standard library calls, such as printf(), to the CPU. We evaluate the impact of these transformations showing results obtained through Vitis HLS for four real-world examples: the disparity and texture-synthesis benchmarks from CortexSuite, which contain dynamic memory allocations and indirect pointers in their hotspots; llama2, a Large Language Model that calls printf() every time it predicts a new word; and the spam-filter benchmark from Rosetta, as a debugging showcase.",
    "title_zh": "HLS到FPGA：通过变换扩展软件区域并将函数卸载到CPU",
    "abstract_zh": "在CPU-FPGA系统中，通常通过使用高层次综合（HLS）将特定代码区域卸载到FPGA上来加速C/C++应用程序。尽管现代FPGA能够实现日益庞大且复杂的电路设计，但可卸载代码区域的规模和多样性仍受限于HLS工具的功能局限（例如，不支持动态内存分配和系统调用）。本文提出了一种自动化的C/C++源到源转换方法，分两步解决这些限制：首先，通过将输入的C/C++程序转换为C的一个子集（如展开数组和结构体），降低其复杂度；其次，对选定的代码区域进行进一步转换，使其具备可综合特性，例如将动态内存分配移出该区域并替换为静态内存，同时将无法综合的C标准库调用（如printf()）卸载至CPU执行。我们通过Vitis HLS对四种真实应用场景进行了评估，结果表明该方法有效：包括CortexSuite中的视差计算与纹理合成基准测试（热点区域包含动态内存分配和间接指针）；Llama2大语言模型（每次预测新词时调用printf()）；以及Rosetta中的垃圾邮件过滤基准测试，作为调试示例展示。"
  },
  {
    "date": "2025-12-31",
    "title": "A Large Language Model-Based Framework for Generating Simulation Models of Power Systems",
    "authors": "Duange Guo, Shanyu Li, Yushi Liu, Xingyu Shi, Shan Jiang, Ye Zhu, Georgios Konstantinou, Mengxuan Shi",
    "publish": "2025 IEEE PES GTD Grand International Conference and Exposition Asia (GTD Asia)",
    "url": "https://doi.org/10.1109/gtdasia60461.2025.11313242",
    "source": "IEEE",
    "abstract": "This paper proposes a dual-agent framework based on a large language model (LLM) to automatically generate power system dynamic simulations. The progress is divided into two Agent parts: Agent I is responsible for creating a plan and transferring the reasoning output into a specific, recognized procedure with functions; Agent II is responsible for generating code, utilizing a database if functions exist, or creating new functions and then storing them in the database. Meanwhile, two feedback loops are used for code generation and the planner to ensure the automation of the entire process. Electromagnetic transient models of the IEEE 9-bus benchmark model are generated in MATLAB/Simulink platform as an example to illustrate that the general commercial LLMs have the potential to be used in the power system domain to perform insightful tasks based on the simulation feedback, and generate the data source during training progress to achieve a self-evolving LLM.",
    "title_zh": "基于大语言模型的电力系统仿真模型生成框架",
    "abstract_zh": "本文提出了一种基于大语言模型（LLM）的双代理框架，用于自动生成电力系统动态仿真。该过程分为两个代理部分：Agent I 负责制定计划，并将推理结果转化为具体、可识别的具有功能的流程；Agent II 负责生成代码，若数据库中已存在所需函数则直接调用，否则创建新函数并将其存入数据库。同时，通过两个反馈回路分别作用于代码生成和规划器，以确保整个过程的自动化。以 IEEE 9 节点基准模型的电磁暂态模型在 MATLAB/Simulink 平台上的生成为例，说明通用商业大语言模型具备在电力系统领域基于仿真反馈执行深入任务的潜力，并能在训练过程中生成数据源，实现大语言模型的自我演化。"
  },
  {
    "date": "2025-12-31",
    "title": "Toward Dynamic Risk Assessment: Machine Learning and LLMs in Software Vulnerability Prioritization",
    "authors": "Mohammed Moustaid, Soufiane Hamida, Abdelaziz Daaif, Bouchaib Cherradi",
    "publish": "2025 12th International Conference on Wireless Networks and Mobile Communications (WINCOM)",
    "url": "https://doi.org/10.1109/wincom65874.2025.11313442",
    "source": "IEEE",
    "abstract": "The rapid growth of software vulnerabilities demands advanced prioritization beyond static scoring systems. Recent works (2020-2025) have applied machine learning (ML) and large language models (LLMs) to predict and rank the risk of vulnerabilities based on features such as CVSS metrics, exploit presence, context and natural language descriptions. This review surveys supervised, unsupervised, and hybrid ML approaches-including neural networks, ensemble classifiers, graph-based models-and LLM-based NLP methods. We examine various model types (e.g. Random Forest, XGBoost, CNN, DistilBERT), data sources (NVD/CVE descriptions, exploit databases, OSINT, telemetry), evaluation metrics (accuracy, F1-score, MSE), and quantitative results. Performance trends indicate ML can improve on CVSS baselines (e.g. <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\boldsymbol{\\sim} \\mathbf{8 3 \\%}$</tex> accuracy), whereas LLMs require domain adaptation to perform well. Finally, we discuss gaps in research, such as limited real-world validation and the need for dynamic, context-aware models.",
    "title_zh": "面向动态风险评估：机器学习与大语言模型在软件漏洞优先级排序中的应用",
    "abstract_zh": "软件漏洞的快速增长要求采用超越静态评分体系的先进优先级评估方法。近年来（2020–2025年），机器学习（ML）与大语言模型（LLMs）被广泛应用于基于CVSS指标、漏洞利用存在性、上下文信息以及自然语言描述等特征，对漏洞风险进行预测与排序。本文综述了监督学习、无监督学习及混合型机器学习方法——包括神经网络、集成分类器、图-based模型——以及基于大语言模型的自然语言处理（NLP）方法。我们分析了多种模型类型（如随机森林、XGBoost、CNN、DistilBERT）、数据来源（NVD/CVE描述、漏洞利用数据库、开源情报OSINT、遥测数据）、评估指标（准确率、F1分数、均方误差）及量化结果。性能趋势表明，机器学习方法在基准上优于CVSS（例如准确率提升至约83%），而大语言模型则需经过领域适配才能表现良好。最后，我们讨论了当前研究中的若干空白，如真实场景验证不足，以及对动态、上下文感知模型的迫切需求。"
  },
  {
    "date": "2025-12-31",
    "title": "Review on Hardware Accelerator Hardware Software Co-Design for Machine Learning",
    "authors": "VedanthSrivatson A, Sivanantham Sathasivam, Prakash Ramachandran",
    "publish": "2025 Innovations in Power and Advanced Computing Technologies (i-PACT)",
    "url": "https://doi.org/10.1109/i-pact65952.2025.11307887",
    "source": "IEEE",
    "abstract": "In recent years, the fast expansion of advanced models of machine learning (ML) and deep learning (DL) has necessitated computing platforms that are not only high-performance but also energy-efficient and adaptable. This review explores developments from 2020 to 2025 in hardware-software co-design (HW/SW co-design) strategies, which enable integration between hardware accelerators and software components. The paper systematically categorizes codesign approaches based on ML methodologies, target platforms, and application domains, highlighting key performance metrics such as throughput, energy efficiency, latency, and accuracy. Through an in-depth analysis of frameworks like RSNN, TinyM2Net-V2, and CODEBench, and toolchains such as Vivado HLS, TVM, and TensorFlow Lite, the review showcases their application in edge AI, computer vision, scientific computing, and secure processing. Quantitative comparisons reveal that FPGA-based designs achieve an optimal balance between performance and power efficiency, outperforming GPUs and ASICs in several energy-constrained scenarios. Challenges such as partitioning complexity, fragmented toolchains, and limited automation are discussed alongside trends in dynamic partial reconfiguration, AI-driven toolchains, and RISC-V-based SoCs. Future research directions emphasize the need for unified frameworks, support for complex models and hardware-aware optimization techniques. This review concludes that hardware/software co-design is essential for enabling scalable, low-power, real-time ML systems, offering a promising pathway toward next-generation intelligent edge computing and high-performance embedded systems.",
    "title_zh": "机器学习硬件加速器的软硬件协同设计综述",
    "abstract_zh": "近年来，机器学习（ML）和深度学习（DL）先进模型的快速扩展，对计算平台提出了更高要求：不仅需要高性能，还需具备能效比高和可适应性强的特点。本文综述了2020至2025年间软硬件协同设计（HW/SW co-design）策略的发展，这些策略实现了硬件加速器与软件组件之间的深度融合。文章系统地根据机器学习方法、目标平台及应用领域对协同设计方法进行了分类，并重点分析了吞吐量、能效、延迟和准确率等关键性能指标。通过对RSNN、TinyM2Net-V2和CODEBench等框架，以及Vivado HLS、TVM和TensorFlow Lite等工具链的深入分析，本文展示了其在边缘人工智能、计算机视觉、科学计算和安全处理等领域的应用。定量对比表明，在多个能源受限场景中，基于FPGA的设计在性能与能效之间实现了最佳平衡，优于GPU和ASIC。同时，文章也探讨了分区复杂性、工具链碎片化以及自动化程度不足等挑战，并展望了动态部分重构、AI驱动的工具链以及基于RISC-V的SoC等发展趋势。未来研究方向强调构建统一的开发框架，支持复杂模型及硬件感知优化技术。本文结论指出，软硬件协同设计对于实现可扩展、低功耗、实时的机器学习系统至关重要，为下一代智能边缘计算和高性能嵌入式系统提供了极具前景的发展路径。"
  },
  {
    "date": "2025-12-31",
    "title": "Cyber Wolf: A Hybrid Optimization Framework Leveraging Heterogeneous Graphs and Multimodal Surrogate Simulators for Fast Analog Sizing",
    "authors": "Nathan Chanez, Pascal Urard, Severin Trochut, Roberto Guizzetti, Kai Wang, Luc Moulin, Marwan Khader, Douglas Mateus Machado",
    "publish": "2025 International Conference on Applied Electrical Engineering and Technology (AEET)",
    "url": "https://doi.org/10.1109/aeet66561.2025.11307005",
    "source": "IEEE",
    "abstract": "In recent years, Artificial Intelligence (AI) has demonstrated its ability to accelerate various tasks by leveraging data to fit equations in neural networks. In the semiconductor industry, circuit design still relies on complex tools that deliver accurate results for simulating and optimizing circuits at the cost of high runtimes. Regarding the analog device sizing problem, researchers have proposed replacing traditional circuit simulators with trained neural networks to accelerate simulations. However, these approaches face limitations in generalizing circuit representations and using such surrogate models. This preliminary work proposes an enhancement in circuit representation compatible with any circuit topology and, through an example, presents a complete methodology for generating databases, representing circuits, and finally including the neural network model into an optimization framework to reduce the optimization time of the sizing of an analog circuit.",
    "title_zh": "网络狼：一种融合异构图与多模态代理模拟器的混合优化框架，用于快速模拟电路尺寸设计",
    "abstract_zh": "近年来，人工智能（AI）通过利用数据拟合神经网络中的方程，展现出加速各类任务的潜力。在半导体行业中，电路设计仍依赖于复杂的工具，这些工具虽然能准确模拟和优化电路，但运行时间成本高昂。针对模拟器件尺寸设计问题，研究人员提出用训练好的神经网络替代传统的电路仿真器，以加快仿真速度。然而，这些方法在泛化电路表示以及使用此类代理模型方面仍存在局限性。本文的初步工作提出了一种适用于任意电路拓扑结构的电路表示方法的改进，并通过一个实例展示了完整的流程：包括数据库生成、电路表示方法以及将神经网络模型集成到优化框架中，从而显著缩短模拟电路尺寸优化的时间。"
  },
  {
    "date": "2025-12-31",
    "title": "Dynamic Fuzzing-Based Whole-System Timing Analysis",
    "authors": "Alwin Berger, Simon Schuster, Peter Wägemann, Peter Ulbrich",
    "publish": "2025 IEEE Real-Time Systems Symposium (RTSS)",
    "url": "https://doi.org/10.1109/rtss66672.2025.00041",
    "source": "IEEE",
    "abstract": "Worst-case timing analysis traditionally begins with estimating the worst-case execution time (WCET) of individual tasks using either static analysis or measurement-based techniques. To derive worst-case response times (WCRTs), engineers typically compose these WCETs with bounds on preemption and operating system overheads. However, WCRTs depend on complex system-level interactions, including task communication, OS behavior, and asynchronous events. Compositional analysis often overestimates, assuming that worst-case conditions across components coincide, admitting infeasible global control-flow paths. whole-system Static techniques refine this by modeling the system holistically but require platform-specific tailoring or extensive annotations. A dynamic equivalent has been missing. We present Fret, the first dynamic whole-system approach for estimating WCRTs. Fret employs feedback-guided fuzzing to uncover timing-critical dependencies, including inter-task communication, task/OS interactions, and interrupt effects, without requiring prior knowledge of inputs or states. Implemented using LibAFL and evaluated on FreeRTOS with realistic benchmarks, FRET consistently outperforms state-of-the-art fuzzing strategies in estimating accurate response times. Although not sound, Fret delivers more than timing estimates: it produces actionable artifacts-worst-case inputs, interrupt schedules, and intertask program-flow information-that complement static analyses and support system validation, runtime monitoring, and robust mixed-criticality scheduling.",
    "title_zh": "基于动态模糊测试的全系统时序分析",
    "abstract_zh": "传统的最坏情况时间分析通常从使用静态分析或基于测量的技术估算各个任务的最坏情况执行时间（WCET）开始。为了推导出最坏情况响应时间（WCRT），工程师通常将这些WCET与抢占和操作系统开销的上限进行组合。然而，WCRT依赖于复杂的系统级交互，包括任务间通信、操作系统行为以及异步事件。组合式分析方法往往存在过度估计的问题，因为它假设各组件的最坏情况条件同时发生，从而引入了不可行的全局控制流路径。全系统静态分析方法通过整体建模系统来改进这一问题，但需要针对特定平台进行定制或添加大量注释。此前一直缺乏一种动态等效方法。我们提出了Fret，这是首个用于估算WCRT的动态全系统方法。Fret采用反馈引导的模糊测试技术，无需预先了解输入或系统状态，即可发现关键时序依赖关系，包括任务间通信、任务与操作系统交互，以及中断的影响。该方法基于LibAFL实现，并在FreeRTOS上使用真实基准进行了评估，结果表明Fret在准确估算响应时间方面持续优于当前最先进的模糊测试策略。尽管Fret并非形式上正确（sound），但它提供的远不止是时间估计：它还能生成可操作的输出——最坏情况输入、中断调度方案以及任务间的程序流信息，这些成果可与静态分析相辅相成，支持系统验证、运行时监控以及鲁棒的混合关键性调度。"
  },
  {
    "date": "2025-12-31",
    "title": "CFU Proving Ground: a Hardware/Software Co-Design Framework for Leveraging a Custom Function Unit and RISC-V Custom Instructions",
    "authors": "Aoba Fujino, Kenji Kise",
    "publish": "2025 IEEE 18th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC)",
    "url": "https://doi.org/10.1109/mcsoc67473.2025.00040",
    "source": "IEEE",
    "abstract": "The mismatches between hardware and software often prolong the development time of application-specific instruction-set processors (ASIPs). While hardware/software codesign approaches have improved development environments, challenges remain regarding usability and flexibility. This research proposes CFU Proving Ground, a hardware/software codesign framework targeting FPGAs that improves performance by leveraging a custom function unit (CFU) as an accelerator. This framework supports agile ASIP development through the design flow of register transfer level and a resource-efficient CFU that supports RISC-V custom instructions. It enhances usability and flexibility by providing lightweight libraries, explicit file dependencies, and flexible memory mapping. We discuss the effectiveness of our framework based on evaluation results using a low-cost FPGA board.",
    "title_zh": "CFU 测试场：一种利用定制功能单元和 RISC-V 自定义指令的软硬件协同设计框架",
    "abstract_zh": "硬件与软件之间的不匹配常常延长了专用指令集处理器（ASIP）的开发周期。尽管软硬件协同设计方法已改善了开发环境，但在可用性和灵活性方面仍存在挑战。本研究提出了一种名为CFU Proving Ground的软硬件协同设计框架，专为FPGA设计，通过引入自定义功能单元（CFU）作为加速器来提升性能。该框架通过寄存器传输级（RTL）的设计流程以及资源高效的CFU支持RISC-V自定义指令，实现了敏捷的ASIP开发。同时，通过提供轻量级库、明确的文件依赖关系以及灵活的内存映射机制，显著提升了框架的易用性与灵活性。我们基于低成本FPGA开发板的评估结果，讨论了该框架的有效性。"
  },
  {
    "date": "2026-1-1",
    "title": "OmniRouter: Budget and Performance Controllable Multi-LLM Routing",
    "authors": "Kai Mei, Wujiang Xu, Minghao Guo, Shuhang Lin, Yongfeng Zhang",
    "publish": "ACM SIGKDD Explorations Newsletter",
    "url": "https://doi.org/10.1145/3787470.3787480",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "OmniRouter：预算与性能可控制的多大模型路由",
    "abstract_zh": "None"
  },
  {
    "date": "2025-12-31",
    "title": "Bridging Cryptographic Robustness and Hardware Efficiency: A Comprehensive Analysis of S-Box Design Methodologies for SoC Integration",
    "authors": "Maitri Iyer, Jai Gopal Pandey",
    "publish": "2025 IEEE 18th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC)",
    "url": "https://doi.org/10.1109/mcsoc67473.2025.00127",
    "source": "IEEE",
    "abstract": "Symmetric-key cryptosystems depend on substitution boxes (S-boxes) for their main source of nonlinearity to enhance security against cryptanalysis. However, the design of S-boxes presents a fundamental challenge of balancing cryptographic properties with resource-efficient hardware implementation. The proposed article provides a comprehensive analysis of the design methodologies of contemporary S-boxes. It categorizes mathematically constructed, heuristic approaches to combinational logic and lookup table-based implementations. These methodologies are evaluated considering both classic security metrics such as nonlinearity and differential uniformity and crucial hardware efficiency metrics including area (gate count), circuit depth, and multiplicative complexity. Furthermore, the paper synthesizes the major open challenges in the field, such as the escalating complexity of achieving resistance against sidechannel attacks and the demanding constraints of lightweight cryptography. Finally, we outline promising future research directions, emphasizing the growing role of automation, machine learning, and application-specific co-design in advancing the development of secure and efficient S-boxes for next-generation computing systems.",
    "title_zh": "连接密码学鲁棒性与硬件效率：面向系统级芯片集成的S盒设计方法全面分析",
    "abstract_zh": "对称密钥密码系统依赖于替换盒（S-box）作为其非线性主要来源，以增强抵御密码分析的安全性。然而，S-box的设计面临着一个根本性挑战：如何在密码学特性与资源高效的硬件实现之间取得平衡。本文提出了一篇全面的分析，探讨了当代S-box的设计方法。文章将设计方法分为三类：数学构造法、基于组合逻辑的启发式方法以及基于查找表的实现方式。这些方法在经典安全度量（如非线性度和差分均匀性）以及关键硬件效率指标（包括面积（门数）、电路深度和乘法复杂度）方面进行了评估。此外，本文还总结了该领域的主要开放性挑战，例如应对侧信道攻击的复杂性日益增加，以及轻量级密码学所面临的严苛约束。最后，我们指出了有前景的未来研究方向，强调自动化、机器学习以及面向应用的协同设计在推动下一代计算系统中安全且高效S-box发展中的日益重要作用。"
  },
  {
    "date": "2025-12-31",
    "title": "A Deep Learning and NLP-Based Platform for Computer Science Development",
    "authors": "Mustafa Hussein, Waleed Mohamed, Abdalla Mohsen, Mennatullah Ibrahiem, Kerolos Samaan, Hajer Essam, Radwa Reda Hossieny, Sally Saad",
    "publish": "2025 Twelfth International Conference on Intelligent Computing and Information Systems (ICICIS)",
    "url": "https://doi.org/10.1109/icicis66182.2025.11313145",
    "source": "IEEE",
    "abstract": "Recently, students and graduates in computer science have encountered increasing difficulty in navigating academic and career pathways due to the absence of centralized, domain-specific platforms. Existing online forums and social media channels are often fragmented and cluttered with irrelevant discussions, sarcasm, and offensive text, limiting their effectiveness in providing accurate guidance and technical support. These challenges disrupt meaningful engagement, hinder information retrieval, and reduce the overall quality of academic discourse within the computer science community. This study introduces FCIS F1 system, an AI-driven platform designed to facilitate structured, safe, and relevant communication among computer science students and graduates. The system integrates deep learning models for multilingual offensive text detection, topic classification, and content filtering. For English offensive text detection, BERT achieved the highest accuracy (92.90%), followed by CNN (92.45%) and Bi-LSTM (90.51%). In Arabic, Naive Bayes outperformed other models with 96.25% accuracy, while Bi-LSTM reached 95.16%, and up to 94.88% in optimized trials. These models demonstrated strong performance across multiple configurations, enabling the platform to automatically moderate content and categorize discussions effectively. FCIS F1 supports a focused and constructive environment that enhances knowledge exchange, academic decision-making, and career development in the field of computer science.",
    "title_zh": "基于深度学习与自然语言处理的计算机科学开发平台",
    "abstract_zh": "近年来，计算机科学领域的学生和毕业生在规划学术与职业发展路径时面临日益严峻的挑战，这主要源于缺乏集中化、领域特定的交流平台。现有的在线论坛和社交媒体渠道往往信息碎片化严重，充斥着无关讨论、讽刺性言论及不当内容，严重影响了其提供精准指导与技术支援的有效性。这些问题不仅干扰了有意义的互动，阻碍了信息检索，还降低了计算机科学社区内学术交流的整体质量。\n\n本研究提出FCIS F1系统——一个由人工智能驱动的平台，旨在促进计算机科学学生与毕业生之间结构化、安全且相关性强的沟通。该系统整合了深度学习模型，用于多语言不当文本检测、话题分类与内容过滤。在英文不当文本检测方面，BERT模型表现最佳，准确率达92.90%，其次为CNN（92.45%）和Bi-LSTM（90.51%）；在阿拉伯语场景中，朴素贝叶斯模型以96.25%的准确率领先，Bi-LSTM达到95.16%，优化实验条件下最高可达94.88%。这些模型在多种配置下均展现出优异性能，使平台能够实现内容的自动审核与话题的高效分类。\n\nFCIS F1系统致力于构建一个专注、积极的交流环境，有效提升知识共享水平，助力学术决策与职业发展，推动计算机科学领域的持续进步。"
  },
  {
    "date": "2025-12-31",
    "title": "Energy-Aware Computing: Cutting CPU Cycles through Intelligent Thread Management and Code Smell Optimization",
    "authors": "Sonali Jolly, Asmita Yadav, Sandeep Kumar Singh",
    "publish": "2025 Seventeenth International Conference on Contemporary Computing (IC3)",
    "url": "https://doi.org/10.1109/ic366947.2025.11290127",
    "source": "IEEE",
    "abstract": "In the field of high-performance computing, increasing resource efficiency and enhancing code quality is essential for minimizing CPU cycles, reducing execution duration, and reducing energy usage. This study introduces an approach for Thread management and Code smell optimization aimed at boosting software performance and reducing the workload on the CPU. The framework achieves significant reductions in overall computational time by employing intelligent thread allocation techniques with respect to memory thereby reducing CPU cycles and speeding up the processing. This paper also incorporates experiments on code smell detection by using Abstract Syntax Tree (AST) traversal and recommendation along with optimization to boost efficiency and quality code. By identifying and addressing code smell indicators of subpar coding practices that negatively affect readability, maintainability has achieved results in reduced CPU cycles and Execution Time. The resulting code demonstrates enhanced modularity and improved performance, minimizing energy usage without sacrificing results. Findings reveal that the optimization decreases in processing duration by 33.34% over non-optimized programs, CPU utilization, and energy consumption across a set of few programs and highlighting the efficacy of thread management. These Experiments contribute to the advancement of energy-efficient software methodologies suitable for modern resource-intensive applications.",
    "title_zh": "节能计算：通过智能线程管理和代码异味优化减少CPU周期",
    "abstract_zh": "在高性能计算领域，提高资源利用效率并提升代码质量对于减少CPU周期、缩短执行时间以及降低能耗至关重要。本研究提出了一种面向线程管理与代码异味优化的方法，旨在提升软件性能并减轻CPU负担。该框架通过采用针对内存特性的智能线程分配技术，显著降低了整体计算时间，减少了CPU周期，并加快了处理速度。此外，本文还通过抽象语法树（AST）遍历实现代码异味检测，并结合推荐与优化策略，进一步提升了代码的效率与质量。通过识别并修复反映不良编程习惯的代码异味指标，有效改善了代码的可读性与可维护性，从而实现了CPU周期和执行时间的降低。优化后的代码展现出更高的模块化程度和更优的性能表现，在不牺牲计算结果的前提下显著降低了能源消耗。实验结果表明，相较于未优化程序，该优化方法使处理时长平均减少33.34%，同时降低了CPU利用率和能耗。这些实验验证了线程管理策略的有效性，为现代资源密集型应用提供了高效节能的软件设计新范式，推动了绿色计算技术的发展。"
  },
  {
    "date": "2025-12-31",
    "title": "Large Language Models: Fundamentals, Necessity, Impacts, Challenges, and Opportunities for Future Applications",
    "authors": "M. Sivakumar, N. Krishnaraj, Amit Kumar Tyagi",
    "publish": "2025 Seventeenth International Conference on Contemporary Computing (IC3)",
    "url": "https://doi.org/10.1109/ic366947.2025.11290699",
    "source": "IEEE",
    "abstract": "The creation of LLMs which focus on specific domains will enable new opportunities for analyzing healthcare conditions while also performing legal tasks in the future. The chapter examines the complete characteristics and evolution of Large Language Models (LLMs) in modern artificial intelligence as well as their essential contribution to contemporary systems. The discussion shows how LLMs can resolve complicated language problems while enabling human-computer communication and multimodal information learning. The review demonstrates how Large Language Models reshape different sectors in society including education and healthcare and they influence the finance sector and entertainment industry. The exceptional performance of LLMs comes with substantial difficulties particularly regarding their costly operations and demanding training requirements alongside empathy-intensive interpretation of their judgment methods. Ongoing research must focus on model efficiency as well as explainable approaches and sustainable AI practices because these challenges exist. The adoption of responsible AI development procedures becomes vital because of ethical risks which include privacy violations as well as bias and misinformation challenges. Building fair and transparent AI-driven technologies requires full cooperation between policymakers, researchers and industry stakeholders to create guidelines focused on accountability. The upcoming era of LLM technology development will rely on fresh approaches to explain analysis methods while adopting power-efficient training processes and enhanced interpretability software frameworks. The deployment of LLMs between practical execution objectives and moral principles enables progress in natural language processing and their suitable implementation throughout various disciplines. The future success of AI depends on how human beings utilize LLM technology with both knowledge and responsibility to generate maximum positive impact alongside risk management.",
    "title_zh": "大型语言模型：基础、必要性、影响、挑战及未来应用前景",
    "abstract_zh": "专注于特定领域的大型语言模型（LLM）的创建，将为未来分析医疗状况以及执行法律任务开辟新的机遇。本章全面探讨了现代人工智能中大型语言模型（LLMs）的特性与演进历程，以及它们对当代系统所做出的关键贡献。讨论表明，LLMs能够解决复杂的语言问题，促进人机交流，并支持多模态信息学习。综述还展示了大型语言模型如何重塑社会多个领域，包括教育、医疗、金融及娱乐产业。尽管LLMs表现出卓越性能，但其背后也伴随着巨大挑战，尤其是在高昂的运行成本、严苛的训练需求，以及对其决策机制进行富有同理心的解读方面。因此，持续的研究必须聚焦于提升模型效率、发展可解释性方法，以及推动可持续的人工智能实践。由于存在隐私侵犯、偏见和虚假信息等伦理风险，负责任的人工智能开发流程变得至关重要。构建公平且透明的AI驱动技术，需要政策制定者、研究人员与行业利益相关方之间的充分协作，共同制定以问责制为核心的指导原则。未来LLM技术的发展将依赖于全新的可解释性分析方法，采用能效更高的训练流程，以及更先进的可解释性软件框架。在实际应用目标与道德准则之间实现平衡，将推动自然语言处理的进步，并确保其在各学科中的恰当应用。人工智能的未来发展，取决于人类是否能够以知识与责任并重的方式运用LLM技术，在最大化积极影响的同时有效管理潜在风险。"
  },
  {
    "date": "2025-12-31",
    "title": "A High Level Synthesis Tool for Multiple FPGAs",
    "authors": "Kazutoshi Wakabayashi, Wataru Takahashi, Hideharu Amano",
    "publish": "2025 IEEE 18th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC)",
    "url": "https://doi.org/10.1109/mcsoc67473.2025.00078",
    "source": "IEEE",
    "abstract": "This paper presents a High Level Synthesis tool which partitions a large-scale circuit into multiple FPGAs. The proposed partitioning method in this tool is designed for stream processing, such as CNN, and aims to partition the circuit into a minimum number of FPGAs and to minimize the through-put degradation caused by the partitioning. Unlike traditional approaches that divide circuits at the RTL or netlist level, in the proposed method, circuits described in the C++ language are divided by high-level synthesis. After partitioning, the RTL descriptions for communication interface circuits between the portioned modules. Also, the RTL descriptions for data stream packing and unpacking circuits are also automatically generated. This stream packing trys to minimize the latency for data transfer between the partitioned modules. Thirdly, the RTL description for data stream sharing circuits is generated. This stream sharing also try to minimize the latency for data transfer. This technique takes advantage of the Data Initial Interval (DII) of the packed data. Lastly, the header information of the packet, such as destination address, slot address, is automatically decided, creating an RTL description. This capability is similar to Network-on-chip (NoC), but this tool does this function with small circuits. The proposed tool allows the designer to synthesize circuit on multiple FPGAs completely automatic. In the experiment, MobileNet-v1 written in C++ language was automatically partitioned into multiple FPGAs, and it was confirmed that the partitioning was done correctly and there was no degradation in throughput.",
    "title_zh": "用于多FPGA的高层次综合工具",
    "abstract_zh": "本文提出了一种高层次综合（High Level Synthesis）工具，可将大规模电路划分为多个FPGA。该工具所采用的划分方法专为流处理应用（如卷积神经网络CNN）设计，旨在将电路划分为最少数量的FPGA，并最小化因划分带来的吞吐量下降。与传统在RTL或网表级别进行电路划分的方法不同，本方法基于用C++语言描述的电路，通过高层次综合实现划分。划分完成后，自动生成各模块间通信接口电路的RTL描述；同时，自动产生数据流打包与解包电路的RTL描述。这种数据流打包技术致力于最小化各划分模块间的数据传输延迟。第三，自动生成用于数据流共享的RTL描述，该共享机制同样旨在降低数据传输延迟。该技术充分利用了打包数据的初始数据间隔（Data Initial Interval, DII）。最后，自动确定数据包的头部信息（如目标地址、槽位地址等），并生成相应的RTL描述。这一功能类似于片上网络（NoC），但本工具仅以小型电路即可实现类似功能。所提出的工具使设计者能够完全自动化地在多个FPGA上进行电路综合。实验中，使用C++编写的MobileNet-v1模型被自动划分至多个FPGA，结果验证了划分过程正确无误，且未造成吞吐量下降。"
  },
  {
    "date": "2025-12-31",
    "title": "Security Vulnerabilities in Unmanned Aerial Vehicle Systems: A Case Study of Ardupilot",
    "authors": "Abdullah Al-Boghdady, Mohammad El-Ramly, Basheer Youssef, Khaled Wassif",
    "publish": "2025 Twelfth International Conference on Intelligent Computing and Information Systems (ICICIS)",
    "url": "https://doi.org/10.1109/icicis66182.2025.11313194",
    "source": "IEEE",
    "abstract": "Unmanned Aerial Vehicle (UAV) systems have become increasingly prominent in recent decades in both civilian and military applications, owing to their ability to perform a wide range of complex tasks quickly and accurately. The software architecture of UAVs is inherently complex, requiring coordinated management of hardware, software, communications, and sensors, along with integration of advanced technologies like the Internet of Things. As a result, UAV systems face significant security and privacy challenges. They are frequently viewed as attractive targets for malicious actors and remain vulnerable to diverse attacks that threaten data confidentiality, integrity, and availability. This paper presents a pilot analytical study of security vulnerabilities in UAV systems by examining the growth rate and density of security related-errors measured as the number of errors per 1K physical Source Lines of Code (SLoC) across four releases of the ArduPilot system. To identify these vulnerabilities, a static analysis tool, Cppcheck, was applied to detect weaknesses categorized under the Common Weakness Enumeration (CWE). In addition, the study explores the evolutionary properties of UAV systems using the CodeScene tool, aiming to establish how these properties correlate with the presence of security vulnerabilities. The findings show that ArduPilot exhibits a consistently high rate of source code vulnerabilities across the analyzed versions. The most frequently detected vulnerabilities were CWE-398, CWE-563, and CWE-758. Overall, this work advances the comprehension of UAV system security by providing detailed analytical insights into source code vulnerabilities, thereby serving as a guiding resource for future research and secure UAV development.",
    "title_zh": "无人机系统中的安全漏洞：以Ardupilot为例的案例研究",
    "abstract_zh": "近年来，无人驾驶飞行器（UAV）系统在民用和军事领域的应用日益突出，这得益于其能够快速、准确地执行多种复杂任务的能力。UAV的软件架构本身极为复杂，需要对硬件、软件、通信系统以及传感器进行协调管理，并集成物联网等先进技术。因此，UAV系统面临严峻的安全与隐私挑战。它们常被视为恶意攻击者的理想目标，且容易受到各类威胁数据机密性、完整性和可用性的攻击。本文通过对ArduPilot系统四个版本中每千行物理源代码（SLoC）所含安全相关错误的数量及其增长速率与密度的分析，开展了一项初步的漏洞研究。为识别这些漏洞，研究采用静态分析工具Cppcheck，检测属于通用弱点枚举（CWE）分类的潜在缺陷。此外，研究还利用CodeScene工具探讨了UAV系统的演化特性，旨在揭示这些特性与安全漏洞之间的关联。研究结果表明，ArduPilot在所分析的各个版本中均表现出持续较高的源代码漏洞率。最常见的漏洞类型为CWE-398、CWE-563和CWE-758。总体而言，本研究通过深入剖析UAV系统源代码中的安全漏洞，深化了对UAV系统安全性的理解，为未来的研究及安全可靠的UAV开发提供了有价值的参考依据。"
  },
  {
    "date": "2025-12-31",
    "title": "Latency-Optimal Quantum Circuits for IoT Networks",
    "authors": "Savo Glisic, Beatriz Lorenzo",
    "publish": "IEEE Access",
    "url": "https://doi.org/10.1109/access.2025.3649805",
    "source": "IEEE",
    "abstract": "Future IoT networks will demand increasingly lower latency for transmitting mission-critical information while simultaneously adopting quantum technologies to enhance security and accelerate information processing. This survey reviews the state-of-the-art in latency-optimal quantum circuit design, focusing on minimizing circuit latency without sacrificing essential functionality. The space is primarily allocated to the network level aspects, exploring balance between the network latency and the energy consumption, programmability and cost, while establishing a proper interface with the problems that should be delegated to the practical design carried on by circuit developers. Section I analyzes how aggressive latency reduction constrains the use of <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">universal</i> and <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">reversible</i> gates—both of which tend to increase circuit depth (latency). Universal gates enable programmability, while reversible gates reduce energy dissipation, yet both can extend circuit latency. Aware of these trade-offs, Section II discusses latency-optimal design methodologies and their implementation aspects. Next we discuss exact minimization of quantum circuits, presenting quantitative improvements. For instance, controlled-T gate implementations demonstrate reductions from 15 → 9 T gates (compression = 15/9), 16 → 12 CNOT gates (compression = 16/12), and 9 → 5 T-latency (compression = 9/5). Similarly, for a 1-bit full adder, reductions include 14 → 8 T gates (14/8), 12 → 10 CNOT gates (12/10), 4 → 2 H gates (2), and 8 → 2 T-latency (4).",
    "title_zh": "面向物联网网络的低延迟量子电路",
    "abstract_zh": "未来的物联网网络将对传输关键任务信息的延迟提出越来越低的要求，同时还将采用量子技术以增强安全性并加速信息处理。本文综述了在延迟优化量子电路设计方面的最新进展，重点在于在不牺牲必要功能的前提下最小化电路延迟。文章主要聚焦于网络层面的问题，探讨网络延迟与能耗、可编程性及成本之间的平衡，并建立与电路开发者实际设计中应处理问题的恰当接口。第一部分分析了激进的延迟降低如何限制通用门和可逆门的使用——这两类门均倾向于增加电路深度（即延迟）。通用门支持可编程性，而可逆门则有助于减少能量耗散，但两者都可能延长电路延迟。意识到这些权衡关系后，第二部分讨论了延迟最优的设计方法及其实施细节。随后，我们介绍了量子电路的精确最小化方法，并展示了量化的改进成果。例如，在受控T门的实现中，T门数量从15个减少至9个（压缩比为15/9），CNOT门数量从16个减少至12个（压缩比为16/12），T延迟从9个降至5个（压缩比为9/5）。同样地，对于一位全加器，其优化结果包括：T门数量从14个减少至8个（压缩比14/8），CNOT门从12个减少至10个（压缩比12/10），H门从4个减少至2个（压缩比2），T延迟从8个降至2个（压缩比4）。"
  },
  {
    "date": "2025-12-31",
    "title": "FPGA-Based Muon Beam Monitoring with Real-Time Fault Recovery",
    "authors": "Yoshiki Yamada, Yoshiki Yamaguchi, Kenya Okabe, Yowichi Fujita, Yoshinori Fukao, Eitaro Hamada, Youichi Igarashi, Masayoshi Shoji, Hironori Uchinoyae, Tetsuichi Kishishita, Kazuki Ueno",
    "publish": "2025 IEEE 18th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC)",
    "url": "https://doi.org/10.1109/mcsoc67473.2025.00063",
    "source": "IEEE",
    "abstract": "This paper presents a scalable and autonomous FPGA-SoC architecture designed for real-time muon-beam monitoring in the COMET experiment at J-PARC. The proposed system integrates a parallel stream-alignment mechanism and on-chip error-correction logic within a deterministic pipeline, achieving a processing latency of <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$0.56 \\sim \\mu \\mathrm{s}$</tex> and an alignment accuracy exceeding 99.99 %. In contrast to our previously reported single-channel prototype, this design introduces full multi-channel scalability, autonomous synchronization, and a hardware-level fault-recovery mechanism. Implemented on a Kintex-7 FPGA and validated on the COMET Phase- <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\alpha$</tex> beamline, the system demonstrated stable operation for more than 10<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">6</sup> beam cycles without any packet loss or timing drift. The architecture supports hierarchical scalability up to 256 channels, enabling real-time adaptability under radiation-exposure conditions. These results confirm that deterministic, low-latency data acquisition with a self-healing capability can be achieved entirely in hardware. The proposed approach establishes a foundation for heterogeneous and reconfigurable SoC systems applicable to next-generation accelerator instrumentation as well as intelligent sensing environments.",
    "title_zh": "基于FPGA的μ子束流监测及实时故障恢复",
    "abstract_zh": "本文提出了一种可扩展且自主的FPGA-SoC架构，专为J-PARC实验室COMET实验中的实时μ子束监测而设计。该系统在确定性流水线中集成了并行流对齐机制与片上纠错逻辑，实现了小于0.56 μs的处理延迟以及超过99.99%的对齐精度。相较于我们此前报道的单通道原型，本设计引入了完整的多通道可扩展性、自主同步功能以及硬件级故障恢复机制。该系统基于Kintex-7 FPGA实现，并在COMET Phase-α束流线上完成验证，连续稳定运行超过10⁶个束流周期，期间未出现任何数据包丢失或时序漂移现象。该架构支持最高达256个通道的分层可扩展性，能够在辐射暴露条件下实现实时自适应。结果表明，完全通过硬件实现的确定性、低延迟数据采集与自愈能力是可行的。所提出的方案为下一代加速器仪器仪表以及智能传感环境中的异构可重构SoC系统奠定了基础。"
  },
  {
    "date": "2025-12-31",
    "title": "MicroIntent: Intent-Based Placement Strategy for Microservice Application in the Compute Continuum Using LLMs",
    "authors": "Koushikur Islam, Guilherme Da Cunha Rodrigues, Bahman Javadi, Rodrigo N. Calheiros",
    "publish": "2025 IEEE International Conference on Smart Internet of Things (SmartIoT)",
    "url": "https://doi.org/10.1109/smartiot66867.2025.00026",
    "source": "IEEE",
    "abstract": "The placement of microservices in the compute continuum plays a vital role in delivering services that comply with customers’ needs, such as reduced latency, storage requirements, quality of service and availability. To achieve customers’ needs in the geographically dispersed architecture of the compute continuum, Service Level Objectives (SLOs) have been largely used in decision-making to place microservices. However, because low-level SLOs increase the barrier to entry for continuum users, placement decisions based on high-level business vocabulary are required if the compute continuum is to be adopted at scale. This paper proposes an architecture for microservices placement decisions in the computing continuum utilizing highlevel user intents described in natural language as input. The approach utilizes Generative Artificial Intelligence to translate the intents to low-level SLOs, which are used along with the infrastructure description to decide where different microservices that compose an application must be deployed so that SLOs are met. We implement and evaluate a prototype of the architecture to demonstrate the approach’s feasibility.",
    "title_zh": "MicroIntent：基于大模型的计算连续体中微服务应用意图驱动部署策略",
    "abstract_zh": "在计算连续体中合理部署微服务对于满足客户的需求（如降低延迟、减少存储需求、保障服务质量与可用性）至关重要。为了在地理分布广泛的计算连续体架构中实现客户需求，服务等级目标（SLOs）已被广泛用于指导微服务的部署决策。然而，由于底层SLOs对连续体用户的使用门槛较高，若要实现计算连续体的大规模应用，必须采用基于高层次业务术语的部署决策方式。本文提出了一种基于自然语言描述的高层次用户意图输入的微服务部署决策架构。该方法利用生成式人工智能将用户意图转化为低层次SLOs，并结合基础设施描述信息，确定构成应用程序的各个微服务应部署在何处，以确保SLO得以满足。我们实现了该架构的原型系统，并进行了评估，以验证该方法的可行性。"
  },
  {
    "date": "2025-12-31",
    "title": "Automating Timing Enclaves for Reactive Programs in Lingua Franca",
    "authors": "Julian Robledo, Jeronimo Castrillon",
    "publish": "2025 IEEE 18th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC)",
    "url": "https://doi.org/10.1109/mcsoc67473.2025.00013",
    "source": "IEEE",
    "abstract": "Lingua Franca (LF) is a coordination language for cyber-physical systems that ensures deterministic execution through the reactor model. While LF's conservative scheduler guarantees correctness, it can hinder performance due to synchronization barriers that limit parallelism. Timing enclaves, a recent extension to LF, mitigate this by partitioning programs into independently scheduled regions with decoupled logical timelines. However, determining optimal enclave configurations is non-trivial and not intuitive to programmers, as it requires balancing parallelism, synchronization overhead, and system constraints. In this paper, we present a framework for Design Space Exploration (DSE) of timing enclaves in LF programs. Our approach automatically analyzes the program's topology, identifies valid enclave partitions, and uses heuristics to explore trade-offs between determinism and latency. We evaluate the effectiveness of our method on synthetic benchmarks and a realworld 4G baseband processing application. Results show that our DSE framework can significantly improve performance while preserving deterministic semantics, highlighting its practical value for complex real-time systems.",
    "title_zh": "在Lingua Franca中为反应式程序自动构建时序环境",
    "abstract_zh": "Lingua Franca（LF）是一种用于网络物理系统的协调语言，通过反应器模型确保执行的确定性。尽管LF采用保守调度策略可保证正确性，但同步屏障会限制并行度，从而影响性能。近期对LF的扩展——时序区（timing enclaves），通过将程序划分为独立调度的区域，并采用解耦的逻辑时间线，有效缓解了这一问题。然而，如何确定最优的区配置并非易事，且对程序员而言并不直观，因为这需要在并行性、同步开销与系统约束之间进行权衡。本文提出了一种针对LF程序中时序区的设计空间探索（DSE）框架。我们的方法能够自动分析程序拓扑结构，识别有效的区划分方案，并利用启发式算法探索确定性与延迟之间的权衡。我们在合成基准测试和一个真实的4G基带处理应用上评估了该方法的有效性。结果表明，所提出的DSE框架能够在保持确定性语义的前提下显著提升性能，凸显其在复杂实时系统中的实际应用价值。"
  },
  {
    "date": "2025-12-31",
    "title": "A Neuro-Symbolic Approach to Translating Natural Language Instructions into Structured, Verifiable Commands",
    "authors": "Shang Li, Zhiwei Zong, Yang Chen, Xin Zhang",
    "publish": "2025 9th International Conference on Communication and Information Systems (ICCIS)",
    "url": "https://doi.org/10.1109/iccis68161.2025.11316866",
    "source": "IEEE",
    "abstract": "Automating procedural tasks from natural language instructions is crucial for industrial applications but is hampered by ambiguity and the need for implicit domain knowledge. While Large Language Models (LLMs) offer unprecedented semantic understanding, their propensity for factual hallucination renders them unreliable for safety-critical systems. This paper presents a novel neuro-symbolic framework to address this challenge. Our approach leverages an LLM for initial instruction translation, guided by Chain-of-Thought prompting for logical transparency and Retrieval-Augmented Generation for contextual grounding. Critically, we introduce a post-hoc verification layer where every structured command candidate generated by the LLM is rigorously validated against a domain-specific knowledge graph for entity correctness, operational legality, and parameter safety. Through a series of case studies, we demonstrate that our framework effectively resolves contextual ambiguities, prevents the execution of factually incorrect commands, and enforces implicit safety constraints that a standalone LLM would miss. Our work provides a robust methodology for creating verifiable and trustworthy instruction-following systems, bridging the gap between linguistic flexibility and operational reliability.",
    "title_zh": "一种将自然语言指令转化为结构化、可验证命令的神经符号方法",
    "abstract_zh": "从自然语言指令自动执行程序化任务对于工业应用至关重要，但受限于语义模糊性以及对隐含领域知识的需求。尽管大型语言模型（LLM）提供了前所未有的语义理解能力，但其容易产生事实性幻觉的特性，使其在安全关键系统中不可靠。本文提出一种新颖的神经符号框架以应对这一挑战。我们的方法利用LLM进行初始指令翻译，并通过思维链提示（Chain-of-Thought prompting）实现逻辑透明性，结合检索增强生成（Retrieval-Augmented Generation）确保上下文的准确性。关键在于，我们引入了一个后处理验证层：对LLM生成的每一个结构化命令候选，均基于特定领域的知识图谱进行严格验证，以确保实体正确性、操作合法性及参数安全性。通过一系列案例研究，我们证明该框架能够有效解决上下文歧义问题，防止执行错误的事实性指令，并强制实施独立LLM可能忽略的隐式安全约束。本研究为构建可验证且可信的指令遵循系统提供了一种稳健的方法，弥合了语言灵活性与操作可靠性之间的鸿沟。"
  },
  {
    "date": "2025-12-31",
    "title": "Promises and Perils of LLM- and Agent-Generated Code",
    "authors": "Premkumar T. Devanbu",
    "publish": "Computer",
    "url": "https://doi.org/10.1109/mc.2025.3627694",
    "source": "IEEE",
    "abstract": "This article draws attention to the fact that, traditionally, software maintenance costs have strongly dominated initial development costs, and calls for more in-depth, focused, specialized studies of the actual impacts of large language model- and agent-generated code.",
    "title_zh": "大语言模型与智能体生成代码的承诺与风险",
    "abstract_zh": "本文指出，传统上软件维护成本远远超过初始开发成本，呼吁对大型语言模型和智能体生成代码的实际影响进行更深入、专注且专门的研究。"
  },
  {
    "date": "2025-12-31",
    "title": "Digital Dice Design Using Pseudo Random Number Generator",
    "authors": "Tee Hui Teo, Maoyang Xiang, Qianrui Lin, Michael Kee Hian Lim",
    "publish": "2025 IEEE 18th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC)",
    "url": "https://doi.org/10.1109/mcsoc67473.2025.00039",
    "source": "IEEE",
    "abstract": "The primary objective of this innovative project is to develop a digital dice that can display dice numbers in real-time. This digital dice operates by utilizing a pseudo-random number generator (PRNG) that employs the XORshift algorithm. This algorithm is coded explicitly in Verilog Hardware Description Language (HDL) and is integrated into a field-programmable gate array (FPGA). The digital dice itself comprises various components such as a tilt sensor, display screen, power management circuit, and a rechargeable battery, all neatly enclosed within a 3D-printed dice casing. When interacting with this digital dice, users can initiate the generation of a random number by simply shaking the device. This action triggers the tilt sensor within the dice, which in turn generates a seed for the PRNG to produce a random number. The numbers displayed by this digital die range from 1 to 100, effectively simulating the various sides of a traditional dice. This comprehensive kit, aptly named SUTDicey, showcases a perfect blend of cutting-edge technology and practical application in the realm of gaming and probability.",
    "title_zh": "使用伪随机数生成器设计数字骰子",
    "abstract_zh": "本创新项目的主要目标是开发一款能够实时显示骰子点数的数字骰子。这款数字骰子通过使用伪随机数生成器（PRNG）实现，其核心算法采用XORshift算法。该算法以Verilog硬件描述语言（HDL）直接编写，并集成于现场可编程门阵列（FPGA）中。数字骰子本身包含多种组件，如倾斜传感器、显示屏、电源管理电路以及可充电电池，所有这些部件均被巧妙地封装在3D打印的骰子外壳内。当用户与该数字骰子互动时，只需轻轻摇动设备即可触发随机数的生成。这一动作会激活骰子内部的倾斜传感器，进而为PRNG提供初始种子值，从而生成一个随机数。该数字骰子所显示的数值范围为1至100，有效模拟了传统骰子的各种面数。这一完整的套件被命名为SUTDicey，充分展现了前沿科技与实际应用在游戏及概率领域中的完美结合。"
  },
  {
    "date": "2025-12-31",
    "title": "Agentic AI: A New Paradigm for Cyber–Physical Systems Cybersecurity",
    "authors": "Georgios Xenos, Dimitrios Serpanos",
    "publish": "Computer",
    "url": "https://doi.org/10.1109/mc.2025.3625738",
    "source": "IEEE",
    "abstract": "Agentic artificial intelligence and advancements in large language models are transforming cyber–physical systems security into a proactive and adaptive paradigm, where intelligent agents autonomously detect, analyze and respond to cyber threats in real time.",
    "title_zh": "代理型人工智能：网络物理系统安全的新范式",
    "abstract_zh": "智能代理型人工智能以及大型语言模型的进步正在将网络物理系统安全转变为一种主动且自适应的范式，其中智能代理能够自主地实时检测、分析并响应网络威胁。"
  },
  {
    "date": "2025-12-31",
    "title": "Implementation of Recursive and Iterative Moving Average Filter for ECG Data in Fixed and Floating Representation",
    "authors": "Susithra N, Nidhiyashree V K, Sivatharini R, Priyanka P, Jasmitaa G",
    "publish": "2025 Second International Conference on Intelligent Technologies for Sustainable Electric and Communications Systems (iTech SECOM)",
    "url": "https://doi.org/10.1109/itechsecom64750.2025.11307544",
    "source": "IEEE",
    "abstract": "Moving Average Filters are used for digital signal processing applications, such as smoothing ECG signals by minimizing high-frequency noise. This work investigates the design and implementation of Moving Average Filters (MAFs) based on Verilog HDL in iterative and recursive methods with fixed-point and floating-point arithmetic. The research compares the four different implementations—fixed-point iterative, fixed-point recursive, floating-point iterative, and floating-point recursive—tested in terms of performance such as resource usage, timing latency, power dissipation, and numerical precision. In the study, real-life ECG records obtained from the MIT-BIH Arrhythmia database are used to emulate the filter performance. Results prove that fixed-point recursive filters achieve the optimal trade-off between accuracy, stability, and hardware efficiency and therefore are suitable for real-time and low-power embedded applications. On the other hand, floating-point implementations provide increased dynamic range but in exchange for increased resource usage and sensitivity to rounding errors. This study aids in choosing the filter design for performance-limited signal processing applications.",
    "title_zh": "基于固定精度与浮点表示的递归与迭代移动平均滤波器在心电图（ECG）数据中的实现",
    "abstract_zh": "移动平均滤波器（Moving Average Filters, MAFs）广泛应用于数字信号处理领域，例如通过抑制高频噪声来平滑心电图（ECG）信号。本研究探讨了基于Verilog HDL的移动平均滤波器在迭代与递归两种方法下的设计与实现，并分别采用定点数和浮点数运算进行分析。研究对比了四种不同实现方式——定点迭代、定点递归、浮点迭代和浮点递归——在资源占用、时序延迟、功耗消耗以及数值精度等方面的性能表现。实验中使用来自MIT-BIH心律失常数据库的真实ECG数据记录来模拟滤波器的实际性能。结果表明，定点递归滤波器在精度、稳定性与硬件效率之间实现了最佳平衡，因而特别适用于实时性要求高且功耗受限的嵌入式应用。相比之下，浮点实现虽然提供了更大的动态范围，但代价是更高的资源开销以及对舍入误差更敏感。本研究为性能受限的信号处理应用中的滤波器设计选择提供了重要参考依据。"
  },
  {
    "date": "2025-12-31",
    "title": "FPGA Implementation of Binary-Weighted Transformer for Prognostics and Health Management of Permanent Magnet Synchronous Motors Using Current Sensors",
    "authors": "Soongyu Kang, Yongchul Jung, Sewoon Oh, Yunho Jung",
    "publish": "IEEE Sensors Letters",
    "url": "https://doi.org/10.1109/lsens.2025.3649802",
    "source": "IEEE",
    "abstract": "In this letter, we propose a prognostics and health management (PHM) method for permanent magnet synchronous motors (PMSMs) in urban air mobility (UAM). The method uses a Transformer and three-phase current sensors. The short-time Fourier transform (STFT) is employed for current signals to capture fault-related information. The Transformer architecture effectively extracts both local and global features from time–frequency representations, enabling high classification performance. However, its high computational cost and large model size hinder deployment in practical industrial applications. To address this challenge, we designed a lightweight binary-weighted Transformer (BWT) for PMSM PHM, reducing the model size to 5.5% of the baseline. The proposed BWT achieves 99.81% classification accuracy across four classes. We also developed a hardware accelerator for matrix multiplication (MM)—the most time-consuming operation in BWT—and implemented it on a field-programmable gate array (FPGA). The proposed SW/HW co-design achieved an 85.55× speedup over the software-only implementation on the ARM microprocessor unit (MPU).",
    "title_zh": "基于电流传感器的永磁同步电机状态监测与健康管理系统中二进制加权Transformer的FPGA实现",
    "abstract_zh": "在本信件中，我们提出了一种用于城市空中交通（UAM）中永磁同步电机（PMSM）的故障预测与健康管理（PHM）方法。该方法采用Transformer架构与三相电流传感器，利用短时傅里叶变换（STFT）对电流信号进行处理，以提取与故障相关的信息。Transformer结构能够有效从时频表示中捕捉局部和全局特征，从而实现高精度分类。然而，其较高的计算成本和庞大的模型规模限制了其在实际工业应用中的部署。为解决这一挑战，我们设计了一种轻量化的二值权重Transformer（BWT），将模型大小缩减至基准模型的5.5%。所提出的BWT在四类故障分类任务中达到了99.81%的分类准确率。此外，我们还开发了一种针对矩阵乘法（MM）——BWT中最耗时的操作——的硬件加速器，并将其在现场可编程门阵列（FPGA）上实现。所提出的软硬件协同设计相较于ARM微处理器单元（MPU）上的纯软件实现，实现了85.55倍的加速效果。"
  },
  {
    "date": "2025-12-31",
    "title": "Fine-Tuning and Optimizing Lightweight LLMs for Domain-Specific Use Cases",
    "authors": "Tapan Kumar Das, Ayan Biswas",
    "publish": "2025 Innovations in Power and Advanced Computing Technologies (i-PACT)",
    "url": "https://doi.org/10.1109/i-pact65952.2025.11307857",
    "source": "IEEE",
    "abstract": "This research presents the development and optimization of a lightweight transformer model tailored for deployment on consumer-grade hardware with limited computational resources. The study aims to address the challenges associated with running large-scale language models on home computers by integrating advanced optimization techniques, including quantization, pruning, and knowledge distillation. These methods are used to reduce the size of the model, improve efficiency, a nd maintain performance while minimizing computational overhead. The proposed methodology includes model design, dataset preprocessing, training optimization, and performance evaluation. The model is fine-tuned on a research paper metadata dataset, ensuring its relevance for academic and research-oriented applications. The implementation is validated on an NVIDIA RTX 4050 (6GB VRAM), with performance metrics that evaluate inference speed, memory utilization, and accuracy trade-offs. By demonstrating an effective balance between computational efficiency and model performance, this research contributes to the broader objective of democratizing transformer-based NLP models for low-resource environments. The findings underscore the feasibility of deploying advanced AI models on consumer-grade hardware, facilitating offline processing, and reducing the dependency on cloud-based infrastructures.",
    "title_zh": "针对特定领域应用微调与优化轻量级大语言模型",
    "abstract_zh": "本研究提出了一种专为计算资源有限的消费级硬件设计并优化的轻量级Transformer模型。该研究旨在解决在家庭计算机上运行大规模语言模型所面临的挑战，通过整合量化、剪枝和知识蒸馏等先进优化技术，有效减小模型规模，提升运行效率，并在最小化计算开销的同时保持模型性能。所提出的方案涵盖模型设计、数据集预处理、训练优化及性能评估等多个环节。模型在科研论文元数据数据集上进行微调，确保其在学术与研究类应用中的相关性。实验在配备NVIDIA RTX 4050（6GB显存）的设备上进行，性能评估指标包括推理速度、内存占用以及准确率之间的权衡。研究结果表明，该方法在计算效率与模型性能之间实现了有效平衡，推动了基于Transformer的自然语言处理模型在低资源环境中的普及。研究发现证实了在消费级硬件上部署先进人工智能模型的可行性，有助于实现离线处理，降低对云端基础设施的依赖。"
  }
]