[
  {
    "date": "2026-02-05",
    "title": "Characterizing and Modeling the GitHub Security Advisories Review Pipeline",
    "authors": "Claudio Segal, Paulo Segal, Carlos Eduardo de Schuller Banjar, Felipe Paixão, Hudson Silva Borges, Paulo Silveira Neto, Eduardo Santana de Almeida, Joanna C. S. Santos, Anton Kocheturov, Gaurav Kumar Srivastava, Daniel Sadoc Menasché",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.06009v1",
    "source": "arXiv",
    "abstract": "GitHub Security Advisories (GHSA) have become a central component of open-source vulnerability disclosure and are widely used by developers and security tools. A distinctive feature of GHSA is that only a fraction of advisories are reviewed by GitHub, while the mechanisms associated with this review process remain poorly understood. In this paper, we conduct a large-scale empirical study of GHSA review processes, analyzing over 288,000 advisories spanning 2019--2025. We characterize which advisories are more likely to be reviewed, quantify review delays, and identify two distinct review-latency regimes: a fast path dominated by GitHub Repository Advisories (GRAs) and a slow path dominated by NVD-first advisories. We further develop a queueing model that accounts for this dichotomy based on the structure of the advisory processing pipeline.",
    "title_zh": "描述与建模GitHub安全通告审核流程",
    "abstract_zh": "GitHub 安全通告（GHSA）已成为开源漏洞披露的核心组成部分，被开发者和安全工具广泛使用。GHSA 的一个显著特点是，仅有少数通告经过 GitHub 的审核，而与这一审核流程相关的机制至今仍不清晰。本文对 GHSA 的审核流程开展了一项大规模实证研究，分析了涵盖 2019 至 2025 年间超过 28.8 万条通告的数据。我们刻画了哪些类型的通告更可能被审核，量化了审核延迟，并识别出两种截然不同的审核延迟模式：一种是受 GitHub Repository Advisories（GRAs）主导的快速通道，另一种则是由以 NVD 为先的通告主导的慢速通道。此外，我们构建了一个排队模型，基于通告处理流程的结构，解释了这一二元分化的现象。"
  },
  {
    "date": "2026-02-05",
    "title": "Persistent Human Feedback, LLMs, and Static Analyzers for Secure Code Generation and Vulnerability Detection",
    "authors": "Ehsan Firouzi, Mohammad Ghafari",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05868v1",
    "source": "arXiv",
    "abstract": "Existing literature heavily relies on static analysis tools to evaluate LLMs for secure code generation and vulnerability detection. We reviewed 1,080 LLM-generated code samples, built a human-validated ground-truth, and compared the outputs of two widely used static security tools, CodeQL and Semgrep, against this corpus. While 61% of the samples were genuinely secure, Semgrep and CodeQL classified 60% and 80% as secure, respectively. Despite the apparent agreement in aggregate statistics, per-sample analysis reveals substantial discrepancies: only 65% of Semgrep's and 61% of CodeQL's reports correctly matched the ground truth. These results question the reliability of static analysis tools as sole evaluators of code security and underscore the need for expert feedback. Building on this insight, we propose a conceptual framework that persistently stores human feedback in a dynamic retrieval-augmented generation pipeline, enabling LLMs to reuse past feedback for secure code generation and vulnerability detection.",
    "title_zh": "持续的人类反馈、大语言模型与静态分析器在安全代码生成与漏洞检测中的应用",
    "abstract_zh": "现有文献在评估大语言模型（LLM）生成安全代码及检测漏洞时，高度依赖静态分析工具。我们审查了1,080个由LLM生成的代码样本，构建了一个经过人工验证的基准真实数据集，并将两种广泛使用的静态安全分析工具——CodeQL和Semgrep的输出结果与该数据集进行了对比。尽管61%的样本实际上属于安全代码，但Semgrep和CodeQL分别将60%和80%的样本判定为安全。尽管总体统计数据看似一致，但逐样本分析揭示了显著差异：仅有65%的Semgrep报告和61%的CodeQL报告与基准真实情况相符。这些结果质疑了将静态分析工具作为代码安全性评估唯一依据的可靠性，并凸显了引入专家反馈的必要性。基于这一发现，我们提出了一种概念性框架，该框架在动态检索增强生成流程中持续存储人类反馈，使LLM能够复用以往的反馈信息，从而在生成安全代码和检测漏洞时实现更可靠的性能。"
  },
  {
    "date": "2026-02-05",
    "title": "DARWIN: Dynamic Agentically Rewriting Self-Improving Network",
    "authors": "Henry Jiang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05848v1",
    "source": "arXiv",
    "abstract": "DARWIN is an evolutionary GPT model, utilizing a genetic-algorithm like optimization structure with several independent GPT agents being trained individually using unique training code. Each iteration, the GPT models are prompted to modify the training code of one another in an attempt to improve their performance in a mutation-like manner, and the best GPT agents are then benchmarked and selected for the next iteration by genetic algorithm. For demonstration purposes and due to budget and time constraints, OpenAI API is used to prompt training code improvements and the nanoGPT framework is used as the training code. DARWIN also utilizes persistent JSON-based memory files to track previous reasoning and changes to code to correlate with improvement to model performance. and a bidirectional interface for HITL intervention allowing the model to request upgrades such as additional datasets, training scripts, and restructuring of file hierarchies. In experiments, DARWIN achieved a 1.26 percent improvement in model FLOPS utilization (MFU) and a 2.07 percent improvement to perplexity in 5 iterations of training over baseline configurations, demonstrating promising capabilities as a foundation for scaling evolutionary GPT training.",
    "title_zh": "达尔文：动态代理重写自我改进网络",
    "abstract_zh": "DARWIN 是一个基于进化机制的 GPT 模型，采用类似遗传算法的优化架构，其中多个独立的 GPT 代理分别使用独特的训练代码进行训练。在每一轮迭代中，GPT 模型会被提示修改彼此的训练代码，以类似突变的方式尝试提升性能，随后通过遗传算法对表现最佳的 GPT 代理进行基准测试并选出下一代迭代的候选者。出于演示目的以及受预算和时间限制，DARWIN 使用 OpenAI API 来生成训练代码的改进建议，并采用 nanoGPT 框架作为训练代码基础。此外，DARWIN 还利用基于持久化 JSON 的内存文件，记录先前的推理过程和代码变更，以便与模型性能的提升进行关联分析。系统还配备双向人机协同（HITL）接口，允许模型主动请求升级，例如增加数据集、改进训练脚本或重构文件结构。在实验中，DARWIN 在 5 轮训练迭代中，相较于基线配置，实现了模型浮点运算利用率（MFU）提升 1.26%，困惑度（perplexity）降低 2.07%，展现出作为大规模进化式 GPT 训练基础的广阔潜力。"
  },
  {
    "date": "2026-02-05",
    "title": "ToMigo: Interpretable Design Concept Graphs for Aligning Generative AI with Creative Intent",
    "authors": "Lena Hegemann, Xinyi Wen, Michael A. Hedderich, Tarmo Nurmi, Hariharan Subramonyam",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05825v1",
    "source": "arXiv",
    "abstract": "Generative AI often produces results misaligned with user intentions, for example, resolving ambiguous prompts in unexpected ways. Despite existing approaches to clarify intent, a major challenge remains: understanding and influencing AI's interpretation of user intent through simple, direct inputs requiring no expertise or rigid procedures. We present ToMigo, representing intent as design concept graphs: nodes represent choices of purpose, content, or style, while edges link them with interpretable explanations. Applied to graphic design, ToMigo infers intent from reference images and text. We derived a schema of node types and edges from pre-study data, informing a multimodal large language model to generate graphs aligning nodes externally with user intent and internally toward a unified design goal. This structure enables users to explore AI reasoning and directly manipulate the design concept. In our user studies, ToMigo received high alignment ratings and captured most user intentions well. Users reported greater control and found interactive features-editable graphs, reflective chats, concept-design realignment-useful for evolving and realizing their design ideas.",
    "title_zh": "ToMigo：用于对齐生成式人工智能与创作意图的可解释设计概念图",
    "abstract_zh": "生成式人工智能常常产生与用户意图不符的结果，例如对模糊的提示词做出出人意料的解读。尽管已有方法尝试澄清用户意图，但一个主要挑战依然存在：如何通过简单、直接的输入，无需专业知识或严格流程，来理解并影响人工智能对用户意图的解读。我们提出了ToMigo，将意图表示为设计概念图：节点代表目的、内容或风格的选择，边则以可解释的说明连接这些节点。在图形设计领域，ToMigo能够根据参考图像和文本推断用户意图。我们基于前期研究数据提炼出节点类型与边的模式，指导多模态大语言模型生成既在外部与用户意图一致、又在内部统一于整体设计目标的概念图。这种结构使用户能够探索AI的推理过程，并直接操控设计概念。在用户研究中，ToMigo获得了高度的意图对齐评分，较好地捕捉了大多数用户意图。用户反馈称，他们获得了更强的控制感，并认为交互功能——如可编辑的图谱、反思性对话、概念与设计的重新对齐——对不断演化和完善设计构想非常有帮助。"
  },
  {
    "date": "2026-02-05",
    "title": "LongR: Unleashing Long-Context Reasoning via Reinforcement Learning with Dense Utility Rewards",
    "authors": "Bowen Ping, Zijun Chen, Yiyao Yu, Tingfeng Hui, Junchi Yan, Baobao Chang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05758v1",
    "source": "arXiv",
    "abstract": "Reinforcement Learning has emerged as a key driver for LLM reasoning. This capability is equally pivotal in long-context scenarios--such as long-dialogue understanding and structured data analysis, where the challenge extends beyond consuming tokens to performing rigorous deduction. While existing efforts focus on data synthesis or architectural changes, recent work points out that relying solely on sparse, outcome-only rewards yields limited gains, as such coarse signals are often insufficient to effectively guide the complex long-context reasoning. To address this, we propose LongR, a unified framework that enhances long-context performance by integrating a dynamic \"Think-and-Read\" mechanism, which interleaves reasoning with document consultation, with a contextual density reward based on relative information gain to quantify the utility of the relevant documents. Empirically, LongR achieves a 9% gain on LongBench v2 and consistent improvements on RULER and InfiniteBench, demonstrating robust efficiency in navigating extensive contexts. Furthermore, LongR consistently enhances performance across diverse RL algorithms (e.g., DAPO, GSPO). Finally, we conduct in-depth analyses to investigate the impact of reasoning chain length on efficiency and the model's robustness against distractors.",
    "title_zh": "LongR：通过密集效用奖励的强化学习实现长上下文推理",
    "abstract_zh": "强化学习已成为推动大语言模型（LLM）推理能力的关键动力。这一能力在长上下文场景中同样至关重要——例如长对话理解与结构化数据分析，其中挑战不仅在于处理大量token，更在于执行严谨的逻辑推理。尽管现有研究主要聚焦于数据合成或架构改进，但近期工作指出，仅依赖稀疏的、仅基于结果的奖励信号难以带来显著提升，因为这类粗粒度信号往往不足以有效引导复杂的长上下文推理过程。为解决这一问题，我们提出LongR，一个统一框架，通过引入动态的“思考-查阅”机制，将推理过程与文档查阅交替进行，并结合基于相对信息增益的上下文密度奖励，以量化相关文档的实际价值。实验表明，LongR在LongBench v2上实现了9%的性能提升，并在RULER和InfiniteBench上均表现出持续改进，展现出在处理长上下文时的强大效率。此外，LongR在多种强化学习算法（如DAPO、GSPO）中均能稳定提升性能。最后，我们通过深入分析，探讨了推理链长度对效率的影响，以及模型对干扰项的鲁棒性。"
  },
  {
    "date": "2026-02-05",
    "title": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications",
    "authors": "Chang Yang, Chuang Zhou, Yilin Xiao, Su Dong, Luyao Zhuang, Yujing Zhang, Zhu Wang, Zijin Hong, Zheng Yuan, Zhishang Xiang, Shengyuan Chen, Huachi Zhou, Qinggang Zhang, Ninghao Liu, Jinsong Su, Xinrun Wang, Yi Chang, Xiao Huang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05665v1",
    "source": "arXiv",
    "abstract": "Memory emerges as the core module in the Large Language Model (LLM)-based agents for long-horizon complex tasks (e.g., multi-turn dialogue, game playing, scientific discovery), where memory can enable knowledge accumulation, iterative reasoning and self-evolution. Among diverse paradigms, graph stands out as a powerful structure for agent memory due to the intrinsic capabilities to model relational dependencies, organize hierarchical information, and support efficient retrieval. This survey presents a comprehensive review of agent memory from the graph-based perspective. First, we introduce a taxonomy of agent memory, including short-term vs. long-term memory, knowledge vs. experience memory, non-structural vs. structural memory, with an implementation view of graph-based memory. Second, according to the life cycle of agent memory, we systematically analyze the key techniques in graph-based agent memory, covering memory extraction for transforming the data into the contents, storage for organizing the data efficiently, retrieval for retrieving the relevant contents from memory to support reasoning, and evolution for updating the contents in the memory. Third, we summarize the open-sourced libraries and benchmarks that support the development and evaluation of self-evolving agent memory. We also explore diverse application scenarios. Finally, we identify critical challenges and future research directions. This survey aims to offer actionable insights to advance the development of more efficient and reliable graph-based agent memory systems. All the related resources, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphMemory.",
    "title_zh": "基于图的智能体记忆：分类、技术与应用",
    "abstract_zh": "记忆在基于大语言模型（LLM）的智能体执行长周期复杂任务（如多轮对话、游戏博弈、科学发现等）中成为核心模块，其作用在于实现知识积累、迭代推理与自我演化。在多种记忆范式中，图结构因其天然具备建模关系依赖、组织层次化信息以及支持高效检索的能力，展现出强大的优势。本文从图视角对智能体记忆进行了全面综述。首先，我们提出一个智能体记忆的分类体系，涵盖短期与长期记忆、知识与经验记忆、非结构化与结构化记忆，并从图结构实现的角度进行阐述。其次，基于智能体记忆的生命周期，系统分析了图结构智能体记忆中的关键技术，包括：记忆提取（将数据转化为记忆内容）、存储（高效组织记忆数据）、检索（从记忆中获取相关知识以支持推理）以及演化（更新记忆内容以实现持续进化）。第三，我们总结了支持自演化智能体记忆开发与评估的开源库与基准测试集，并探讨了其在多样化应用场景中的实践。最后，我们指出了当前面临的关键挑战与未来研究方向。本综述旨在为构建更高效、更可靠的图结构智能体记忆系统提供切实可行的洞见。所有相关资源，包括研究论文、开源数据与项目，均已整理并发布于 GitHub 仓库：https://github.com/DEEP-PolyU/Awesome-GraphMemory，以供社区共享与使用。"
  },
  {
    "date": "2026-02-05",
    "title": "ORACL: Optimized Reasoning for Autoscaling via Chain of Thought with LLMs for Microservices",
    "authors": "Haoyu Bai, Muhammed Tawfiqul Islam, Minxian Xu, Rajkumar Buyya",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05292v1",
    "source": "arXiv",
    "abstract": "Applications are moving away from monolithic designs to microservice and serverless architectures, where fleets of lightweight and independently deployable components run on public clouds. Autoscaling serves as the primary control mechanism for balancing resource utilization and quality of service, yet existing policies are either opaque learned models that require substantial per-deployment training or brittle hand-tuned rules that fail to generalize. We investigate whether large language models can act as universal few-shot resource allocators that adapt across rapidly evolving microservice deployments. We propose ORACL, Optimized Reasoning for Autoscaling via Chain of Thought with LLMs for Microservices, a framework that leverages prior knowledge and chain-of-thought reasoning to diagnose performance regressions and recommend resource allocations. ORACL transforms runtime telemetry, including pods, replicas, CPU and memory usage, latency, service-level objectives, and fault signals, into semantic natural-language state descriptions and invokes an LLM to produce an interpretable intermediate reasoning trace. This reasoning identifies likely root causes, prunes the action space, and issues safe allocation decisions under policy constraints. Experiments on representative open-source microservice workloads show that ORACL improves root-cause identification accuracy by 15 percent, accelerates training by up to 24x, and improves quality of service by 6 percent in short-term scenarios, without deployment-specific retraining.",
    "title_zh": "ORACL：基于思维链的大型语言模型用于微服务自动扩缩容的优化推理",
    "abstract_zh": "应用程序正从传统的单体架构转向微服务和无服务器架构，其中大量轻量级、可独立部署的组件运行在公有云上。自动扩缩容作为平衡资源利用率与服务质量的主要控制机制，但现有的扩缩容策略要么是难以理解的、需要大量部署训练的机器学习模型，要么是脆弱的手动调优规则，难以泛化。我们探讨了大型语言模型是否能够作为通用的少样本资源分配器，在快速演进的微服务部署中实现自适应。为此，我们提出了 ORACL（Optimized Reasoning for Autoscaling via Chain of Thought with LLMs for Microservices）框架，该框架利用先验知识和思维链推理，诊断性能退化问题并推荐资源分配方案。ORACL 将运行时遥测数据（包括 Pod、副本数、CPU 和内存使用率、延迟、服务等级目标以及故障信号等）转化为语义化的自然语言状态描述，并调用大语言模型生成可解释的中间推理过程。该推理过程能够识别潜在的根本原因，缩小动作空间，并在策略约束下做出安全的资源分配决策。在代表性开源微服务工作负载上的实验表明，ORACL 将根本原因识别准确率提升了 15%，训练速度加快了最高达 24 倍，且在短期场景下将服务质量提升了 6%，且无需针对具体部署进行重新训练。"
  },
  {
    "date": "2026-02-05",
    "title": "Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities",
    "authors": "Pengyi Li, Elizaveta Goncharova, Andrey Kuznetsov, Ivan Oseledets",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05281v1",
    "source": "arXiv",
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths.",
    "title_zh": "回归基础：通过生成概率重新审视强化学习在大语言模型推理中的探索问题",
    "abstract_zh": "基于可验证奖励的强化学习（RLVR）已成为提升大语言模型（LLMs）推理能力不可或缺的范式。然而，标准的策略优化方法（如组相对策略优化，GRPO）往往收敛至低熵策略，导致严重的模式崩溃，输出多样性受限。本文从采样概率动态的角度分析该问题，发现标准目标函数过度强化最高似然路径，从而抑制了其他有效推理路径的存在。为解决此问题，我们提出一种新颖的优势重加权机制（ARM），旨在均衡所有正确回答的置信度水平。通过将提示困惑度（Prompt Perplexity）与答案置信度纳入优势估计，我们的方法动态调整奖励信号，抑制过度自信推理路径的梯度更新，同时将概率质量重新分配至尚未充分探索的正确解法。实验结果表明，该方法显著提升了生成多样性与响应熵，同时保持了优异的准确率，有效实现了推理任务中探索与利用之间的更优平衡。在Qwen2.5与DeepSeek模型上，针对数学与编程基准的实证研究显示，ProGRPO显著缓解了熵崩溃问题。具体而言，在Qwen2.5-7B模型上，我们的方法在Pass@1指标上比GRPO提升5.7%，尤其在Pass@32指标上更是高出13.9%，充分展现了其在生成多样化正确推理路径方面的卓越能力。"
  },
  {
    "date": "2026-02-05",
    "title": "Hallucination-Resistant Security Planning with a Large Language Model",
    "authors": "Kim Hammar, Tansu Alpcan, Emil Lupu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05279v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) are promising tools for supporting security management tasks, such as incident response planning. However, their unreliability and tendency to hallucinate remain significant challenges. In this paper, we address these challenges by introducing a principled framework for using an LLM as decision support in security management. Our framework integrates the LLM in an iterative loop where it generates candidate actions that are checked for consistency with system constraints and lookahead predictions. When consistency is low, we abstain from the generated actions and instead collect external feedback, e.g., by evaluating actions in a digital twin. This feedback is then used to refine the candidate actions through in-context learning (ICL). We prove that this design allows to control the hallucination risk by tuning the consistency threshold. Moreover, we establish a bound on the regret of ICL under certain assumptions. To evaluate our framework, we apply it to an incident response use case where the goal is to generate a response and recovery plan based on system logs. Experiments on four public datasets show that our framework reduces recovery times by up to 30% compared to frontier LLMs.",
    "title_zh": "基于大语言模型的抗幻觉安全规划",
    "abstract_zh": "大型语言模型（LLMs）是支持安全管理任务（如事件响应规划）的有前景工具。然而，其不可靠性以及容易产生幻觉的问题仍然是重大挑战。本文通过提出一种系统性的框架，将LLM作为安全管理系统中的决策支持工具，以应对这些挑战。我们的框架将LLM置于一个迭代循环中，使其生成候选行动方案，并通过系统约束条件和前瞻预测来验证其一致性。当一致性较低时，我们不采纳生成的行动，而是通过外部反馈（例如在数字孪生环境中评估行动）来获取信息，随后利用上下文学习（ICL）对候选行动进行优化。我们证明，该设计可通过调节一致性阈值来控制幻觉风险。此外，在一定假设条件下，我们建立了ICL的后悔值上界。为评估该框架，我们将其应用于事件响应场景，目标是基于系统日志生成响应与恢复计划。在四个公开数据集上的实验表明，与前沿LLM相比，我们的框架可将恢复时间减少高达30%。"
  },
  {
    "date": "2026-02-05",
    "title": "Copyright Detective: A Forensic System to Evidence LLMs Flickering Copyright Leakage Risks",
    "authors": "Guangwei Zhang, Jianing Zhu, Cheng Qian, Neil Gong, Rada Mihalcea, Zhaozhuo Xu, Jingrui He, Jiaqi Ma, Yun Huang, Chaowei Xiao, Bo Li, Ahmed Abbasi, Dongwon Lee, Heng Ji, Denghui Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05252v1",
    "source": "arXiv",
    "abstract": "We present Copyright Detective, the first interactive forensic system for detecting, analyzing, and visualizing potential copyright risks in LLM outputs. The system treats copyright infringement versus compliance as an evidence discovery process rather than a static classification task due to the complex nature of copyright law. It integrates multiple detection paradigms, including content recall testing, paraphrase-level similarity analysis, persuasive jailbreak probing, and unlearning verification, within a unified and extensible framework. Through interactive prompting, response collection, and iterative workflows, our system enables systematic auditing of verbatim memorization and paraphrase-level leakage, supporting responsible deployment and transparent evaluation of LLM copyright risks even with black-box access.",
    "title_zh": "版权侦探：一种用于检测大语言模型闪烁式版权泄露风险的取证系统",
    "abstract_zh": "我们提出了“版权侦探”（Copyright Detective），这是首个用于检测、分析和可视化大语言模型（LLM）输出中潜在版权风险的交互式取证系统。由于版权法本身的复杂性，该系统将版权侵权与合规性判断视为一个证据发现过程，而非静态的分类任务。系统在统一且可扩展的框架内整合了多种检测范式，包括内容回忆测试、改写级别相似性分析、说服性越狱探测以及遗忘验证。通过交互式提示、响应收集和迭代工作流，本系统能够系统性地审计原文记忆与改写级别信息泄露问题，即使在仅具备黑箱访问权限的情况下，也能支持大语言模型的负责任部署与透明化的版权风险评估。"
  },
  {
    "date": "2026-02-05",
    "title": "RAG without Forgetting: Continual Query-Infused Key Memory",
    "authors": "Yuntong Hu, Sha Li, Naren Ramakrishnan, Liang Zhao",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05152v1",
    "source": "arXiv",
    "abstract": "Retrieval-augmented generation (RAG) systems commonly improve robustness via query-time adaptations such as query expansion and iterative retrieval. While effective, these approaches are inherently stateless: adaptations are recomputed for each query and discarded thereafter, precluding cumulative learning and repeatedly incurring inference-time cost. Index-side approaches like key expansion introduce persistence but rely on offline preprocessing or heuristic updates that are weakly aligned with downstream task utility, leading to semantic drift and noise accumulation. We propose Evolving Retrieval Memory (ERM), a training-free framework that transforms transient query-time gains into persistent retrieval improvements. ERM updates the retrieval index through correctness-gated feedback, selectively attributes atomic expansion signals to the document keys they benefit, and progressively evolves keys via stable, norm-bounded updates. We show that query and key expansion are theoretically equivalent under standard similarity functions and prove convergence of ERM's selective updates, amortizing optimal query expansion into a stable index with zero inference-time overhead. Experiments on BEIR and BRIGHT across 13 domains demonstrate consistent gains in retrieval and generation, particularly on reasoning-intensive tasks, at native retrieval speed.",
    "title_zh": "无需遗忘的RAG：持续查询注入式键记忆",
    "abstract_zh": "检索增强生成（RAG）系统通常通过查询时的适应性方法（如查询扩展和迭代检索）来提升鲁棒性。尽管这些方法有效，但其本质是无状态的：每次查询都需要重新计算适应性操作，并在使用后丢弃，这导致无法实现累积学习，且反复产生推理时的开销。而索引端的方法（如键扩展）虽然引入了持久性，却依赖离线预处理或启发式更新，这些更新与下游任务的实际效用关联较弱，容易引发语义漂移和噪声累积。我们提出一种无需训练的框架——演化检索记忆（Evolving Retrieval Memory, ERM），将瞬时的查询时收益转化为持久的检索性能提升。ERM通过正确性门控的反馈机制更新检索索引，将原子级的扩展信号有选择地归因于其实际受益的文档键，并通过稳定且范数有界的更新逐步演化键向量。我们证明，在标准相似度函数下，查询扩展与键扩展在理论上等价，并进一步证明了ERM选择性更新的收敛性，从而将最优查询扩展的收益以零推理开销的方式固化到稳定的索引中。在BEIR和BRIGHT数据集上覆盖13个领域的实验表明，ERM在检索与生成任务中均实现了持续提升，尤其在需要复杂推理的任务上表现突出，且保持了原生检索速度。"
  },
  {
    "date": "2026-02-05",
    "title": "Capture the Flags: Family-Based Evaluation of Agentic LLMs via Semantics-Preserving Transformations",
    "authors": "Shahin Honarvar, Amber Gorzynski, James Lee-Jones, Harry Coppock, Marek Rei, Joseph Ryan, Alastair F. Donaldson",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05523v1",
    "source": "arXiv",
    "abstract": "Agentic large language models (LLMs) are increasingly evaluated on cybersecurity tasks using capture-the-flag (CTF) benchmarks. However, existing pointwise benchmarks have limited ability to shed light on the robustness and generalisation abilities of agents across alternative versions of the source code. We introduce CTF challenge families, whereby a single CTF is used as the basis for generating a family of semantically-equivalent challenges via semantics-preserving program transformations. This enables controlled evaluation of agent robustness to source code transformations while keeping the underlying exploit strategy fixed. We introduce a new tool, Evolve-CTF, that generates CTF families from Python challenges using a range of transformations. Using Evolve-CTF to derive families from Cybench and Intercode challenges, we evaluate 13 agentic LLM configurations with tool access. We find that models are remarkably robust to intrusive renaming and code insertion-based transformations, but that composed transformations and deeper obfuscation affect performance by requiring more sophisticated use of tools. We also find that enabling explicit reasoning has little effect on solution success rates across challenge families. Our work contributes a valuable technique and tool for future LLM evaluations, and a large dataset characterising the capabilities of current state-of-the-art models in this domain.",
    "title_zh": "捕获旗帜：基于家庭的代理型大语言模型语义保持变换评估",
    "abstract_zh": "代理型大语言模型（LLMs）在网络安全任务中的评估越来越多地采用夺旗赛（CTF）基准。然而，现有的逐点评估基准在揭示代理在源代码不同版本下的鲁棒性和泛化能力方面存在局限。为此，我们提出了“CTF挑战家族”的概念，即以单一CTF挑战为基础，通过语义保持的程序变换生成一系列语义等价的挑战。这种方法能够在保持底层利用策略不变的前提下，对代理在源代码变换下的鲁棒性进行受控评估。我们开发了一款新工具Evolve-CTF，能够基于Python类CTF挑战，通过多种变换自动生成CTF挑战家族。利用Evolve-CTF从Cybench和Intercode挑战中衍生出挑战家族，我们评估了13种具有工具访问权限的代理型LLM配置。研究发现，模型对侵入性重命名和基于代码插入的变换表现出显著的鲁棒性，但复合变换和更深层次的混淆会显著影响性能，要求模型更 sophisticated 地使用工具。此外，我们还发现，启用显式推理对各类挑战家族中的解题成功率影响甚微。本研究为未来LLM评估提供了一种有价值的评估方法与工具，并构建了一个大规模数据集，用以刻画当前最先进模型在该领域的能力。"
  },
  {
    "date": "2026-02-05",
    "title": "SciDef: Automating Definition Extraction from Academic Literature with Large Language Models",
    "authors": "Filip Kučera, Christoph Mandl, Isao Echizen, Radu Timofte, Timo Spinde",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05413v1",
    "source": "arXiv",
    "abstract": "Definitions are the foundation for any scientific work, but with a significant increase in publication numbers, gathering definitions relevant to any keyword has become challenging. We therefore introduce SciDef, an LLM-based pipeline for automated definition extraction. We test SciDef on DefExtra & DefSim, novel datasets of human-extracted definitions and definition-pairs' similarity, respectively. Evaluating 16 language models across prompting strategies, we demonstrate that multi-step and DSPy-optimized prompting improve extraction performance. To evaluate extraction, we test various metrics and show that an NLI-based method yields the most reliable results. We show that LLMs are largely able to extract definitions from scientific literature (86.4% of definitions from our test-set); yet future work should focus not just on finding definitions, but on identifying relevant ones, as models tend to over-generate them. Code & datasets are available at https://github.com/Media-Bias-Group/SciDef.",
    "title_zh": "SciDef：利用大型语言模型从学术文献中自动提取定义",
    "abstract_zh": "定义是任何科学工作的基础，但随着出版物数量的显著增加，针对任意关键词收集相关定义变得愈发困难。为此，我们提出了 SciDef，一个基于大语言模型（LLM）的自动化定义提取流程。我们在两个新数据集——DefExtra（人工提取的定义数据集）和 DefSim（定义对相似性数据集）上测试了 SciDef。通过对16种语言模型在不同提示策略下的评估，我们证明了多步提示和 DSPy 优化提示能够显著提升定义提取的性能。为评估提取效果，我们测试了多种评估指标，结果表明基于自然语言推理（NLI）的方法能提供最可靠的结果。实验显示，大语言模型在从科学文献中提取定义方面表现良好（在我们的测试集中成功提取了86.4%的定义）；然而，未来的研究不应仅局限于发现定义，更应关注识别出相关定义，因为当前模型容易产生大量冗余或不相关的定义。代码与数据集已公开，详见 https://github.com/Media-Bias-Group/SciDef。"
  },
  {
    "date": "2026-02-05",
    "title": "SEAL: Symbolic Execution with Separation Logic (Competition Contribution)",
    "authors": "Tomáš Brablec, Tomáš Dacík, Tomáš Vojnar",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05703v1",
    "source": "arXiv",
    "abstract": "SEAL is a static analyser for the verification of programs that manipulate unbounded linked data structures. It is based on separation logic to represent abstract memory states and, unlike other separation-logic-based approaches, it employs a general-purpose separation logic solver Astral for satisfiability and entailment checking, which itself is based on translation to SMT. This design results in a modular architecture intended to be easier to extend and to combine with reasoning in other theories. Although still a prototype, SEAL achieved competitive results in the LinkedLists base category and was one of only four analysers capable of verifying programs with unbounded lists. We believe that the tool's extensibility, combined with further development, can lead to significant improvements in future competitions.",
    "title_zh": "SEAL：带分离逻辑的符号执行（竞赛贡献）",
    "abstract_zh": "SEAL 是一种用于验证操作无界链式数据结构程序的静态分析工具。它基于分离逻辑来表示抽象的内存状态，与其它基于分离逻辑的方法不同，SEAL 采用通用的分离逻辑求解器 Astral 进行可满足性和蕴含关系的检查，而 Astral 本身通过翻译为 SMT 问题来实现。这种设计使得 SEAL 具有模块化架构，更易于扩展，并可与其他理论的推理机制相结合。尽管目前仍处于原型阶段，SEAL 在 LinkedLists 基准类别中已取得具有竞争力的结果，是少数能够验证涉及无界链表程序的四个分析工具之一。我们相信，该工具的可扩展性，结合进一步的开发，有望在未来竞赛中带来显著的性能提升。"
  },
  {
    "date": "2026-02-05",
    "title": "MedErrBench: A Fine-Grained Multilingual Benchmark for Medical Error Detection and Correction with Clinical Expert Annotations",
    "authors": "Congbo Ma, Yichun Zhang, Yousef Al-Jazzazi, Ahamed Foisal, Laasya Sharma, Yousra Sadqi, Khaled Saleh, Jihad Mallat, Farah E. Shamout",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05692v1",
    "source": "arXiv",
    "abstract": "Inaccuracies in existing or generated clinical text may lead to serious adverse consequences, especially if it is a misdiagnosis or incorrect treatment suggestion. With Large Language Models (LLMs) increasingly being used across diverse healthcare applications, comprehensive evaluation through dedicated benchmarks is crucial. However, such datasets remain scarce, especially across diverse languages and contexts. In this paper, we introduce MedErrBench, the first multilingual benchmark for error detection, localization, and correction, developed under the guidance of experienced clinicians. Based on an expanded taxonomy of ten common error types, MedErrBench covers English, Arabic and Chinese, with natural clinical cases annotated and reviewed by domain experts. We assessed the performance of a range of general-purpose, language-specific, and medical-domain language models across all three tasks. Our results reveal notable performance gaps, particularly in non-English settings, highlighting the need for clinically grounded, language-aware systems. By making MedErrBench and our evaluation protocols publicly-available, we aim to advance multilingual clinical NLP to promote safer and more equitable AI-based healthcare globally. The dataset is available in the supplementary material. An anonymized version of the dataset is available at: https://github.com/congboma/MedErrBench.",
    "title_zh": "MedErrBench：一个具有临床专家标注的细粒度多语言医学错误检测与修正基准",
    "abstract_zh": "现有或生成的临床文本中的不准确之处可能导致严重不良后果，尤其是在误诊或错误治疗建议的情况下。随着大型语言模型（LLMs）在各类医疗应用中日益普及，通过专用基准进行综合评估变得至关重要。然而，这类数据集仍然稀缺，尤其是在多种语言和不同临床情境下的覆盖仍显不足。本文介绍了MedErrBench，这是首个由经验丰富的临床医生指导开发的多语言基准，用于错误检测、定位与修正。基于扩展后的十类常见错误类型，MedErrBench涵盖英语、阿拉伯语和中文，所有临床案例均由领域专家进行标注与审核。我们评估了多种通用型、语言特定型及医疗领域专用语言模型在三项任务上的表现。结果表明，模型在非英语语境下存在显著性能差距，凸显了开发具备临床依据且语言敏感的系统的重要性。通过公开发布MedErrBench及我们的评估协议，我们旨在推动多语言临床自然语言处理的发展，促进全球范围内更安全、更公平的基于人工智能的医疗健康服务。数据集详见补充材料，匿名版本可访问：https://github.com/congboma/MedErrBench。"
  },
  {
    "date": "2026-02-05",
    "title": "Beyond Cosine Similarity",
    "authors": "Xinbo Ai",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05266v1",
    "source": "arXiv",
    "abstract": "Cosine similarity, the standard metric for measuring semantic similarity in vector spaces, is mathematically grounded in the Cauchy-Schwarz inequality, which inherently limits it to capturing linear relationships--a constraint that fails to model the complex, nonlinear structures of real-world semantic spaces. We advance this theoretical underpinning by deriving a tighter upper bound for the dot product than the classical Cauchy-Schwarz bound. This new bound leads directly to recos, a similarity metric that normalizes the dot product by the sorted vector components. recos relaxes the condition for perfect similarity from strict linear dependence to ordinal concordance, thereby capturing a broader class of relationships. Extensive experiments across 11 embedding models--spanning static, contextualized, and universal types--demonstrate that recos consistently outperforms traditional cosine similarity, achieving higher correlation with human judgments on standard Semantic Textual Similarity (STS) benchmarks. Our work establishes recos as a mathematically principled and empirically superior alternative, offering enhanced accuracy for semantic analysis in complex embedding spaces.",
    "title_zh": "余弦相似度之外",
    "abstract_zh": "余弦相似度是向量空间中衡量语义相似性的标准度量方法，其数学基础源于柯西-施瓦茨不等式，这使得它本质上只能捕捉线性关系——这一限制无法有效建模现实世界语义空间中复杂的非线性结构。本文通过推导出比经典柯西-施瓦茨界更紧的点积上界，推进了该理论基础。这一新界直接导出了recos这一相似度度量方法，该方法通过排序后的向量分量对点积进行归一化。recos将完全相似的条件从严格的线性相关性放宽为序数一致性，从而能够捕捉更广泛的关系类型。在11种不同类型的嵌入模型（涵盖静态、上下文相关和通用型）上的大量实验表明，recos在标准语义文本相似性（STS）基准测试中始终优于传统的余弦相似度，与人类判断的相关性更高。我们的研究确立了recos作为一种数学上严谨且实证表现更优的替代方案，为复杂嵌入空间中的语义分析提供了更高的准确性。"
  },
  {
    "date": "2026-02-05",
    "title": "Structured Context Engineering for File-Native Agentic Systems: Evaluating Schema Accuracy, Format Effectiveness, and Multi-File Navigation at Scale",
    "authors": "Damon McMillan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05447v1",
    "source": "arXiv",
    "abstract": "Large Language Model agents increasingly operate external systems through programmatic interfaces, yet practitioners lack empirical guidance on how to structure the context these agents consume. Using SQL generation as a proxy for programmatic agent operations, we present a systematic study of context engineering for structured data, comprising 9,649 experiments across 11 models, 4 formats (YAML, Markdown, JSON, Token-Oriented Object Notation [TOON]), and schemas ranging from 10 to 10,000 tables. Our findings challenge common assumptions. First, architecture choice is model-dependent: file-based context retrieval improves accuracy for frontier-tier models (Claude, GPT, Gemini; +2.7%, p=0.029) but shows mixed results for open source models (aggregate -7.7%, p<0.001), with deficits varying substantially by model. Second, format does not significantly affect aggregate accuracy (chi-squared=2.45, p=0.484), though individual models, particularly open source, exhibit format-specific sensitivities. Third, model capability is the dominant factor, with a 21 percentage point accuracy gap between frontier and open source tiers that dwarfs any format or architecture effect. Fourth, file-native agents scale to 10,000 tables through domain-partitioned schemas while maintaining high navigation accuracy. Fifth, file size does not predict runtime efficiency: compact formats can consume significantly more tokens at scale due to format-unfamiliar search patterns. These findings provide practitioners with evidence-based guidance for deploying LLM agents on structured systems, demonstrating that architectural decisions should be tailored to model capability rather than assuming universal best practices.",
    "title_zh": "面向文件原生智能体系统的结构化上下文工程：大规模评估模式准确性、格式有效性与多文件导航能力",
    "abstract_zh": "大型语言模型代理越来越多地通过程序化接口操作外部系统，但从业者在如何组织这些代理所消耗的上下文方面缺乏实证指导。本文以SQL生成作为程序化代理操作的代理，对结构化数据的上下文工程进行了系统性研究，涵盖11种模型、4种格式（YAML、Markdown、JSON、面向标记的对象表示法[TOON]）以及从10到10,000张表的多种数据模式，共进行了9,649次实验。研究结果挑战了若干常见假设：第一，架构选择具有模型依赖性：对于前沿模型（Claude、GPT、Gemini），基于文件的上下文检索可提升准确率（+2.7%，p=0.029），但对开源模型则效果参差不齐（总体下降7.7%，p<0.001），且不同模型的性能差距差异显著；第二，格式对整体准确率无显著影响（卡方值=2.45，p=0.484），尽管个别模型，尤其是开源模型，表现出对特定格式的敏感性；第三，模型能力是决定性因素，前沿模型与开源模型之间存在21个百分点的准确率差距，远超任何格式或架构带来的影响；第四，原生文件处理的代理可通过领域分区模式实现对10,000张表的可扩展性，同时保持高导航准确率；第五，文件大小并不能预测运行效率：紧凑格式在大规模场景下可能因格式不熟悉导致的搜索模式而消耗更多token。这些发现为在结构化系统上部署LLM代理的实践者提供了基于证据的指导，表明架构决策应根据模型能力量身定制，而非假设存在普适的最佳实践。"
  },
  {
    "date": "2026-02-05",
    "title": "DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching",
    "authors": "Yuxing Lu, Yucheng Hu, Xukai Zhao, Jiuxin Cao",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.06039v1",
    "source": "arXiv",
    "abstract": "Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager's round goal, each agent outputs lightweight natural-language query (need) and \\key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. +6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.",
    "title_zh": "DyTopo：通过语义匹配实现多智能体推理的动态拓扑路由",
    "abstract_zh": "由提示大型语言模型构建的多智能体系统能够提升多轮推理能力，然而现有大多数流程依赖于固定、全局的通信模式，难以适应迭代式问题求解过程中各阶段的不同需求。我们提出了DyTopo，一种由管理者引导的多智能体框架，该框架在每一轮中重构一个稀疏有向通信图。在管理者本轮目标的指导下，每个智能体输出轻量级的自然语言查询（需求）和关键（供给）描述符；DyTopo对这些描述符进行嵌入并执行语义匹配，仅在生成的边路径上传输私有消息。在代码生成和数学推理基准测试中，使用四种不同的大型语言模型作为后端，DyTopo始终显著优于最强基线模型（平均提升6.2%）。除了提升准确率外，DyTopo还通过动态演化的通信图生成可解释的协作轨迹，使研究者能够定性分析通信路径在各轮次间的重构过程。"
  },
  {
    "date": "2026-02-05",
    "title": "Multi-Token Prediction via Self-Distillation",
    "authors": "John Kirchenbauer, Abhimanyu Hans, Brian Bartoldson, Micah Goldblum, Ashwinee Panda, Tom Goldstein",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.06019v1",
    "source": "arXiv",
    "abstract": "Existing techniques for accelerating language model inference, such as speculative decoding, require training auxiliary speculator models and building and deploying complex inference pipelines. We consider a new approach for converting a pretrained autoregressive language model from a slow single next token prediction model into a fast standalone multi-token prediction model using a simple online distillation objective. The final model retains the exact same implementation as the pretrained initial checkpoint and is deployable without the addition of any auxiliary verifier or other specialized inference code. On GSM8K, our method produces models that can decode more than $3\\times$ faster on average at $<5\\%$ drop in accuracy relative to single token decoding performance.",
    "title_zh": "通过自蒸馏实现多标记预测",
    "abstract_zh": "现有的加速语言模型推理技术，如推测解码（speculative decoding），需要训练辅助的推测模型，并构建和部署复杂的推理流水线。我们提出一种新方法，通过简单的在线蒸馏目标，将预训练的自回归语言模型从缓慢的单标记预测模型转换为快速的独立多标记预测模型。最终模型与预训练初始检查点保持完全相同的实现方式，无需添加任何辅助验证器或其他专用推理代码即可部署。在GSM8K数据集上，我们的方法所生成的模型平均解码速度提升超过3倍，同时准确率相对于单标记解码性能的下降不足5%。"
  },
  {
    "date": "2026-02-05",
    "title": "Layer-wise LoRA fine-tuning: a similarity metric approach",
    "authors": "Keith Ando Ogawa, Bruno Lopes Yamamoto, Lucas Lauton de Alcantara, Lucas Pellicer, Rosimeire Pereira Costa, Edson Bollis, Anna Helena Reali Costa, Artur Jordao",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05988v1",
    "source": "arXiv",
    "abstract": "Pre-training Large Language Models (LLMs) on web-scale datasets becomes fundamental for advancing general-purpose AI. In contrast, enhancing their predictive performance on downstream tasks typically involves adapting their knowledge through fine-tuning. Parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA), aim to reduce the computational cost of this process by freezing the pre-trained model and updating a smaller number of parameters. In comparison to full fine-tuning, these methods achieve over 99\\% reduction in trainable parameter count, depending on the configuration. Unfortunately, such a reduction may prove insufficient as LLMs continue to grow in scale. In this work, we address the previous problem by systematically selecting only a few layers to fine-tune using LoRA or its variants. We argue that not all layers contribute equally to the model adaptation. Leveraging this, we identify the most relevant layers to fine-tune by measuring their contribution to changes in internal representations. Our method is orthogonal to and readily compatible with existing low-rank adaptation techniques. We reduce the trainable parameters in LoRA-based techniques by up to 50\\%, while maintaining the predictive performance across different models and tasks. Specifically, on encoder-only architectures, this reduction in trainable parameters leads to a negligible predictive performance drop on the GLUE benchmark. On decoder-only architectures, we achieve a small drop or even improvements in the predictive performance on mathematical problem-solving capabilities and coding tasks. Finally, this effectiveness extends to multimodal models, for which we also observe competitive results relative to fine-tuning with LoRA modules in all layers. Code is available at: https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA",
    "title_zh": "逐层LoRA微调：一种相似性度量方法",
    "abstract_zh": "在大规模网络数据集上预训练大型语言模型（LLMs）已成为推动通用人工智能发展的基础。相比之下，提升模型在下游任务上的预测性能通常需要通过微调来调整其知识。参数高效的微调技术，如低秩适应（LoRA），旨在通过冻结预训练模型并仅更新少量参数，从而降低该过程的计算成本。与全量微调相比，这些方法可将可训练参数数量减少超过99%（具体取决于配置）。然而，随着LLM规模的持续扩大，这种参数减少程度可能仍显不足。本文通过系统性地选择仅少数层进行LoRA或其变体的微调，解决了上述问题。我们认为，并非所有模型层对模型适应都具有同等贡献。基于这一观点，我们通过测量各层对内部表示变化的贡献程度，识别出最相关的微调层。我们的方法与现有的低秩适应技术正交且易于兼容。在保持不同模型和任务上预测性能不变的前提下，我们将基于LoRA的技术中可训练参数减少了高达50%。具体而言，在仅编码器架构上，可训练参数的减少在GLUE基准测试上导致的预测性能下降可忽略不计；在仅解码器架构上，我们在数学问题求解能力和编程任务上的预测性能出现微小下降，甚至有所提升。最后，该方法的有效性也扩展到了多模态模型，在所有任务上均取得了与全层LoRA微调相当甚至更优的性能。代码已公开：https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA"
  },
  {
    "date": "2026-02-05",
    "title": "Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations",
    "authors": "Wei Liu, Jiawei Xu, Yingru Li, Longtao Zheng, Tianjian Li, Qian Liu, Junxian He",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05885v1",
    "source": "arXiv",
    "abstract": "High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr.Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr.Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in https://www.github.com/hkust-nlp/KernelGYM.",
    "title_zh": "Dr. Kernel：为 Triton 内核生成而正确实现的强化学习",
    "abstract_zh": "高质量的内核代码对于可扩展的人工智能系统至关重要，而让大语言模型（LLM）生成此类代码将显著推动AI的发展。然而，训练LLM完成这一任务需要充足的训练数据、稳健的训练环境，且整个过程容易受到奖励欺骗（reward hacking）和“懒惰优化”（lazy optimization）的影响。在这些情况下，模型可能通过操纵训练奖励，优先追求表面的正确性而非实质性的性能提升。\n\n本文系统地研究了用于内核生成的强化学习（RL）方法。我们首先设计了KernelGYM——一个稳健的分布式GPU训练环境，支持奖励欺骗检测、多轮交互下的数据收集以及长期强化学习训练。基于KernelGYM，我们探索了有效的多轮强化学习方法，并发现GRPO（Generalized Reward Policy Optimization）中因自我包含（self-inclusion）导致的策略梯度偏差问题。为解决该问题，我们提出了一种新的方法——**轮次级Reinforce-Leave-One-Out（Turn-level Reinforce-Leave-One-Out, TRLOO）**，以实现多轮RL中无偏的优势估计。\n\n为进一步缓解懒惰优化问题，我们引入了不匹配校正机制以提升训练稳定性，并提出**基于性能分析的奖励机制（Profiling-based Rewards, PR）** 和 **基于性能分析的拒绝采样方法（Profiling-based Rejection Sampling, PRS）**，有效克服了传统奖励信号的局限性。\n\n最终训练得到的模型Dr.Kernel-14B在Kernelbench测试中表现达到与Claude-4.5-Sonnet相当的水平。此外，我们还研究了Dr.Kernel-14B在测试阶段的序列式扩展（sequential test-time scaling）。在Kernelbench Level-2子集上，31.6%的生成内核实现了至少1.2倍于Torch参考实现的加速，优于Claude-4.5-Sonnet（26.7%）和GPT-5（28.6%）。当从所有轮次中选择最优候选结果时，该1.2倍加速率进一步提升至47.8%。\n\n本文所有资源，包括训练环境、训练代码、模型权重及数据集，均已开源，详见：https://www.github.com/hkust-nlp/KernelGYM。"
  },
  {
    "date": "2026-02-05",
    "title": "Agent2Agent Threats in Safety-Critical LLM Assistants: A Human-Centric Taxonomy",
    "authors": "Lukas Stappen, Ahmet Erkan Turan, Johann Hagerer, Georg Groh",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05877v1",
    "source": "arXiv",
    "abstract": "The integration of Large Language Model (LLM)-based conversational agents into vehicles creates novel security challenges at the intersection of agentic AI, automotive safety, and inter-agent communication. As these intelligent assistants coordinate with external services via protocols such as Google's Agent-to-Agent (A2A), they establish attack surfaces where manipulations can propagate through natural language payloads, potentially causing severe consequences ranging from driver distraction to unauthorized vehicle control. Existing AI security frameworks, while foundational, lack the rigorous \"separation of concerns\" standard in safety-critical systems engineering by co-mingling the concepts of what is being protected (assets) with how it is attacked (attack paths). This paper addresses this methodological gap by proposing a threat modeling framework called AgentHeLLM (Agent Hazard Exploration for LLM Assistants) that formally separates asset identification from attack path analysis. We introduce a human-centric asset taxonomy derived from harm-oriented \"victim modeling\" and inspired by the Universal Declaration of Human Rights, and a formal graph-based model that distinguishes poison paths (malicious data propagation) from trigger paths (activation actions). We demonstrate the framework's practical applicability through an open-source attack path suggestion tool AgentHeLLM Attack Path Generator that automates multi-stage threat discovery using a bi-level search strategy.",
    "title_zh": "Agent2Agent威胁在安全关键型大语言模型助手中的研究：一种以人为中心的分类法",
    "abstract_zh": "将基于大语言模型（LLM）的对话代理集成到车辆中，带来了智能代理AI、汽车安全与多代理通信交叉领域的新型安全挑战。当这些智能助手通过Google的代理间协议（Agent-to-Agent, A2A）等与外部服务协同工作时，便形成了攻击面，恶意操纵可通过自然语言载荷传播，可能引发从驾驶员分心到未经授权控制车辆等一系列严重后果。现有的AI安全框架虽然具有基础性意义，但在安全关键系统工程中缺乏严格的“职责分离”标准，其将被保护对象（资产）与攻击路径（攻击方式）的概念混为一谈。本文通过提出一种名为AgentHeLLM（LLM助手的代理危害探索）的威胁建模框架，填补了这一方法论空白。该框架在形式上将资产识别与攻击路径分析相分离。我们引入了一种以人为中心的资产分类体系，该体系基于以“受害者建模”为导向的伤害分析，并受到《世界人权宣言》的启发；同时提出一种形式化的图模型，明确区分“污染路径”（恶意数据传播路径）与“触发路径”（激活动作路径）。通过一个开源的攻击路径建议工具——AgentHeLLM攻击路径生成器，我们展示了该框架的实际应用价值，该工具采用双层搜索策略，可自动化实现多阶段威胁发现。"
  },
  {
    "date": "2026-02-05",
    "title": "Stop Rewarding Hallucinated Steps: Faithfulness-Aware Step-Level Reinforcement Learning for Small Reasoning Models",
    "authors": "Shuo Nie, Hexuan Deng, Chao Wang, Ruiyu Fang, Xuebo Liu, Shuangyong Song, Yu Li, Min Zhang, Xuelong Li",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05897v1",
    "source": "arXiv",
    "abstract": "As large language models become smaller and more efficient, small reasoning models (SRMs) are crucial for enabling chain-of-thought (CoT) reasoning in resource-constrained settings. However, they are prone to faithfulness hallucinations, especially in intermediate reasoning steps. Existing mitigation methods based on online reinforcement learning rely on outcome-based rewards or coarse-grained CoT evaluation, which can inadvertently reinforce unfaithful reasoning when the final answer is correct. To address these limitations, we propose Faithfulness-Aware Step-Level Reinforcement Learning (FaithRL), introducing step-level supervision via explicit faithfulness rewards from a process reward model, together with an implicit truncated resampling strategy that generates contrastive signals from faithful prefixes. Experiments across multiple SRMs and Open-Book QA benchmarks demonstrate that FaithRL consistently reduces hallucinations in both the CoT and final answers, leading to more faithful and reliable reasoning. Code is available at https://github.com/Easy195/FaithRL.",
    "title_zh": "停止奖励幻觉步骤：面向小型推理模型的忠实性感知步骤级强化学习",
    "abstract_zh": "随着大型语言模型不断缩小并变得更加高效，小型推理模型（SRMs）在资源受限的环境中实现思维链（CoT）推理变得至关重要。然而，这些模型在中间推理步骤中容易产生忠实性幻觉。现有的基于在线强化学习的缓解方法依赖于基于结果的奖励或粗粒度的CoT评估，当最终答案正确时，可能会无意中强化不忠实的推理过程。为解决这些局限性，我们提出了**忠实性感知的步骤级强化学习**（FaithRL），通过来自过程奖励模型的显式忠实性奖励实现步骤级监督，并结合一种隐式的截断重采样策略，从忠实的前缀中生成对比信号。在多个SRMs和开放书问答（Open-Book QA）基准上的实验表明，FaithRL能够持续减少CoT及最终答案中的幻觉，从而实现更忠实、更可靠的推理。代码已公开，地址为：https://github.com/Easy195/FaithRL。"
  },
  {
    "date": "2026-02-05",
    "title": "Multi-Field Tool Retrieval",
    "authors": "Yichen Tang, Weihang Su, Yiqun Liu, Qingyao Ai",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05366v1",
    "source": "arXiv",
    "abstract": "Integrating external tools enables Large Language Models (LLMs) to interact with real-world environments and solve complex tasks. Given the growing scale of available tools, effective tool retrieval is essential to mitigate constraints of LLMs' context windows and ensure computational efficiency. Existing approaches typically treat tool retrieval as a traditional ad-hoc retrieval task, matching user queries against the entire raw tool documentation. In this paper, we identify three fundamental challenges that limit the effectiveness of this paradigm: (i) the incompleteness and structural inconsistency of tool documentation; (ii) the significant semantic and granular mismatch between user queries and technical tool documents; and, most importantly, (iii) the multi-aspect nature of tool utility, that involves distinct dimensions, such as functionality, input constraints, and output formats, varying in format and importance. To address these challenges, we introduce Multi-Field Tool Retrieval, a framework designed to align user intent with tool representations through fine-grained, multi-field modeling. Experimental results show that our framework achieves SOTA performance on five datasets and a mixed benchmark, exhibiting superior generalizability and robustness.",
    "title_zh": "多字段工具检索",
    "abstract_zh": "集成外部工具使大型语言模型（LLMs）能够与现实世界环境交互并解决复杂任务。随着可用工具规模的不断增长，有效的工具检索对于缓解LLM上下文窗口的限制并确保计算效率至关重要。现有方法通常将工具检索视为传统的即兴检索任务，将用户查询与完整的原始工具文档进行匹配。本文指出，这一范式存在三个根本性挑战：（i）工具文档的不完整性与结构不一致性；（ii）用户查询与技术性工具文档之间存在显著的语义和粒度差异；以及最重要的是，（iii）工具功能具有多维度特性，涉及功能、输入约束和输出格式等不同方面，这些方面在格式和重要性上各不相同。为应对这些挑战，我们提出了多字段工具检索（Multi-Field Tool Retrieval）框架，通过细粒度的多字段建模，实现用户意图与工具表示之间的精准对齐。实验结果表明，我们的框架在五个数据集和一个混合基准上均达到了当前最优（SOTA）性能，展现出卓越的泛化能力和鲁棒性。"
  },
  {
    "date": "2026-02-05",
    "title": "AgentXRay: White-Boxing Agentic Systems via Workflow Reconstruction",
    "authors": "Ruijie Shi, Houbin Zhang, Yuecheng Han, Yuheng Wang, Jingru Fan, Runde Yang, Yufan Dang, Huatao Li, Dewen Liu, Yuan Cheng, Chen Qian",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05353v1",
    "source": "arXiv",
    "abstract": "Large Language Models have shown strong capabilities in complex problem solving, yet many agentic systems remain difficult to interpret and control due to opaque internal workflows. While some frameworks offer explicit architectures for collaboration, many deployed agentic systems operate as black boxes to users. We address this by introducing Agentic Workflow Reconstruction (AWR), a new task aiming to synthesize an explicit, interpretable stand-in workflow that approximates a black-box system using only input--output access. We propose AgentXRay, a search-based framework that formulates AWR as a combinatorial optimization problem over discrete agent roles and tool invocations in a chain-structured workflow space. Unlike model distillation, AgentXRay produces editable white-box workflows that match target outputs under an observable, output-based proxy metric, without accessing model parameters. To navigate the vast search space, AgentXRay employs Monte Carlo Tree Search enhanced by a scoring-based Red-Black Pruning mechanism, which dynamically integrates proxy quality with search depth. Experiments across diverse domains demonstrate that AgentXRay achieves higher proxy similarity and reduces token consumption compared to unpruned search, enabling deeper workflow exploration under fixed iteration budgets.",
    "title_zh": "AgentXRay：通过工作流重构实现智能体系统的白盒分析",
    "abstract_zh": "大型语言模型在复杂问题求解方面展现了强大的能力，然而许多智能体系统由于其内部工作流程不透明，仍难以解释和控制。尽管一些框架提供了明确的协作架构，但大多数已部署的智能体系统对用户而言仍如同黑箱。为此，我们提出了智能体工作流重构（Agentic Workflow Reconstruction, AWR）这一新任务，旨在仅通过输入-输出访问，合成一个显式且可解释的替代工作流，以近似黑箱系统的行为。我们提出了AgentXRay——一种基于搜索的框架，将AWR建模为链式结构工作流空间中离散智能体角色与工具调用的组合优化问题。与模型蒸馏不同，AgentXRay生成的是可编辑的白盒工作流，能够在可观测的、基于输出的代理指标下匹配目标输出，且无需访问模型参数。为应对庞大的搜索空间，AgentXRay采用蒙特卡洛树搜索（MCTS），并引入基于评分的红黑剪枝机制，动态融合代理质量与搜索深度。在多个不同领域的实验表明，AgentXRay在代理相似性方面表现更优，同时相比未剪枝的搜索显著降低了令牌消耗，在固定迭代预算下实现了更深层次的工作流探索。"
  },
  {
    "date": "2026-02-05",
    "title": "SynAT: Enhancing Security Knowledge Bases via Automatic Synthesizing Attack Tree from Crowd Discussions",
    "authors": "Ziyou Jiang, Lin Shi, Guowei Yang, Xuyan Ma, Fenglong Li, Qing Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05329v1",
    "source": "arXiv",
    "abstract": "Cyber attacks have become a serious threat to the security of software systems. Many organizations have built their security knowledge bases to safeguard against attacks and vulnerabilities. However, due to the time lag in the official release of security information, these security knowledge bases may not be well maintained, and using them to protect software systems against emergent security risks can be challenging. On the other hand, the security posts on online knowledge-sharing platforms contain many crowd security discussions and the knowledge in those posts can be used to enhance the security knowledge bases. This paper proposes SynAT, an automatic approach to synthesize attack trees from crowd security posts. Given a security post, SynAT first utilize the Large Language Model (LLM) and prompt learning to restrict the scope of sentences that may contain attack information; then it utilizes a transition-based event and relation extraction model to extract the events and relations simultaneously from the scope; finally, it applies heuristic rules to synthesize the attack trees with the extracted events and relations. An experimental evaluation is conducted on 5,070 Stack Overflow security posts, and the results show that SynAT outperforms all baselines in both event and relation extraction, and achieves the highest tree similarity in attack tree synthesis. Furthermore, SynAT has been applied to enhance HUAWEI's security knowledge base as well as public security knowledge bases CVE and CAPEC, which demonstrates SynAT's practicality.",
    "title_zh": "SynAT：通过从众包讨论中自动合成攻击树来增强安全知识库",
    "abstract_zh": "网络攻击已成为软件系统安全的重大威胁。许多组织建立了安全知识库，以防范攻击和漏洞。然而，由于官方安全信息发布存在时间延迟，这些安全知识库往往难以及时维护，因此利用它们来应对新兴安全风险颇具挑战。另一方面，在线知识共享平台上的安全帖子中包含大量来自公众的安全讨论，其中蕴含的知识可用于丰富和完善安全知识库。本文提出SynAT，一种从众包安全帖子中自动生成攻击树的自动化方法。给定一篇安全帖子，SynAT首先利用大语言模型（LLM）和提示学习技术，限定可能包含攻击信息的句子范围；随后，采用基于转移的事件与关系抽取模型，从该范围内同时提取事件和关系；最后，通过启发式规则将提取出的事件与关系整合，生成攻击树。在5,070篇Stack Overflow安全帖子上的实验评估表明，SynAT在事件和关系抽取任务中均优于所有基线方法，并在攻击树生成任务中实现了最高的树相似度。此外，SynAT已成功应用于提升华为公司的安全知识库，以及公开的安全知识库CVE和CAPEC，充分展示了其实际应用价值。"
  },
  {
    "date": "2026-02-05",
    "title": "ProAct: Agentic Lookahead in Interactive Environments",
    "authors": "Yangbin Yu, Mingyu Yang, Junyou Li, Yiming Gao, Feiyu Liu, Yijun Yang, Zichuan Lin, Jiafei Lyu, Yicheng Liu, Zhicong Lu, Deheng Ye, Jie Jiang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05327v1",
    "source": "arXiv",
    "abstract": "Existing Large Language Model (LLM) agents struggle in interactive environments requiring long-horizon planning, primarily due to compounding errors when simulating future states. To address this, we propose ProAct, a framework that enables agents to internalize accurate lookahead reasoning through a two-stage training paradigm. First, we introduce Grounded LookAhead Distillation (GLAD), where the agent undergoes supervised fine-tuning on trajectories derived from environment-based search. By compressing complex search trees into concise, causal reasoning chains, the agent learns the logic of foresight without the computational overhead of inference-time search. Second, to further refine decision accuracy, we propose the Monte-Carlo Critic (MC-Critic), a plug-and-play auxiliary value estimator designed to enhance policy-gradient algorithms like PPO and GRPO. By leveraging lightweight environment rollouts to calibrate value estimates, MC-Critic provides a low-variance signal that facilitates stable policy optimization without relying on expensive model-based value approximation. Experiments on both stochastic (e.g., 2048) and deterministic (e.g., Sokoban) environments demonstrate that ProAct significantly improves planning accuracy. Notably, a 4B parameter model trained with ProAct outperforms all open-source baselines and rivals state-of-the-art closed-source models, while demonstrating robust generalization to unseen environments. The codes and models are available at https://github.com/GreatX3/ProAct",
    "title_zh": "ProAct：交互环境中智能前瞻机制",
    "abstract_zh": "现有的大型语言模型（LLM）代理在需要长期规划的交互式环境中表现不佳，主要原因是模拟未来状态时误差不断累积。为解决这一问题，我们提出 ProAct 框架，通过两阶段训练范式使代理能够内化准确的前瞻推理能力。首先，我们引入基于环境搜索轨迹的“有根基的前瞻蒸馏”（Grounded LookAhead Distillation, GLAD），在监督微调过程中，代理学习由环境驱动的搜索轨迹。通过将复杂的搜索树压缩为简洁且具有因果关系的推理链，代理在无需推理时进行昂贵搜索计算的前提下，掌握了前瞻思维的逻辑。其次，为进一步提升决策准确性，我们提出一种即插即用的辅助价值估计器——蒙特卡洛评论家（Monte-Carlo Critic, MC-Critic），可增强 PPO 和 GRPO 等策略梯度算法。MC-Critic 利用轻量级环境滚动回放校准价值估计，提供低方差信号，从而实现稳定策略优化，而无需依赖计算成本高昂的基于模型的价值近似。在随机环境（如 2048）和确定性环境（如 Sokoban）上的实验表明，ProAct 显著提升了规划准确性。值得注意的是，一个仅 40 亿参数的模型在 ProAct 训练下，性能超越所有开源基线，并媲美当前最先进的闭源模型，同时展现出对未见过环境的强大泛化能力。代码与模型已开源，地址为：https://github.com/GreatX3/ProAct"
  },
  {
    "date": "2026-02-05",
    "title": "Does Programming Language Matter? An Empirical Study of Fuzzing Bug Detection",
    "authors": "Tatsuya Shirai, Olivier Nourry, Yutaro Kashiwa, Kenji Fujiwara, Hajimu Iida",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05312v1",
    "source": "arXiv",
    "abstract": "Fuzzing has become a popular technique for automatically detecting vulnerabilities and bugs by generating unexpected inputs. In recent years, the fuzzing process has been integrated into continuous integration workflows (i.e., continuous fuzzing), enabling short and frequent testing cycles. Despite its widespread adoption, prior research has not examined whether the effectiveness of continuous fuzzing varies across programming languages. This study conducts a large-scale cross-language analysis to examine how fuzzing bug characteristics and detection efficiency differ among languages. We analyze 61,444 fuzzing bugs and 999,248 builds from 559 OSS-Fuzz projects categorized by primary language. Our findings reveal that (i) C++ and Rust exhibit higher fuzzing bug detection frequencies, (ii) Rust and Python show low vulnerability ratios but tend to expose more critical vulnerabilities, (iii) crash types vary across languages and unreproducible bugs are more frequent in Go but rare in Rust, and (iv) Python attains higher patch coverage but suffers from longer time-to-detection. These results demonstrate that fuzzing behavior and effectiveness are strongly shaped by language design, providing insights for language-aware fuzzing strategies and tool development.",
    "title_zh": "编程语言重要吗？一项关于模糊测试漏洞检测的实证研究",
    "abstract_zh": "模糊测试（Fuzzing）已成为一种通过生成意外输入来自动检测漏洞和缺陷的流行技术。近年来，模糊测试过程已融入持续集成工作流（即持续模糊测试），实现了短周期、高频次的测试。尽管该技术被广泛采用，但以往研究尚未探讨持续模糊测试在不同编程语言间的有效性是否存在差异。本研究开展了一项大规模的跨语言分析，以考察不同编程语言在模糊测试发现的缺陷特征和检测效率方面的差异。我们分析了来自559个OSS-Fuzz项目的61,444个模糊测试缺陷以及999,248次构建记录，并按主要编程语言进行分类。研究结果表明：（i）C++和Rust的模糊测试缺陷发现频率更高；（ii）Rust和Python的漏洞比例较低，但更倾向于暴露严重漏洞；（iii）不同语言的崩溃类型存在差异，Go语言中不可复现的缺陷更为常见，而Rust中则极为罕见；（iv）Python在补丁覆盖率方面表现更优，但缺陷检测时间较长。这些发现表明，模糊测试的行为与效果深受语言设计的影响，为开发面向特定语言的模糊测试策略和工具提供了重要启示。"
  },
  {
    "date": "2026-02-05",
    "title": "EGSS: Entropy-guided Stepwise Scaling for Reliable Software Engineering",
    "authors": "Chenhui Mao, Yuanting Lei, Zhixiang Wei, Ming Liang, Zhixiang Wang, Jingxuan Xu, Dajun Chen, Wei Jiang, Yong Li",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05242v1",
    "source": "arXiv",
    "abstract": "Agentic Test-Time Scaling (TTS) has delivered state-of-the-art (SOTA) performance on complex software engineering tasks such as code generation and bug fixing. However, its practical adoption remains limited due to significant computational overhead, primarily driven by two key challenges: (1) the high cost associated with deploying excessively large ensembles, and (2) the lack of a reliable mechanism for selecting the optimal candidate solution, ultimately constraining the performance gains that can be realized. To address these challenges, we propose Entropy-Guided Stepwise Scaling (EGSS), a novel TTS framework that dynamically balances efficiency and effectiveness through entropy-guided adaptive search and robust test-suite augmentation. Extensive experiments on SWE-Bench-Verified demonstrate that EGSS consistently boosts performance by 5-10% across all evaluated models. Specifically, it increases the resolved ratio of Kimi-K2-Intruct from 63.2% to 72.2%, and GLM-4.6 from 65.8% to 74.6%. Furthermore, when paired with GLM-4.6, EGSS achieves a new state-of-the-art among open-source large language models. In addition to these accuracy improvements, EGSS reduces inference-time token usage by over 28% compared to existing TTS methods, achieving simultaneous gains in both effectiveness and computational efficiency.",
    "title_zh": "EGSS：基于熵的逐步缩放方法，用于可靠的软件工程",
    "abstract_zh": "代理型测试时扩展（Agentic Test-Time Scaling, TTS）在代码生成、缺陷修复等复杂软件工程任务上已实现当前最优（SOTA）性能。然而，由于存在显著的计算开销，其实际应用仍受到限制，主要源于两个关键挑战：（1）部署过大集成模型所带来的高昂成本；（2）缺乏可靠的机制来选择最优候选解决方案，从而限制了性能提升的潜力。为应对这些挑战，我们提出了一种新型TTS框架——熵引导的分步扩展（Entropy-Guided Stepwise Scaling, EGSS），该框架通过熵引导的自适应搜索与稳健的测试用例增强，动态平衡了效率与效果。在SWE-Bench-Verified上的大量实验表明，EGSS在所有评估模型上均实现了5%-10%的性能提升。具体而言，其将Kimi-K2-Instruct的修复成功率从63.2%提升至72.2%，将GLM-4.6的修复率从65.8%提升至74.6%。此外，当与GLM-4.6结合使用时，EGSS在开源大语言模型中达到了新的SOTA水平。除了显著的准确率提升外，EGSS相比现有TTS方法，推理阶段的token消耗减少了超过28%，实现了效果与计算效率的双重提升。"
  },
  {
    "date": "2026-02-05",
    "title": "FedMosaic: Federated Retrieval-Augmented Generation via Parametric Adapters",
    "authors": "Zhilin Liang, Yuxiang Wang, Zimu Zhou, Hainan Zhang, Boyi Liu, Yongxin Tong",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05235v1",
    "source": "arXiv",
    "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by grounding generation in external knowledge to improve factuality and reduce hallucinations. Yet most deployments assume a centralized corpus, which is infeasible in privacy aware domains where knowledge remains siloed. This motivates federated RAG (FedRAG), where a central LLM server collaborates with distributed silos without sharing raw documents. In context RAG violates this requirement by transmitting verbatim documents, whereas parametric RAG encodes documents into lightweight adapters that merge with a frozen LLM at inference, avoiding raw-text exchange. We adopt the parametric approach but face two unique challenges induced by FedRAG: high storage and communication from per-document adapters, and destructive aggregation caused by indiscriminately merging multiple adapters. We present FedMosaic, the first federated RAG framework built on parametric adapters. FedMosaic clusters semantically related documents into multi-document adapters with document-specific masks to reduce overhead while preserving specificity, and performs selective adapter aggregation to combine only relevance-aligned, nonconflicting adapters. Experiments show that FedMosaic achieves an average 10.9% higher accuracy than state-of-the-art methods in four categories, while lowering storage costs by 78.8% to 86.3% and communication costs by 91.4%, and never sharing raw documents.",
    "title_zh": "FedMosaic：基于参数化适配器的联邦检索增强生成",
    "abstract_zh": "检索增强生成（RAG）通过将生成过程与外部知识相结合，提升了大型语言模型（LLMs）的事实准确性并减少了幻觉现象。然而，大多数现有部署依赖于集中式知识库，这在注重隐私的领域中难以实现，因为知识通常分散在各个独立的孤岛中。为此，我们提出联邦检索增强生成（FedRAG），即中央LLM服务器与分布式知识孤岛协作，而无需共享原始文档。传统的上下文RAG方法通过直接传输完整文档来实现，违反了隐私保护的要求；而参数化RAG则将文档编码为轻量级适配器，在推理时与冻结的LLM融合，避免了原始文本的交换。我们采用参数化方法，但面临联邦RAG带来的两个独特挑战：每个文档对应的适配器导致的高存储与通信开销，以及 indiscriminately（不加区分地）合并多个适配器所引发的破坏性聚合问题。\n\n为此，我们提出FedMosaic——首个基于参数化适配器的联邦RAG框架。FedMosaic通过将语义相关的文档聚类为多文档适配器，并引入文档特定的掩码机制，在降低系统开销的同时保持了生成的精准性；同时，采用选择性适配器聚合策略，仅融合语义相关且无冲突的适配器。实验结果表明，FedMosaic在四个类别任务中平均准确率比现有最先进方法高出10.9%，同时将存储成本降低78.8%至86.3%，通信成本降低91.4%，且始终不共享任何原始文档。"
  },
  {
    "date": "2026-02-05",
    "title": "Steering Large Reasoning Models towards Concise Reasoning via Flow Matching",
    "authors": "Yawei Li, Benjamin Bergner, Yinghan Zhao, Vihang Prakash Patil, Bei Chen, Cheng Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05539v1",
    "source": "arXiv",
    "abstract": "Large Reasoning Models (LRMs) excel at complex reasoning tasks, but their efficiency is often hampered by overly verbose outputs. Prior steering methods attempt to address this issue by applying a single, global vector to hidden representations -- an approach grounded in the restrictive linear representation hypothesis. In this work, we introduce FlowSteer, a nonlinear steering method that goes beyond uniform linear shifts by learning a complete transformation between the distributions associated with verbose and concise reasoning. This transformation is learned via Flow Matching as a velocity field, enabling precise, input-dependent control over the model's reasoning process. By aligning steered representations with the distribution of concise-reasoning activations, FlowSteer yields more compact reasoning than the linear shifts. Across diverse reasoning benchmarks, FlowSteer demonstrates strong task performance and token efficiency compared to leading inference-time baselines. Our work demonstrates that modeling the full distributional transport with generative techniques offers a more effective and principled foundation for controlling LRMs.",
    "title_zh": "通过流匹配引导大型推理模型走向简洁推理",
    "abstract_zh": "大型推理模型（LRMs）在复杂推理任务中表现出色，但其效率常因输出过于冗长而受到制约。以往的引导方法试图通过应用单一的全局向量作用于隐藏表示来解决这一问题，这种方法基于一种限制性较强的线性表示假设。在本研究中，我们提出了FlowSteer，一种非线性引导方法，它超越了均匀的线性偏移，通过学习冗长推理与简洁推理之间分布的完整变换来实现更精细的控制。该变换通过流匹配（Flow Matching）以速度场的形式进行学习，从而实现对模型推理过程的精确、输入相关的调控。通过将引导后的表示与简洁推理激活值的分布对齐，FlowSteer生成的推理过程比线性偏移更加紧凑。在多种推理基准测试中，FlowSteer在任务表现和令牌效率方面均优于当前领先的推理时基线方法。我们的工作表明，利用生成式技术建模完整的分布传输，为控制大型推理模型提供了一个更有效且更具原则性的基础。"
  },
  {
    "date": "2026-02-05",
    "title": "Relying on LLMs: Student Practices and Instructor Norms are Changing in Computer Science Education",
    "authors": "Xinrui Lin, Heyan Huang, Shumin Shi, John Vines",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05506v1",
    "source": "arXiv",
    "abstract": "Prior research has raised concerns about students' over-reliance on large language models (LLMs) in higher education. This paper examines how Computer Science students and instructors engage with LLMs across five scenarios: \"Writing\", \"Quiz\", \"Programming\", \"Project-based learning\", and \"Information retrieval\". Through user studies with 16 students and 6 instructors, we identify 7 key intents, including increasingly complex student practices. Findings reveal varying levels of conflict between student practices and instructor norms, ranging from clear conflict in \"Writing-generation\" and \"(Programming) quiz-solving\", through partial conflict in \"Programming project-implementation\" and \"Project-based learning\", to broad agreement in \"Writing-revision & ideation\", \"(Programming) quiz-correction\" and \"Info-query & summary\". We document instructors are shifting from prohibiting to recognizing students' use of LLMs for high-quality work, integrating usage records into assessment grading. Finally, we propose LLM design guidelines: deploying default guardrails with game-like and empathetic interaction to prevent students from \"deserting\" LLMs, especially for \"Writing-generation\", while utilizing comprehension checks in low-conflict intents to promote learning.",
    "title_zh": "依赖大语言模型：计算机科学教育中的学生实践与教师规范正在发生变化",
    "abstract_zh": "先前的研究已对高等教育中学生过度依赖大型语言模型（LLMs）的现象表示担忧。本文探讨了计算机科学专业的学生与教师在五个场景中如何使用LLMs：写作、测验、编程、基于项目的学习以及信息检索。通过对16名学生和6名教师进行用户研究，我们识别出7个关键使用意图，其中包括学生日益复杂的实践行为。研究发现，学生实践与教师规范之间存在不同程度的冲突：在“写作生成”和“（编程）测验解答”场景中存在明显冲突；在“编程项目实现”和“基于项目的学习”中存在部分冲突；而在“写作修改与构思”、“（编程）测验订正”以及“信息查询与摘要”中则表现出广泛共识。研究还发现，教师正从禁止学生使用LLMs转向认可其在高质量学习成果中的作用，并将使用记录纳入评估与评分体系。最后，本文提出了一系列LLM设计建议：在默认设置中部署类似游戏化和富有同理心的交互机制，以防止学生“抛弃”LLMs，尤其是在“写作生成”场景中；同时，在低冲突意图场景中引入理解性检测机制，以促进学习效果。"
  },
  {
    "date": "2026-02-05",
    "title": "Generative Ontology: When Structured Knowledge Learns to Create",
    "authors": "Benny Cheung",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05636v1",
    "source": "arXiv",
    "abstract": "Traditional ontologies excel at describing domain structure but cannot generate novel artifacts. Large language models generate fluently but produce outputs that lack structural validity, hallucinating mechanisms without components, goals without end conditions. We introduce Generative Ontology, a framework that synthesizes these complementary strengths: ontology provides the grammar; the LLM provides the creativity. Generative Ontology encodes domain knowledge as executable Pydantic schemas that constrain LLM generation via DSPy signatures. A multi-agent pipeline assigns specialized roles to different ontology domains: a Mechanics Architect designs game systems, a Theme Weaver integrates narrative, a Balance Critic identifies exploits. Each agent carrying a professional \"anxiety\" that prevents shallow, agreeable outputs. Retrieval-augmented generation grounds novel designs in precedents from existing exemplars, while iterative validation ensures coherence between mechanisms and components. We demonstrate the framework through GameGrammar, a system for generating complete tabletop game designs. Given a thematic prompt (\"bioluminescent fungi competing in a cave ecosystem\"), the pipeline produces structurally complete, playable game specifications with mechanisms, components, victory conditions, and setup instructions. These outputs satisfy ontological constraints while remaining genuinely creative. The pattern generalizes beyond games. Any domain with expert vocabulary, validity constraints, and accumulated exemplars (music composition, software architecture, culinary arts) is a candidate for Generative Ontology. We argue that constraints do not limit creativity but enable it: just as grammar makes poetry possible, ontology makes structured generation possible.",
    "title_zh": "生成本体论：当结构化知识学会创造",
    "abstract_zh": "传统本体论在描述领域结构方面表现出色，但无法生成新颖的实体。大型语言模型虽然能流畅输出，却常常产生缺乏结构有效性的内容——它们会虚构出没有组件的机制、没有终态的目标。我们提出“生成式本体”（Generative Ontology）这一框架，融合二者互补的优势：本体提供语法结构，大模型赋予创造力。生成式本体将领域知识编码为可执行的 Pydantic 模型，并通过 DSPy 签名约束大模型的生成过程。一个多智能体流水线为不同本体领域分配专业角色：机械架构师设计游戏系统，主题编织者整合叙事，平衡评审员识别漏洞。每个智能体都带有专业的“焦虑感”，以避免产生浅显、迎合性的输出。基于检索增强生成的技术使新设计扎根于既有范例，而迭代验证则确保机制与组件之间的一致性。我们通过 GameGrammar 系统展示了该框架的应用：这是一个用于生成完整桌游设计的系统。当输入一个主题提示（“在洞穴生态系统中发光真菌之间的竞争”），该流水线能够产出结构完整、可玩性强的游戏方案，包含机制、组件、胜利条件和设置说明。这些输出既符合本体约束，又保持了真正的创造性。这一模式可推广至其他领域。任何具备专家术语、有效性约束以及积累范例的领域（如音乐创作、软件架构、烹饪艺术）都适合应用生成式本体。我们认为，约束并非对创造力的限制，而是其前提：正如语法规则让诗歌成为可能，本体也使结构化生成成为可能。"
  },
  {
    "date": "2026-02-05",
    "title": "PatchGuru: Patch Oracle Inference from Natural Language Artifacts with Large Language Models",
    "authors": "Thanh Le-Cong, Bach Le, Toby Murray, Michael Pradel, Cristian Cadar",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05270v1",
    "source": "arXiv",
    "abstract": "As software systems evolve, patches may unintentionally alter program behavior. Validating patches against their intended semantics is difficult due to incomplete regression tests and informal, non-executable natural language (NL) descriptions of patch intent. We present PatchGuru, the first automated technique that infers executable patch specifications from real-world pull requests (PRs). Given a PR, PatchGuru uses large language models (LLMs) to extract developer intent from NL artifacts and synthesizes patch oracles: under-approximate yet practical specifications expressed as runtime assertions in comparison programs that integrate pre- and post-patch versions. Patch oracles focus on patch-relevant behaviors, enable automated validation, and support cross-version properties. PatchGuru iteratively refines inferred oracles by comparing pre- and post-patch behaviors, identifies violations, filters inconsistencies via self-review, and generates bug reports. We evaluate PatchGuru on 400 recent PRs from four widely used open-source Python projects. PatchGuru reports 39 warnings with a precision of 0.62, yielding 24 confirmed true positives, including 12 previously unknown bugs, 11 of which were subsequently fixed by developers. Compared to the state-of-the-art technique Testora, PatchGuru detects 17 more bugs (24 vs. 7) while improving precision from 0.32 to 0.62. PatchGuru incurs an average cost of 8.9 minutes and USD 0.07 per PR. These results suggest that PatchGuru complements code review and regression testing by providing executable documentation and automated validation of patch intent.",
    "title_zh": "PatchGuru：利用大语言模型从自然语言文档中推断补丁以修复Oracle",
    "abstract_zh": "随着软件系统的不断发展，补丁可能会无意中改变程序的行为。由于回归测试不完整，以及补丁意图通常以非形式化、不可执行的自然语言（NL）描述，验证补丁是否符合其预期语义变得十分困难。本文提出 PatchGuru，这是首个能够从真实世界拉取请求（PR）中自动推断可执行补丁规范的技术。给定一个 PR，PatchGuru 利用大规模语言模型（LLMs）从自然语言文档中提取开发者的意图，并合成“补丁预言”——一种下近似但实用的规范，以运行时断言的形式表达在集成补丁前后版本的对比程序中。补丁预言聚焦于与补丁相关的程序行为，支持自动化验证，并能处理跨版本属性。PatchGuru 通过比较补丁前后的程序行为，迭代地优化推断出的预言，识别行为违规，利用自我审查机制过滤不一致之处，并生成缺陷报告。我们在四个广泛使用的开源 Python 项目中对 400 个近期 PR 进行了评估。PatchGuru 共报告了 39 个警告，精确率为 0.62，其中 24 个为真实缺陷，包括 12 个此前未知的漏洞，其中 11 个已被开发者修复。与当前最先进的技术 Testora 相比，PatchGuru 多检测出 17 个漏洞（24 vs. 7），同时将精确率从 0.32 提升至 0.62。PatchGuru 每个 PR 的平均开销为 8.9 分钟和 0.07 美元。结果表明，PatchGuru 通过提供可执行的文档和补丁意图的自动化验证，有效补充了代码审查和回归测试。"
  },
  {
    "date": "2026-02-05",
    "title": "Hybrid Gated Flow (HGF): Stabilizing 1.58-bit LLMs via Selective Low-Rank Correction",
    "authors": "David Alejandro Trejo Pizzo",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05269v1",
    "source": "arXiv",
    "abstract": "The deployment of Large Language Models (LLMs) on edge devices is fundamentally constrained by the \"Memory Wall\" -- a hardware limitation where memory bandwidth, not compute, becomes the bottleneck. Recent 1.58-bit quantization techniques (e.g., BitNet b1.58) dramatically reduce memory footprint but typically incur a perplexity degradation of 20-25% compared to FP16 baselines. In this work, we introduce Hybrid Gated Flow (HGF), a dual-stream architecture that couples a 1.58-bit ternary backbone with a learnable, low-rank FP16 correction path controlled by adaptive gates. Through extensive experiments on the TinyStories dataset across two training regimes (2500 and 3500 steps), we demonstrate that HGF 5.4 achieves a validation loss of 0.9306 compared to BitNet's 1.0294, recovering approximately 55% of the quality gap between pure ternary quantization and the FP16 baseline (0.8490). This recovery is achieved with only ~12-15% memory overhead beyond the ternary backbone. Furthermore, we provide empirical evidence for an emergent phenomenon: quantization as structural regularization. While a full-precision differential attention baseline (Diff_Only) exhibited training instability with validation loss exceeding 1.68, the ternary-anchored HGF maintained robust convergence throughout training. Finally, we report preliminary results extending this architecture to 1.2B and 3B parameter models trained on SlimPajama and FineWeb-Edu. These larger-scale experiments confirm that the architectural stability and quality recovery observed in small-scale proxies scale linearly to production-grade language modeling regimes.",
    "title_zh": "混合门控流（HGF）：通过选择性低秩修正稳定1.58比特大语言模型",
    "abstract_zh": "将大型语言模型（LLMs）部署在边缘设备上，其根本限制在于“内存墙”——即硬件瓶颈表现为内存带宽而非计算能力成为性能瓶颈。近期的1.58位量化技术（如BitNet b1.58）虽大幅降低了内存占用，但通常相较于FP16基线模型，困惑度（perplexity）会下降20%-25%。在本研究中，我们提出了一种双流架构——混合门控流（Hybrid Gated Flow, HGF），该架构将1.58位三值主干网络与一个可学习、低秩的FP16校正路径相结合，并通过自适应门控机制进行控制。在TinyStories数据集上，针对两种训练策略（2500步和3500步）的大量实验表明，HGF 5.4的验证损失达到0.9306，显著优于BitNet的1.0294，恢复了纯三值量化与FP16基线之间约55%的质量差距（FP16基线为0.8490）。这一性能提升仅带来约12%-15%的额外内存开销，远低于传统方法。此外，我们提供了实证证据，揭示了一种新兴现象：量化即结构正则化。尽管全精度微分注意力基线模型（Diff_Only）在训练中表现出严重不稳定性，验证损失超过1.68，而以三值网络为锚点的HGF在整个训练过程中始终保持稳健收敛。最后，我们报告了将该架构扩展至1.2B和3B参数模型的初步结果，这些模型在SlimPajama和FineWeb-Edu数据集上进行训练。大规模实验结果表明，小规模模型中观察到的架构稳定性与质量恢复能力可线性扩展至生产级语言建模场景。"
  },
  {
    "date": "2026-02-05",
    "title": "OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions",
    "authors": "Fangzhi Xu, Hang Yan, Qiushi Sun, Jinyang Wu, Zixian Huang, Muye Huang, Jingyang Gong, Zichen Ding, Kanzhi Cheng, Yian Wang, Xinyu Che, Zeyi Sun, Jian Zhang, Zhangyue Yin, Haoran Luo, Xuanjing Huang, Ben Kao, Jun Liu, Qika Lin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05843v1",
    "source": "arXiv",
    "abstract": "The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena",
    "title_zh": "奥德赛竞技场：面向长时程、主动式与归纳性交互的大语言模型基准测试",
    "abstract_zh": "大型语言模型（LLMs）的迅猛发展推动了能够自主导航复杂环境的智能体的兴起。然而，现有的评估方法主要采用演绎范式，即智能体基于显式提供的规则和静态目标执行任务，通常局限于有限的规划范围。关键问题在于，这种范式忽视了智能体从经验中自主发现潜在转换规律的归纳必要性——而这正是实现智能体前瞻性与维持战略连贯性的核心所在。为弥合这一差距，我们提出了 OdysseyArena，将智能体评估重新聚焦于长周期、主动且具有归纳性质的交互过程。我们形式化并实现了四个基本原型，将抽象的动态转换关系转化为具体的交互环境。在此基础上，我们构建了 OdysseyArena-Lite，用于标准化基准测试，提供一组包含120个任务的评测集，以衡量智能体在归纳效率与长周期探索方面的能力。进一步地，我们推出了 OdysseyArena-Challenge，旨在通过极端交互周期（如超过200步）对智能体的稳定性进行压力测试。对15种以上领先LLM的广泛实验表明，即便是前沿模型在归纳场景下仍存在明显不足，揭示了在复杂环境中实现自主发现的关键瓶颈。我们的代码与数据已公开于 https://github.com/xufangzhi/Odyssey-Arena"
  },
  {
    "date": "2026-02-05",
    "title": "A Dual-Loop Agent Framework for Automated Vulnerability Reproduction",
    "authors": "Bin Liu, Yanjie Zhao, Zhenpeng Chen, Guoai Xu, Haoyu Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05721v1",
    "source": "arXiv",
    "abstract": "Automated vulnerability reproduction from CVE descriptions requires generating executable Proof-of-Concept (PoC) exploits and validating them in target environments. This process is critical in software security research and practice, yet remains time-consuming and demands specialized expertise when performed manually. While LLM agents show promise for automating this task, existing approaches often conflate exploring attack directions with fixing implementation details, which leads to unproductive debugging loops when reproduction fails. To address this, we propose Cve2PoC, an LLM-based dual-loop agent framework following a plan-execute-evaluate paradigm. The Strategic Planner analyzes vulnerability semantics and target code to produce structured attack plans. The Tactical Executor generates PoC code and validates it through progressive verification. The Adaptive Refiner evaluates execution results and routes failures to different loops: the \\textit{Tactical Loop} for code-level refinement, while the \\textit{Strategic Loop} for attack strategy replanning. This dual-loop design enables the framework to escape ineffective debugging by matching remediation to failure type. Evaluation on two benchmarks covering 617 real-world vulnerabilities demonstrates that Cve2PoC achieves 82.9\\% and 54.3\\% reproduction success rates on SecBench.js and PatchEval, respectively, outperforming the best baseline by 11.3\\% and 20.4\\%. Human evaluation confirms that generated PoCs achieve comparable code quality to human-written exploits in readability and reusability.",
    "title_zh": "一种用于自动化漏洞复现的双回路智能体框架",
    "abstract_zh": "从CVE描述中自动复现漏洞需要生成可执行的漏洞利用证明（PoC）并验证其在目标环境中的有效性。这一过程在软件安全研究与实践中至关重要，但若采用人工方式执行，往往耗时且需要高度专业化的技能。尽管大型语言模型（LLM）代理在自动化该任务方面展现出潜力，但现有方法常将探索攻击路径与修复实现细节混为一谈，导致复现失败时陷入无效的调试循环。为解决这一问题，我们提出Cve2PoC——一种基于LLM的双循环代理框架，遵循“规划-执行-评估”的范式。战略规划器（Strategic Planner）分析漏洞语义与目标代码，生成结构化的攻击计划；战术执行器（Tactical Executor）则生成PoC代码，并通过渐进式验证进行测试。当执行失败时，自适应优化器（Adaptive Refiner）会根据失败类型进行判断：若为代码层面问题，则引导至“战术循环”（Tactical Loop）进行代码级优化；若为攻击策略缺陷，则转入“战略循环”（Strategic Loop）重新规划攻击策略。这种双循环设计使系统能够根据失败原因精准调整修复方向，有效避免无效调试。在涵盖617个真实漏洞的两个基准测试集（SecBench.js和PatchEval）上的评估表明，Cve2PoC在两个数据集上的漏洞复现成功率分别达到82.9%和54.3%，分别优于最佳基线11.3%和20.4%。人工评估进一步证实，生成的PoC在可读性和可复用性方面与人工编写的漏洞利用代码相当。"
  },
  {
    "date": "2026-02-05",
    "title": "Towards Green AI: Decoding the Energy of LLM Inference in Software Development",
    "authors": "Lola Solovyeva, Fernando Castor",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05712v1",
    "source": "arXiv",
    "abstract": "Context: AI-assisted tools are increasingly integrated into software development workflows, but their reliance on large language models (LLMs) introduces substantial computational and energy costs. Understanding and reducing the energy footprint of LLM inference is therefore essential for sustainable software development. Objective: In this study, we conduct a phase-level analysis of LLM inference energy consumption, distinguishing between the (1) prefill, where the model processes the input and builds internal representations, and (2) decoding, where output tokens are generated using the stored state. Method: We investigate six 6B-7B and four 3B-4B transformer-based models, evaluating them on code-centric benchmarks HumanEval for code generation and LongBench for code understanding. Results: Our findings show that, within both parameter groups, models exhibit distinct energy patterns across phases. Furthermore, we observed that increases in prefill cost amplify the energy cost per token during decoding, with amplifications ranging from 1.3% to 51.8% depending on the model. Lastly, three out of ten models demonstrate babbling behavior, adding excessive content to the output that unnecessarily inflates energy consumption. We implemented babbling suppression for code generation, achieving energy savings ranging from 44% to 89% without affecting generation accuracy. Conclusion: These findings show that prefill costs influence decoding, which dominates energy consumption, and that babbling suppression can yield up to 89% energy savings. Reducing inference energy therefore requires both mitigating babbling behavior and limiting impact of prefill on decoding.",
    "title_zh": "迈向绿色AI：解码软件开发中大型语言模型推理的能耗",
    "abstract_zh": "上下文：AI辅助工具正越来越多地融入软件开发工作流程，但其对大型语言模型（LLM）的依赖带来了巨大的计算和能源开销。因此，理解并降低LLM推理过程中的能源足迹，对于实现可持续的软件开发至关重要。  \n目标：本研究对LLM推理过程中的能源消耗进行了阶段级分析，区分了（1）预填充阶段（prefill），即模型处理输入并构建内部表示；以及（2）解码阶段（decoding），即利用存储的状态逐个生成输出标记。  \n方法：我们评估了六款6B-7B参数规模和四款3B-4B参数规模的基于Transformer的模型，测试其在代码生成任务（HumanEval基准）和代码理解任务（LongBench基准）上的表现。  \n结果：研究发现，在两个参数组中，不同模型在各阶段表现出显著不同的能源消耗模式。此外，我们观察到预填充阶段成本的增加会显著放大解码阶段每生成一个标记的能源消耗，放大比例在1.3%至51.8%之间，具体取决于模型。最后，十款模型中有三款表现出“胡言乱语”（babbling）行为，即在输出中添加大量冗余内容，导致能源消耗不必要地增加。我们针对代码生成任务实施了胡言乱语抑制机制，实现了44%至89%的能源节省，且未影响生成准确性。  \n结论：研究结果表明，预填充阶段的成本会影响解码阶段，而解码阶段是能源消耗的主要来源。通过抑制胡言乱语行为，可实现高达89%的能源节约。因此，降低推理过程的能源消耗，需要同时减少胡言乱语行为，并控制预填充对解码阶段的影响。"
  },
  {
    "date": "2026-02-05",
    "title": "Toward Operationalizing Rasmussen: Drift Observability on the Simplex for Evolving Systems",
    "authors": "Anatoly A. Krasnovsky",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05483v1",
    "source": "arXiv",
    "abstract": "Monitoring drift into failure is hindered by Euclidean anomaly detection that can conflate safe operational trade-offs with risk accumulation in signals expressed as shares, and by architectural churn that makes fixed schemas (and learned models) stale before rare boundary events occur. Rasmussen's dynamic safety model motivates drift under competing pressures, but operationalizing it for software is difficult because many high-value operational signals (effort, remaining margin, incident impact) are compositional and their parts evolve. We propose a vision for drift observability on the simplex: model drift and boundary proximity in Aitchison geometry to obtain coordinate-invariant direction and distance-to-safety in interpretable balance coordinates. To remain comparable under churn, a monitor would continuously refresh its part inventory and policy-defined boundaries from engineering artifacts and apply lineage-aware aggregation. We outline early-warning diagnostics and falsifiable hypotheses for future evaluation.",
    "title_zh": "迈向实现拉姆塞斯：对演化系统的单纯形上的漂移可观察性",
    "abstract_zh": "监测漂移至故障的过程受到欧几里得异常检测的限制，这种检测方法容易将安全的操作权衡与以份额形式表达的信号中的风险累积混淆在一起；同时，架构频繁变更导致固定模式（以及学习模型）在罕见边界事件发生前就已过时。Rasmussen 的动态安全模型为在多重压力下发生漂移提供了理论依据，但将其应用于软件系统却十分困难，因为许多高价值的运行信号（如工作量、剩余缓冲、事件影响）具有组合性，其组成部分也在不断演化。我们提出一种在单纯形空间中实现漂移可观测性的愿景：在 Aitchison 几何中建模漂移与边界接近度，从而在可解释的平衡坐标系中获得坐标不变的方向和到安全边界的距离。为应对架构变更带来的影响，监控系统需持续从工程资产中刷新其组成部分清单和策略定义的边界，并采用具备血缘感知能力的聚合方法，以保持监测的一致性与可比性。我们还提出了早期预警诊断方法以及可用于未来评估的可证伪假设。"
  },
  {
    "date": "2026-02-05",
    "title": "Late-to-Early Training: LET LLMs Learn Earlier, So Faster and Better",
    "authors": "Ji Zhao, Yufei Gu, Shitong Shao, Xun Zhou, Liang Xiang, Zeke Xie",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05393v1",
    "source": "arXiv",
    "abstract": "As Large Language Models (LLMs) achieve remarkable empirical success through scaling model and data size, pretraining has become increasingly critical yet computationally prohibitive, hindering rapid development. Despite the availability of numerous pretrained LLMs developed at significant computational expense, a fundamental real-world question remains underexplored: \\textit{Can we leverage existing small pretrained models to accelerate the training of larger models?} In this paper, we propose a Late-to-Early Training (LET) paradigm that enables LLMs to explicitly learn later knowledge in earlier steps and earlier layers. The core idea is to guide the early layers of an LLM during early training using representations from the late layers of a pretrained (i.e. late training phase) model. We identify two key mechanisms that drive LET's effectiveness: late-to-early-step learning and late-to-early-layer learning. These mechanisms significantly accelerate training convergence while robustly enhancing both language modeling capabilities and downstream task performance, enabling faster training with superior performance. Extensive experiments on 1.4B and 7B parameter models demonstrate LET's efficiency and effectiveness. Notably, when training a 1.4B LLM on the Pile dataset, our method achieves up to 1.6$\\times$ speedup with nearly 5\\% improvement in downstream task accuracy compared to standard training, even when using a pretrained model with 10$\\times$ fewer parameters than the target model.",
    "title_zh": "晚进早学：让大语言模型更早学习，从而更快、更优",
    "abstract_zh": "随着大规模语言模型（LLMs）通过扩大模型和数据规模取得了显著的实证成功，预训练阶段变得愈发关键，但其计算成本也日益高昂，严重制约了模型的快速迭代与发展。尽管已有大量预训练LLM在巨大计算投入下完成，一个根本性的现实问题仍鲜有探讨：**我们能否利用现有的小型预训练模型来加速更大模型的训练？** 本文提出了一种“晚到早训练”（Late-to-Early Training, LET）范式，使LLM能够在早期训练阶段和早期层中显式地学习到后期知识。其核心思想是：在早期训练过程中，利用一个已预训练完成（即处于后期训练阶段）的模型的深层表示来指导目标模型的浅层网络。我们识别出驱动LET有效性的两个关键机制：**晚到早步学习**（late-to-early-step learning）和**晚到早层学习**（late-to-early-layer learning）。这两种机制显著加速了训练收敛过程，同时稳健地提升了语言建模能力与下游任务表现，实现了更快训练速度与更优性能的统一。在14亿（1.4B）和70亿（7B）参数模型上的大量实验验证了LET的高效性与有效性。特别地，在Pile数据集上训练一个1.4B参数的LLM时，我们的方法相比标准训练实现了高达1.6倍的加速，下游任务准确率提升近5%，即使所使用的预训练模型参数量仅为目标模型的十分之一。"
  },
  {
    "date": "2026-02-05",
    "title": "Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening",
    "authors": "Zhenxiong Yu, Zhi Yang, Zhiheng Jin, Shuhe Wang, Heng Zhang, Yanlin Fei, Lingfeng Zeng, Fangqi Lou, Shuo Zhang, Tu Hu, Jingping Liu, Rongze Chen, Xingyu Zhu, Kunyi Wang, Chaofa Yuan, Xin Guo, Zhaowei Liu, Feipeng Zhang, Jie Huang, Huacan Wang, Ronghao Chen, Liwen Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05386v1",
    "source": "arXiv",
    "abstract": "As large language models (LLMs) evolve into autonomous agents, their real-world applicability has expanded significantly, accompanied by new security challenges. Most existing agent defense mechanisms adopt a mandatory checking paradigm, in which security validation is forcibly triggered at predefined stages of the agent lifecycle. In this work, we argue that effective agent security should be intrinsic and selective rather than architecturally decoupled and mandatory. We propose Spider-Sense framework, an event-driven defense framework based on Intrinsic Risk Sensing (IRS), which allows agents to maintain latent vigilance and trigger defenses only upon risk perception. Once triggered, the Spider-Sense invokes a hierarchical defence mechanism that trades off efficiency and precision: it resolves known patterns via lightweight similarity matching while escalating ambiguous cases to deep internal reasoning, thereby eliminating reliance on external models. To facilitate rigorous evaluation, we introduce S$^2$Bench, a lifecycle-aware benchmark featuring realistic tool execution and multi-stage attacks. Extensive experiments demonstrate that Spider-Sense achieves competitive or superior defense performance, attaining the lowest Attack Success Rate (ASR) and False Positive Rate (FPR), with only a marginal latency overhead of 8.3\\%.",
    "title_zh": "蜘蛛感应：基于分层自适应筛选的高效智能体防御内在风险感知",
    "abstract_zh": "随着大型语言模型（LLMs）逐步演变为自主智能体，其在现实世界中的应用范围显著扩展，同时也带来了新的安全挑战。现有的大多数智能体防御机制采用强制检查范式，即在智能体生命周期的预设阶段强制触发安全验证。本文认为，有效的智能体安全应是内在的、选择性的，而非架构上分离且强制性的。为此，我们提出了Spider-Sense框架——一种基于内在风险感知（Intrinsic Risk Sensing, IRS）的事件驱动防御机制，使智能体能够保持隐性警觉，并仅在感知到风险时才触发防御。一旦触发，Spider-Sense会启动分层防御机制，在效率与精度之间进行权衡：对于已知模式，通过轻量级相似性匹配快速处理；对于模糊情况，则升级至深度内部推理，从而完全摆脱对外部模型的依赖。为支持严谨的评估，我们提出了S$^2$Bench，一个具备生命周期感知能力的基准测试平台，包含真实的工具执行场景和多阶段攻击。大量实验表明，Spider-Sense在防御性能上表现优异，达到了最低的攻击成功率（ASR）和误报率（FPR），同时仅带来8.3%的微小延迟开销。"
  },
  {
    "date": "2026-02-05",
    "title": "Split Personality Training: Revealing Latent Knowledge Through Alternate Personalities",
    "authors": "Florian Dietz, William Wale, Oscar Gilg, Robert McCarthy, Felix Michalak, Gustavo Ewbank Rodrigues Danon, Miguelito de Guzman, Dietrich Klakow",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05532v1",
    "source": "arXiv",
    "abstract": "Detecting misalignment in large language models is challenging because models may learn to conceal misbehavior during training. Standard auditing techniques fall short: black-box methods often cannot distinguish misaligned outputs from benign ones, and mechanistic interpretability does not scale with model capabilities. We introduce Split Personality Training (SPT), which fine-tunes a second ``honest persona'' into LoRA parameters that remain inactive during normal operation. After the main model responds, we activate the LoRA adapter and insert a trigger string, enabling the honest persona to review the response while accessing the main model's latent states. We test our method on the Anthropic Auditing Game Model Organism, a benchmark where Llama-3.3-70B is trained to exploit reward hacks while concealing this behavior. SPT achieves 96% overall accuracy, whereas Anthropic reports near 0% accuracy. The honest persona reveals latent knowledge inaccessible to external observers, such as the fictional biases the compromised model was trained on.",
    "title_zh": "分裂人格训练：通过不同人格揭示潜在知识",
    "abstract_zh": "在大型语言模型中检测偏差行为极具挑战性，因为模型可能在训练过程中学会隐藏其不当行为。传统的审计技术难以奏效：黑箱方法通常无法区分偏差输出与正常输出，而机制可解释性方法又无法随模型能力的提升而扩展。我们提出了一种名为“分裂人格训练”（Split Personality Training, SPT）的新方法，该方法通过微调一个第二“诚实人格”进入LoRA参数，该参数在正常运行时保持非激活状态。在主模型生成回应后，我们激活LoRA适配器并插入一个触发字符串，使诚实人格能够审查回应，同时访问主模型的潜在状态。我们在Anthropic审计游戏模型生物体（a benchmark）上测试了该方法，该基准中Llama-3.3-70B模型被训练为利用奖励漏洞，同时隐藏此类行为。SPT实现了96%的整体准确率，而Anthropic报告的准确率接近0%。诚实人格揭示了外部观察者无法获取的潜在知识，例如被攻陷模型所学习到的虚构偏见。"
  },
  {
    "date": "2026-02-05",
    "title": "ArkTS-CodeSearch: A Open-Source ArkTS Dataset for Code Retrieval",
    "authors": "Yulong He, Artem Ermakov, Sergey Kovalchuk, Artem Aliev, Dmitry Shalymov",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05550v1",
    "source": "arXiv",
    "abstract": "ArkTS is a core programming language in the OpenHarmony ecosystem, yet research on ArkTS code intelligence is hindered by the lack of public datasets and evaluation benchmarks. This paper presents a large-scale ArkTS dataset constructed from open-source repositories, targeting code retrieval and code evaluation tasks. We design a single-search task, where natural language comments are used to retrieve corresponding ArkTS functions. ArkTS repositories are crawled from GitHub and Gitee, and comment-function pairs are extracted using tree-sitter-arkts, followed by cross-platform deduplication and statistical analysis of ArkTS function types. We further evaluate all existing open-source code embedding models on the single-search task and perform fine-tuning using both ArkTS and TypeScript training datasets, resulting in a high-performing model for ArkTS code understanding. This work establishes the first systematic benchmark for ArkTS code retrieval. Both the dataset and our fine-tuned model will be released publicly and are available at https://huggingface.co/hreyulog/embedinggemma_arkts and https://huggingface.co/datasets/hreyulog/arkts-code-docstring,establishing the first systematic benchmark for ArkTS code retrieval.",
    "title_zh": "ArkTS-CodeSearch：一个用于代码检索的开源ArkTS数据集",
    "abstract_zh": "ArkTS 是 OpenHarmony 生态系统中的核心编程语言，然而由于缺乏公开的数据集和评估基准，针对 ArkTS 代码智能的研究受到严重制约。本文构建了一个大规模的 ArkTS 数据集，该数据集来源于开源代码仓库，旨在支持代码检索与代码评估任务。我们设计了一项单次搜索任务，即通过自然语言注释来检索对应的 ArkTS 函数。ArkTS 代码仓库从 GitHub 和 Gitee 上爬取，利用 tree-sitter-arkts 工具提取注释-函数对，并进行了跨平台去重处理，同时对 ArkTS 函数类型进行了统计分析。在此基础上，我们评估了所有现有的开源代码嵌入模型在单次搜索任务上的表现，并基于 ArkTS 和 TypeScript 的训练数据集进行了微调，最终获得了一个在 ArkTS 代码理解任务中表现优异的模型。本研究首次建立了针对 ArkTS 代码检索的系统性基准。相关数据集及我们微调后的模型将公开发布，地址分别为：https://huggingface.co/hreyulog/embedinggemma_arkts 和 https://huggingface.co/datasets/hreyulog/arkts-code-docstring，标志着 ArkTS 代码检索领域首个系统性基准的建立。"
  },
  {
    "date": "2026-02-05",
    "title": "Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents",
    "authors": "Quan M. Tran, Zhuo Huang, Wenbin Zhang, Bo Han, Koji Yatani, Masashi Sugiyama, Tongliang Liu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05810v1",
    "source": "arXiv",
    "abstract": "Autonomous agents excel in self-improvement through reflection and iterative refinement, which reuse successful task trajectories as in-context examples to assist subsequent reasoning. However, shifting across tasks often introduces a context mismatch. Hence, existing approaches either discard the trajectories or manipulate them using heuristics, leading to a non-negligible fine-tuning cost or unguaranteed performance. To bridge this gap, we reveal a context-trajectory correlation, where shifts of context are highly parallel with shifts of trajectory. Based on this finding, we propose BrIdge contextual gap FoR imprOvised trajectory STeering (Bifrost), a training-free method that leverages context differences to precisely guide the adaptation of previously solved trajectories towards the target task, mitigating the misalignment caused by context shifts. Our trajectory adaptation is conducted at the representation level using agent hidden states, ensuring trajectory transformation accurately aligns with the target context in a shared space. Across diverse benchmarks, Bifrost consistently outperforms existing trajectory reuse and finetuned self-improvement methods, demonstrating that agents can effectively leverage past experiences despite substantial context shifts.",
    "title_zh": "Bifrost：引导战略轨迹以弥合上下文差距，助力自我提升型智能体",
    "abstract_zh": "自主智能体通过反思与迭代优化，在自我改进方面表现出色，能够复用成功的任务轨迹作为上下文示例，以辅助后续推理。然而，任务之间的切换常导致上下文不匹配问题。现有方法要么丢弃已有轨迹，要么使用启发式方法对其进行调整，这要么带来显著的微调成本，要么无法保证性能。为弥合这一差距，我们揭示了上下文与轨迹之间存在相关性：上下文的变化与轨迹的变化高度平行。基于这一发现，我们提出了 Bifrost（BrIdge contextual gap FoR imprOvised trajectory STeering，用于修复上下文间隙的轨迹自适应引导方法），一种无需训练的方法。Bifrost 利用上下文差异，精准引导已解决轨迹向目标任务的适应，有效缓解了因上下文变化引起的错位问题。我们的轨迹适应在表示层进行，通过智能体的隐藏状态实现，确保轨迹变换在共享空间中准确对齐目标上下文。在多个不同基准测试中，Bifrost 均显著优于现有的轨迹复用与微调型自我改进方法，证明了智能体即使在面临显著上下文变化时，仍能有效利用过往经验。"
  },
  {
    "date": "2026-02-05",
    "title": "Automated Customization of LLMs for Enterprise Code Repositories Using Semantic Scopes",
    "authors": "Ulrich Finkler, Irene Manotas, Wei Zhang, Geert Janssen, Octavian Popescu, Shyam Ramji",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05780v1",
    "source": "arXiv",
    "abstract": "Code completion (CC) is a task frequently used by developers when working in collaboration with LLM-based programming assistants. Despite the increased performance of LLMs on public benchmarks, out of the box LLMs still have a hard time generating code that aligns with a private code repository not previously seen by the model's training data. Customizing code LLMs to a private repository provides a way to improve the model performance. In this paper we present our approach for automated LLM customization based on semantic scopes in the code. We evaluate LLMs on real industry cases with two private enterprise code repositories with two customization strategies: Retrieval-Augmented Generation (RAG) and supervised Fine-Tuning (FT). Our mechanism for ingesting the repository's data and formulating the training data pairs with semantic scopes helps models to learn the underlying patterns specific to the repository, providing more precise code to developers and helping to boost their productivity. The code completions of moderately sized customized models can be significantly better than those of uncustomized models of much larger capacity. We also include an analysis of customization on two public benchmarks and present opportunities for future work.",
    "title_zh": "使用语义范围实现企业代码仓库中大语言模型的自动化定制",
    "abstract_zh": "代码补全（Code Completion, CC）是开发者在与基于大语言模型（LLM）的编程助手协作时经常使用的一项任务。尽管大语言模型在公开基准测试中的表现不断提升，但未经定制的通用模型在生成与未曾在其训练数据中出现过的私有代码仓库相匹配的代码时仍面临困难。针对特定私有代码库对代码类大语言模型进行定制，是提升模型性能的有效途径。本文提出了一种基于代码语义作用域的自动化LLM定制方法。我们在两个真实的工业案例中，利用两个私有企业级代码仓库，评估了两种定制策略——检索增强生成（Retrieval-Augmented Generation, RAG）和监督微调（Supervised Fine-Tuning, FT）的效果。我们设计的数据摄入机制以及结合语义作用域构建训练数据对的方法，使模型能够学习到该代码库特有的底层模式，从而为开发者提供更精准的代码补全结果，有效提升开发效率。实验表明，经过适度规模定制的模型，其代码补全质量显著优于参数量更大但未定制的模型。此外，我们还对两种定制方法在两个公开基准上的表现进行了分析，并探讨了未来研究的方向。"
  },
  {
    "date": "2026-02-05",
    "title": "Toward Quantum-Safe Software Engineering: A Vision for Post-Quantum Cryptography Migration",
    "authors": "Lei Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05759v1",
    "source": "arXiv",
    "abstract": "The quantum threat to cybersecurity has accelerated the standardization of Post-Quantum Cryptography (PQC). Migrating legacy software to these quantum-safe algorithms is not a simple library swap, but a new software engineering challenge: existing vulnerability detection, refactoring, and testing tools are not designed for PQC's probabilistic behavior, side-channel sensitivity, and complex performance trade-offs. To address these challenges, this paper outlines a vision for a new class of tools and introduces the Automated Quantum-safe Adaptation (AQuA) framework, with a three-pillar agenda for PQC-aware detection, semantic refactoring, and hybrid verification, thereby motivating Quantum-Safe Software Engineering (QSSE) as a distinct research direction.",
    "title_zh": "迈向量子安全的软件工程：后量子密码学迁移的愿景",
    "abstract_zh": "量子威胁加速了后量子密码学（PQC）的标准化进程。将传统软件迁移至这些抗量子算法并非简单的库替换，而是一项全新的软件工程挑战：现有的漏洞检测、重构和测试工具并未针对PQC的 probabilistic（概率性）行为、侧信道敏感性以及复杂的性能权衡进行设计。为应对这些挑战，本文提出了一种新型工具的愿景，并介绍了自动化量子安全适配（AQuA）框架，该框架以三大支柱——PQC感知的漏洞检测、语义化重构和混合验证——为核心，推动量子安全软件工程（QSSE）成为一项独立的研究方向。"
  },
  {
    "date": "2026-02-05",
    "title": "Multi-Task GRPO: Reliable LLM Reasoning Across Tasks",
    "authors": "Shyam Sundhar Ramesh, Xiaotong Ji, Matthieu Zimmer, Sangwoong Yoon, Zhiyong Wang, Haitham Bou Ammar, Aurelien Lucchi, Ilija Bogunovic",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05547v1",
    "source": "arXiv",
    "abstract": "RL-based post-training with GRPO is widely used to improve large language models on individual reasoning tasks. However, real-world deployment requires reliable performance across diverse tasks. A straightforward multi-task adaptation of GRPO often leads to imbalanced outcomes, with some tasks dominating optimization while others stagnate. Moreover, tasks can vary widely in how frequently prompts yield zero advantages (and thus zero gradients), which further distorts their effective contribution to the optimization signal. To address these issues, we propose a novel Multi-Task GRPO (MT-GRPO) algorithm that (i) dynamically adapts task weights to explicitly optimize worst-task performance and promote balanced progress across tasks, and (ii) introduces a ratio-preserving sampler to ensure task-wise policy gradients reflect the adapted weights. Experiments on both 3-task and 9-task settings show that MT-GRPO consistently outperforms baselines in worst-task accuracy. In particular, MT-GRPO achieves 16-28% and 6% absolute improvement on worst-task performance over standard GRPO and DAPO, respectively, while maintaining competitive average accuracy. Moreover, MT-GRPO requires 50% fewer training steps to reach 50% worst-task accuracy in the 3-task setting, demonstrating substantially improved efficiency in achieving reliable performance across tasks.",
    "title_zh": "多任务GRPO：跨任务的可靠大模型推理",
    "abstract_zh": "基于强化学习的后训练方法，如GRPO，在提升大语言模型在单个推理任务上的表现方面已被广泛应用。然而，实际部署场景要求模型在多样化任务上均具备可靠的性能。直接将GRPO扩展至多任务场景往往导致结果失衡：部分任务在优化过程中占据主导地位，而其他任务则进展缓慢。此外，不同任务中提示（prompt）产生零优势（从而导致零梯度）的频率差异显著，这进一步扭曲了各任务对优化信号的有效贡献。为解决上述问题，我们提出了一种新型的多任务GRPO算法——MT-GRPO，该算法具有两个关键特性：(i) 动态调整任务权重，以显式优化最差任务的性能，促进各任务间的均衡进展；(ii) 引入一种保持比例的采样器，确保任务级别的策略梯度能够准确反映调整后的权重。在三任务和九任务设置下的实验表明，MT-GRPO在最差任务准确率方面始终优于各类基线方法。具体而言，相较于标准GRPO和DAPO，MT-GRPO在最差任务性能上分别实现了16%-28%和6%的绝对提升，同时保持了具有竞争力的平均准确率。此外，在三任务设置下，MT-GRPO仅需50%的训练步数即可达到50%的最差任务准确率，充分证明了其在实现跨任务可靠性能方面的显著效率优势。"
  },
  {
    "date": "2026-02-05",
    "title": "ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation",
    "authors": "Yiwen Duan, Jing Ye, Xinpei Zhao",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05472v1",
    "source": "arXiv",
    "abstract": "The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \\textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \\textbf{costly} to scale, \\textbf{brittle} across domains, and \\textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \\textbf{ALIVE} (\\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \\emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty. Empirical evaluations across mathematical reasoning, code generation, and general logical inference benchmarks demonstrate that ALIVE consistently mitigates reward signal limitations. With identical data and compute, it achieves accuracy gains, markedly improved cross-domain generalization, and higher self-correction rates. These results indicate that the reasoning trinity fosters a self-sustaining trajectory of capability growth, positioning ALIVE as a scalable foundation for general-purpose reasoning alignment without human-in-the-loop supervision.",
    "title_zh": "ALIVE：通过对抗性学习与指导性语言评估唤醒大语言模型的推理能力",
    "abstract_zh": "在大型语言模型（LLMs）中实现专家级推理能力的过程中，一个持续存在的瓶颈是所谓的“奖励瓶颈”：传统的强化学习（RL）依赖于标量奖励信号，这些信号在扩展性上代价高昂，在不同领域间表现脆弱，且对解决方案背后的逻辑完全无知。这种对外部、贫乏信号的依赖，阻碍了模型发展出对推理原则的深层、自洽的理解。为此，我们提出了**ALIVE**（*基于指导性语言评估的对抗学习*），一种无需人工干预的对齐框架，它突破了标量奖励优化的局限，转向内在推理能力的获取。ALIVE 基于“认知协同”原则，将问题提出、求解与评判统一于单一策略模型之中，从而内化“正确性”的逻辑。通过结合对抗学习与指导性语言反馈，ALIVE 使模型能够直接从原始语料库中内化评估标准，将外部批评有效转化为内在的推理能力。在数学推理、代码生成和通用逻辑推理等多个基准上的实证评估表明，ALIVE 能够持续缓解奖励信号的局限性。在相同的数据和计算资源下，其在准确率、跨领域泛化能力以及自我修正率方面均表现出显著提升。这些结果表明，推理三要素共同构建了一个自我维持的能力增长轨迹，使 ALIVE 成为一种无需人工监督的、可扩展的通用推理对齐基础。"
  },
  {
    "date": "2026-02-05",
    "title": "Emergence-as-Code for Self-Governing Reliable Systems",
    "authors": "Anatoly A. Krasnovsky",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05458v1",
    "source": "arXiv",
    "abstract": "SLO-as-code has made per-service} reliability declarative, but user experience is defined by journeys whose reliability is an emergent property of microservice topology, routing, redundancy, timeouts/fallbacks, shared failure domains, and tail amplification. As a result, journey objectives (e.g., \"checkout p99 < 400 ms\") are often maintained outside code and drift as the system evolves, forcing teams to either miss user expectations or over-provision and gate releases with ad-hoc heuristics. We propose Emergence-as-Code (EmaC), a vision for making journey reliability computable and governable via intent plus evidence. An EmaC spec declares journey intent (objective, control-flow operators, allowed actions) and binds it to atomic SLOs and telemetry. A runtime inference component consumes operational artifacts (e.g., tracing and traffic configuration) to synthesize a candidate journey model with provenance and confidence. From the last accepted model, the EmaC compiler/controller derives bounded journey SLOs and budgets under explicit correlation assumptions (optimistic independence vs. pessimistic shared fate), and emits control-plane artifacts (burn-rate alerts, rollout gates, action guards) that are reviewable in a Git workflow. An anonymized artifact repository provides a runnable example specification and generated outputs.",
    "title_zh": "代码化涌现：自治理可靠系统的实现",
    "abstract_zh": "SLO-as-code 已经使每个服务的可靠性变得可声明化，但用户体验则由用户旅程定义，而旅程的可靠性是微服务拓扑、路由策略、冗余机制、超时与降级、共享故障域以及尾部放大效应等要素共同作用的涌现属性。因此，旅程目标（例如“结账操作的 p99 延迟 < 400ms”）往往被维护在代码之外，随着系统演进而逐渐偏离，迫使团队要么无法满足用户期望，要么过度配置资源，并依赖临时的启发式规则来限制发布。我们提出“涌现即代码”（Emergence-as-Code, EmaC）的愿景，旨在通过意图加证据的方式，使旅程可靠性变得可计算且可治理。EmaC 规范声明了旅程的意图（目标、控制流操作符、允许的操作），并将其与原子级 SLO 和遥测数据绑定。运行时推理组件会消费运行时产物（如链路追踪数据和流量配置），合成具备来源追溯和置信度的候选旅程模型。基于上一次被接受的模型，EmaC 编译器/控制器在明确的相关性假设下（如“乐观独立”或“悲观共享命运”），推导出有界旅程 SLO 和预算，并生成可审查的控制平面产物（如烧毁速率告警、发布门禁、操作保护机制），这些产物可通过 Git 工作流进行审查。一个匿名化的产物仓库提供了可运行的规范示例及生成的输出。"
  },
  {
    "date": "2026-02-05",
    "title": "Orthogonal Model Merging",
    "authors": "Sihan Yang, Kexuan Shi, Weiyang Liu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05943v1",
    "source": "arXiv",
    "abstract": "Merging finetuned Large Language Models (LLMs) has become increasingly important for integrating diverse capabilities into a single unified model. However, prevailing model merging methods rely on linear arithmetic in Euclidean space, which often destroys the intrinsic geometric properties of pretrained weights, such as hyperspherical energy. To address this, we propose Orthogonal Model Merging (OrthoMerge), a method that performs merging operations on the Riemannian manifold formed by the orthogonal group to preserve the geometric structure of the model's weights. By mapping task-specific orthogonal matrices learned by Orthogonal Finetuning (OFT) to the Lie algebra, OrthoMerge enables a principled yet efficient integration that takes into account both the direction and intensity of adaptations. In addition to directly leveraging orthogonal matrices obtained by OFT, we further extend this approach to general models finetuned with non-OFT methods (i.e., low-rank finetuning, full finetuning) via an Orthogonal-Residual Decoupling strategy. This technique extracts the orthogonal components of expert models by solving the orthogonal Procrustes problem, which are then merged on the manifold of the orthogonal group, while the remaining linear residuals are processed through standard additive merging. Extensive empirical results demonstrate the effectiveness of OrthoMerge in mitigating catastrophic forgetting and maintaining model performance across diverse tasks.",
    "title_zh": "正交模型合并",
    "abstract_zh": "将微调后的大型语言模型（LLMs）进行合并，正日益成为将多种能力整合到单一统一模型中的关键手段。然而，现有的模型合并方法大多依赖于欧几里得空间中的线性算术运算，这往往破坏了预训练权重固有的几何特性，例如超球面能量。为解决这一问题，我们提出了一种正交模型合并方法（Orthogonal Model Merging, OrthoMerge），该方法在由正交群构成的黎曼流形上执行合并操作，从而保留模型权重的几何结构。通过将正交微调（Orthogonal Finetuning, OFT）所学习到的任务特定正交矩阵映射到李代数，OrthoMerge 实现了一种既具有理论依据又高效的方法，能够同时考虑微调的方向与强度。除了直接利用 OFT 获得的正交矩阵外，我们进一步将该方法扩展至通过非 OFT 方法（如低秩微调、全量微调）进行微调的通用模型，引入了一种正交-残差解耦策略。该技术通过求解正交普鲁克斯特问题（orthogonal Procrustes problem），提取专家模型中的正交分量，并在正交群的流形上进行合并，而剩余的线性残差则通过标准的加法合并方式进行处理。大量实验证明，OrthoMerge 在缓解灾难性遗忘、保持模型在多样化任务上的性能方面具有显著效果。"
  },
  {
    "date": "2026-02-05",
    "title": "Codified Finite-state Machines for Role-playing",
    "authors": "Letian Peng, Yupeng Hou, Kun Zhou, Jingbo Shang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05905v1",
    "source": "arXiv",
    "abstract": "Modeling latent character states is crucial for consistent and engaging role-playing (RP) with large language models (LLMs). Yet, existing prompting-based approaches mainly capture surface actions, often failing to track the latent states that drive interaction. We revisit finite-state machines (FSMs), long used in game design to model state transitions. While effective in small, well-specified state spaces, traditional hand-crafted, rule-based FSMs struggle to adapt to the open-ended semantic space of RP. To address this, we introduce Codified Finite-State Machines (CFSMs), a framework that automatically codifies textual character profiles into FSMs using LLM-based coding. CFSMs extract key states and transitions directly from the profile, producing interpretable structures that enforce character consistency. To further capture uncertainty and variability, we extend CFSMs into Codified Probabilistic Finite-State Machines (CPFSMs), where transitions are modeled as probability distributions over states. Through both synthetic evaluations and real-world RP scenarios in established artifacts, we demonstrate that CFSM and CPFSM outperform generally applied baselines, verifying effectiveness not only in structured tasks but also in open-ended stochastic state exploration.",
    "title_zh": "角色扮演中的编码有限状态机",
    "abstract_zh": "建模潜在角色状态对于与大型语言模型（LLMs）进行一致且引人入胜的角色扮演（RP）至关重要。然而，现有的基于提示的方法主要捕捉表面行为，往往无法有效追踪驱动交互的潜在状态。我们重新审视了有限状态机（FSMs），这种技术在游戏设计中长期用于建模状态转换。尽管在小规模、明确指定的状态空间中表现良好，但传统的手工构建、基于规则的FSM难以适应角色扮演中开放式的语义空间。为解决这一问题，我们提出了编码有限状态机（CFSMs）框架，该框架利用基于LLM的代码生成技术，自动将文本角色描述编码为FSM。CFSMs能够直接从角色描述中提取关键状态与转换，生成可解释的结构，从而确保角色行为的一致性。为进一步捕捉不确定性与多样性，我们将CFSMs扩展为编码概率有限状态机（CPFSMs），其中状态间的转换被建模为状态上的概率分布。通过合成评估以及在已建立的虚构作品中的真实角色扮演场景测试，我们证明CFSM与CPFSM在性能上显著优于广泛采用的基线方法，验证了其不仅在结构化任务中有效，而且在开放式的随机状态探索中同样具备强大表现力。"
  },
  {
    "date": "2026-02-05",
    "title": "ContextBench: A Benchmark for Context Retrieval in Coding Agents",
    "authors": "Han Li, Letian Zhu, Bohan Zhang, Rili Feng, Jiaming Wang, Yue Pan, Earl T. Barr, Sarro Federica, Zhaoyang Chu, He Ye",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05892v1",
    "source": "arXiv",
    "abstract": "LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval (\"The Bitter Lesson\" of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks. Data and code are available at: https://cioutn.github.io/context-bench/.",
    "title_zh": "ContextBench：面向编码智能体的上下文检索基准测试",
    "abstract_zh": "基于大语言模型（LLM）的编程代理在自动化问题修复基准测试中表现出色，但现有评估方法主要关注最终任务的成功率，难以揭示代理在解决问题过程中如何检索和使用代码上下文。为此，我们提出了ContextBench，一个面向上下文检索过程的评估基准。ContextBench包含来自8种编程语言、66个代码仓库的1,136个问题修复任务，每个任务均配有由人工标注的“黄金上下文”（gold contexts）。我们进一步构建了一个自动化评估框架，能够追踪代理在任务执行过程中的行为轨迹，并衡量其在问题解决过程中上下文的召回率、精确率和效率。\n\n利用ContextBench，我们评估了四种前沿LLM和五种编程代理。结果表明：复杂的代理架构设计仅带来微弱的上下文检索性能提升（即“编程代理的苦涩教训”）；LLM在实践中始终更倾向于高召回率而非高精确率；且代理在探索的上下文与实际使用的上下文之间存在显著差距。\n\nContextBench通过引入中间阶段的“黄金上下文”指标，弥补了现有端到端基准测试的不足，揭示了问题修复过程的内部机制。这些中间上下文信号为引导LLM在软件任务中的推理提供了宝贵线索。数据与代码已公开，访问地址为：https://cioutn.github.io/context-bench/。"
  },
  {
    "date": "2026-02-05",
    "title": "When Elo Lies: Hidden Biases in Codeforces-Based Evaluation of Large Language Models",
    "authors": "Shenyu Zheng, Ximing Dong, Xiaoshuang Liu, Gustavo Oliva, Chong Chun Yong, Dayi Lin, Boyuan Chen, Shaowei Wang, Ahmed E. Hassan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05891v1",
    "source": "arXiv",
    "abstract": "As Large Language Models (LLMs) achieve breakthroughs in complex reasoning, Codeforces-based Elo ratings have emerged as a prominent metric for evaluating competitive programming capabilities. However, these ratings are often reported without critical experimental details, leading to significant discrepancies illustrated by recent reports where the score of the same model version fluctuated by nearly 500 points. This paper presents a systematic empirical study on the hidden factors biasing Elo evaluations: (1) the temporal ordering of submissions, (2) contest difficulty selection, and (3) run to run stochastic variability of LLMs. Utilizing a controlled benchmark of 37 recent Codeforces contests and 13,691 generated test cases, we demonstrate that Elo scores are highly sensitive to these parameters. Our findings reveal that varying submission orders can shift scores by 394 points, while contest selection can cause differences of up to 1,122 points for the same model. Run to run performance exhibits substantial instability, with a maximum difference of 349 points in mean scores observed when evaluating identical contests. We conclude that direct Elo comparisons are unreliable and potentially misleading without strict standardization and transparent reporting of experimental settings.",
    "title_zh": "当 Elo 说谎：基于 Codeforces 的大语言模型评估中的隐藏偏差",
    "abstract_zh": "随着大型语言模型（LLMs）在复杂推理任务中取得突破性进展，基于Codeforces的Elo评分已成为评估编程竞赛能力的主流指标。然而，这些评分通常缺乏关键的实验细节，导致显著差异：近期报告指出，同一模型版本的得分在不同测试中波动幅度接近500分。本文系统性地研究了影响Elo评估结果的隐藏因素，包括：（1）提交的时间顺序，（2）比赛难度的选择，以及（3）LLM运行间的随机性波动。基于一个包含37场近期Codeforces比赛和13,691个生成测试用例的受控基准，我们证明Elo评分对这些参数极为敏感。研究发现，提交顺序的改变可导致评分波动达394分，而比赛选择的不同会使同一模型的得分差异高达1,122分。此外，相同比赛在多次运行中表现出显著的性能不稳定性，平均得分的最大差异达349分。我们得出结论：在缺乏严格标准化和透明实验设置报告的情况下，直接进行Elo评分比较是不可靠且可能产生误导的。"
  },
  {
    "date": "2026-02-05",
    "title": "VisRefiner: Learning from Visual Differences for Screenshot-to-Code Generation",
    "authors": "Jie Deng, Kaichun Yao, Libo Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05998v1",
    "source": "arXiv",
    "abstract": "Screenshot-to-code generation aims to translate user interface screenshots into executable frontend code that faithfully reproduces the target layout and style. Existing multimodal large language models perform this mapping directly from screenshots but are trained without observing the visual outcomes of their generated code. In contrast, human developers iteratively render their implementation, compare it with the design, and learn how visual differences relate to code changes. Inspired by this process, we propose VisRefiner, a training framework that enables models to learn from visual differences between rendered predictions and reference designs. We construct difference-aligned supervision that associates visual discrepancies with corresponding code edits, allowing the model to understand how appearance variations arise from implementation changes. Building on this, we introduce a reinforcement learning stage for self-refinement, where the model improves its generated code by observing both the rendered output and the target design, identifying their visual differences, and updating the code accordingly. Experiments show that VisRefiner substantially improves single-step generation quality and layout fidelity, while also endowing models with strong self-refinement ability. These results demonstrate the effectiveness of learning from visual differences for advancing screenshot-to-code generation.",
    "title_zh": "VisRefiner：从视觉差异中学习以实现截图到代码的生成",
    "abstract_zh": "截图转代码生成旨在将用户界面的截图转化为可执行的前端代码，以忠实还原目标布局与样式。现有的多模态大语言模型直接从截图中进行映射，但其训练过程中并未观察到生成代码的实际视觉效果。相比之下，人类开发者会通过迭代渲染实现结果、与设计稿进行对比，并学习视觉差异与代码修改之间的关系。受此过程启发，我们提出了VisRefiner——一种训练框架，使模型能够从渲染结果与参考设计之间的视觉差异中学习。我们构建了差异对齐的监督信号，将视觉差异与相应的代码修改关联起来，从而使模型理解外观变化如何源于实现上的调整。在此基础上，我们引入了强化学习阶段以实现自我优化：模型通过观察渲染输出与目标设计，识别两者间的视觉差异，并据此更新代码，逐步提升生成质量。实验结果表明，VisRefiner显著提升了单步生成的质量与布局保真度，同时赋予模型强大的自我优化能力。这些成果证明了从视觉差异中学习对于推动截图转代码生成技术发展的有效性。"
  },
  {
    "date": "2026-02-05",
    "title": "From Human-Human Collaboration to Human-Agent Collaboration: A Vision, Design Philosophy, and an Empirical Framework for Achieving Successful Partnerships Between Humans and LLM Agents",
    "authors": "Bingsheng Yao, Chaoran Chen, April Yi Wang, Sherry Tongshuang Wu, Toby Jia-jun Li, Dakuo Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05987v1",
    "source": "arXiv",
    "abstract": "The emergence of Large Language Model (LLM) agents enables us to build agent-based intelligent systems that move beyond the role of a \"tool\" to become genuine collaborators with humans, thereby realizing a novel human-agent collaboration paradigm. Our vision is that LLM agents should resemble remote human collaborators, which allows HCI researchers to ground the future exploration in decades of research on trust, awareness, and common ground in remote human collaboration, while also revealing the unique opportunities and challenges that emerge when one or more partners are AI agents. This workshop establishes a foundational research agenda for the new era by posing the question: How can the rich understanding of remote human collaboration inspire and inform the design and study of human-agent collaboration? We will bring together an interdisciplinary group from HCI, CSCW, and AI to explore this critical transition. The 180-minute workshop will be highly interactive, featuring a keynote speaker, a series of invited lightning talks, and an exploratory group design session where participants will storyboard novel paradigms of human-agent partnership. Our goal is to enlighten the research community by cultivating a shared vocabulary and producing a research agenda that charts the future of collaborative agents.",
    "title_zh": "从人与人协作到人与智能体协作：实现人类与大语言模型智能体成功合作的愿景、设计哲学与实证框架",
    "abstract_zh": "大型语言模型（LLM）代理的出现使我们能够构建基于代理的智能系统，这些系统不再仅仅扮演“工具”的角色，而是真正成为人类的合作者，从而实现一种全新的“人机协作”范式。我们的愿景是，LLM代理应如同远程的人类合作者一般，这使得人机交互（HCI）研究者能够基于数十年来关于远程人类协作中的信任、协同意识和共同基础的研究成果，为未来探索奠定基础，同时揭示当一方或多方合作者为AI代理时所出现的独特机遇与挑战。本次研讨会旨在确立新时代的基础研究议程，提出核心问题：对远程人类协作的深入理解，如何启发并指导人机协作的设计与研究？我们将汇聚来自人机交互（HCI）、计算机支持的协同工作（CSCW）和人工智能（AI）等领域的跨学科专家，共同探讨这一关键转变。本次180分钟的研讨会将高度互动，包含主题演讲、一系列特邀闪电演讲，以及一个探索性小组设计环节，参与者将共同绘制人机协作新模式的叙事板。我们的目标是通过建立共享术语体系，产出一份研究议程，为协作型智能代理的未来发展指明方向，从而启迪整个研究社区。"
  },
  {
    "date": "2026-02-05",
    "title": "SAGE: Benchmarking and Improving Retrieval for Deep Research Agents",
    "authors": "Tiansheng Hu, Yilun Zhao, Canyu Zhang, Arman Cohan, Chen Zhao",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05975v1",
    "source": "arXiv",
    "abstract": "Deep research agents have emerged as powerful systems for addressing complex queries. Meanwhile, LLM-based retrievers have demonstrated strong capability in following instructions or reasoning. This raises a critical question: can LLM-based retrievers effectively contribute to deep research agent workflows? To investigate this, we introduce SAGE, a benchmark for scientific literature retrieval comprising 1,200 queries across four scientific domains, with a 200,000 paper retrieval corpus.We evaluate six deep research agents and find that all systems struggle with reasoning-intensive retrieval. Using DR Tulu as backbone, we further compare BM25 and LLM-based retrievers (i.e., ReasonIR and gte-Qwen2-7B-instruct) as alternative search tools. Surprisingly, BM25 significantly outperforms LLM-based retrievers by approximately 30%, as existing agents generate keyword-oriented sub-queries. To improve performance, we propose a corpus-level test-time scaling framework that uses LLMs to augment documents with metadata and keywords, making retrieval easier for off-the-shelf retrievers. This yields 8% and 2% gains on short-form and open-ended questions, respectively.",
    "title_zh": "SAGE：深度研究智能体的检索基准测试与优化",
    "abstract_zh": "深度研究代理系统已发展成为应对复杂问题的强大工具。与此同时，基于大语言模型（LLM）的检索器在遵循指令或进行推理方面展现出强大的能力。这引发了一个关键问题：基于大语言模型的检索器能否有效融入深度研究代理的工作流程？为探究这一问题，我们提出了SAGE，一个科学文献检索基准测试集，涵盖四个科学领域的1200个查询，检索语料库包含20万篇论文。我们评估了六种深度研究代理系统，发现所有系统在需要复杂推理的检索任务中均表现不佳。以DR Tulu为基底模型，我们进一步比较了BM25与基于LLM的检索器（即ReasonIR和gte-Qwen2-7B-instruct）作为替代搜索工具的表现。令人意外的是，BM25的性能比基于LLM的检索器高出约30%，原因在于现有代理生成的子查询仍以关键词为导向。为提升性能，我们提出了一种基于语料库层面的测试时扩展框架，利用大语言模型为文档自动添加元数据和关键词，从而降低对现成检索器的检索难度。该方法在短文本问答和开放式问题上分别带来了8%和2%的性能提升。"
  },
  {
    "date": "2026-02-05",
    "title": "Can We Classify Flaky Tests Using Only Test Code? An LLM-Based Empirical Study",
    "authors": "Alexander Berndt, Vekil Bekmyradov, Rainer Gemulla, Marcus Kessel, Thomas Bach, Sebastian Baltes",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05465v1",
    "source": "arXiv",
    "abstract": "Flaky tests yield inconsistent results when they are repeatedly executed on the same code revision. They interfere with automated quality assurance of code changes and hinder efficient software testing. Previous work evaluated approaches to train machine learning models to classify flaky tests based on identifiers in the test code. However, the resulting classifiers have been shown to lack generalizability, hindering their applicability in practical environments. Recently, pre-trained Large Language Models (LLMs) have shown the capability to generalize across various tasks. Thus, they represent a promising approach to address the generalizability problem of previous approaches. In this study, we evaluated three LLMs (two general-purpose models, one code-specific model) using three prompting techniques on two benchmark datasets from prior studies on flaky test classification. Furthermore, we manually investigated 50 samples from the given datasets to determine whether classifying flaky tests based only on test code is feasible for humans. Our findings indicate that LLMs struggle to classify flaky tests given only the test code. The results of our best prompt-model combination were only marginally better than random guessing. In our manual analysis, we found that the test code does not necessarily contain sufficient information for a flakiness classification. Our findings motivate future work to evaluate LLMs for flakiness classification with additional context, for example, using retrieval-augmented generation or agentic AI.",
    "title_zh": "仅使用测试代码能否对不稳定的测试进行分类？一项基于大语言模型的实证研究",
    "abstract_zh": "间歇性测试在相同代码版本上反复执行时会产生不一致的结果，干扰代码变更的自动化质量保证，阻碍高效软件测试。以往的研究尝试通过训练机器学习模型，基于测试代码中的标识符来分类间歇性测试。然而，这些模型的分类器被证明缺乏泛化能力，限制了其在实际环境中的应用。最近，预训练的大语言模型（LLMs）展现出在多种任务中良好的泛化能力，因此为解决以往方法的泛化问题提供了有前景的途径。在本研究中，我们使用三种提示技术，在两个先前研究中用于间歇性测试分类的基准数据集上，评估了三种LLMs（两种通用模型，一种代码专用模型）。此外，我们手动分析了数据集中50个样本，以判断仅基于测试代码进行间歇性测试分类对人类是否可行。研究结果表明，仅凭测试代码，LLMs难以有效分类间歇性测试。我们最佳提示-模型组合的结果仅略优于随机猜测。在人工分析中，我们发现测试代码本身并不一定包含足够信息以进行准确的间歇性分类。这些发现为未来研究提供了方向：应探索在引入额外上下文信息（例如，通过检索增强生成或代理型AI）的情况下，评估LLMs在间歇性测试分类中的表现。"
  },
  {
    "date": "2026-02-05",
    "title": "MentorCollab: Selective Large-to-Small Inference-Time Guidance for Efficient Reasoning",
    "authors": "Haojin Wang, Yike Wang, Shangbin Feng, Hannaneh Hajishirzi, Yulia Tsvetkov",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05307v1",
    "source": "arXiv",
    "abstract": "Large reasoning models (LRMs) achieve strong performance by producing long chains of thought, but their inference costs are high and often generate redundant reasoning. Small language models (SLMs) are far more efficient, yet struggle on multi-step reasoning tasks. A natural idea is to let a large model guide a small one at inference time as a mentor, yet existing collaboration methods often promote imitation, resulting in verbose reasoning without consistent error correction. We propose MentorCollab, an inference-time collaboration method in which an LRM selectively and sparsely guides an SLM, rather than taking over generation. At randomly sampled token positions, we probe for divergences between the two models and use a lightweight verifier to decide whether the SLM should follow a short lookahead segment from its mentor or continue on its own. Across 15 SLM--LRM pairs and 3 domains (math reasoning, general knowledge, and commonsense reasoning), our method improves performance in 12 settings, with average gains of 3.0% and up to 8.0%, while adopting only having 18.4% tokens generated by the expensive mentor model on average. We find that short segments and selective probing are sufficient for effective collaboration. Our results show that selective inference-time guidance restores large-model reasoning ability without substantial inference overhead.",
    "title_zh": "MentorCollab：面向高效推理的择优式大模型到小模型推理时指导",
    "abstract_zh": "大型推理模型（LRMs）通过生成长串的思维链实现强大性能，但其推理成本高昂，且常产生冗余推理。小型语言模型（SLMs）则高效得多，但在多步推理任务上表现不佳。一个自然的想法是在推理时让大型模型作为导师指导小型模型，然而现有的协作方法往往导致模仿行为，造成冗长的推理过程，且缺乏一致的错误修正能力。我们提出MentorCollab，一种推理时的协作方法：大型模型（LRM）以选择性、稀疏的方式指导小型模型（SLM），而非完全接管生成过程。在随机采样的词元位置，我们检测两模型之间的分歧，并通过一个轻量级验证器判断SLM是否应采纳来自导师的简短前瞻片段，或继续自主生成。在15组SLM-LRM配对及3个领域（数学推理、通用知识、常识推理）中，我们的方法在12个设置中提升了性能，平均提升达3.0%，最高提升达8.0%，同时平均仅使用18.4%由昂贵的导师模型生成的词元。我们发现，短片段与选择性探测已足以实现高效协作。结果表明，选择性推理时的指导能够在不带来显著推理开销的前提下，恢复大型模型的推理能力。"
  },
  {
    "date": "2026-02-05",
    "title": "The Single-Multi Evolution Loop for Self-Improving Model Collaboration Systems",
    "authors": "Shangbin Feng, Kishan Panaganti, Yulia Tsvetkov, Wenhao Yu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05182v1",
    "source": "arXiv",
    "abstract": "Model collaboration -- systems where multiple language models (LMs) collaborate -- combines the strengths of diverse models with cost in loading multiple LMs. We improve efficiency while preserving the strengths of collaboration by distilling collaborative patterns into a single model, where the model is trained on the outputs of the model collaboration system. At inference time, only the distilled model is employed: it imitates the collaboration while only incurring the cost of a single model. Furthermore, we propose the single-multi evolution loop: multiple LMs collaborate, each distills from the collaborative outputs, and these post-distillation improved LMs collaborate again, forming a collective evolution ecosystem where models evolve and self-improve by interacting with an environment of other models. Extensive experiments with 7 collaboration strategies and 15 tasks (QA, reasoning, factuality, etc.) demonstrate that: 1) individual models improve by 8.0% on average, absorbing the strengths of collaboration while reducing the cost to a single model; 2) the collaboration also benefits from the stronger and more synergistic LMs after distillation, improving over initial systems without evolution by 14.9% on average. Analysis reveals that the single-multi evolution loop outperforms various existing evolutionary AI methods, is compatible with diverse model/collaboration/distillation settings, and helps solve problems where the initial model/system struggles to.",
    "title_zh": "单-多进化循环在自我改进模型协作系统中的应用",
    "abstract_zh": "模型协作——即多个语言模型（LMs）协同工作的系统——结合了不同模型的优势，但同时也带来了加载多个模型所带来的成本问题。我们通过将协作模式提炼为单一模型，实现了效率提升，同时保留了协作的优势：该单一模型在协作系统输出的基础上进行训练。在推理阶段，仅需使用这个经过提炼的模型，它能够模仿协作过程，同时仅承担单个模型的开销。此外，我们提出了“单-多进化循环”机制：多个语言模型先进行协作，每个模型再从协作输出中进行提炼，提炼后的模型再次参与协作，形成一个集体进化的生态系统，模型通过与其他模型的交互不断进化和自我提升。在7种协作策略和15项任务（包括问答、推理、事实性等）上的大量实验表明：1）单个模型平均提升8.0%，在吸收协作优势的同时，将成本降低至单个模型的水平；2）协作系统本身也因提炼后更强大、更具协同效应的模型而获益，相比未经过进化的初始系统，平均提升达14.9%。分析显示，单-多进化循环优于多种现有进化型AI方法，兼容多种模型、协作与提炼设置，并能有效解决初始模型或系统难以应对的问题。"
  },
  {
    "date": "2026-02-05",
    "title": "CoSA: Compressed Sensing-Based Adaptation of Large Language Models",
    "authors": "Songtao Wei, Yi Li, Bohan Zhang, Zhichun Guo, Ying Huang, Yuede Ji, Miao Yin, Guanpeng Li, Bingzhe Li",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.05148v1",
    "source": "arXiv",
    "abstract": "Parameter-Efficient Fine-Tuning (PEFT) has emerged as a practical paradigm for adapting large language models (LLMs) without updating all parameters. Most existing approaches, such as LoRA and PiSSA, rely on low-rank decompositions of weight updates. However, the low-rank assumption may restrict expressivity, particularly in task-specific adaptation scenarios where singular values are distributed relatively uniformly. To address this limitation, we propose CoSA (Compressed Sensing-Based Adaptation), a new PEFT method extended from compressed sensing theory. Instead of constraining weight updates to a low-rank subspace, CoSA expresses them through fixed random projection matrices and a compact learnable core. We provide a formal theoretical analysis of CoSA as a synthesis process, proving that weight updates can be compactly encoded into a low-dimensional space and mapped back through random projections. Extensive experimental results show that CoSA provides a principled perspective for efficient and expressive multi-scale model adaptation. Specifically, we evaluate CoSA on 10 diverse tasks, including natural language understanding and generation, employing 5 models of different scales from RoBERTa, Llama, and Qwen families. Across these settings, CoSA consistently matches or outperforms state-of-the-art PEFT methods.",
    "title_zh": "CoSA：基于压缩感知的大语言模型适配",
    "abstract_zh": "参数高效微调（PEFT）作为一种实用范式，使得在不更新所有参数的情况下适应大型语言模型（LLMs）成为可能。现有的大多数方法，如LoRA和PiSSA，依赖于权重更新的低秩分解。然而，低秩假设可能限制模型的表达能力，尤其是在任务特定适应场景中，当奇异值分布相对均匀时尤为明显。为解决这一局限性，我们提出了一种基于压缩感知理论的新PEFT方法——CoSA（基于压缩感知的适应）。与将权重更新限制在低秩子空间不同，CoSA通过固定的随机投影矩阵和一个紧凑的可学习核心来表达权重更新。我们从理论上对CoSA作为合成过程进行了形式化分析，证明权重更新可以被紧凑地编码到低维空间，并通过随机投影映射回原空间。大量实验结果表明，CoSA为高效且具有表达力的多尺度模型适应提供了严谨的理论视角。具体而言，我们在10个多样化的任务上评估了CoSA，涵盖自然语言理解与生成任务，使用了来自RoBERTa、Llama和Qwen系列的5种不同规模的模型。在这些设置中，CoSA始终达到或超越当前最先进的PEFT方法。"
  },
  {
    "date": "2026-2-5",
    "title": "Clean Code, Better Models: Enhancing LLM Performance with Smell-Cleaned Dataset",
    "authors": "Zhipeng Xue, Xiaoting Zhang, Zhipeng Gao, Xing Hu, Shan Gao, Xin Xia, Shanping Li",
    "publish": "ACM Transactions on Software Engineering and Methodology",
    "url": "https://doi.org/10.1145/3793252",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "整洁代码，更优模型：通过净化数据集提升大语言模型性能",
    "abstract_zh": "None"
  }
]