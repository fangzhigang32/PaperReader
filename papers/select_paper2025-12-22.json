[
  {
    "date": "2025-12-22",
    "title": "Semantically-Equivalent Transformations-Based Backdoor Attacks against Neural Code Models: Characterization and Mitigation",
    "authors": "Junyao Ye, Zhen Li, Xi Tang, Shouhuai Xu, Deqing Zou, Zhongsheng Yuan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.19215v1",
    "source": "arXiv",
    "abstract": "Neural code models have been increasingly incorporated into software development processes. However, their susceptibility to backdoor attacks presents a significant security risk. The state-of-the-art understanding focuses on injection-based attacks, which insert anomalous patterns into software code. These attacks can be neutralized by standard sanitization techniques. This status quo may lead to a false sense of security regarding backdoor attacks. In this paper, we introduce a new kind of backdoor attacks, dubbed Semantically-Equivalent Transformation (SET)-based backdoor attacks, which use semantics-preserving low-prevalence code transformations to generate stealthy triggers. We propose a framework to guide the generation of such triggers. Our experiments across five tasks, six languages, and models like CodeBERT, CodeT5, and StarCoder show that SET-based attacks achieve high success rates (often >90%) while preserving model utility. The attack proves highly stealthy, evading state-of-the-art defenses with detection rates on average over 25.13% lower than injection-based counterparts. We evaluate normalization-based countermeasures and find they offer only partial mitigation, confirming the attack's robustness. These results motivate further investigation into scalable defenses tailored to SET-based attacks.",
    "title_zh": "基于语义等价变换的后门攻击及其在神经代码模型中的表征与缓解",
    "abstract_zh": "神经码模型正越来越多地被融入软件开发流程中。然而，它们容易受到后门攻击的威胁，带来了显著的安全风险。目前最先进的研究主要集中在基于注入的攻击，这类攻击通过在软件代码中插入异常模式来实现。这些攻击通常可以通过标准的净化技术加以消除。然而，这种现状可能使人对后门攻击产生一种虚假的安全感。本文提出了一种新型后门攻击——语义等价变换（Semantically-Equivalent Transformation, SET）驱动的后门攻击，该攻击利用保持语义不变但低频出现的代码变换生成隐蔽的触发器。我们提出一个框架以指导此类触发器的生成。在五个任务、六种编程语言以及CodeBERT、CodeT5和StarCoder等模型上的实验表明，SET-based攻击在保持模型功能的前提下，取得了极高的成功率（通常超过90%）。该攻击表现出极强的隐蔽性，平均比基于注入的攻击在先进防御机制下的检测率低25.13%以上。我们评估了基于归一化的应对措施，发现其仅能提供部分缓解效果，进一步验证了该攻击的强大鲁棒性。这些结果促使我们进一步探索针对SET型攻击的可扩展防御机制。"
  },
  {
    "date": "2025-12-22",
    "title": "Recontextualization Mitigates Specification Gaming without Modifying the Specification",
    "authors": "Ariana Azarbal, Victor Gillioz, Vladimir Ivanov, Bryce Woodworth, Jacob Drori, Nevan Wichers, Aram Ebtekar, Alex Cloud, Alexander Matt Turner",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.19027v1",
    "source": "arXiv",
    "abstract": "Developers often struggle to specify correct training labels and rewards. Perhaps they don't need to. We propose recontextualization, which reduces how often language models \"game\" training signals, performing misbehaviors those signals mistakenly reinforce. We show recontextualization prevents models from learning to 1) prioritize evaluation metrics over chat response quality; 2) special-case code to pass incorrect tests; 3) lie to users; and 4) become sycophantic. Our method works by generating completions from prompts discouraging misbehavior and then recontextualizing them as though they were in response to prompts permitting misbehavior. Recontextualization trains language models to resist misbehavior even when instructions permit it. This mitigates the reinforcement of misbehavior from misspecified training signals, reducing specification gaming without improving the supervision signal.",
    "title_zh": "重新语境化在不修改规范的情况下缓解了规范规避问题",
    "abstract_zh": "开发者常常难以准确指定训练标签和奖励机制。也许他们根本不需要这么做。我们提出了“再语境化”（recontextualization）方法，该方法能够减少语言模型“钻训练信号的空子”的情况，避免模型学习到那些被错误强化的不当行为。我们证明，再语境化可以防止模型学会以下四类行为：1）优先考虑评估指标而非聊天回复质量；2）为通过错误测试而特殊处理代码；3）欺骗用户；4）一味讨好用户。该方法通过生成那些明确反对不当行为的补全内容，然后将其重新语境化为仿佛是针对允许不当行为的提示所作的回答。这种再语境化训练使语言模型即使在指令允许不当行为的情况下，也能抵抗这些行为。这种方法有效缓解了因训练信号设定不当而强化不良行为的问题，在不改进监督信号的前提下，减少了对规范的“钻空子”现象。"
  },
  {
    "date": "2025-12-22",
    "title": "DREAM: Dynamic Red-teaming across Environments for AI Models",
    "authors": "Liming Lu, Xiang Gu, Junyu Huang, Jiawei Du, Yunhuai Liu, Yongbin Zhou, Shuchao Pang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.19016v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) are increasingly used in agentic systems, where their interactions with diverse tools and environments create complex, multi-stage safety challenges. However, existing benchmarks mostly rely on static, single-turn assessments that miss vulnerabilities from adaptive, long-chain attacks. To fill this gap, we introduce DREAM, a framework for systematic evaluation of LLM agents against dynamic, multi-stage attacks. At its core, DREAM uses a Cross-Environment Adversarial Knowledge Graph (CE-AKG) to maintain stateful, cross-domain understanding of vulnerabilities. This graph guides a Contextualized Guided Policy Search (C-GPS) algorithm that dynamically constructs attack chains from a knowledge base of 1,986 atomic actions across 349 distinct digital environments. Our evaluation of 12 leading LLM agents reveals a critical vulnerability: these attack chains succeed in over 70% of cases for most models, showing the power of stateful, cross-environment exploits. Through analysis of these failures, we identify two key weaknesses in current agents: contextual fragility, where safety behaviors fail to transfer across environments, and an inability to track long-term malicious intent. Our findings also show that traditional safety measures, such as initial defense prompts, are largely ineffective against attacks that build context over multiple interactions. To advance agent safety research, we release DREAM as a tool for evaluating vulnerabilities and developing more robust defenses.",
    "title_zh": "DREAM：面向AI模型的跨环境动态对抗测试",
    "abstract_zh": "大型语言模型（LLMs）在智能体系统中的应用日益广泛，其与多种工具和环境的交互带来了复杂且多阶段的安全挑战。然而，现有的评估基准大多依赖静态、单轮次的测试，难以捕捉自适应、长链攻击所暴露的漏洞。为填补这一空白，我们提出了DREAM框架，用于系统性地评估LLM智能体在动态、多阶段攻击下的表现。DREAM的核心是一个跨环境对抗知识图谱（CE-AKG），该图谱能够持续维护对跨领域漏洞的状态化理解。基于此图谱，我们设计了上下文感知的引导策略搜索算法（C-GPS），从包含1,986个原子动作、覆盖349种不同数字环境的知识库中动态构建攻击链。对12个主流LLM智能体的评估结果显示，这些攻击链在大多数模型上成功率超过70%，凸显了状态化、跨环境攻击的强大威力。通过对失败案例的深入分析，我们识别出当前智能体存在的两大关键缺陷：一是上下文脆弱性，即安全行为无法在不同环境中有效迁移；二是缺乏对长期恶意意图的追踪能力。此外，我们的研究还表明，传统的安全措施（如初始防御提示）在面对通过多轮交互逐步建立上下文的攻击时，基本无效。为推动智能体安全研究的发展，我们公开发布DREAM，作为评估漏洞和开发更鲁棒防御机制的工具。"
  },
  {
    "date": "2025-12-22",
    "title": "Modular Layout Synthesis (MLS): Front-end Code via Structure Normalization and Constrained Generation",
    "authors": "Chong Liu, Ming Zhang, Fei Li, Hao Zhou, Xiaoshuang Chen, Ye Yuan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.18996v1",
    "source": "arXiv",
    "abstract": "Automated front-end engineering drastically reduces development cycles and minimizes manual coding overhead. While Generative AI has shown promise in translating designs to code, current solutions often produce monolithic scripts, failing to natively support modern ecosystems like React, Vue, or Angular. Furthermore, the generated code frequently suffers from poor modularity, making it difficult to maintain. To bridge this gap, we introduce Modular Layout Synthesis (MLS), a hierarchical framework that merges visual understanding with structural normalization. Initially, a visual-semantic encoder maps the screen capture into a serialized tree topology, capturing the essential layout hierarchy. Instead of simple parsing, we apply heuristic deduplication and pattern recognition to isolate reusable blocks, creating a framework-agnostic schema. Finally, a constraint-based generation protocol guides the LLM to synthesize production-ready code with strict typing and component props. Evaluations show that MLS significantly outperforms existing baselines, ensuring superior code reusability and structural integrity across multiple frameworks",
    "title_zh": "模块化布局合成（MLS）：通过结构归一化与约束生成的前端代码",
    "abstract_zh": "自动化前端工程大幅缩短了开发周期，并显著降低了手动编码的负担。尽管生成式AI在将设计转化为代码方面展现出巨大潜力，但现有解决方案往往生成的是结构臃肿的单一脚本，无法原生支持React、Vue或Angular等现代前端生态。此外，生成的代码通常模块性差，难以维护。为弥合这一差距，我们提出了一种分层框架——模块化布局合成（Modular Layout Synthesis, MLS），该框架融合了视觉理解与结构规范化技术。首先，一个视觉语义编码器将屏幕截图映射为序列化的树状拓扑结构，精准捕捉布局的层级关系。不同于简单的解析方式，我们采用启发式去重和模式识别技术，提取可复用的代码块，构建出与框架无关的抽象 schema。最后，基于约束的生成协议引导大语言模型（LLM）生成具备严格类型检查和组件属性定义的生产级代码。评估结果表明，MLS显著优于现有基线方法，在多个前端框架中均实现了更高的代码复用率与更强的结构完整性。"
  },
  {
    "date": "2025-12-22",
    "title": "Binary Neural Network Implementation for Handwritten Digit Recognition on FPGA",
    "authors": "Emir Devlet Ertörer, Cem Ünsalan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.19304v1",
    "source": "arXiv",
    "abstract": "Binary neural networks provide a promising solution for low-power, high-speed inference by replacing expensive floating-point operations with bitwise logic. This makes them well-suited for deployment on resource-constrained platforms such as FPGAs. In this study, we present a fully custom BNN inference accelerator for handwritten digit recognition, implemented entirely in Verilog without the use of high-level synthesis tools. The design targets the Xilinx Artix-7 FPGA and achieves real-time classification at 80\\,MHz with low power consumption and predictable timing. Simulation results demonstrate 84\\% accuracy on the MNIST test set and highlight the advantages of manual HDL design for transparent, efficient, and flexible BNN deployment in embedded systems. The complete project including training scripts and Verilog source code are available at GitHub repo for reproducibility and future development.",
    "title_zh": "基于FPGA的手写数字识别二值神经网络实现",
    "abstract_zh": "二值神经网络通过将昂贵的浮点运算替换为位级逻辑运算，为低功耗、高速推理提供了一种有前景的解决方案。这使其非常适合部署在资源受限的平台（如FPGA）上。在本研究中，我们提出了一种完全定制的BNN推理加速器，用于手写数字识别，整个设计均使用Verilog实现，未采用高层次综合工具。该设计针对Xilinx Artix-7 FPGA，可在80 MHz频率下实现实时分类，具有低功耗和可预测的时序特性。仿真结果表明，在MNIST测试集上达到了84%的准确率，充分展示了手动HDL设计在嵌入式系统中实现透明、高效且灵活的BNN部署的优势。完整的项目代码（包括训练脚本和Verilog源码）已发布于GitHub仓库，以确保可复现性并支持未来开发。"
  },
  {
    "date": "2025-12-22",
    "title": "When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models",
    "authors": "Michael S. Zhang, Rishi A. Ruia, Arnav Kewalram, Saathvik Dharmapuram, Utkarsh Sharma, Kevin Zhu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.18934v1",
    "source": "arXiv",
    "abstract": "Catastrophic forgetting poses a fundamental challenge in continual learning, particularly when models are quantized for deployment efficiency. We systematically investigate the interplay between quantization precision (FP16, INT8, INT4) and replay buffer strategies in large language models, revealing unexpected dynamics. While FP16 achieves superior initial task performance (74.44% on NLU), we observe a striking inversion on subsequent tasks: quantized models outperform FP16 by 8-15% on final task forward accuracy, with INT4 achieving nearly double FP16's performance on Code generation (40% vs 20%). Critically, even minimal replay buffers (0.1%) dramatically improve retention - increasing NLU retention after Math training from 45% to 65% across all precision levels - with INT8 consistently achieving the optimal balance between learning plasticity and knowledge retention. We hypothesize that quantization-induced noise acts as implicit regularization, preventing the overfitting to new task gradients that plagues high-precision models. These findings challenge the conventional wisdom that higher precision is always preferable, suggesting instead that INT8 quantization offers both computational efficiency and superior continual learning dynamics. Our results provide practical guidelines for deploying compressed models in continual learning scenarios: small replay buffers (1-2%) suffice for NLU tasks, while Math and Code benefit from moderate buffers (5-10%), with quantized models requiring less replay than FP16 to achieve comparable retention. Code is available at https://github.com/Festyve/LessIsMore.",
    "title_zh": "少即是多：8位量化提升大语言模型的持续学习能力",
    "abstract_zh": "灾难性遗忘是持续学习中的一个根本性挑战，尤其是在模型量化以提升部署效率时。我们系统地研究了大语言模型中量化精度（FP16、INT8、INT4）与回放缓冲区策略之间的相互作用，揭示出一些出人意料的动态现象。尽管FP16在初始任务上表现出更优的性能（NLU任务达到74.44%），但在后续任务中却出现了显著反转：量化模型在最终任务的前向准确率上比FP16高出8%-15%，其中INT4在代码生成任务上的表现几乎达到FP16的两倍（40% vs 20%）。关键的是，即使极小的回放缓冲区（仅0.1%）也能显著改善知识保留——在完成数学任务后，NLU任务的保留率从45%提升至65%，且这一效果在所有精度级别下均成立；而INT8始终在学习可塑性与知识保留之间保持最佳平衡。我们推测，量化引入的噪声起到了隐式的正则化作用，有效防止了高精度模型因过度拟合新任务梯度而导致的过拟合问题。这些发现挑战了“精度越高越优”的传统观念，表明INT8量化不仅具备计算效率优势，还能带来更优越的持续学习性能。我们的研究结果为压缩模型在持续学习场景中的实际部署提供了实用指导：对于NLU任务，1%-2%的小型回放缓冲区已足够；而对于数学和代码任务，则建议使用中等规模的缓冲区（5%-10%），且量化模型所需回放量远低于FP16即可实现相当的知识保留效果。代码已开源，地址为 https://github.com/Festyve/LessIsMore。"
  },
  {
    "date": "2025-12-22",
    "title": "Beyond Language Boundaries: Uncovering Programming Language Families for Code Language Models",
    "authors": "Shangbo Yun, Xiaodong Gu, Jianghong Huang, Beijun Shen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.19509v1",
    "source": "arXiv",
    "abstract": "The rapid proliferation of diverse programming languages presents both opportunities and challenges for developing multilingual code LLMs. While existing techniques often train code LLMs by simply aggregating multilingual code data, few explore the deeper relationships between programming languages(PLs) and how such relationships can be utilized to optimize the training and inference of code LLMs. In this work, we investigate 2 fundamental questions: 1) What are the deep linguistic relationships among PLs? and 2) How can these relationships be leveraged to improve multilingual code LLMs? We propose an embedding-based framework to uncover the latent families of PLs. Our approach begins by defining 21 primary linguistic features of programming languages, such as variable definition, control structures, and method declarations, and then employs LLMs to generate feature-aligned code samples across multiple languages. By embedding these semantically parallel code snippets from 19 languages, we construct a similarity matrix and perform hierarchical clustering to uncover inherent language relationships. Our analysis reveals clear hierarchical structures among programming languages. Closely related languages form well-defined clusters (e.g., C, C++, Java, and Swift group together), while Go exhibits as a central language with the highest cross-language similarity. Building on the uncovered language families, we propose three strategies to enhance multilingual LLM training: transfer learning across linguistically related languages, linguistic proximity-guided curriculum learning, and centroid-based intermediary code translation. Experiments on 4 code intelligence tasks demonstrate that our methods significantly improve multilingual LLM performance. This work offers a universal perspective on programming languages and advances more effective strategies for multilingual code LLM training.",
    "title_zh": "超越语言界限：为代码语言模型揭示编程语言家族",
    "abstract_zh": "编程语言的快速多样化为多语言代码大模型（LLM）的开发带来了机遇与挑战。尽管现有方法通常通过简单聚合多语言代码数据来训练代码LLM，但很少深入探讨编程语言（PLs）之间的深层关系，以及如何利用这些关系来优化代码LLM的训练与推理过程。在本研究中，我们探讨两个核心问题：1）编程语言之间存在哪些深层次的语言学关系？2）这些关系如何被用于提升多语言代码LLM的性能？为此，我们提出了一种基于嵌入的框架，以揭示编程语言的潜在家族结构。\n\n我们的方法首先定义了编程语言的21个基本语言特征，如变量定义、控制结构和方法声明等，并利用大语言模型生成跨多种语言的语义对齐代码样本。通过对来自19种编程语言的语义平行代码片段进行嵌入表示，我们构建了一个相似性矩阵，并采用层次聚类分析来揭示编程语言间的内在关联。分析结果表明，编程语言呈现出清晰的层级结构：语义相近的语言形成明确的聚类（例如C、C++、Java和Swift构成一组），而Go则表现出中心化特征，具有最高的跨语言相似性。\n\n基于所揭示的语言家族结构，我们提出了三种策略以增强多语言LLM的训练效果：1）在语言学上相关的语言间进行迁移学习；2）基于语言亲缘性的课程学习（curriculum learning）；3）基于质心的中间代码翻译机制。在4项代码智能任务上的实验表明，我们的方法显著提升了多语言LLM的性能。本工作为编程语言提供了统一的视角，推动了更高效、更有效的多语言代码LLM训练策略的发展。"
  },
  {
    "date": "2025-12-22",
    "title": "More code, less validation: Risk factors for over-reliance on AI coding tools among scientists",
    "authors": "Gabrielle O'Brien, Alexis Parker, Nasir Eisty, Jeffrey Carver",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.19644v1",
    "source": "arXiv",
    "abstract": "Programming is essential to modern scientific research, yet most scientists report inadequate training for the software development their work demands. Generative AI tools capable of code generation may support scientific programmers, but user studies indicate risks of over-reliance, particularly among inexperienced users. We surveyed 868 scientists who program, examining adoption patterns, tool preferences, and factors associated with perceived productivity. Adoption is highest among students and less experienced programmers, with variation across fields. Scientific programmers overwhelmingly prefer general-purpose conversational interfaces like ChatGPT over developer-specific tools. Both inexperience and limited use of development practices (like testing, code review, and version control) are associated with greater perceived productivity-but these factors interact, suggesting formal practices may partially compensate for inexperience. The strongest predictor of perceived productivity is the number of lines of generated code typically accepted at once. These findings suggest scientific programmers using generative AI may gauge productivity by code generation rather than validation, raising concerns about research code integrity.",
    "title_zh": "更多代码，更少验证：科学家过度依赖AI编程工具的风险因素",
    "abstract_zh": "编程对于现代科学研究至关重要，然而大多数科学家表示，他们在软件开发方面的培训严重不足，无法满足工作需求。能够生成代码的生成式人工智能工具或许能为科学编程人员提供帮助，但用户研究表明，尤其是经验较少的使用者存在过度依赖的风险。我们对868名从事编程的科学家进行了调查，研究了他们对工具的采用模式、偏好以及与感知工作效率相关的因素。结果显示，学生和经验较少的程序员在使用这些工具方面最为积极，不同学科领域之间也存在差异。科学编程人员普遍更倾向于使用通用型对话式界面（如ChatGPT），而非专为开发者设计的工具。无论是编程经验不足，还是较少使用开发实践（如测试、代码审查和版本控制），都与更高的感知工作效率相关；但这些因素之间存在交互作用，表明正式的开发实践可在一定程度上弥补经验的不足。感知工作效率最强的预测因子是每次通常接受的生成代码行数。这些发现表明，使用生成式AI的科学编程人员可能以代码生成量来衡量效率，而非代码验证，这引发了对科研代码质量与完整性的担忧。"
  },
  {
    "date": "2025-12-22",
    "title": "Increasing the Thinking Budget is Not All You Need",
    "authors": "Ignacio Iacobacci, Zhaozhi Qian, Faroq AL-Tam, Muhammad AL-Qurishi, Riad Souissi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.19585v1",
    "source": "arXiv",
    "abstract": "Recently, a new wave of thinking-capable Large Language Models has emerged, demonstrating exceptional capabilities across a wide range of reasoning benchmarks. Early studies have begun to explore how the amount of compute in terms of the length of the reasoning process, the so-called thinking budget, impacts model performance. In this work, we propose a systematic investigation of the thinking budget as a key parameter, examining its interaction with various configurations such as self-consistency, reflection, and others. Our goal is to provide an informative, balanced comparison framework that considers both performance outcomes and computational cost. Among our findings, we discovered that simply increasing the thinking budget is not the most effective use of compute. More accurate responses can instead be achieved through alternative configurations, such as self-consistency and self-reflection.",
    "title_zh": "增加思维预算并非万能之策",
    "abstract_zh": "最近，具备思维能力的大规模语言模型掀起了一股新潮流，在各类推理基准测试中展现出卓越的能力。早期研究已开始探索计算资源（以推理过程的长度来衡量，即所谓的“思维预算”）对模型性能的影响。在本项工作中，我们系统性地将思维预算作为一个关键参数进行研究，考察其与多种配置（如自一致性、反思等）之间的相互作用。我们的目标是建立一个信息丰富且平衡的比较框架，兼顾性能结果与计算成本。研究发现，单纯增加思维预算并非计算资源的最佳利用方式；通过采用其他配置策略，例如自一致性与自反思，反而能够获得更准确的响应。"
  },
  {
    "date": "2025-12-22",
    "title": "A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis",
    "authors": "Katharina Stengg, Christian Macho, Martin Pinzger",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.19481v1",
    "source": "arXiv",
    "abstract": "Understanding source code changes and their impact on other code entities is a crucial skill in software development. However, the analysis of code changes and their impact is often performed manually and therefore is time-consuming. Recent advancements in AI, and in particular large language models (LLMs) show promises to help developers in various code analysis tasks. However, the extent to which this potential can be utilized for understanding code changes and their impact is underexplored. To address this gap, we study the capabilities of GPT-5 and GPT-5-mini to predict the code entities impacted by given source code changes. We construct a dataset containing information about seed-changes, change pairs, and change types for each commit. Existing datasets lack crucial information about seed changes and impacted code entities. Our experiments evaluate the LLMs in two configurations: (1) seed-change information and the parent commit tree and (2) seed-change information, the parent commit tree, and the diff hunk of each seed change. We found that both LLMs perform poorly in the two experiments, whereas GPT-5 outperforms GPT-5-mini. Furthermore, the provision of the diff hunks helps both models to slightly improve their performance.",
    "title_zh": "一个关于使用GPT-5进行代码变更影响分析的数据集及初步研究",
    "abstract_zh": "理解源代码变更及其对其他代码实体的影响是软件开发中的一项关键技能。然而，代码变更及其影响的分析通常依赖人工完成，因此耗时较长。近年来，人工智能（AI）尤其是大型语言模型（LLMs）在多个代码分析任务中展现出巨大潜力，有望帮助开发者提升效率。但目前对于如何充分挖掘这些模型在理解代码变更及其影响方面的潜力，仍缺乏深入探索。为填补这一空白，我们研究了GPT-5和GPT-5-mini在预测给定源代码变更所影响的代码实体方面的能力。我们构建了一个数据集，其中包含每个提交的种子变更（seed-change）、变更对（change pairs）以及变更类型的信息。现有数据集普遍缺少关于种子变更及受影响代码实体的关键信息。我们的实验评估了LLM在两种配置下的表现：（1）仅提供种子变更信息和父提交树；（2）在上述基础上，额外提供每个种子变更的差异片段（diff hunk）。实验结果表明，两种配置下，两个模型的表现均不理想，但GPT-5的表现优于GPT-5-mini。此外，提供差异片段信息使两个模型的性能均有轻微提升。"
  },
  {
    "date": "2025-12-22",
    "title": "Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems",
    "authors": "Prathamesh Devadiga",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.19250v1",
    "source": "arXiv",
    "abstract": "Traditional auto-parallelizing compilers, reliant on rigid heuristics, struggle with the complexity of modern heterogeneous systems. This paper presents a comprehensive evaluation of small (approximately 1B parameter) language-model-driven compiler auto-parallelization. We evaluate three models: gemma3, llama3.2, and qwen2.5, using six reasoning strategies across 11 real-world kernels drawn from scientific computing, graph algorithms, and machine learning. Our system is benchmarked against strong compiler baselines, including LLVM Polly, TVM, and Triton. Across 376 total evaluations, the proposed approach achieves an average speedup of 6.81x and a peak performance of 43.25x on convolution operations. We analyze scalability, verify correctness using multiple sanitizers, and confirm robustness across diverse compilers and hardware platforms. Our results demonstrate that small, efficient language models can serve as powerful reasoning engines for complex compiler optimization tasks.",
    "title_zh": "小语言模型作为编译器专家：面向异构系统的自动并行化",
    "abstract_zh": "传统自动并行化编译器依赖于僵化的启发式规则，在应对现代异构系统复杂性时面临挑战。本文对小型（约10亿参数）语言模型驱动的编译器自动并行化进行了全面评估。我们评估了三种模型：Gemma3、Llama3.2 和 Qwen2.5，采用六种不同的推理策略，针对来自科学计算、图算法和机器学习领域的11个真实世界内核进行测试。我们的系统与多个强大的编译器基准（包括 LLVM Polly、TVM 和 Triton）进行了对比。在总计376次评估中，所提出的方法平均实现了6.81倍的加速，而在卷积操作上达到了最高43.25倍的性能提升。我们分析了系统的可扩展性，通过多种内存检测工具验证了正确性，并确认了其在不同编译器和硬件平台上的鲁棒性。结果表明，小型且高效的语言模型可作为复杂编译器优化任务的强大推理引擎。"
  },
  {
    "date": "2025-12-22",
    "title": "Can abstract concepts from LLM improve SLM performance?",
    "authors": "Siddharth Tandon",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.19069v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) excel at diverse tasks, but their deployment on resource-constrained devices remains challenging. Existing methods like quantization, pruning, and distillation can reduce memory footprint but often demand extensive experimentation and careful infrastructure design. Leveraging existing techniques for extracting high-level concepts (represented as steering vectors) from larger models, we investigate their transferability to smaller language models (SLM) during inference. We demonstrate through extensive experimentation that these concepts can be effectively transferred to smaller models, irrespective of their family (e.g., Phi, Llama, Qwen), leading to performance improvements across a wide range of tasks. Furthermore, we introduce inference-time scaling to enhance performance by dynamically adjusting the steering intensity which has resulted in a 7-15\\% of accuracy improvement for Qwen3-0.6B.",
    "title_zh": "大语言模型（LLM）中的抽象概念能否提升小语言模型（SLM）的性能？",
    "abstract_zh": "大型语言模型（LLMs）在多种任务中表现出色，但其在资源受限设备上的部署仍面临挑战。现有的量化、剪枝和蒸馏等方法虽能降低内存占用，但通常需要大量实验以及精细的基础设施设计。我们利用现有技术从大模型中提取高层次概念（以控制向量的形式表示），并研究这些概念在推理阶段向小型语言模型（SLM）迁移的可行性。通过大量实验，我们证明了这些概念可有效迁移到小型模型中，无论其模型家族（如Phi、Llama、Qwen）如何，均能在广泛的任务上带来性能提升。此外，我们引入了推理时的动态缩放机制，通过实时调整控制强度来增强模型表现，使Qwen3-0.6B模型的准确率提升了7%-15%。"
  },
  {
    "date": "2025-12-22",
    "title": "Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement",
    "authors": "Saman Forouzandeh, Wei Peng, Parham Moradi, Xinghuo Yu, Mahdi Jalili",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.18950v1",
    "source": "arXiv",
    "abstract": "We present MACLA, a framework that decouples reasoning from learning by maintaining a frozen large language model while performing all adaptation in an external hierarchical procedural memory. MACLA extracts reusable procedures from trajectories, tracks reliability via Bayesian posteriors, selects actions through expected-utility scoring, and refines procedures by contrasting successes and failures. Across four benchmarks (ALFWorld, WebShop, TravelPlanner, InterCodeSQL), MACLA achieves 78.1 percent average performance, outperforming all baselines. On ALFWorld unseen tasks, MACLA reaches 90.3 percent with 3.1 percent positive generalization. The system constructs memory in 56 seconds, 2800 times faster than the state-of-the-art LLM parameter-training baseline, compressing 2851 trajectories into 187 procedures. Experimental results demonstrate that structured external memory with Bayesian selection and contrastive refinement enables sample-efficient, interpretable, and continually improving agents without LLM parameter updates.",
    "title_zh": "通过贝叶斯选择与对比精炼学习用于大语言模型智能体的分层过程记忆",
    "abstract_zh": "我们提出MACLA，一种将推理与学习解耦的框架。该框架通过保持一个冻结的大语言模型，并在外部分层过程记忆中完成所有适应性调整。MACLA从轨迹中提取可复用的程序，利用贝叶斯后验追踪其可靠性，通过期望效用评分选择动作，并通过对比成功与失败来优化程序。在四个基准测试（ALFWorld、WebShop、TravelPlanner、InterCodeSQL）中，MACLA实现了78.1%的平均性能，优于所有基线方法。在ALFWorld的未见任务上，MACLA达到了90.3%的性能，实现了3.1%的正向泛化。系统仅用56秒构建记忆，比当前最先进的LLM参数训练基线快2800倍，将2851条轨迹压缩为187个程序。实验结果表明，结合贝叶斯选择与对比优化的结构化外部记忆，能够在不更新LLM参数的情况下，实现样本高效、可解释且持续改进的智能体。"
  },
  {
    "date": "2025-12-22",
    "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
    "authors": "Jiacheng Guo, Ling Yang, Peter Chen, Qixin Xiao, Yinjie Wang, Xinzhe Juan, Jiahao Qiu, Ke Shen, Mengdi Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.19682v1",
    "source": "arXiv",
    "abstract": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $α$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.",
    "title_zh": "GenEnv：大语言模型智能体与环境模拟器之间的难度对齐协同进化",
    "abstract_zh": "训练具备能力的大型语言模型（LLM）智能体，其关键瓶颈在于真实世界交互数据成本高昂且静态不变。为此，我们提出了GenEnv框架，建立了一个难度对齐的协同进化博弈机制，使智能体与可扩展的生成式环境模拟器共同演进。与传统方法在静态数据集上训练模型不同，GenEnv实现了“数据动态演化”：模拟器充当动态课程策略，持续生成针对智能体“最近发展区”的定制化任务。这一过程由一种简单而有效的$α$-课程奖励机制引导，确保任务难度始终与智能体当前能力相匹配。我们在五个基准测试（包括API-Bank、ALFWorld、BFCL、Bamboogle和TravelPlanner）上评估了GenEnv的表现。在这些任务中，GenEnv相较于7B规模的基线模型，性能提升最高达**+40.3%**，并达到或超越了更大模型的平均表现。相比基于Gemini 2.5 Pro的离线数据增强方法，GenEnv在使用数据量减少3.3倍的情况下仍取得了更优性能。通过从静态监督转向自适应模拟，GenEnv为高效扩展智能体能力提供了一条数据高效的路径。"
  },
  {
    "date": "2025-12-22",
    "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
    "authors": "Yuqiao Tan, Minzheng Wang, Shizhu He, Huanxuan Liao, Chengfeng Zhao, Qiunan Lu, Tian Liang, Jun Zhao, Kang Liu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.19673v1",
    "source": "arXiv",
    "abstract": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.",
    "title_zh": "自下而上的策略优化：您的语言模型策略中秘密包含内部策略",
    "abstract_zh": "现有的强化学习（RL）方法将大语言模型（LLMs）视为一个单一的整体策略，忽略了其内部机制。因此，理解策略在不同层与模块间的演化过程，对于实现更精准的优化以及揭示复杂的推理机制至关重要。本文通过利用Transformer残差流的内在分解特性，以及隐藏状态组合与反嵌入矩阵之间的等价性所对应的可采样策略，对语言模型的策略进行了分解。该分解揭示了“内部层策略”——对应于各层的贡献——以及“内部模块策略”，这些策略与每层中的自注意力机制和前馈网络（FFN）组件相对应。通过对内部策略熵的分析，我们发现：(a) 早期层保持较高的熵以促进探索，而顶层则趋于接近零熵，以实现精细化调整，且不同模型系列的收敛模式存在差异；(b) Llama模型在最后一层的预测空间迅速收敛，而Qwen系列模型，尤其是Qwen3，则展现出更接近人类的、逐步结构化的推理模式。基于上述发现，我们提出了自下而上的策略优化（Bottom-up Policy Optimization, BuPO），一种新型的强化学习范式，能够在训练初期直接优化内部层策略。通过在低层对齐训练目标，BuPO重构了基础推理能力，并取得了卓越的性能表现。在复杂推理基准上的大量实验验证了本方法的有效性。我们的代码已开源，地址为：https://github.com/Trae1ounG/BuPO。"
  },
  {
    "date": "2025-12-22",
    "title": "CodeSimpleQA: Scaling Factuality in Code Large Language Models",
    "authors": "Jian Yang, Wei Zhang, Yizhi Li, Shawn Guo, Haowen Wang, Aishan Liu, Ge Zhang, Zili Wang, Zhoujun Li, Xianglong Liu, Weifeng Lv",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.19424v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) have made significant strides in code generation, achieving impressive capabilities in synthesizing code snippets from natural language instructions. However, a critical challenge remains in ensuring LLMs generate factually accurate responses about programming concepts, technical implementations, etc. Most previous code-related benchmarks focus on code execution correctness, overlooking the factual accuracy of programming knowledge. To address this gap, we present CodeSimpleQA, a comprehensive bilingual benchmark designed to evaluate the factual accuracy of code LLMs in answering code-related questions, which contains carefully curated question-answer pairs in both English and Chinese, covering diverse programming languages and major computer science domains. Further, we create CodeSimpleQA-Instruct, a large-scale instruction corpus with 66M samples, and develop a post-training framework combining supervised fine-tuning and reinforcement learning. Our comprehensive evaluation of diverse LLMs reveals that even frontier LLMs struggle with code factuality. Our proposed framework demonstrates substantial improvements over the base model, underscoring the critical importance of factuality-aware alignment in developing reliable code LLMs.",
    "title_zh": "CodeSimpleQA：在代码大语言模型中提升事实性",
    "abstract_zh": "大型语言模型（LLMs）在代码生成方面取得了显著进展，能够从自然语言指令中合成出高质量的代码片段。然而，确保LLM在编程概念、技术实现等方面生成内容的客观准确性仍是一个关键挑战。以往大多数与代码相关的评估基准主要关注代码执行的正确性，而忽视了编程知识本身的事实准确性。为弥补这一空白，我们提出了CodeSimpleQA——一个全面的双语评估基准，旨在衡量代码LLM在回答代码相关问题时的事实准确性。该基准包含精心设计的中英文问答对，覆盖多种编程语言及计算机科学的主要领域。此外，我们构建了包含6600万样本的大规模指令数据集CodeSimpleQA-Instruct，并开发了一种结合监督微调与强化学习的后训练框架。对多种LLM的综合评估表明，即使前沿模型在代码事实性方面仍存在明显不足。我们提出的框架相较于基础模型表现出显著提升，凸显了在构建可靠代码LLM过程中，注重事实准确性的对齐机制至关重要。"
  },
  {
    "date": "2025-12-22",
    "title": "Generation of Programmatic Rules for Document Forgery Detection Using Large Language Models",
    "authors": "Valentin Schmidberger, Manuel Eberhardinger, Setareh Maghsudi, Johannes Maucher",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.19228v1",
    "source": "arXiv",
    "abstract": "Document forgery poses a growing threat to legal, economic, and governmental processes, requiring increasingly sophisticated verification mechanisms. One approach involves the use of plausibility checks, rule-based procedures that assess the correctness and internal consistency of data, to detect anomalies or signs of manipulation. Although these verification procedures are essential for ensuring data integrity, existing plausibility checks are manually implemented by software engineers, which is time-consuming. Recent advances in code generation with large language models (LLMs) offer new potential for automating and scaling the generation of these checks. However, adapting LLMs to the specific requirements of an unknown domain remains a significant challenge. This work investigates the extent to which LLMs, adapted on domain-specific code and data through different fine-tuning strategies, can generate rule-based plausibility checks for forgery detection on constrained hardware resources. We fine-tune open-source LLMs, Llama 3.1 8B and OpenCoder 8B, on structured datasets derived from real-world application scenarios and evaluate the generated plausibility checks on previously unseen forgery patterns. The results demonstrate that the models are capable of generating executable and effective verification procedures. This also highlights the potential of LLMs as scalable tools to support human decision-making in security-sensitive contexts where comprehensibility is required.",
    "title_zh": "基于大语言模型的文档伪造检测程序规则生成",
    "abstract_zh": "文件伪造对法律、经济和政府流程构成了日益严重的威胁，亟需更加复杂的验证机制。一种方法是采用合理性检查（plausibility checks），即基于规则的程序，用于评估数据的正确性与内部一致性，以检测异常或篡改迹象。尽管这些验证程序对于保障数据完整性至关重要，但现有的合理性检查通常由软件工程师手动实现，耗时费力。近年来，大型语言模型（LLMs）在代码生成方面的进展为自动化并规模化生成此类检查提供了新的可能。然而，将LLMs适配到未知领域的特定需求仍面临重大挑战。本文研究了通过不同微调策略，在领域特定代码与数据上进行调整的LLMs，在资源受限硬件条件下生成基于规则的合理性检查以检测伪造行为的能力。我们对开源LLM模型Llama 3.1 8B和OpenCoder 8B在源自真实应用场景的结构化数据集上进行了微调，并在先前未见过的伪造模式上评估了生成的合理性检查。结果表明，这些模型能够生成可执行且有效的验证程序。这同时也凸显了LLMs作为可扩展工具的巨大潜力，可在需要可理解性的安全敏感场景中辅助人类决策。"
  },
  {
    "date": "2025-12-22",
    "title": "BanglaForge: LLM Collaboration with Self-Refinement for Bangla Code Generation",
    "authors": "Mahir Labib Dihan, Sadif Ahmed, Md Nafiu Rahman",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.19122v1",
    "source": "arXiv",
    "abstract": "Bangla is a low-resource language for code generation, lacking large-scale annotated datasets and tools to transform natural language specifications into executable programs. This makes Bangla-to-code generation a challenging task requiring innovative solutions. To address this, we introduce BanglaForge, a novel framework for generating code from Bangla function descriptions. BanglaForge leverages a retrieval-augmented dual-model collaboration paradigm with self-refinement, combining in-context learning, llm-based translation, systematic prompt engineering, and iterative self-refinement based on execution feedback, where a coder generates initial solutions and a reviewer enhances them for robustness. On the BLP-2025 Bangla Code Generation benchmark, BanglaForge achieves a competitive Pass@1 accuracy of 84.00%, demonstrating the effectiveness of retrieval, model collaboration, and self-refinement for low-resource Bangla code generation.",
    "title_zh": "BanglaForge：具有自修正能力的大型语言模型协作用于孟加拉语代码生成",
    "abstract_zh": "孟加拉语是一种代码生成领域的低资源语言，缺乏大规模的标注数据集以及将自然语言规范转换为可执行程序的工具。这使得从孟加拉语生成代码成为一个极具挑战性的任务，亟需创新性解决方案。为此，我们提出了 BanglaForge——一种从孟加拉语函数描述生成代码的新颖框架。BanglaForge 采用检索增强的双模型协作范式，并结合自我优化机制，融合了上下文学习、基于大语言模型的翻译、系统化的提示工程以及基于执行反馈的迭代自我优化：由编码器生成初始解决方案，评审者则对其进行改进以提升鲁棒性。在 BLP-2025 孟加拉语代码生成基准测试中，BanglaForge 达到了 84.00% 的竞争性 Pass@1 准确率，充分证明了检索、模型协作和自我优化在低资源孟加拉语代码生成中的有效性。"
  },
  {
    "date": "2025-12-22",
    "title": "FASTRIC: Prompt Specification Language for Verifiable LLM Interactions",
    "authors": "Wen-Long Jin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.18940v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) execute complex multi-turn interaction protocols but lack formal specifications to verify execution against designer intent. We introduce FASTRIC, a Prompt Specification Language that makes implicit Finite State Machines (FSMs) explicit in natural language prompts, enabling conformance verification through execution trace analysis. The LLM serves as intelligent execution agent: interpreting designer-encoded FSMs to execute specified behavioral roles. Unlike symbolic specification languages requiring parsers and compilers, FASTRIC leverages LLMs as unified infrastructure-simultaneously parser, interpreter, runtime environment, and development assistant. FASTRIC guides designers to articulate seven FSM elements (Final States, Agents, States, Triggers, Roles, Initial State, Constraints) structuring multi-turn interactions. Specification formality-ranging from implicit descriptions that frontier models infer to explicit step-by-step instructions for weaker models-serves as a design parameter. We introduce procedural conformance as verification metric measuring execution adherence to FSM specifications. Testing a 3-state kindergarten tutoring FSM across four formality levels and three model scales (14.7B, 685B, 1T+ parameters) reveals optimal specification formality is a function of model capacity. DeepSeek-V3.2 (685B) achieves perfect conformance (1.00) at L2-L4; ChatGPT-5 (~1T) peaks at L3 (0.90) before collapsing at L4 (0.39); Phi4 (14.7B) shows no stable optimum with high variance (SD=0.16-0.36). These findings reveal model-specific formality ranges-\"Goldilocks zones\"-where specifications provide sufficient structure without over-constraint, establishing Prompt Specification Engineering for creating verifiable interaction protocols, transforming multi-turn interaction design from heuristic art to systematic engineering with measurable procedural guarantees.",
    "title_zh": "FASTRIC：用于可验证大语言模型交互的提示规范语言",
    "abstract_zh": "大型语言模型（LLMs）能够执行复杂的多轮交互协议，但缺乏形式化规范来验证其执行是否符合设计意图。我们提出了FASTRIC——一种提示规范语言，将自然语言提示中隐含的有限状态机（FSM）显式化，从而通过执行轨迹分析实现合规性验证。在该框架中，LLM作为智能执行代理：解析设计师编码的FSM，并据此执行指定的行为角色。与需要解析器和编译器的符号化规范语言不同，FASTRIC利用LLM作为统一基础设施——同时承担解析器、解释器、运行时环境和开发助手的角色。\n\nFASTRIC引导设计者明确表达七类FSM要素：终态（Final States）、参与者（Agents）、状态（States）、触发条件（Triggers）、角色（Roles）、初始状态（Initial State）以及约束（Constraints），以结构化方式组织多轮交互。规范的形式性程度——从前沿模型可推断的隐式描述，到为较弱模型设计的显式分步指令——成为一项可调节的设计参数。\n\n我们引入“过程合规性”（procedural conformance）作为验证指标，用于衡量执行过程对FSM规范的遵循程度。在四种形式性层级（L1-L4）和三种模型规模（147亿、6850亿、1万亿+参数）下对一个三状态幼儿园教学FSM进行测试，结果表明：最优规范形式性是模型能力的函数。DeepSeek-V3.2（685B）在L2至L4层级均达到完美合规性（1.00）；ChatGPT-5（约1万亿参数）在L3层级达到峰值（0.90），但在L4层级骤降至0.39；Phi4（147亿参数）则未表现出稳定最优值，且波动较大（标准差为0.16–0.36）。\n\n这些发现揭示了针对不同模型的“黄金区域”（Goldilocks zones）——即在提供足够结构的同时避免过度约束的最佳形式性范围。这标志着“提示规范工程”（Prompt Specification Engineering）的建立，使多轮交互协议的设计从依赖直觉的经验艺术，转变为具备可度量过程保障的系统化工程实践。"
  },
  {
    "date": "2025-12-22",
    "title": "Understanding Chain-of-Thought in Large Language Models via Topological Data Analysis",
    "authors": "Chenghao Li, Chaoning Zhang, Yi Lu, Shuxu Chen, Xudong Wang, Jiaquan Zhang, Zhicheng Wang, Zhengxun Jin, Kuien Liu, Sung-Ho Bae, Guoqing Wang, Yang Yang, Hen Tao Shen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.19135v1",
    "source": "arXiv",
    "abstract": "With the development of large language models (LLMs), particularly with the introduction of the long reasoning chain technique, the reasoning ability of LLMs in complex problem-solving has been significantly enhanced. While acknowledging the power of long reasoning chains, we cannot help but wonder: Why do different reasoning chains perform differently in reasoning? What components of the reasoning chains play a key role? Existing studies mainly focus on evaluating reasoning chains from a functional perspective, with little attention paid to their structural mechanisms. To address this gap, this work is the first to analyze and evaluate the quality of the reasoning chain from a structural perspective. We apply persistent homology from Topological Data Analysis (TDA) to map reasoning steps into semantic space, extract topological features, and analyze structural changes. These changes reveal semantic coherence, logical redundancy, and identify logical breaks and gaps. By calculating homology groups, we assess connectivity and redundancy at various scales, using barcode and persistence diagrams to quantify stability and consistency. Our results show that the topological structural complexity of reasoning chains correlates positively with accuracy. More complex chains identify correct answers sooner, while successful reasoning exhibits simpler topologies, reducing redundancy and cycles, enhancing efficiency and interpretability. This work provides a new perspective on reasoning chain quality assessment and offers guidance for future optimization.",
    "title_zh": "通过拓扑数据分析理解大语言模型中的思维链",
    "abstract_zh": "随着大型语言模型（LLMs）的发展，尤其是长推理链技术的引入，LLMs在复杂问题求解中的推理能力得到了显著提升。尽管我们承认长推理链的强大作用，但仍不禁产生疑问：为何不同的推理链在推理表现上存在差异？推理链中的哪些组成部分起关键作用？现有研究主要从功能角度评估推理链，而对其内在结构机制关注甚少。为弥补这一空白，本文首次从结构视角对推理链的质量进行分析与评估。我们运用拓扑数据分析（TDA）中的持久同调方法，将推理步骤映射到语义空间，提取拓扑特征，并分析其结构变化。这些变化揭示了语义连贯性、逻辑冗余，以及逻辑断裂和缺失之处。通过计算同调群，我们在不同尺度上评估连通性与冗余度，并利用条形码图（barcode）和持久性图（persistence diagram）量化推理过程的稳定性与一致性。研究结果表明，推理链的拓扑结构复杂度与推理准确率呈正相关：结构更复杂的推理链能更早识别正确答案；而成功的推理则表现出更简洁的拓扑结构，减少了冗余与循环，从而提升了效率与可解释性。本工作为推理链质量评估提供了新的视角，并为未来优化提供了重要指导。"
  },
  {
    "date": "2025-12-22",
    "title": "AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards",
    "authors": "Zihan Lin, Xiaohan Wang, Hexiong Yang, Jiajun Chai, Jie Cao, Guojun Yin, Wei Lin, Ran He",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.19126v1",
    "source": "arXiv",
    "abstract": "While reinforcement learning (RL) shows promise in training tool-use large language models (LLMs) using verifiable outcome rewards, existing methods largely overlook the potential of explicit reasoning rewards to bolster reasoning and tool utilization. Furthermore, natively combining reasoning and outcome rewards may yield suboptimal performance or conflict with the primary optimization objective. To address this, we propose advantage-weighted policy optimization (AWPO) -- a principled RL framework that effectively integrates explicit reasoning rewards to enhance tool-use capability. AWPO incorporates variance-aware gating and difficulty-aware weighting to adaptively modulate advantages from reasoning signals based on group-relative statistics, alongside a tailored clipping mechanism for stable optimization. Extensive experiments demonstrate that AWPO achieves state-of-the-art performance across standard tool-use benchmarks, significantly outperforming strong baselines and leading closed-source models in challenging multi-turn scenarios. Notably, with exceptional parameter efficiency, our 4B model surpasses Grok-4 by 16.0 percent in multi-turn accuracy while preserving generalization capability on the out-of-distribution MMLU-Pro benchmark.",
    "title_zh": "AWPO：通过显式整合推理奖励提升大型语言模型的工具使用能力",
    "abstract_zh": "尽管强化学习（RL）在利用可验证结果奖励训练工具使用大型语言模型（LLMs）方面展现出巨大潜力，但现有方法大多忽视了显式推理奖励在增强推理能力与工具使用方面的潜在价值。此外，直接原生结合推理奖励与结果奖励可能导致性能次优，甚至与主要优化目标产生冲突。为解决这一问题，我们提出了一种基于优势加权的策略优化方法（AWPO）——一种系统化的强化学习框架，能够有效整合显式推理奖励，从而提升模型的工具使用能力。AWPO引入了感知方差的门控机制和感知难度的加权策略，根据组内相对统计信息自适应地调节来自推理信号的优势值，并采用专门设计的截断机制以确保优化过程的稳定性。大量实验表明，AWPO在标准工具使用基准测试中达到了当前最优性能，显著优于多个强基线模型，并在复杂的多轮交互场景中超越了部分闭源模型。尤为突出的是，我们的4B参数量模型在多轮任务准确率上比Grok-4高出16.0%，同时在分布外的MMLU-Pro基准测试中仍保持出色的泛化能力，展现出卓越的参数效率。"
  },
  {
    "date": "2025-12-22",
    "title": "PEAK: A Performance Engineering AI-Assistant for GPU Kernels Powered by Natural Language Transformations",
    "authors": "Muhammad Usman Tariq, Abhinav Jangda, Angelica Moreira, Madan Musuvathi, Tyler Sorensen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.19018v1",
    "source": "arXiv",
    "abstract": "Advancements in large language models (LLMs) are showing promising impact in software development and programming assistance. However, these models struggle when operating on low-level backend code. This challenge is exacerbated in the domain of GPU kernels, where performance-critical details are coupled to rapidly evolving hardware characteristics and available code examples are sparse. In this work, we introduce PEAK, a Performance Engineering AI-Assistant for GPU Kernels powered by natural language transformations. PEAK utilizes the key insight that iterative code transformations (optimizations) can straightforwardly be written in natural language, and then carried out by LLMs. Thus, these transformations can be rapidly developed, encoding general portable optimizations, but also easily specialized to specific GPU devices and even kernels. These natural transformations are supported by a modular and extensible infrastructure that additionally performs validation and performance evaluation. We demonstrate the flexibility of PEAK by instantiating it for three backends, CUDA, HIP, and HLSL, and create 16 natural transformations for optimizing matrix multiplication kernels. We show that our resulting implementations are competitive with vendor libraries when available, and for HLSL (without a library) our implementations match the hardware documented FLOPS. PEAK allows the fine-grained exploration of several research questions around how LLMs behave in this domain, including characterizing transformations and their errors; and how performance evolves along optimization sequences. PEAK provides an interface that can either be utilized by performance engineers to improve productivity, or driven completely autonomously (e.g., by an AI agent), providing a forward-compatible design that can continue to improve with advances in AI capabilities.",
    "title_zh": "PEAK：一种基于自然语言转换的GPU内核性能工程AI助手",
    "abstract_zh": "大型语言模型（LLMs）在软件开发和编程辅助领域展现出令人瞩目的进展。然而，这些模型在处理低层后端代码时仍面临挑战。这一问题在GPU内核领域尤为突出，因为性能关键的细节与快速演进的硬件特性紧密耦合，且可用的代码示例极为稀少。在本研究中，我们提出了PEAK——一种基于自然语言转换的GPU内核性能工程AI助手。PEAK的核心思想是：迭代式的代码变换（优化）可以轻松地用自然语言描述，然后由大语言模型执行。因此，这些变换能够快速开发，既可编码通用的可移植优化，也可轻松针对特定GPU设备甚至具体内核进行定制。这些自然语言形式的变换依托于一个模块化且可扩展的基础设施，该系统还具备验证和性能评估功能。\n\n我们通过为三种后端（CUDA、HIP 和 HLSL）实例化PEAK，并创建了16种用于优化矩阵乘法内核的自然语言变换，展示了PEAK的灵活性。实验结果表明，我们的实现方案在有厂商库可用的情况下具有竞争力；而在HLSL（无现成库支持）场景下，我们的实现达到了硬件文档所记录的浮点运算峰值性能（FLOPS）。PEAK为深入探索LLM在此领域中的行为提供了精细的研究框架，包括对变换及其错误的特征分析，以及优化序列中性能演变规律的研究。\n\nPEAK提供了一个接口，既可供性能工程师使用以提升工作效率，也可完全由AI代理自主驱动，具备面向未来的兼容性设计，能够随着人工智能技术的进步持续进化与优化。"
  },
  {
    "date": "2025-12-22",
    "title": "JEPA-Reasoner: Decoupling Latent Reasoning from Token Generation",
    "authors": "Bingyang Kelvin Liu, Ziyu Patrick Chen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2512.19171v1",
    "source": "arXiv",
    "abstract": "While Joint-Embedding Predictive Architecture (JEPA) has emerged as a powerful architecture for learning rich latent representations, it fundamentally lacks generative abilities. Meanwhile, latent space reasoning attempts for Transformer models like COCONUT do improve performance, but they ultimately rely on token-by-token generation, which still accumulates compounding error and relies on context information to gain reasoning insights. To address these limitations, we propose JEPA-Reasoner, a novel JEPA model enhanced with generative ability that reasons in latent space. We augment it with a separate action-taker model, Talker, to produce human-readable sentences. Our approach demonstrates that decoupling latent space reasoning and token generation enables JEPA-Reasoner to produce mixed latent vectors that might lay the foundation for multi-threaded reasoning, while performing autoregressive generation with superior robustness to compounding error.",
    "title_zh": "JEPA-Reasoner：将潜在推理与标记生成解耦",
    "abstract_zh": "尽管联合嵌入预测架构（JEPA）已成为学习丰富潜在表示的强大架构，但其本质上缺乏生成能力。与此同时，像COCONUT这样的Transformer模型虽通过潜在空间推理提升了性能，但最终仍依赖于逐标记生成，这导致误差不断累积，并且需要依赖上下文信息来获得推理见解。为解决这些局限性，我们提出JEPA-Reasoner，一种增强生成能力的新型JEPA模型，能够在潜在空间中进行推理。我们引入一个独立的执行者模型——Talker，用于生成人类可读的句子。我们的方法表明，将潜在空间推理与标记生成解耦，使JEPA-Reasoner能够生成混合的潜在向量，为多线程推理奠定基础，同时在自回归生成过程中展现出对误差累积更强的鲁棒性。"
  },
  {
    "date": "2025-12-22",
    "title": "LLM-Driven Design for Fluid Antenna Systems",
    "authors": "Chao Wang, Bowen Zhang, Zan Li, Kai-Kit Wong, Hao Xu, Gan Zheng, Chan-Byoung Chae",
    "publish": "IEEE Wireless Communications Letters",
    "url": "https://doi.org/10.1109/lwc.2025.3647421",
    "source": "IEEE",
    "abstract": "We address the joint optimization of port selection and precoder design for a FAS to minimize power consumption under a minimum signal-to-interference-plus-noise ratio (SINR) constraint. This formulation results in a challenging mixed-integer nonconvex problem. To solve it, we introduce a novel large language model (LLM)-based hyper-heuristic. Our approach leverages alternating optimization to decompose the problem into a convex precoder subproblem and a port selection subproblem. For the latter, we employ a genetic algorithm (GA) that is automatically enhanced by a dual-LLM architecture. This architecture uses one LLM to generate crossover operators and another to critique and refine them, eliminating manual tuning. Under the simulation conditions of 10 dB SINR, 10 users, and antenna sizes between 8×8 cm and 13×13 cm, the proposed LLM-enhanced GA outperforms the conventional GA by an average of 22.09%.",
    "title_zh": "基于大语言模型的流体天线系统设计",
    "abstract_zh": "我们针对FAS（智能反射面）的端口选择与预编码设计联合优化问题，旨在最小化功耗的同时满足最低信干噪比（SINR）约束。该问题建模为一个复杂的混合整数非凸优化问题。为求解此问题，我们提出一种基于大语言模型（LLM）的新型超启发式算法。我们的方法采用交替优化策略，将原问题分解为一个凸的预编码子问题和一个端口选择子问题。对于后者，我们引入一种由双LLM架构自动增强的遗传算法（GA）：其中一个LLM负责生成交叉算子，另一个则对这些算子进行评估与优化，从而避免了人工调参过程。在10 dB SINR、10个用户以及天线尺寸介于8×8 cm至13×13 cm的仿真条件下，所提出的LLM增强型GA相比传统GA平均性能提升22.09%。"
  },
  {
    "date": "2025-12-22",
    "title": "Talk-To-Your-Data: Bridging Human Language and SQL with AI",
    "authors": "Harini T, Annsi Jerolin E, J Gold Beulah Patturose, R Priscilla",
    "publish": "2025 Third International Conference on Cyber Physical Systems, Power Electronics and Electric Vehicles (ICPEEV)",
    "url": "https://doi.org/10.1109/icpeev67897.2025.11291234",
    "source": "IEEE",
    "abstract": "By enabling natural language explores driven by Large Language Models (LLMs), the “SQL Chatbot” project SEKS to modify the interaction between users and databases. For nontechnical users, traditional database querying challenges since it depends on SQL knowledge. This work demonstrates a method to interpret natural language input and translate it into SQL queries by combining LangChain with concurrent LLMs. The system upgrades structured database real-time interaction, so clarifying data retrieval access. We evaluate the efficiency of the model, talk on implementation difficulties, and suggest future paths to enhance performance and improve applicability.",
    "title_zh": "对话式数据：通过人工智能连接人类语言与SQL",
    "abstract_zh": "通过利用由大型语言模型（LLMs）驱动的自然语言探索，“SQL聊天机器人”项目旨在改变用户与数据库之间的交互方式。对于非技术用户而言，传统的数据库查询存在困难，因为其依赖于对SQL知识的掌握。本研究提出一种方法，结合LangChain与并行运行的大型语言模型，将自然语言输入解析并转换为SQL查询语句。该系统提升了结构化数据库的实时交互能力，从而简化了数据检索的访问流程。我们评估了模型的效率，讨论了实施过程中的挑战，并提出了未来改进性能和增强应用范围的可行路径。"
  },
  {
    "date": "2025-12-22",
    "title": "Anti-patterns Detection in Microservice-Based Software Architectures Using Graph Analysis",
    "authors": "Farid Ahmadpour Mobarakeh, Fereidoon Shams Aliee",
    "publish": "2025 9th Iranian Conference on Advances in Enterprise Architecture (ICAEA)",
    "url": "https://doi.org/10.1109/icaea69058.2025.11301540",
    "source": "IEEE",
    "abstract": "Microservice architectures promise scalability, flexibility, and rapid delivery, yet their distributed nature makes them susceptible to architectural anti-patterns that erode maintainability, reliability, and performance. This paper presents a graph-based, runtime methodology for detecting such issues in systems orchestrated with Kubernetes and observed via the Istio service mesh. Leveraging mesh telemetry, we automatically construct a Service Dependency Graph (SDG) that captures inter-service communication and operational context; network-analysis metrics and structural queries are then applied to surface problematic architectural structures. The approach scales to large clusters, runs continuously, and requires neither static code inspection nor intrusive changes to application deployments. By turning production telemetry into actionable architectural signals, it enables early remediation and continuous governance of microservice systems.",
    "title_zh": "基于图分析的微服务软件架构中的反模式检测",
    "abstract_zh": "微服务架构承诺具备可扩展性、灵活性和快速交付能力，但其分布式特性也使其容易受到架构反模式的影响，从而损害系统的可维护性、可靠性和性能。本文提出一种基于图的运行时方法，用于检测由Kubernetes编排并通过Istio服务网格观测的系统中的此类问题。通过利用服务网格的遥测数据，我们能够自动构建服务依赖图（SDG），以捕捉服务间的通信关系及运行时上下文；随后应用网络分析指标与结构化查询，识别出存在潜在问题的架构模式。该方法可扩展至大规模集群，支持持续运行，且无需静态代码分析或对应用部署进行侵入性修改。通过将生产环境的遥测数据转化为可操作的架构信号，该方法实现了微服务系统的早期修复与持续治理。"
  },
  {
    "date": "2025-12-22",
    "title": "An Explainable Control Framework (XCF): A Novel Fuzzy Model-Agnostic Explanation (FMAE) Approach with Case Study",
    "authors": "Faliang Yin, David Watson, Hak-Keung Lam",
    "publish": "2025 International Conference on Advanced Robotics and Mechatronics (ICARM)",
    "url": "https://doi.org/10.1109/icarm65671.2025.11293635",
    "source": "IEEE",
    "abstract": "Explainable Artificial Intelligence (XAI) can enhance the transparency and trustworthiness of control systems by delivering decision-making insights that are readily understandable to humans. This is especially crucial in scenarios where data-driven controllers employing closed box approaches, or mathematically rigorous yet complex controllers, are adopted. However, existing XAI methods typically focus on predictive tasks such as classification and regression, with limited applications in control. To bridge this gap, this paper integrates XAI techniques into control systems by proposing an eXplainable Control Framework (XCF) which explains how controllers determine their control actions and their underlying working mechanism. A novel XAI method, fuzzy model-agnostic explanation (FMAE), is introduced as an instantiation of the proposed framework. The innovations and contributions of this work include applying and adjusting the hierarchical structure, model-agnostic algorithm and user interface of FMAE within the proposed XCF to better align with the needs of explainable control. To validate the proposed XCF based on FMAE in real control scenarios, we conduct a case study on the cartpole inverted pendulum system. The generated explanations comprise comprehensible IF-THEN rules revealing the decision logic of controller and the salience values of the system states illustrating their contributions to control output. Furthermore, we employ large language models (LLMs) to refine the explanations. Experimental results based on the designed evaluation metrics demonstrate that the explainer accurately approximates the behavior of real controller. This work enhances control system explainability and usability for accessible design, analysis, and deployment.",
    "title_zh": "可解释控制框架（XCF）：一种新型模糊模型无关解释（FMAE）方法及其案例研究",
    "abstract_zh": "可解释人工智能（XAI）通过提供人类易于理解的决策洞察，能够提升控制系统的透明度与可信度。这一点在采用“黑箱”数据驱动控制器或数学上严谨但结构复杂的控制器的应用场景中尤为重要。然而，现有的XAI方法大多集中于分类、回归等预测任务，其在控制领域的应用仍十分有限。为弥合这一差距，本文提出了一种可解释控制框架（XCF），将XAI技术融入控制系统，旨在阐明控制器如何做出控制决策及其内在工作机制。作为该框架的具体实现，本文引入了一种新型XAI方法——模糊模型无关解释法（FMAE）。本研究的创新与贡献在于：在所提出的XCF框架内，对FMAE的层次化结构、模型无关算法以及用户界面进行了适配性调整，使其更契合可解释控制的实际需求。为验证基于FMAE的XCF在真实控制场景中的有效性，本文以倒立摆小车系统为例开展案例研究。生成的解释包含清晰易懂的“如果-那么”规则，揭示了控制器的决策逻辑；同时提供了系统状态的显著性数值，展示了各状态变量对控制输出的贡献程度。此外，本文还利用大语言模型（LLM）对解释内容进行优化和润色。基于设计的评估指标，实验结果表明，该解释器能够准确逼近真实控制器的行为。本研究显著提升了控制系统的可解释性与可用性，为控制系统的可访问性设计、分析与部署提供了有力支持。"
  },
  {
    "date": "2025-12-22",
    "title": "Self-Optimizing Enterprise Architecture for Digital Government: Telemetry-Driven Maturity and Generative Co-Design",
    "authors": "Sadegh Aref, Marzieh Hatami, Seyedakbar Mostafavi",
    "publish": "2025 9th Iranian Conference on Advances in Enterprise Architecture (ICAEA)",
    "url": "https://doi.org/10.1109/icaea69058.2025.11301497",
    "source": "IEEE",
    "abstract": "Digital government increasingly depends on enterprise architecture (EA) to align strategy with technology portfolios, yet current practices remain static, survey-based, and detached from operational data. This paper introduces a self-optimizing EA framework that integrates telemetry-driven maturity monitoring, generative co-design, and an AI governance layer. Telemetry from service logs and performance metrics feeds a real-time maturity orchestrator, while an AI meta-layer performs predictive analytics and generates policy-compliant BPMN workflows and capability maps under human oversight. A knowledge-graph backbone links policies, processes, and technologies to enable reasoning and interoperability across agencies. A proof-of-concept prototype demonstrates near real-time maturity updates (latency below five seconds), artefact accuracy of up to 100 percent after review, and complete governance traceability. The results indicate that EA in government can evolve from static documentation toward an adaptive, evidence-driven, and trustworthy foundation for digital transformation.",
    "title_zh": "面向数字政府的自优化企业架构：基于遥测驱动的成熟度与生成式协同设计",
    "abstract_zh": "数字政府日益依赖企业架构（EA）将战略与技术组合相统一，但当前的做法仍停留在静态、基于调查的模式，与实际运营数据脱节。本文提出了一种自优化的企业架构框架，整合了基于遥测数据的成熟度监控、生成式协同设计以及人工智能治理层。服务日志和性能指标产生的遥测数据被输入实时成熟度编排器，而AI元层在人工监督下执行预测分析，并生成符合政策要求的BPMN工作流与能力图谱。以知识图谱为骨干，该框架将政策、流程与技术相连接，实现跨机构间的推理与互操作性。一个概念验证原型展示了接近实时的成熟度更新（延迟低于五秒）、经审查后最高达100%的成果物准确率，以及完整的治理可追溯性。结果表明，政府领域的企业架构可从静态文档演进为一种适应性强、基于证据且值得信赖的数字化转型基础。"
  },
  {
    "date": "2025-12-22",
    "title": "Application of Infrastructure as Code (IaC) in Multi-Cloud Network Infrastructure Management Using Terraform and Aviatrix on AWS and Azure",
    "authors": "Akhyar Lubis, Elviawaty Muisa Zamzami, Ade Candra, Herman Mawengkang",
    "publish": "2025 13th International Conference on Cyber and IT Service Management (CITSM)",
    "url": "https://doi.org/10.1109/citsm67730.2025.11291297",
    "source": "IEEE",
    "abstract": "Managing network infrastructure across multiple cloud providers introduces operational complexity, security risks, and inconsistencies in configuration. This paper investigates the application of Infrastructure as Code (IaC) for automating multi-cloud network deployment using Terraform and Aviatrix on AWS and Azure. A three-phase implementation model is proposed: initial infrastructure provisioning, configuration enhancement, and secure automation through CI/CD pipelines. Each phase is executed using a structured IaC workflow, involving collaborative roles of NetOps, DevOps, and SecOps teams. Functional validation demonstrates successful connectivity between isolated cloud environments and effective enforcement of egress filtering through FQDN-based policies. The research highlights the effectiveness of combining Terraform with Aviatrix in managing scalable, secure, and automated multi-cloud networks, offering a replicable approach aligned with modern DevSecOps practices. Limitations related to performance metrics and real-world deployment scenarios are also discussed.",
    "title_zh": "使用 Terraform 和 Aviatrix 在 AWS 与 Azure 上实现多云网络基础设施管理的基础设施即代码（IaC）应用",
    "abstract_zh": "在多个云服务提供商之间管理网络基础设施会带来操作复杂性、安全风险以及配置不一致等问题。本文研究了利用基础设施即代码（IaC）技术，通过Terraform与Aviatrix在AWS和Azure环境中实现多云网络部署的自动化。文章提出一个三阶段实施模型：初始基础设施部署、配置优化增强，以及通过CI/CD流水线实现的安全自动化。每个阶段均采用结构化的IaC工作流程，并由网络运维（NetOps）、开发运维（DevOps）和安全运维（SecOps）团队协同完成。功能验证表明，该方案成功实现了隔离云环境间的连通性，并通过基于FQDN的策略有效实施了出站流量过滤。研究结果表明，将Terraform与Aviatrix相结合，能够有效管理可扩展、安全且自动化的多云网络，提供一种符合现代DevSecOps实践的可复用解决方案。同时，本文也讨论了在性能指标评估及实际部署场景中存在的局限性。"
  },
  {
    "date": "2025-12-22",
    "title": "Generative AI-Empowered Secure Communications in Space-Air-Ground Integrated Networks: A Survey and Tutorial",
    "authors": "Chenbo Hu, Ruichen Zhang, Bo Li, Xu Jiang, Nan Zhao, Marco Di Renzo, Dusit Niyato, Arumugam Nallanathan, George K. Karagiannidis",
    "publish": "IEEE Communications Surveys &amp; Tutorials",
    "url": "https://doi.org/10.1109/comst.2025.3646700",
    "source": "IEEE",
    "abstract": "Space-air-ground integrated networks (SAGINs) face unprecedented security challenges due to their inherent characteristics, such as multidimensional heterogeneity and dynamic topologies. These characteristics fundamentally undermine conventional security methods and traditional artificial intelligence (AI)-driven solutions. Generative AI (GAI) is a transformative approach that can safeguard SAGIN security by synthesizing data, understanding semantics, and making autonomous decisions. This survey fills existing review gaps by examining GAI-empowered secure communications across SAGINs, with a focus on core models such as generative adversarial networks (GANs), variational autoencoders (VAEs), generative diffusion models (GDMs), and large language models (LLMs). First, we introduce secured SAGINs and highlight GAI’s advantages over traditional AI for SAGIN security defenses. Then, we explain how GAI mitigates failures of authenticity, breaches of confidentiality, tampering of integrity, and disruptions of availability across the physical, data link, and network layers of SAGINs. We present three step-by-step tutorials to discuss how to apply GAI to solve specific security problems across different layers and segments of SAGINs utilizing concrete methods, emphasizing its generative paradigm beyond traditional AI. Finally, we outline open issues and future research directions, including lightweight deployment, adversarial robustness, cross-domain governance, and heterogeneous compatibility, to provide major insights into GAI’s role in shaping next-generation SAGIN security.",
    "title_zh": "生成式人工智能赋能的空-天-地一体化网络中安全通信：综述与教程",
    "abstract_zh": "空-天-地一体化网络（SAGINs）由于其固有的多维异构性和动态拓扑等特性，正面临前所未有的安全挑战。这些特性从根本上削弱了传统安全方法以及基于传统人工智能（AI）的解决方案的有效性。生成式人工智能（GAI）作为一种变革性技术，能够通过数据合成、语义理解与自主决策来保障SAGIN的安全。本综述填补了现有研究的空白，系统探讨了GAI赋能下的SAGIN安全通信，重点关注生成对抗网络（GANs）、变分自编码器（VAEs）、生成扩散模型（GDMs）和大语言模型（LLMs）等核心模型。首先，我们介绍了安全化的SAGIN架构，并突出GAI相较于传统AI在SAGIN安全防御中的优势；其次，阐述了GAI如何在物理层、数据链路层和网络层中有效应对身份真实性失效、机密性泄露、完整性篡改及可用性中断等安全威胁。随后，我们提供了三个循序渐进的实践教程，结合具体方法，详细说明如何将GAI应用于解决SAGIN不同层级与子段中的特定安全问题，强调其超越传统AI的生成式范式。最后，我们梳理了当前存在的开放性问题与未来研究方向，包括轻量化部署、对抗鲁棒性、跨域治理机制以及异构兼容性等，为深入理解GAI在塑造下一代SAGIN安全体系中的关键作用提供了重要洞见。"
  },
  {
    "date": "2025-12-22",
    "title": "Identifying Linux Kernel Instability Due to Poor RCU Synchronization",
    "authors": "Oisin O’Sullivan, Eoin O’Connell, Colin Flanagan",
    "publish": "2025 35th Irish Signals and Systems Conference (ISSC)",
    "url": "https://doi.org/10.1109/issc67739.2025.11291433",
    "source": "IEEE",
    "abstract": "Read-Copy-Update (RCU) is widely used in the Linux kernel to manage concurrent access to shared data structures. However, improper synchronization when removing RCU-protected hash table entries can lead to stale pointers, inconsistent lookups, and critical use-after-free (UAF) vulnerabilities. This paper investigates a driver-level synchronization issue arising from the omission of explicit synchronize_rcu() calls during hash table updates, using a discovered weakness in the Intel® ICE network driver’s Virtual Function (VF) management. Previous kernel vulnerabilities, such as a bug in the Reliable Datagram Sockets (RDS) subsystem, show how improper RCU synchronization can directly cause kernel crashes. Experimental results demonstrate that removing VF entries without proper synchronization leaves transient stale entries, delays memory reclamation, and results in significant memory fragmentation under rapid insert/delete workloads. RCU hash tables are widely deployed in Linux kernel subsystems such as networking, virtualization, and file systems; improper synchronization can cause memory fragmentation, kernel instability, and out-of-memory (OOM) conditions. Mitigations are proposed, recommending explicit insertion of synchronize_rcu() calls to ensure timely and safe memory reclamation. These findings reinforce established best practices for RCU synchronization, highlighting their importance for maintaining kernel stability and memory safety.",
    "title_zh": "由于RCU同步不良导致的Linux内核不稳定问题识别",
    "abstract_zh": "读-拷贝-更新（RCU）在Linux内核中被广泛用于管理对共享数据结构的并发访问。然而，在删除受RCU保护的哈希表条目时若同步不当，可能导致悬空指针、查找结果不一致，并引发严重的使用后释放（UAF）漏洞。本文研究了一种由在哈希表更新过程中遗漏显式调用`synchronize_rcu()`所导致的驱动层同步问题，以英特尔® ICE网络驱动程序中虚拟功能（VF）管理存在的一个缺陷为例进行分析。此前内核中的漏洞，如可靠数据报套接字（RDS）子系统中的一个缺陷，已表明不当的RCU同步可直接导致内核崩溃。实验结果表明，在未进行适当同步的情况下移除VF条目，会产生临时的过期条目，延迟内存回收，并在快速插入/删除的工作负载下造成显著的内存碎片化。RCU哈希表广泛部署于Linux内核的多个子系统中，包括网络、虚拟化和文件系统；若同步不当，可能引发内存碎片化、内核不稳定以及内存不足（OOM）等问题。本文提出了相应的缓解措施，建议显式插入`synchronize_rcu()`调用，以确保内存能够及时且安全地回收。这些发现进一步验证了RCU同步的既定最佳实践，凸显其对于维护内核稳定性与内存安全的重要性。"
  },
  {
    "date": "2025-12-22",
    "title": "Integration of AI-Augmented CloudOps with EAM and IT4IT: A Strategic–Operational Framework for Intelligent Cloud Infrastructure Management",
    "authors": "Maryam Mayabi Joghal, Alireza Shameli-Sendi",
    "publish": "2025 9th Iranian Conference on Advances in Enterprise Architecture (ICAEA)",
    "url": "https://doi.org/10.1109/icaea69058.2025.11301455",
    "source": "IEEE",
    "abstract": "In the era of digital transformation, Enterprise Architecture Management (EAM) must evolve from a static blueprint into a dynamic framework that strategically integrates emerging technologies across the IT lifecycle. This paper examines the role of intelligent Cloud Operations (AI-Augmented CloudOps) in linking EAM with the IT4IT operating model. First, AI-Augmented CloudOps is viewed as an operational capability in the Detect-to-Correct value stream, using AI to enable predictive monitoring, failure detection, and self-healing. Second, it is treated as a digital product whose lifecycle spans all four IT4IT value streams. The proposed model was implemented in a real-world organization, where OpenStack and Ceph-based automation enabled secure and sovereign cloud deployment with continuous architectural compliance. The system automatically generates optimized architecture recommendations based on existing organizational assets and performs fully offline and automated deployment. The results demonstrate significant reductions in incident response time, downtime, and operational costs. Moreover, the model enables the delivery of stable, secure, and governance-compliant digital services— particularly critical in highly regulated industries—while also addressing challenges such as technical complexity, the need for specialized expertise, and ensuring security in offline environments.",
    "title_zh": "人工智能增强的云运管（CloudOps）与企业应用管理（EAM）及IT4IT的融合：智能云基础设施管理的战略-运营框架",
    "abstract_zh": "在数字化转型的时代，企业架构管理（EAM）必须从静态蓝图演变为一种动态框架，战略性地将新兴技术整合到整个IT生命周期中。本文探讨了智能云运维（AI增强型CloudOps）在连接EAM与IT4IT运营模型中的作用。首先，AI增强型CloudOps被视为“检测至修复”价值流中的一个运营能力，利用人工智能实现预测性监控、故障检测和自愈功能；其次，它被视作一个数字产品，其生命周期贯穿IT4IT的全部四个价值流。所提出的模型已在一家真实企业中实施，基于OpenStack和Ceph的自动化技术实现了安全且自主可控的云部署，并确保持续的架构合规性。系统能够根据现有组织资产自动生成优化的架构建议，并实现完全离线环境下的自动化部署。结果表明，该方案显著降低了事件响应时间、停机时间和运营成本。此外，该模型能够交付稳定、安全且符合治理要求的数字服务——这一点在高度监管的行业中尤为关键，同时有效应对了技术复杂性、专业人才需求以及离线环境中安全保障等挑战。"
  },
  {
    "date": "2025-12-22",
    "title": "AI Tactics, Now",
    "authors": "Stephen J. Andriole",
    "publish": "IT Professional",
    "url": "https://doi.org/10.1109/mitp.2025.3632495",
    "source": "IEEE",
    "abstract": "This article discusses the tactical steps organizations should take to apply artificial intelligence effectively. The focus shifts from high-level strategy to ground-level implementation. The first step requires the establishment of an artificial intelligence (AI) governance framework, which includes internal policies, regulatory compliance, oversight mechanisms, and documentation requirements. Next, organizations must build out a small, rapid prototyping squad to work on quick, high-impact pilot projects. This squad should use process mining to identify specific, broken internal processes that need fixing. The team should then select the appropriate AI tool based on the broken process’s need: machine learning for prediction, generative AI for creation, or an AI agent for autonomous actions. Success is measured via prototypes by assessing performance versus the human baseline.",
    "title_zh": "人工智能策略，现在",
    "abstract_zh": "本文讨论了组织应采取的战术步骤，以有效应用人工智能。重点从高层战略转向基层实施。第一步是建立人工智能（AI）治理框架，包括内部政策、合规要求、监督机制和文档记录要求。接下来，组织必须组建一支小型、快速原型开发团队，专注于开展快速且影响重大的试点项目。该团队应使用流程挖掘技术，识别出需要修复的具体、存在问题的内部流程。随后，根据问题流程的需求，选择合适的AI工具：预测任务采用机器学习，创造任务采用生成式AI，自主行动则采用AI代理。成功与否通过原型测试来衡量，即评估其性能相对于人工基准的表现。"
  },
  {
    "date": "2025-12-22",
    "title": "From Scenario to Case Model: Generating CMMN Models from Natural Language with Large Language Models",
    "authors": "Kheira Cherad, Imen Benzarti, Abderrahmane Leshob, Hafedh Mili",
    "publish": "2025 IEEE International Conference on E-Business Engineering (ICEBE)",
    "url": "https://doi.org/10.1109/icebe68123.2025.00017",
    "source": "IEEE",
    "abstract": "Translating rich, user-centric narratives into formal process models remains a major challenge in model-driven engineering, particularly for flexible and adaptive workflows. This paper introduces a novel two-step method that leverages GPT4.0 to first enrich natural language scenarios with behavioral design principles and then generate executable CMMN models. We evaluate four prompt strategies across diverse e-commerce cases. Findings show that role-playing prompts effectively guide scenario enrichment, while a combined strategy integrating full guidance, few-shot examples, and role-playing produces the most accurate and semantically aligned CMMN models. This work lays the groundwork for LLM-driven, human-centric modeling and opens new directions for integrating cognitive insights into automated model synthesis.",
    "title_zh": "从场景到案例模型：利用大语言模型从自然语言生成CMMN模型",
    "abstract_zh": "将丰富的、以用户为中心的叙事转化为正式的过程模型，仍是模型驱动工程中的一个重大挑战，尤其是在灵活且可适应的工作流领域。本文提出一种新颖的两步法，利用GPT4.0首先在自然语言场景中融入行为设计原则，进而生成可执行的CMMN模型。我们在多种电子商务案例中评估了四种提示策略。研究发现，角色扮演类提示能有效引导场景的丰富化；而结合全面指导、少量示例和角色扮演的综合策略，能够生成最准确且语义对齐的CMMN模型。本研究为基于大语言模型的、以人为本的建模奠定了基础，并为将认知洞察融入自动化模型生成开辟了新的方向。"
  },
  {
    "date": "2025-12-22",
    "title": "Implementation of PytoVHDL Framework for Automatic Code Transliteration and Acceleration of FPGA-based SoC Design",
    "authors": "Yevhenii Holopotyliuk, Vladyslav Romashchenko, Michael Brutscheck",
    "publish": "2025 35th Irish Signals and Systems Conference (ISSC)",
    "url": "https://doi.org/10.1109/issc67739.2025.11291512",
    "source": "IEEE",
    "abstract": "The development of multiple projects and hardware prototypes frequently requires collaboration between software and hardware teams, who are focused on a certain programming languages. This can result in communication challenges during sharing ideas and solutions. It leads to integration difficulties by the fundamentally distinct nature of the programming languages. Also, knowledge gaps may arise, increasing the risk of code failures and extending of development timelines. In such scenarios, teams often need specialists in both software and hardware description languages. In case, if it can be complicated, developers and researchers have created the frameworks such as MyHDL, HDPython, AmaranthHDL etc. Using the Python syntax and decorators on the top-layer, the frameworks can convert Python source-code into VHDL instructions, initiating further FPGA-Prototyping. In parallel, recent achievements in Large Language Models (LLMs) like CodeT5, CodeBERT have introduced new functionality for automating code transliteration tasks. The mentioned models are pretrained by Go, Java, PHP and Python languages, which are quite common for software developers. However, the support of hardware languages such as VHDL or Verilog needs additional investigations. Due to the following fact, the proposed investigations, represent an investigation and realisation of PytoVHDL framework. Inspiring by MyHDL, AI-HDLCoder, the framework aims to bridge the gap between Python and VHDL programmers. Making numerous experiments with middle layer interface, the proposed work shows conversion results for systems that include combination logic or can be sufficient for digital signal processing.",
    "title_zh": "PytoVHDL框架在FPGA-based SoC设计中的自动代码转译与加速实现",
    "abstract_zh": "多个项目和硬件原型的开发通常需要软件与硬件团队之间的协作，而这两个团队往往专注于特定的编程语言。这种差异可能导致在交流想法和解决方案时出现沟通障碍，进而由于编程语言本质上的根本差异，造成集成困难。此外，知识鸿沟的出现可能增加代码出错的风险，并延长开发周期。在这些情况下，团队往往需要同时精通软件和硬件描述语言的专业人才。为应对这一复杂性，开发者和研究人员已创建了诸如MyHDL、HDPython、AmaranthHDL等框架。这些框架利用Python语法及顶层装饰器，能够将Python源代码转换为VHDL指令，从而启动后续的FPGA原型设计。与此同时，大型语言模型（LLMs）如CodeT5、CodeBERT近年来取得的进展，为自动化代码转写任务带来了新功能。这些模型通过Go、Java、PHP和Python等语言进行预训练，这些语言对软件开发者而言非常常见。然而，对于VHDL或Verilog等硬件语言的支持仍需进一步研究。基于上述背景，本文提出并实现了一个名为PytoVHDL的框架。该框架受MyHDL和AI-HDLCoder的启发，旨在弥合Python与VHDL程序员之间的差距。通过大量针对中间层接口的实验，本工作展示了对包含组合逻辑或足以满足数字信号处理需求的系统进行转换的结果。"
  },
  {
    "date": "2025-12-22",
    "title": "Comparion of Source-Selection Between KNN-TDS Method and Median Z-Score in Multi-Source Cross-Project Defect Prediction Using Neural Network",
    "authors": "Rizal Broer Bahaweres, Nabila Fatia Kasmizar",
    "publish": "2025 13th International Conference on Cyber and IT Service Management (CITSM)",
    "url": "https://doi.org/10.1109/citsm67730.2025.11291213",
    "source": "IEEE",
    "abstract": "Software defect prediction is a critical activity in software development. this work was continuous work from our research road map. Historical data is required to predict software defects, but such data is not always available for every project. In cross-project prediction, data availability is not a problem as data from other projects is available, for example, in repositories like PROMISE. However, this technique has a limitation in that the differences between the source and target projects can affect the accuracy of the predictions. Therefore, multi-source cross-project defect prediction has been developed to combine data from multiple sources to reduce the impact of domain differences between source and target projects. Previous studies have shown that appropriate selection of training data can improve cross-project defect prediction quality. In this study, we propose a distance-based strategy for selecting training data based on the distributional characteristics of available data. We evaluated the proposed strategy in a large case study with 11 datasets from different projects. We compared two source selection methods, namely KNN-TDS and median z-score, to obtain the best performance. Finally, we used the Neural Network algorithm to train the training data. The results showed that the average F1-score of the KNN-TDS method was 6% higher than the median z-score method.",
    "title_zh": "基于神经网络的多源跨项目缺陷预测中KNN-TDS方法与中位数Z-score法的源选择比较",
    "abstract_zh": "软件缺陷预测是软件开发中的关键活动。本研究工作是我们研究路线图的持续性成果。在进行软件缺陷预测时，需要依赖历史数据，但并非每个项目都能获得此类数据。在跨项目预测中，由于其他项目的数据可被利用（例如在PROMISE等代码库中），数据可用性问题得以解决。然而，该方法存在一个局限性：源项目与目标项目之间的差异会影响预测的准确性。因此，多源跨项目缺陷预测应运而生，通过整合多个数据源的信息，以降低源项目与目标项目之间领域差异的影响。以往的研究表明，合理选择训练数据可以提升跨项目缺陷预测的性能。在本研究中，我们提出了一种基于距离的训练数据选择策略，该策略依据已有数据的分布特征进行筛选。我们在包含11个不同项目数据集的大规模案例研究中评估了该策略的有效性，并比较了两种源数据选择方法——KNN-TDS和中位z-score，以确定最佳性能表现。最后，我们采用神经网络算法对所选训练数据进行建模。实验结果表明，KNN-TDS方法的平均F1分数比中位z-score方法高出6%。"
  },
  {
    "date": "2025-12-22",
    "title": "EvaRAG: Evaluating Advanced RAG Techniques with Indexing and Distance Metrics",
    "authors": "Harun Elkiran, Jawad Rasheed",
    "publish": "IEEE Access",
    "url": "https://doi.org/10.1109/access.2025.3646665",
    "source": "IEEE",
    "abstract": "Retrieval Augmented Generation (RAG) has emerged as a powerful paradigm for enhancing large language models (LLMs) with external knowledge. Yet, the performance of RAG pipelines is susceptible to design choices across retrieval, similarity metrics, indexing, and reranking. Despite growing adoption, little systematic work has explored the trade-offs between retrieval quality, semantic accuracy, computational efficiency, and cost in RAG systems. This study addresses this gap by conducting a comprehensive evaluation of RAG configurations across multiple dimensions. We propose a benchmarking framework that systematically varies retrievers (Fusion, HyDe, Hierarchical, SCaNN), indexing methods (HNSW, IVF, Flat), similarity metrics (Cosine, Inner Product, L2), and rerankers (BGE, minilm) over datasets of three scales (small, medium, and large). Performance is assessed through coverage, recall, MRR, and nDCG, while semantic quality is measured using correctness, faithfulness, and relevance. Efficiency is quantified via latency, throughput, and computational cost. Our experiments reveal that HNSW–IP–Fusion–minilm achieves the strongest semantic performance, with Coverage Retrieval of 0.942, Correctness of 0.909, and Faithfulness of 0.970, making it ideal for accuracy-critical tasks. Conversely, IVF–L2–Hierarchical demonstrates the lowest latency (1.736 ns) and cost, making it suitable for real-time deployments. Reranker analysis shows modest but consistent gains for minilm over BGE, while HyDe excels in precision at the expense of efficiency. Notably, no single configuration dominates; optimal designs depend on the application’s needs, whether it is maximizing semantic accuracy, minimizing latency, or striking a balance between the two. By demonstrating concrete trade-offs, this work provides a practical foundation for scaling RAG pipelines across diverse domains, including information retrieval, enterprise search, and knowledge-intensive reasoning.",
    "title_zh": "EvaRAG：基于索引与距离度量的高级RAG技术评估",
    "abstract_zh": "检索增强生成（Retrieval Augmented Generation, RAG）已成为提升大语言模型（LLM）外部知识能力的强大范式。然而，RAG流水线的性能极易受到检索策略、相似度度量、索引方法及重排序等设计选择的影响。尽管RAG被广泛采用，但针对检索质量、语义准确性、计算效率与成本之间权衡关系的系统性研究仍十分有限。本研究填补了这一空白，通过多维度全面评估不同RAG配置的表现。我们提出了一套基准测试框架，系统地调整多种检索器（Fusion、HyDe、Hierarchical、SCaNN）、索引方法（HNSW、IVF、Flat）、相似度度量（余弦相似度、内积、L2距离）以及重排序器（BGE、minilm），并在小、中、大规模三个数据集上进行实验。性能评估涵盖覆盖率、召回率、平均秩（MRR）和归一化折损累计增益（nDCG），语义质量则通过正确性、忠实性和相关性进行衡量。效率方面，以延迟、吞吐量和计算成本为量化指标。实验结果表明，HNSW–IP–Fusion–minilm配置在语义表现上最为出色，其覆盖率为0.942，正确性达0.909，忠实性高达0.970，特别适用于对准确性要求极高的任务场景。相反，IVF–L2–Hierarchical配置展现出最低的延迟（1.736纳秒）和最低成本，非常适合实时部署。重排序器分析显示，minilm相较于BGE虽提升幅度不大，但表现稳定；而HyDe在精度方面表现优异，但牺牲了效率。值得注意的是，不存在一种“万能”配置能够全面胜出；最优设计需根据具体应用场景的需求而定——无论是追求最高语义准确性、最小延迟，还是两者之间的平衡。本研究通过揭示这些具体的权衡关系，为RAG流水线在信息检索、企业搜索以及知识密集型推理等多样化领域的规模化应用提供了切实可行的指导基础。"
  }
]