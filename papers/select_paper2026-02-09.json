[
  {
    "date": "2026-02-09",
    "title": "On Protecting Agentic Systems' Intellectual Property via Watermarking",
    "authors": "Liwen Wang, Zongjie Li, Yuchong Xie, Shuai Wang, Dongdong She, Wei Wang, Juergen Rahmel",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08401v1",
    "source": "arXiv",
    "abstract": "The evolution of Large Language Models (LLMs) into agentic systems that perform autonomous reasoning and tool use has created significant intellectual property (IP) value. We demonstrate that these systems are highly vulnerable to imitation attacks, where adversaries steal proprietary capabilities by training imitation models on victim outputs. Crucially, existing LLM watermarking techniques fail in this domain because real-world agentic systems often operate as grey boxes, concealing the internal reasoning traces required for verification. This paper presents AGENTWM, the first watermarking framework designed specifically for agentic models. AGENTWM exploits the semantic equivalence of action sequences, injecting watermarks by subtly biasing the distribution of functionally identical tool execution paths. This mechanism allows AGENTWM to embed verifiable signals directly into the visible action trajectory while remaining indistinguishable to users. We develop an automated pipeline to generate robust watermark schemes and a rigorous statistical hypothesis testing procedure for verification. Extensive evaluations across three complex domains demonstrate that AGENTWM achieves high detection accuracy with negligible impact on agent performance. Our results confirm that AGENTWM effectively protects agentic IP against adaptive adversaries, who cannot remove the watermarks without severely degrading the stolen model's utility.",
    "title_zh": "通过水印技术保护代理系统知识产权",
    "abstract_zh": "大型语言模型（LLM）向具备自主推理与工具使用能力的代理系统演进，已创造出巨大的知识产权（IP）价值。本文揭示了这类系统在模仿攻击下极为脆弱：攻击者可通过在目标系统的输出数据上训练模仿模型，窃取其专有功能。关键问题是，现有的LLM水印技术在此场景中失效，因为现实中的代理系统通常以“灰箱”模式运行，隐藏了验证所必需的内部推理轨迹。本文提出AGENTWM——首个专为代理模型设计的水印框架。AGENTWM利用动作序列的语义等价性，通过微妙地偏移功能等效的工具执行路径分布来注入水印信号。该机制能够在保持用户不可察觉的前提下，将可验证的信号直接嵌入可见的动作轨迹中。我们构建了一个自动化流水线，用于生成鲁棒的水印方案，并设计了一套严格的统计假设检验流程以实现验证。在三个复杂领域的广泛评估表明，AGENTWM在几乎不影响代理性能的情况下实现了高精度的检测能力。实验结果证实，AGENTWM能有效抵御自适应攻击者，后者若想移除水印，必将严重损害被窃模型的可用性。"
  },
  {
    "date": "2026-02-09",
    "title": "Software Testing at the Network Layer: Automated HTTP API Quality Assessment and Security Analysis of Production Web Applications",
    "authors": "Ali Hassaan Mughal, Muhammad Bilal",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08242v1",
    "source": "arXiv",
    "abstract": "Modern web applications rely heavily on client-side API calls to fetch data, render content, and communicate with backend services. However, the quality of these network interactions (redundant requests, missing cache headers, oversized payloads, and excessive third-party dependencies) is rarely tested in a systematic way. Moreover, many of these quality deficiencies carry security implications: missing cache headers enable cache poisoning, excessive third-party dependencies expand the supply-chain attack surface, and error responses risk leaking server internals. In this study, we present an automated software testing framework that captures and analyzes the complete HTTP traffic of 18 production websites spanning 11 categories (e-commerce, news, government, developer tools, travel, and more). Using automated browser instrumentation via Playwright, we record 108 HAR (HTTP Archive) files across 3 independent runs per page, then apply 8 heuristic-based anti-pattern detectors to produce a composite quality score (0-100) for each site. Our results reveal a wide quality spectrum: minimalist server-rendered sites achieve perfect scores of 100, while content-heavy commercial sites score as low as 56.8. We identify redundant API calls and missing cache headers as the two most pervasive anti-patterns, each affecting 67% of sites, while third-party overhead exceeds 20% on 72% of sites. One utility site makes 2,684 requests per page load, which is 447x more than the most minimal site. To protect site reputations, all identities are anonymized using category-based pseudonyms. We provide all analysis scripts, anonymized results, and reproducibility instructions as an open artifact. This work establishes an empirical baseline for HTTP API call quality across the modern web and offers a reproducible testing framework that researchers and practitioners can apply to their own applications.",
    "title_zh": "网络层的软件测试：生产环境Web应用的自动化HTTP API质量评估与安全分析",
    "abstract_zh": "现代网络应用严重依赖客户端API调用以获取数据、渲染内容并与其他后端服务通信。然而，这些网络交互的质量（如冗余请求、缺失缓存头、过大的数据负载以及过多的第三方依赖）很少得到系统性测试。此外，许多此类质量问题还可能带来安全风险：缺失的缓存头可能导致缓存投毒，过多的第三方依赖会扩大供应链攻击面，而错误响应则可能泄露服务器内部信息。在本研究中，我们提出了一种自动化软件测试框架，通过Playwright自动化浏览器注入技术，捕获并分析了18个生产环境网站的完整HTTP流量，这些网站覆盖11个类别（电子商务、新闻、政府、开发者工具、旅游等）。我们对每个页面进行了三次独立运行，共生成108个HAR（HTTP档案）文件，随后应用8个基于启发式的反模式检测器，为每个网站生成一个综合质量评分（0-100分）。研究结果揭示了显著的质量差异：采用极简服务端渲染的网站获得了满分100分，而内容密集型商业网站的得分低至56.8。我们识别出冗余API调用和缺失缓存头是两种最普遍的反模式，分别影响了67%的网站；同时，72%的网站中第三方资源开销超过20%。其中一个工具类网站在每次页面加载时发起2,684次请求，是最低开销网站的447倍。为保护网站声誉，所有身份信息均通过基于类别的伪名进行匿名化处理。我们已将所有分析脚本、匿名化结果及可复现性说明作为开放资源发布。本研究为现代网络中HTTP API调用质量建立了一个实证基准，并提供了一个可复现的测试框架，供研究人员和实践者用于评估其自身应用的质量。"
  },
  {
    "date": "2026-02-09",
    "title": "Dreaming in Code for Curriculum Learning in Open-Ended Worlds",
    "authors": "Konstantinos Mitsides, Maxence Faldor, Antoine Cully",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08194v1",
    "source": "arXiv",
    "abstract": "Open-ended learning frames intelligence as emerging from continual interaction with an ever-expanding space of environments. While recent advances have utilized foundation models to programmatically generate diverse environments, these approaches often focus on discovering isolated behaviors rather than orchestrating sustained progression. In complex open-ended worlds, the large combinatorial space of possible challenges makes it difficult for agents to discover sequences of experiences that remain consistently learnable. To address this, we propose Dreaming in Code (DiCode), a framework in which foundation models synthesize executable environment code to scaffold learning toward increasing competence. In DiCode, \"dreaming\" takes the form of materializing code-level variations of the world. We instantiate DiCode in Craftax, a challenging open-ended benchmark characterized by rich mechanics and long-horizon progression. Empirically, DiCode enables agents to acquire long-horizon skills, achieving a $16\\%$ improvement in mean return over the strongest baseline and non-zero success on late-game combat tasks where prior methods fail. Our results suggest that code-level environment design provides a practical mechanism for curriculum control, enabling the construction of intermediate environments that bridge competence gaps in open-ended worlds. Project page and source code are available at https://konstantinosmitsides.github.io/dreaming-in-code and https://github.com/konstantinosmitsides/dreaming-in-code.",
    "title_zh": "在开放世界中进行课程学习的代码梦境",
    "abstract_zh": "开放式学习框架将智能视为持续与不断扩展的环境空间互动中涌现的产物。尽管近期研究利用基础模型自动生成多样化的环境，但这些方法往往侧重于发现孤立的行为，而非实现持续的进展。在复杂的开放式世界中，可能挑战的组合空间极为庞大，导致智能体难以发现一系列始终可学习的经验序列。为解决这一问题，我们提出了“代码中的梦境”（Dreaming in Code, DiCode）框架，该框架利用基础模型合成可执行的环境代码，以引导学习逐步提升能力。在DiCode中，“梦境”体现为在代码层面生成世界的各种变体。我们将在Craftax这一具有挑战性的开放式基准中实现DiCode，该基准以丰富的机制和长周期进展为特征。实验结果表明，DiCode使智能体能够掌握长周期技能，在平均回报上比最强基线提升16%，并在以往方法失败的后期战斗任务中实现了非零成功率。我们的研究结果表明，基于代码的环境设计为课程控制提供了一种切实可行的机制，能够构建中间环境，弥合开放式世界中能力上的差距。项目主页和源代码可访问：https://konstantinosmitsides.github.io/dreaming-in-code 和 https://github.com/konstantinosmitsides/dreaming-in-code。"
  },
  {
    "date": "2026-02-09",
    "title": "Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI",
    "authors": "Feiyu Wu, Xu Zheng, Yue Qu, Zhuocheng Wang, Zicheng Feng, Hui Li",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08373v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repairs. We introduce the Verifiable Iterative Refinement Framework (VIRF), a neuro-symbolic architecture that shifts the paradigm from passive safety gatekeeping to active collaboration. Our core contribution is a tutor-apprentice dialogue where a deterministic Logic Tutor, grounded in a formal safety ontology, provides causal and pedagogical feedback to an LLM planner. This enables intelligent plan repairs rather than mere avoidance. We also introduce a scalable knowledge acquisition pipeline that synthesizes safety knowledge bases from real-world documents, correcting blind spots in existing benchmarks. In challenging home safety tasks, VIRF achieves a perfect 0 percent Hazardous Action Rate (HAR) and a 77.3 percent Goal-Condition Rate (GCR), which is the highest among all baselines. It is highly efficient, requiring only 1.1 correction iterations on average. VIRF demonstrates a principled pathway toward building fundamentally trustworthy and verifiably safe embodied agents.",
    "title_zh": "基于可验证逻辑的生成式规划器：一种可信具身人工智能的混合架构",
    "abstract_zh": "大型语言模型（LLMs）在具身人工智能的规划任务中展现出巨大潜力，但其固有的随机性缺乏形式化推理能力，导致在物理部署中难以提供严格的安保障。当前的方法通常依赖不可靠的LLM进行安全检查，或简单地拒绝不安全的计划，却无法提供修复方案。我们提出了可验证迭代精炼框架（Verifiable Iterative Refinement Framework, VIRF），这是一种神经符号架构，将安全机制的范式从被动的安全审查转变为积极的协作模式。我们的核心贡献在于构建了一种“导师-学徒”对话机制：由基于形式化安全本体的确定性逻辑导师，为LLM规划器提供因果性和教学性的反馈，从而实现智能的计划修复，而非简单的规避。此外，我们还设计了一条可扩展的知识获取管道，能够从真实世界文档中合成安全知识库，弥补现有基准测试中的知识盲区。在具有挑战性的家庭安全任务中，VIRF实现了0%的危险动作率（Hazardous Action Rate, HAR）和77.3%的目标达成率（Goal-Condition Rate, GCR），在所有基线方法中表现最优。该框架高效可靠，平均仅需1.1次修正迭代。VIRF为构建根本可信、可验证安全的具身智能体提供了一条严谨可行的技术路径。"
  },
  {
    "date": "2026-02-09",
    "title": "Next Concept Prediction in Discrete Latent Space Leads to Stronger Language Models",
    "authors": "Yuliang Liu, Yunchong Song, Yixuan Wang, Kewen Ge, Alex Lamb, Qipeng Guo, Kai Chen, Bowen Zhou, Zhouhan Lin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08984v1",
    "source": "arXiv",
    "abstract": "We propose Next Concept Prediction (NCP), a generative pretraining paradigm built on top of Next Token Prediction (NTP). NCP predicts discrete concepts that span multiple tokens, thereby forming a more challenging pretraining objective. Our model, ConceptLM, quantizes hidden states using Vector Quantization and constructs a concept vocabulary. It leverages both NCP and NTP to drive parameter updates and generates a concept to guide the generation of the following tokens. We train ConceptLM from scratch at scales ranging from 70M to 1.5B parameters with up to 300B training data, including Pythia and GPT-2 backbones. Results on 13 benchmarks show that NCP yields consistent performance gains over traditional token-level models. Furthermore, continual pretraining experiments on an 8B-parameter Llama model indicate that NCP can further improve an NTP-trained model. Our analysis suggests that NCP leads to more powerful language models by introducing a harder pretraining task, providing a promising path toward better language modeling.",
    "title_zh": "离散潜在空间中的下一概念预测可带来更强的语言模型",
    "abstract_zh": "我们提出了下一概念预测（Next Concept Prediction, NCP），这是一种建立在下一标记预测（Next Token Prediction, NTP）之上的生成式预训练范式。NCP 旨在预测跨越多个标记的离散概念，从而形成更具挑战性的预训练目标。我们的模型 ConceptLM 通过向量量化（Vector Quantization）对隐藏状态进行量化，并构建概念词汇表。该模型同时利用 NCP 和 NTP 来驱动参数更新，并生成一个概念以指导后续标记的生成。我们在 7000 万到 15 亿参数规模下从零开始训练 ConceptLM，使用最多达 3000 亿的训练数据，涵盖 Pythia 和 GPT-2 的骨干网络。在 13 个基准测试上的结果表明，NCP 相较于传统的基于标记级别的模型，能够持续带来性能提升。此外，在一个 80 亿参数的 Llama 模型上进行的持续预训练实验也表明，NCP 能够进一步提升经过 NTP 训练的模型性能。我们的分析表明，NCP 通过引入更具挑战性的预训练任务，使语言模型更具表现力，为实现更优的语言建模提供了一条有前景的路径。"
  },
  {
    "date": "2026-02-09",
    "title": "SkillRL: Evolving Agents via Recursive Skill-Augmented Reinforcement Learning",
    "authors": "Peng Xia, Jianwen Chen, Hanyang Wang, Jiaqi Liu, Kaide Zeng, Yu Wang, Siwei Han, Yiyang Zhou, Xujiang Zhao, Haifeng Chen, Zeyu Zheng, Cihang Xie, Huaxiu Yao",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08234v1",
    "source": "arXiv",
    "abstract": "Large Language Model (LLM) agents have shown stunning results in complex tasks, yet they often operate in isolation, failing to learn from past experiences. Existing memory-based methods primarily store raw trajectories, which are often redundant and noise-heavy. This prevents agents from extracting high-level, reusable behavioral patterns that are essential for generalization. In this paper, we propose SkillRL, a framework that bridges the gap between raw experience and policy improvement through automatic skill discovery and recursive evolution. Our approach introduces an experience-based distillation mechanism to build a hierarchical skill library SkillBank, an adaptive retrieval strategy for general and task-specific heuristics, and a recursive evolution mechanism that allows the skill library to co-evolve with the agent's policy during reinforcement learning. These innovations significantly reduce the token footprint while enhancing reasoning utility. Experimental results on ALFWorld, WebShop and seven search-augmented tasks demonstrate that SkillRL achieves state-of-the-art performance, outperforming strong baselines over 15.3% and maintaining robustness as task complexity increases. Code is available at this https://github.com/aiming-lab/SkillRL.",
    "title_zh": "SkillRL：通过递归技能增强的强化学习演化智能体",
    "abstract_zh": "大型语言模型（LLM）代理在复杂任务中已展现出惊人的性能，但它们通常孤立运行，难以从过往经验中学习。现有的基于记忆的方法主要存储原始轨迹，而这些轨迹往往冗余且充满噪声，导致代理无法提取高层次、可复用的行为模式，而这对于实现泛化能力至关重要。本文提出了一种名为SkillRL的框架，通过自动技能发现与递归演化，弥合原始经验与策略优化之间的鸿沟。我们的方法引入了一种基于经验的提炼机制，构建了一个分层的技能库SkillBank；设计了一种自适应检索策略，用于获取通用及任务特定的启发式规则；并提出了一种递归演化机制，使技能库能够与代理策略在强化学习过程中共同进化。这些创新显著减少了令牌（token）使用量，同时提升了推理效率。在ALFWorld、WebShop以及七个搜索增强型任务上的实验结果表明，SkillRL达到了当前最优性能，相比强基线平均提升超过15.3%，且在任务复杂度增加时仍保持稳健表现。代码已开源，地址为：https://github.com/aiming-lab/SkillRL。"
  },
  {
    "date": "2026-02-09",
    "title": "Fork, Explore, Commit: OS Primitives for Agentic Exploration",
    "authors": "Cong Wang, Yusheng Zheng",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08199v1",
    "source": "arXiv",
    "abstract": "AI agents increasingly perform agentic exploration: pursuing multiple solution paths in parallel and committing only the successful one. Because each exploration path may modify files and spawn processes, agents require isolated environments with atomic commit and rollback semantics for both filesystem state and process state. We introduce the branch context, a new OS abstraction that provides: (1) copy-on-write state isolation with independent filesystem views and process groups, (2) a structured lifecycle of fork, explore, and commit/abort, (3) first-commit-wins resolution that automatically invalidates sibling branches, and (4) nestable contexts for hierarchical exploration. We realize branch contexts in Linux through two complementary components. First, BranchFS is a FUSE-based filesystem that gives each branch context an isolated copy-on-write workspace, with O(1) creation, atomic commit to the parent, and automatic sibling invalidation, all without root privileges. BranchFS is open sourced in https://github.com/multikernel/branchfs. Second, branch() is a proposed Linux syscall that spawns processes into branch contexts with reliable termination, kernel-enforced sibling isolation, and first-commit-wins coordination. Preliminary evaluation of BranchFS shows sub-350 us branch creation independent of base filesystem size, and modification-proportional commit overhead (under 1 ms for small changes).",
    "title_zh": "分叉、探索、提交：智能体探索的系统原语",
    "abstract_zh": "AI代理正越来越多地执行代理式探索：并行尝试多种解决方案路径，仅保留成功的一条。由于每条探索路径都可能修改文件并创建进程，因此代理需要具备原子提交与回滚语义的隔离环境，以确保文件系统状态和进程状态的一致性。我们提出了“分支上下文”（branch context）这一新的操作系统抽象，它提供以下特性：（1）基于写时复制的状态隔离，支持独立的文件系统视图和进程组；（2）结构化的生命周期——包括 fork、探索、提交/中止三个阶段；（3）首次提交优先的冲突解决机制，可自动使兄弟分支失效；（4）支持嵌套的上下文，实现分层探索。\n\n我们在 Linux 系统中通过两个互补组件实现了分支上下文。首先，BranchFS 是一个基于 FUSE 的文件系统，为每个分支上下文提供一个隔离的写时复制工作区，支持 O(1) 时间复杂度的创建、对父级的原子提交，以及自动的兄弟分支失效，且无需 root 权限。BranchFS 已开源，地址为 https://github.com/multikernel/branchfs。其次，branch() 是一项提议中的 Linux 系统调用，用于将进程启动到分支上下文中，支持可靠的终止、内核强制的兄弟分支隔离，以及首次提交优先的协调机制。对 BranchFS 的初步评估表明，分支创建时间低于 350 微秒，且与基础文件系统大小无关；提交开销与修改量成正比（对于小规模修改，提交耗时低于 1 毫秒）。"
  },
  {
    "date": "2026-02-09",
    "title": "DyMA-Fuzz: Dynamic Direct Memory Access Abstraction for Re-hosted Monolithic Firmware Fuzzing",
    "authors": "Guy Farrelly, Michael Chesser, Seyit Camtepe, Damith C. Ranasinghe",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08750v1",
    "source": "arXiv",
    "abstract": "The rise of smart devices in critical domains--including automotive, medical, industrial--demands robust firmware testing. Fuzzing firmware in re-hosted environments is a promising method for automated testing at scale, but remains difficult due to the tight coupling of code with a microcontroller's peripherals. Existing fuzzing frameworks primarily address input challenges in providing inputs for Memory-Mapped I/O or interrupts, but largely overlook Direct Memory Access (DMA), a key high-throughput interface used that bypasses the CPU. We introduce DyMA-Fuzz to extend recent advances in stream-based fuzz input injection to DMA-driven interfaces in re-hosted environments. It tackles key challenges--vendor-specific descriptors, heterogeneous DMA designs, and varying descriptor locations--using runtime analysis techniques to infer DMA memory access patterns and automatically inject fuzzing data into target buffers, without manual configuration or datasheets. Evaluated on 94 firmware samples and 8 DMA-guarded CVE benchmarks, DyMA-Fuzz reveals vulnerabilities and execution paths missed by state-of-the-art tools and achieves up to 122% higher code coverage. These results highlight DyMA-Fuzz as a practical and effective advancement in automated firmware testing and a scalable solution for fuzzing complex embedded systems.",
    "title_zh": "DyMA-Fuzz：用于重构单体固件模糊测试的动态直接内存访问抽象",
    "abstract_zh": "智能设备在汽车、医疗、工业等关键领域中的广泛应用，对固件测试的可靠性提出了更高要求。在重新托管（re-hosted）环境中对固件进行模糊测试（fuzzing）是一种具有前景的大规模自动化测试方法，但由于代码与微控制器外设之间存在紧密耦合，该方法仍面临诸多挑战。现有的模糊测试框架主要关注如何为内存映射I/O或中断提供输入，却普遍忽视了直接内存访问（DMA）这一关键的高吞吐接口——DMA能够绕过CPU直接进行数据传输。为此，我们提出了DyMA-Fuzz，将近期基于流的模糊输入注入技术扩展至重新托管环境中的DMA驱动接口。DyMA-Fuzz通过运行时分析技术，有效应对了若干核心挑战：厂商特定的描述符、异构的DMA设计以及描述符位置的差异，能够自动推断DMA的内存访问模式，并将模糊测试数据注入目标缓冲区，无需人工配置或依赖数据手册。在94个固件样本和8个受DMA保护的CVE基准测试中进行评估，DyMA-Fuzz成功发现了现有先进工具未能覆盖的漏洞和执行路径，代码覆盖率最高提升达122%。这些结果表明，DyMA-Fuzz不仅在自动化固件测试方面具有实际应用价值，也为复杂嵌入式系统的可扩展模糊测试提供了有效解决方案。"
  },
  {
    "date": "2026-02-09",
    "title": "PRISM: A Principled Framework for Multi-Agent Reasoning via Gain Decomposition",
    "authors": "Yiming Yang, Zhuoyuan Li, Fanxiang Zeng, Hao Fu, Yue Liu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08586v1",
    "source": "arXiv",
    "abstract": "Multi-agent collaboration has emerged as a promising paradigm for enhancing reasoning capabilities of Large Language Models (LLMs). However, existing approaches remain largely heuristic, lacking principled guidance on what drives performance gains and how to systematically optimize multi-agent reasoning. Specifically, it remains unclear why multi-agent collaboration outperforms single-agent reasoning and which design choices contribute most to these gains, making it difficult to build better systems. We address this gap by introducing a unified theoretical framework that decomposes multi-agent reasoning gains into three conceptually independent dimensions: Exploration for diverse solution coverage, Information for high-fidelity feedback, and Aggregation for principled consensus. Through this lens, existing methods can be understood as special cases that optimize only subsets of these dimensions. Building upon this decomposition, a novel framework called PRISM (Propose-Review-Integrate Synthesis for Multi-agent Reasoning) is proposed, which jointly maximizes all three dimensions through role-based diversity, execution-grounded feedback with evidence-based cross-evaluation, and iterative synthesis with closed-loop validation. Extensive experiments across mathematical reasoning, code generation, and function calling benchmarks demonstrate that PRISM achieves state-of-the-art performance with superior compute-efficiency compared to methods optimizing partial dimensions. The theoretical framework provides actionable design principles for future multi-agent reasoning systems.",
    "title_zh": "PRISM：一种基于收益分解的多智能体推理原则性框架",
    "abstract_zh": "多智能体协作已成为提升大语言模型（LLM）推理能力的一种有前景的范式。然而，现有方法大多依赖启发式策略，缺乏对性能提升本质原因的理论指导，也缺少系统化优化多智能体推理的有效方法。具体而言，目前仍不清楚为何多智能体协作优于单智能体推理，以及哪些设计选择对性能提升贡献最大，这使得构建更优系统变得困难。为填补这一空白，本文提出一个统一的理论框架，将多智能体推理的性能增益分解为三个概念上相互独立的维度：探索（以覆盖多样化的解决方案）、信息（以获取高保真反馈）和聚合（以实现有原则的共识）。通过这一视角，现有方法可被理解为仅优化其中部分维度的特例。基于该分解，本文进一步提出一种名为PRISM（多智能体推理中的提议-评审-整合合成框架）的新方法，通过角色分工实现多样性、基于执行过程的反馈结合基于证据的交叉评估，以及带有闭环验证的迭代合成，从而协同最大化上述三个维度。在数学推理、代码生成和函数调用等多个基准上的大量实验表明，PRISM在性能上达到当前最优水平，同时相比仅优化部分维度的方法展现出更优的计算效率。该理论框架为未来多智能体推理系统的设计提供了可操作的指导原则。"
  },
  {
    "date": "2026-02-09",
    "title": "Automating Computational Reproducibility in Social Science: Comparing Prompt-Based and Agent-Based Approaches",
    "authors": "Syed Mehtab Hussain Shah, Frank Hopfgartner, Arnim Bleier",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08561v1",
    "source": "arXiv",
    "abstract": "Reproducing computational research is often assumed to be as simple as rerunning the original code with provided data. In practice, missing packages, fragile file paths, version conflicts, or incomplete logic frequently cause analyses to fail, even when materials are shared. This study investigates whether large language models and AI agents can automate the diagnosis and repair of such failures, making computational results easier to reproduce and verify. We evaluate this using a controlled reproducibility testbed built from five fully reproducible R-based social science studies. Realistic failures were injected, ranging from simple issues to complex missing logic, and two automated repair workflows were tested in clean Docker environments. The first workflow is prompt-based, repeatedly querying language models with structured prompts of varying context, while the second uses agent-based systems that inspect files, modify code, and rerun analyses autonomously. Across prompt-based runs, reproduction success ranged from 31-79 percent, with performance strongly influenced by prompt context and error complexity. Complex cases benefited most from additional context. Agent-based workflows performed substantially better, with success rates of 69-96 percent across all complexity levels. These results suggest that automated workflows, especially agent-based systems, can significantly reduce manual effort and improve reproduction success across diverse error types. Unlike prior benchmarks, our testbed isolates post-publication repair under controlled failure modes, allowing direct comparison of prompt-based and agent-based approaches.",
    "title_zh": "社会科学中计算可重现性的自动化：基于提示与基于代理方法的比较",
    "abstract_zh": "复现计算研究通常被认为只需重新运行原始代码并使用提供的数据即可完成。然而在实践中，缺失的软件包、脆弱的文件路径、版本冲突或逻辑不完整等问题常常导致分析失败，即使研究材料已公开共享。本研究探讨了大型语言模型和AI代理是否能够自动化诊断并修复此类失败，从而让计算结果更易于复现与验证。我们通过一个基于五个完全可复现的R语言社会科学研究构建的受控复现测试平台来评估这一设想。在其中注入了从简单问题到复杂逻辑缺失等各类现实故障，并在干净的Docker环境中测试了两种自动修复流程。第一种流程基于提示（prompt-based），通过结构化提示以不同上下文反复调用语言模型；第二种流程采用基于代理的系统，能够自主检查文件、修改代码并重新运行分析。在基于提示的实验中，复现成功率在31%至79%之间波动，且表现显著受提示上下文和错误复杂度的影响，复杂案例尤其受益于更多上下文信息。而基于代理的流程表现明显更优，在所有复杂度水平下成功率达到69%至96%。这些结果表明，自动化流程——尤其是基于代理的系统——能够显著降低人工工作量，并在多种错误类型下提升复现成功率。与以往基准不同，我们的测试平台在受控的故障模式下聚焦于发表后的修复过程，从而实现了对基于提示与基于代理方法的直接对比。"
  },
  {
    "date": "2026-02-09",
    "title": "Characterizing, Evaluating, and Optimizing Complex Reasoning",
    "authors": "Haoran Zhang, Yafu Li, Zhi Wang, Zhilin Wang, Shunkai Zhang, Xiaoye Qu, Yu Cheng",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08498v1",
    "source": "arXiv",
    "abstract": "Large Reasoning Models (LRMs) increasingly rely on reasoning traces with complex internal structures. However, existing work lacks a unified answer to three fundamental questions: (1) what defines high-quality reasoning, (2) how to reliably evaluate long, implicitly structured reasoning traces, and (3) how to use such evaluation signals for reasoning optimization. To address these challenges, we provide a unified perspective. (1) We introduce the ME$^2$ principle to characterize reasoning quality along macro- and micro-level concerning efficiency and effectiveness. (2) Built on this principle, we model reasoning traces as directed acyclic graphs (DAGs) and develop a DAG-based pairwise evaluation method, capturing complex reasoning structures. (3) Based on this method, we construct the TRM-Preference dataset and train a Thinking Reward Model (TRM) to evaluate reasoning quality at scale. Experiments show that thinking rewards serve as an effective optimization signal. At test time, selecting better reasoning leads to better outcomes (up to 19.3% gain), and during RL training, thinking rewards enhance reasoning and performance (up to 3.9% gain) across diverse tasks.",
    "title_zh": "复杂推理的表征、评估与优化",
    "abstract_zh": "大型推理模型（LRMs）越来越多地依赖于具有复杂内部结构的推理轨迹。然而，现有研究尚未对三个基本问题提供统一解答：（1）高质量推理的定义是什么？（2）如何可靠地评估长且隐式结构的推理轨迹？（3）如何利用此类评估信号进行推理优化？为应对这些挑战，我们提出了一种统一的视角：（1）我们引入ME$^2$原则，从宏观和微观两个层面，从效率与效果两个维度刻画推理质量；（2）基于该原则，我们将推理轨迹建模为有向无环图（DAG），并开发了一种基于DAG的成对评估方法，以捕捉复杂的推理结构；（3）基于该方法，我们构建了TRM-Preference数据集，并训练了一个思维奖励模型（TRM），实现对推理质量的大规模评估。实验表明，思维奖励可作为有效的优化信号：在测试阶段，选择更优的推理路径可显著提升结果（最高提升19.3%）；在强化学习训练过程中，思维奖励能有效提升推理能力与整体性能（最高提升3.9%），且在多种任务中均表现出色。"
  },
  {
    "date": "2026-02-09",
    "title": "From Assistant to Double Agent: Formalizing and Benchmarking Attacks on OpenClaw for Personalized Local AI Agent",
    "authors": "Yuhang Wang, Feiming Xu, Zheng Lin, Guangyu He, Yuzhe Huang, Haichang Gao, Zhenxing Niu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08412v1",
    "source": "arXiv",
    "abstract": "Although large language model (LLM)-based agents, exemplified by OpenClaw, are increasingly evolving from task-oriented systems into personalized AI assistants for solving complex real-world tasks, their practical deployment also introduces severe security risks. However, existing agent security research and evaluation frameworks primarily focus on synthetic or task-centric settings, and thus fail to accurately capture the attack surface and risk propagation mechanisms of personalized agents in real-world deployments. To address this gap, we propose Personalized Agent Security Bench (PASB), an end-to-end security evaluation framework tailored for real-world personalized agents. Building upon existing agent attack paradigms, PASB incorporates personalized usage scenarios, realistic toolchains, and long-horizon interactions, enabling black-box, end-to-end security evaluation on real systems. Using OpenClaw as a representative case study, we systematically evaluate its security across multiple personalized scenarios, tool capabilities, and attack types. Our results indicate that OpenClaw exhibits critical vulnerabilities at different execution stages, including user prompt processing, tool usage, and memory retrieval, highlighting substantial security risks in personalized agent deployments. The code for the proposed PASB framework is available at https://github.com/AstorYH/PASB.",
    "title_zh": "从助手到双重间谍：面向个性化本地AI代理OpenClaw的攻击形式化与基准测试",
    "abstract_zh": "尽管以 OpenClaw 为代表的基于大语言模型（LLM）的智能体正逐步从任务导向型系统演变为能够解决复杂现实任务的个性化 AI 助手，但其实际部署也带来了严重的安全风险。然而，现有的智能体安全研究与评估框架主要聚焦于合成环境或任务中心化场景，难以准确捕捉个性化智能体在真实部署中所面临的攻击面及其风险传播机制。为填补这一空白，我们提出了 Personalized Agent Security Bench（PASB）——一个专为真实世界个性化智能体设计的端到端安全评估框架。PASB 在现有智能体攻击范式的基础上，融入了个性化使用场景、真实工具链以及长周期交互，实现了对真实系统的黑盒、端到端安全评估。以 OpenClaw 作为代表性案例，我们系统地评估了其在多种个性化场景、工具能力及攻击类型下的安全性。结果表明，OpenClaw 在不同执行阶段均存在关键漏洞，包括用户提示处理、工具调用以及记忆检索等环节，凸显了个性化智能体部署中的重大安全风险。本文提出的 PASB 框架代码已开源，地址为：https://github.com/AstorYH/PASB。"
  },
  {
    "date": "2026-02-09",
    "title": "Modalities, a PyTorch-native Framework For Large-scale LLM Training and Research",
    "authors": "Max Lübbering, Timm Ruland, Richard Rutmann, Felix Stollenwerk, David Fitzek, Michael Fromm, Alexander Weber, Rafet Sifa, Nicolas Flores-Herr, Joachim Köhler, Mehdi Ali",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08387v1",
    "source": "arXiv",
    "abstract": "Today's LLM (pre-) training and research workflows typically allocate a significant amount of compute to large-scale ablation studies. Despite the substantial compute costs of these ablations, existing open-source frameworks provide limited tooling for these experiments, often forcing researchers to write their own wrappers and scripts. We propose Modalities, an end-to-end PyTorch-native framework that integrates data-driven LLM research with large-scale model training from two angles. Firstly, by integrating state-of-the-art parallelization strategies, it enables both efficient pretraining and systematic ablations at trillion-token and billion-parameter scale. Secondly, Modalities adopts modular design with declarative, self-contained configuration, enabling reproducibility and extensibility levels that are difficult to achieve out-of-the-box with existing LLM training frameworks.",
    "title_zh": "模态（Modalities），一个原生基于 PyTorch 的大规模大语言模型训练与研究框架",
    "abstract_zh": "当前的大规模语言模型（预）训练与研究工作流程通常需要投入大量计算资源进行大规模消融实验。尽管这些消融实验的计算成本高昂，现有的开源框架在支持此类实验方面仍缺乏足够的工具，研究人员往往不得不自行编写封装代码和脚本。为此，我们提出了Modalities——一个端到端的原生PyTorch框架，从两个方面将数据驱动的LLM研究与大规模模型训练紧密结合：首先，通过集成最先进的并行化策略，该框架能够在万亿级token和数十亿参数规模下，同时实现高效的预训练与系统化的消融实验；其次，Modalities采用模块化设计，支持声明式、自包含的配置方式，从而实现了难以通过现有LLM训练框架直接获得的可复现性与可扩展性水平。"
  },
  {
    "date": "2026-02-09",
    "title": "Towards Real-World Industrial-Scale Verification: LLM-Driven Theorem Proving on seL4",
    "authors": "Jianyu Zhang, Fuyuan Zhang, Jiayi Lu, Jilin Hu, Xiaoyi Yin, Long Zhang, Feng Yang, Yongwang Zhao",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08384v1",
    "source": "arXiv",
    "abstract": "Formal methods (FM) are reliable but costly to apply, often requiring years of expert effort in industrial-scale projects such as seL4, especially for theorem proving. Recent advances in large language models (LLMs) have made automated theorem proving increasingly feasible. However, most prior work focuses on mathematics-oriented benchmarks such as miniF2F, with limited evaluation on real-world verification projects. The few studies that consider industrial-scale verification mostly rely on closed-source models with hundreds of billions of parameters, which cannot be locally deployed and incur substantial usage costs. In this paper, we propose AutoReal, an LLM-driven theorem proving method for real-world industrial-scale systems with support for lightweight local deployment. We evaluate AutoReal on the seL4-Isabelle verification project as a representative and challenging case study. AutoReal incorporates two key improvements: (1) chain-of-thought (CoT)-based proof training, which teaches the LLM the reasoning behind proof steps and enables step-wise explanations alongside proofs, and (2) context augmentation, which leverages proof context from the project to enhance LLM-driven proving. Based on the AutoReal methodology, we fine-tune a base model to obtain AutoReal-Prover, a compact 7B-scale prover for industrial-scale theorem proving. AutoReal-Prover achieves a 51.67% proof success rate on 660 theorems from seL4-designated Important Theories across all 10 seL4 proof categories, substantially outperforming prior attempts on seL4 (27.06%). To evaluate generalization, we further apply AutoReal-Prover to three security-related projects from the Archive of Formal Proofs (AFP), covering all 451 theorems and achieving a proof success rate of 53.88%. Overall, this work advances the application of LLM-driven theorem proving in real-world industrial-scale verification.",
    "title_zh": "迈向现实世界工业级验证：基于大语言模型的 seL4 定理证明",
    "abstract_zh": "形式化方法（FM）虽然可靠，但在工业级项目（如 seL4）中应用成本高昂，通常需要专家多年努力，尤其是在定理证明方面。近年来，大型语言模型（LLMs）的进展使得自动化定理证明变得日益可行。然而，以往大多数研究主要集中在数学导向的基准测试（如 miniF2F），对真实世界验证项目的评估十分有限。少数关注工业级验证的研究大多依赖于参数量达数千亿的闭源模型，这些模型无法本地部署，且使用成本高昂。本文提出 AutoReal，一种面向真实世界工业级系统的 LLM 驱动定理证明方法，支持轻量级本地部署。我们以 seL4-Isabelle 验证项目作为代表性且具有挑战性的案例进行评估。AutoReal 包含两项关键改进：（1）基于思维链（CoT）的证明训练，使 LLM 学习证明步骤背后的推理逻辑，从而在生成证明的同时提供逐步解释；（2）上下文增强技术，利用项目本身的证明上下文来提升 LLM 驱动的证明能力。基于 AutoReal 方法论，我们对基础模型进行微调，得到一个紧凑的 7B 规模定理证明器——AutoReal-Prover，专为工业级定理证明设计。在 seL4-指定的重要理论中，AutoReal-Prover 在全部 10 个 seL4 证明类别共 660 个定理上实现了 51.67% 的证明成功率，显著优于以往在 seL4 上的尝试（27.06%）。为进一步评估泛化能力，我们将 AutoReal-Prover 应用于来自形式化证明档案（AFP）的三个安全相关项目，覆盖全部 451 个定理，取得了 53.88% 的证明成功率。总体而言，本工作推动了 LLM 驱动定理证明在真实世界工业级验证中的实际应用。"
  },
  {
    "date": "2026-02-09",
    "title": "Does Your Reasoning Model Implicitly Know When to Stop Thinking?",
    "authors": "Zixuan Huang, Xin Xia, Yuxi Ren, Jianbin Zheng, Xuanda Wang, Zhixia Zhang, Hongyan Xie, Songshi Liang, Zehao Chen, Xuefeng Xiao, Fuzhen Zhuang, Jianxin Li, Yikun Ban, Deqing Wang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08354v1",
    "source": "arXiv",
    "abstract": "Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.",
    "title_zh": "你的推理模型是否隐含地知道何时停止思考？",
    "abstract_zh": "近年来，大型推理模型（LRMs）通过采用长思维链（CoTs）在复杂推理任务上取得了显著进步。然而，这种方法常常导致大量冗余，降低了计算效率，并在实时应用中造成显著延迟。近期研究发现，更长的推理链并不一定与正确性正相关，甚至可能损害模型的准确性。通过对这一现象的深入分析，我们意外地发现并实证验证了：LRMs 本质上知道何时应停止思考，但这一能力被当前的采样范式所掩盖。受此启发，我们提出了 SAGE（Self-Aware Guided Efficient Reasoning），一种全新的采样范式，旨在释放模型内在的高效推理潜力。此外，将 SAGE 作为混合采样策略融入基于群体的强化学习（SAGE-RL），使 SAGE-RL 能够有效将 SAGE 发现的高效推理模式整合到标准的 pass@1 推理中，显著提升了 LRMs 在多个具有挑战性的数学基准测试中的推理准确率与效率。"
  },
  {
    "date": "2026-02-09",
    "title": "When Actions Go Off-Task: Detecting and Correcting Misaligned Actions in Computer-Use Agents",
    "authors": "Yuting Ning, Jaylen Jones, Zhehao Zhang, Chentao Ye, Weitong Ruan, Junyi Li, Rahul Gupta, Huan Sun",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08995v1",
    "source": "arXiv",
    "abstract": "Computer-use agents (CUAs) have made tremendous progress in the past year, yet they still frequently produce misaligned actions that deviate from the user's original intent. Such misaligned actions may arise from external attacks (e.g., indirect prompt injection) or from internal limitations (e.g., erroneous reasoning). They not only expose CUAs to safety risks, but also degrade task efficiency and reliability. This work makes the first effort to define and study misaligned action detection in CUAs, with comprehensive coverage of both externally induced and internally arising misaligned actions. We further identify three common categories in real-world CUA deployment and construct MisActBench, a benchmark of realistic trajectories with human-annotated, action-level alignment labels. Moreover, we propose DeAction, a practical and universal guardrail that detects misaligned actions before execution and iteratively corrects them through structured feedback. DeAction outperforms all existing baselines across offline and online evaluations with moderate latency overhead: (1) On MisActBench, it outperforms baselines by over 15% absolute in F1 score; (2) In online evaluation, it reduces attack success rate by over 90% under adversarial settings while preserving or even improving task success rate in benign environments.",
    "title_zh": "当行为偏离任务：计算机使用代理中错位行为的检测与纠正",
    "abstract_zh": "在过去一年中，计算机使用代理（CUAs）取得了巨大进展，但仍频繁产生与用户原始意图不符的错误行为。这些不一致的行为可能源于外部攻击（如间接提示注入）或内部局限性（如错误推理）。这类行为不仅使CUAs面临安全风险，还降低了任务的效率与可靠性。本文首次系统地定义并研究了CUA中的不一致行为检测问题，全面覆盖了由外部引发和内部产生的各类不一致行为。我们进一步在真实场景的CUA部署中识别出三种常见模式，并构建了MisActBench——一个包含真实操作轨迹、经人工标注且在动作级别具有对齐标签的基准数据集。此外，我们提出了DeAction，一种实用且通用的防护机制，能够在行为执行前检测不一致行为，并通过结构化反馈进行迭代修正。在离线与在线评估中，DeAction均显著优于现有基线方法，且仅带来适度的延迟开销：（1）在MisActBench上，其F1分数相比基线提升超过15个百分点；（2）在在线评估中，在对抗性环境下将攻击成功率降低超过90%，同时在良性环境中保持甚至提升了任务成功率。"
  },
  {
    "date": "2026-02-09",
    "title": "Discovering Interpretable Algorithms by Decompiling Transformers to RASP",
    "authors": "Xinting Huang, Aleksandra Bakalova, Satwik Bhattamishra, William Merrill, Michael Hahn",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08857v1",
    "source": "arXiv",
    "abstract": "Recent work has shown that the computations of Transformers can be simulated in the RASP family of programming languages. These findings have enabled improved understanding of the expressive capacity and generalization abilities of Transformers. In particular, Transformers have been suggested to length-generalize exactly on problems that have simple RASP programs. However, it remains open whether trained models actually implement simple interpretable programs. In this paper, we present a general method to extract such programs from trained Transformers. The idea is to faithfully re-parameterize a Transformer as a RASP program and then apply causal interventions to discover a small sufficient sub-program. In experiments on small Transformers trained on algorithmic and formal language tasks, we show that our method often recovers simple and interpretable RASP programs from length-generalizing transformers. Our results provide the most direct evidence so far that Transformers internally implement simple RASP programs.",
    "title_zh": "通过反编译Transformer模型以发现可解释的算法——RASP",
    "abstract_zh": "近期研究显示，Transformer 的计算过程可以在 RASP 程序语言家族中被模拟。这些发现有助于更深入地理解 Transformer 的表达能力与泛化性能。特别是有研究指出，当问题具有简单的 RASP 程序时，Transformer 能够精确地实现长度泛化。然而，目前仍不清楚经过训练的模型是否实际上实现了简单且可解释的程序。本文提出了一种通用方法，用于从训练好的 Transformer 中提取此类程序。其核心思想是将 Transformer 精确地重参数化为 RASP 程序，然后通过因果干预识别出一个简洁且充分的子程序。在针对小规模 Transformer 模型在算法任务和形式语言任务上的实验中，我们发现该方法通常能够从实现长度泛化的 Transformer 中恢复出简单且可解释的 RASP 程序。我们的结果为 Transformer 内部实际实现简单 RASP 程序提供了迄今为止最直接的证据。"
  },
  {
    "date": "2026-02-09",
    "title": "Taming Scylla: Understanding the multi-headed agentic daemon of the coding seas",
    "authors": "Micah Villmow",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08765v1",
    "source": "arXiv",
    "abstract": "LLM-based tools are automating more software development tasks at a rapid pace, but there is no rigorous way to evaluate how different architectural choices -- prompts, skills, tools, multi-agent setups -- materially affect both capability and cost. This paper introduces Scylla, an evaluation framework for benchmarking agentic coding tools through structured ablation studies that uses seven testing tiers (T0-T6) progressively adding complexity to isolate what directly influences results and how. The key metric is Cost-of-Pass (CoP): the expected dollar cost to get one correct solution, which directly quantifies the trade-off between complexity and efficiency. The framework is model-agnostic, designed to work with any CLI tool; this paper demonstrates it with Claude Sonnet 4.5, using multiple LLM judges (Opus 4.5, Sonnet 4.5, Haiku 4.5) from the same vendor for evaluation consensus, where judges score results using direct tests, human-designed LLM-evaluated rubrics, and qualitative assessment. The result is a reproducible framework that quantifies trade-offs between agent complexity and actual outcomes, suggesting that architectural complexity does not always improve quality.",
    "title_zh": "驯服斯库拉：理解代码之海中多头的代理守护者",
    "abstract_zh": "基于大语言模型（LLM）的工具正在以惊人的速度自动化越来越多的软件开发任务，但目前尚缺乏严谨的方法来评估不同架构选择——如提示设计、技能配置、工具集成以及多智能体设置等——对能力与成本的实际影响。本文提出了Scylla，一个用于基准测试智能体式编程工具的评估框架。该框架通过结构化的消融研究，采用七个逐步增加复杂度的测试层级（T0-T6），以隔离并识别直接影响结果的因素及其作用机制。其核心指标为“通过成本”（Cost-of-Pass, CoP）：即获得一个正确解决方案所需的预期美元成本，该指标直接量化了复杂性与效率之间的权衡关系。该框架具有模型无关性，可适配任何命令行工具；本文以Claude Sonnet 4.5为例进行演示，并使用同一供应商提供的多个LLM评判器（Opus 4.5、Sonnet 4.5、Haiku 4.5）进行评估共识，评判器通过直接测试、人工设计的LLM评估标准以及定性分析对结果进行打分。最终，该框架实现了可复现的评估体系，能够量化智能体架构复杂性与实际产出之间的权衡关系，研究结果表明：架构复杂性并不总能提升质量。"
  },
  {
    "date": "2026-02-09",
    "title": "Craig Interpolation in Program Verification",
    "authors": "Philipp Rümmer",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08532v1",
    "source": "arXiv",
    "abstract": "Craig interpolation is used in program verification for automating key tasks such as the inference of loop invariants and the computation of program abstractions. This chapter covers some of the most important techniques that have been developed in this context over the last years, focusing on two aspects: the derivation of Craig interpolants modulo the theories and data types used in verification and the basic design of verification algorithms applying interpolation.",
    "title_zh": "程序验证中的Craig插值",
    "abstract_zh": "Craig插值法在程序验证中被用于自动化许多关键任务，例如循环不变量的推断以及程序抽象的计算。本章涵盖了近年来在此领域内发展起来的一些最重要技术，重点聚焦于两个方面：在验证中所使用的理论和数据类型基础上推导Craig插值式的方法，以及应用插值技术的验证算法的基本设计。"
  },
  {
    "date": "2026-02-09",
    "title": "Reinforcement Inference: Leveraging Uncertainty for Self-Correcting Language Model Reasoning",
    "authors": "Xinhai Sun",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08520v1",
    "source": "arXiv",
    "abstract": "Modern large language models (LLMs) are often evaluated and deployed under a \\emph{one-shot, greedy} inference protocol, especially in professional settings that require deterministic behavior. This regime can systematically under-estimate a fixed model's true capability: many errors arise not from missing knowledge, but from premature commitment under internal ambiguity. We introduce \\emph{Reinforcement Inference}, an entropy-aware inference-time control strategy that uses the model's own uncertainty to selectively invoke a second, more deliberate reasoning attempt, enabling stronger performance \\emph{without any retraining}. On 12,032 MMLU-Pro questions across 14 subjects, using DeepSeek-v3.2 with deterministic decoding in a zero-shot setting, Reinforcement Inference improves accuracy from 60.72\\% to 84.03\\%, while only incurring 61.06\\% additional inference calls. A 100\\% re-asking ablation reaches 84.35\\%, indicating that uncertainty-aware selection captures most of the attainable improvement with substantially less compute. Moreover, a \\emph{prompt-only} ablation underperforms the baseline, suggesting that the gains are not explained by generic `` your output had high entropy, think step-by-step'' prompting alone. Beyond providing a practical inference-time upgrade, our results suggest a broader \\emph{entropy-aware} paradigm for measuring and expanding model capability: because modern decoder-based models generate outputs autoregressively, entropy and related confidence measures arise naturally as first-class control signals during generation. The resulting gap between one-pass greedy inference and uncertainty-conditioned deliberation offers a diagnostic lens on an LLM's latent reasoning horizon and motivates future training objectives that explicitly constrain correctness--confidence alignment.",
    "title_zh": "强化推理：利用不确定性实现语言模型的自我修正推理",
    "abstract_zh": "现代大型语言模型（LLMs）在评估和部署时，通常采用一种“单次、贪心”（one-shot, greedy）的推理协议，尤其是在需要确定性行为的专业场景中。然而，这种推理方式往往会系统性地低估固定模型的真实能力：许多错误并非源于知识缺失，而是由于在内部存在不确定性时过早做出承诺。为此，我们提出了**强化推理**（Reinforcement Inference），一种基于熵感知的推理时控制策略，利用模型自身的不确定性，有选择地触发第二次更审慎的推理尝试，从而在**无需任何重新训练**的前提下实现更强的性能表现。\n\n在涵盖14个学科的12,032道MMLU-Pro题目上，使用DeepSeek-v3.2模型，在零样本设置下采用确定性解码，强化推理将准确率从60.72%提升至84.03%，同时仅增加61.06%的推理调用次数。一个“100%重新提问”的消融实验达到了84.35%的准确率，表明基于不确定性的选择机制已捕获了绝大部分可实现的性能提升，且所需计算资源显著更低。此外，仅通过提示（prompt-only）方式的消融实验表现反而低于基线，说明性能提升并非仅由“你的输出熵很高，逐步思考”这类通用提示所驱动。\n\n除了提供一种实用的推理时优化方案，我们的研究还揭示了一种更广泛的**熵感知**范式，用于衡量和拓展模型能力：由于现代基于解码器的模型采用自回归方式生成输出，熵及其相关置信度指标自然地成为生成过程中的第一类控制信号。由此产生的“单次贪心推理”与“基于不确定性条件的审慎推理”之间的差距，为诊断大模型潜在的推理能力边界提供了新的视角，并为未来训练目标的设计指明方向——即显式地约束“正确性”与“置信度”之间的对齐关系。"
  },
  {
    "date": "2026-02-09",
    "title": "Beyond Correctness: Learning Robust Reasoning via Transfer",
    "authors": "Hyunseok Lee, Soheil Abbasloo, Jihoon Tack, Jinwoo Shin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08489v1",
    "source": "arXiv",
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently strengthened LLM reasoning, but its focus on final answer correctness leaves a critical gap: it does not ensure the robustness of the reasoning process itself. We adopt a simple philosophical view, robust reasoning should remain useful beyond the mind that produced it, and treat reasoning as a form of meaning transfer that must survive truncation, reinterpretation, and continuation. Building on this principle, we introduce Reinforcement Learning with Transferable Reward (RLTR), which operationalizes robustness via transfer reward that tests whether a partial reasoning prefix from one model can guide a separate model to the correct answer. This encourages LLMs to produce reasoning that is stable, interpretable, and genuinely generalizable. Our approach improves sampling consistency while improving final answer accuracy, and it reaches comparable performance in substantially fewer training steps. For example, on MATH500, RLTR achieves a +3.6%p gain in Maj@64 compared to RLVR and matches RLVR's average accuracy with roughly 2.5x fewer training steps, providing both more reliable reasoning and significantly more sample efficient.",
    "title_zh": "超越正确性：通过迁移学习鲁棒推理",
    "abstract_zh": "基于可验证奖励的强化学习（RLVR）最近显著提升了大语言模型（LLM）的推理能力，但其仅关注最终答案正确性的局限性带来了一个关键问题：无法确保推理过程本身的鲁棒性。我们秉持一种简单的哲学观点：鲁棒的推理应当超越其产生者的思想而依然具有价值，因此将推理视为一种意义传递过程，必须能够经受住截断、重新解释和延续的考验。基于这一原则，我们提出了可迁移奖励强化学习（RLTR），通过“可迁移奖励”来实现鲁棒性——该奖励测试一个模型生成的部分推理前缀，是否能有效引导另一个独立模型得出正确答案。这一机制促使LLM生成稳定、可解释且真正具备泛化能力的推理过程。我们的方法在提升最终答案准确率的同时，显著增强了采样一致性，并在远少于传统方法的训练步数内达到相当甚至更优的性能。例如，在MATH500数据集上，RLTR相较于RLVR在Maj@64指标上实现了+3.6个百分点的提升，且仅需约2.5倍的训练步数便达到RLVR的平均准确率，既提供了更可靠的推理过程，又实现了显著更高的样本效率。"
  },
  {
    "date": "2026-02-09",
    "title": "When Does Context Help? Error Dynamics of Contextual Information in Large Language Models",
    "authors": "Dingzirui Wang, Xuanliang Zhang, Keyan Xu, Qingfu Zhu, Wanxiang Che, Yang Deng",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08294v1",
    "source": "arXiv",
    "abstract": "Contextual information at inference time, such as demonstrations, retrieved knowledge, or interaction history, can substantially improve large language models (LLMs) without parameter updates, yet its theoretical role remains poorly understood beyond specific settings such as in-context learning (ICL). We present a unified theoretical framework for analyzing the effect of arbitrary contextual information in Transformer-based LLMs. Our analysis characterizes contextual influence through output error dynamics. In a single-layer Transformer, we prove that the context-conditioned error vector decomposes additively into the baseline error vector and a contextual correction vector. This yields necessary geometric conditions for error reduction: the contextual correction must align with the negative baseline error and satisfy a norm constraint. We further show that the contextual correction norm admits an explicit upper bound determined by context-query relevance and complementarity. These results extend to multi-context and multi-layer Transformers. Experiments across ICL, retrieval-augmented generation, and memory evolution validate our theory and motivate a principled context selection strategy that improves performance by $0.6\\%$.",
    "title_zh": "上下文何时有帮助？大型语言模型中上下文信息的误差动态分析",
    "abstract_zh": "推理时的上下文信息，如示范样本、检索到的知识或交互历史，可以在不更新参数的情况下显著提升大型语言模型（LLMs）的性能，然而其理论作用在超出特定场景（如上下文学习，ICL）之外仍缺乏深入理解。本文提出了一种统一的理论框架，用于分析基于Transformer的LLM中任意上下文信息的影响。我们的分析通过输出误差动态来刻画上下文的影响。在单层Transformer中，我们证明了条件于上下文的误差向量可分解为基线误差向量与一个上下文修正向量的叠加。这给出了误差减少的必要几何条件：上下文修正向量必须与负的基线误差对齐，并满足一个范数约束。我们进一步证明，上下文修正向量的范数存在一个由上下文与查询的相关性及互补性决定的显式上界。这些结论可推广至多上下文和多层Transformer。在上下文学习、检索增强生成以及记忆演化等多个场景中的实验验证了理论的有效性，并启发了一种基于原则的上下文选择策略，该策略将模型性能提升了0.6%。"
  },
  {
    "date": "2026-02-09",
    "title": "CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute",
    "authors": "Chen Jin, Ryutaro Tanno, Tom Diethe, Philip Teare",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08948v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consumes full-trace confidence to decide whether to halt, re-examine, or try a different approach, enabling targeted self-correction with an average of 2.7 refinement steps per problem and roughly 190-fold token reduction relative to 512-sample baselines. Across diverse reasoning benchmarks and three open-source models, the controller achieves 92.6 percent precision when it confidently halts, indicating that confidence dynamics reliably signal correctness without ground-truth verification. We extend this to CoRefine-Tree, a hybrid sequential-parallel variant that adaptively balances exploration and exploitation, with easy serving integration and verifier compatibility. By treating confidence as a control signal rather than a correctness guarantee, CoRefine provides a modular primitive for scalable reasoning and agentic settings with imperfect verifiers.",
    "title_zh": "CoRefine：基于置信度引导的自精炼自适应测试时计算",
    "abstract_zh": "大型语言模型（LLMs）通常依赖于测试时的并行解码（例如，512个样本）来提升推理准确性，但这会带来巨大的计算开销。我们提出了CoRefine，一种基于置信度引导的自精炼方法，通过在冻结的LLM之上部署一个轻量级的211k参数Conv1D控制器，仅用极少量的token即可实现具有竞争力的准确率。该控制器利用完整的置信度轨迹，决定是否停止、重新审视，或尝试不同的方法，从而实现有针对性的自我修正，平均每道题仅需2.7次精炼步骤，相比512样本基线，token消耗减少了约190倍。在多个推理基准测试及三种开源模型上，当控制器自信地停止时，其准确率可达92.6%，表明置信度动态能够可靠地指示正确性，而无需依赖真实标签验证。我们进一步将该方法扩展为CoRefine-Tree，一种混合的串行-并行变体，可自适应地平衡探索与利用，具备易于部署和与验证器兼容的优点。通过将置信度视为控制信号而非正确性的保证，CoRefine为可扩展的推理和代理式场景提供了一个模块化的基础组件，尤其适用于验证器不完美的实际应用。"
  },
  {
    "date": "2026-02-09",
    "title": "ModARO: A Modular Approach to Architecture Reconstruction of Distributed Microservice Codebases",
    "authors": "Oscar Manglaras, Alex Farkas, Thomas Woolford, Christoph Treude, Markus Wagner",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08181v1",
    "source": "arXiv",
    "abstract": "Microservice architectures promote small, independently developed services, but increase overall architectural complexity. It is crucial that developers understand the architecture and how changes to a service affect the overall system, but rapid and independent development of services increases the risk of architectural drift and discourages the creation and maintenance of documentation. Automatic architecture reconstruction can help avoid these issues, but it is difficult to reuse reconstruction code across multiple projects, as all use different combinations of technologies and project-specific conventions. Reconstruction of architecture-level details is further complicated by the tendency to split microservices into separate repositories, preventing a full view of the system from any one codebase. In this paper, we present and evaluate ModARO, an approach to microservice architecture reconstruction that allows writing modular reconstruction code ('extractors') for any technologies and reusing them across different projects, independent of the surrounding technology stack or whether or not the services are split into multiple codebases. We demonstrate the effectiveness of our approach by configuring ModARO to reconstruct 10 open source projects, and we validate the usefulness and usability of ModARO against a state-of-the-art baseline in a user study with 8 industry practitioners. Using this approach, developers can assemble or create extractors tailored to their technology stacks and distribute architecture reconstruction across repositories, enabling integration into repository CI/CD pipelines.",
    "title_zh": "ModARO：一种用于分布式微服务代码库架构重建的模块化方法",
    "abstract_zh": "微服务架构促进了小型、独立开发的服务，但同时也增加了整体架构的复杂性。开发者必须理解架构本身以及服务变更对整个系统的影响，然而服务的快速且独立开发，又增加了架构漂移的风险，并抑制了文档的创建与维护。自动架构重建有助于避免这些问题，但重建代码难以在多个项目间复用，因为各项目使用的技术组合和特定项目约定各不相同。此外，由于微服务倾向于拆分到独立的代码仓库中，使得单个代码库无法提供系统的完整视图，进一步增加了架构级细节重建的难度。本文提出并评估了一种名为ModARO的微服务架构重建方法，该方法允许为任意技术编写模块化的重建代码（“提取器”），并可在不同项目间复用，且不受周围技术栈或服务是否拆分到多个代码库的影响。我们通过配置ModARO成功重建了10个开源项目，验证了该方法的有效性；并通过一项包含8名行业实践者的用户研究，证明了ModARO相较于当前最先进的基线方法在实用性和可用性方面的优势。借助该方法，开发者可根据自身技术栈组装或创建提取器，并将架构重建任务分布到各个代码仓库中，从而轻松集成到代码仓库的CI/CD流水线中。"
  },
  {
    "date": "2026-02-09",
    "title": "G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design",
    "authors": "Baoyun Zhao, He Wang, Liang Zeng",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08253v1",
    "source": "arXiv",
    "abstract": "While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, a generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. A cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions.",
    "title_zh": "G-LNS：基于生成式大邻域搜索的LLM自动启发式设计",
    "abstract_zh": "尽管大型语言模型（LLMs）在自动启发式设计（AHD）领域近期展现出巨大潜力，但现有方法通常围绕构造性优先规则或参数化局部搜索引导来构建启发式，从而将搜索空间限制在固定的启发式形式内。这种设计在结构探索能力上存在局限，难以在复杂的组合优化问题（COPs）中摆脱深度局部最优。本文提出G-LNS，一种基于生成式进化的框架，将基于LLM的AHD拓展至大规模邻域搜索（LNS）算子的自动化设计。与以往独立演化启发式的做法不同，G-LNS利用LLM协同进化紧密耦合的破坏（destroy）与修复（repair）算子对。通过一种协作式评估机制，显式捕捉二者之间的交互关系，从而发现能够协同实现有效结构破坏与重构的互补性算子逻辑。在旅行商问题（TSP）和容量约束车辆路径问题（CVRP）等具有挑战性的COP基准测试中进行的大量实验表明，G-LNS显著优于基于LLM的AHD方法以及强大的传统求解器。所发现的启发式不仅能在更小的计算预算下逼近最优解，还展现出在多样化且未见实例分布上的强大泛化能力。"
  },
  {
    "date": "2026-02-09",
    "title": "iGRPO: Self-Feedback-Driven LLM Reasoning",
    "authors": "Ali Hatamizadeh, Shrimai Prabhumoye, Igor Gitman, Ximing Lu, Seungju Han, Wei Ping, Yejin Choi, Jan Kautz",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09000v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\\% and 79.64\\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.",
    "title_zh": "iGRPO：由自我反馈驱动的大型语言模型推理",
    "abstract_zh": "大型语言模型（LLMs）在解决复杂数学问题方面展现出巨大潜力，但仍难以持续生成准确且一致的解答。强化学习（RL）是一种通过任务特定奖励来对齐这些模型的框架，能够有效提升整体质量和可靠性。组相对策略优化（GRPO）是一种高效、无需价值函数的近端策略优化（PPO）替代方法，其核心在于利用组相对奖励归一化。本文提出迭代组相对策略优化（iGRPO），作为GRPO的两阶段扩展，通过模型生成的草稿引入动态自条件机制。在第一阶段，iGRPO采样多个探索性草稿，并使用与优化过程相同的标量奖励信号选择奖励最高的草稿；在第二阶段，将该最优草稿附加至原始提示中，对草稿条件化的精炼过程执行类似GRPO的更新，训练策略在已有最强尝试的基础上进一步提升。在相同采样预算下，iGRPO在多种基础模型（如Nemotron-H-8B-Base-8K和DeepSeek-R1 Distilled）上均持续优于GRPO，验证了其在多样化推理基准上的有效性。此外，将iGRPO应用于在AceReason-Math数据集上训练的OpenReasoning-Nemotron-7B模型，在AIME24和AIME25上分别取得85.62%和79.64%的新SOTA成绩。消融实验进一步表明，该精炼框架不仅适用于GRPO变体，还能从生成式评判器中获益，并通过延缓熵坍缩来改变学习动态。这些结果凸显了基于迭代自反馈的强化学习在推动可验证数学推理方面的巨大潜力。"
  },
  {
    "date": "2026-02-09",
    "title": "Comparing AI Coding Agents: A Task-Stratified Analysis of Pull Request Acceptance",
    "authors": "Giovanni Pinna, Jingzhi Gong, David Williams, Federica Sarro",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08915v1",
    "source": "arXiv",
    "abstract": "The rapid adoption of AI-powered coding assistants is transforming software development practices, yet systematic comparisons of their effectiveness across different task types and over time remain limited. This paper presents an empirical study comparing five popular agents (OpenAI Codex, GitHub Copilot, Devin, Cursor, and Claude Code), analyzing 7,156 pull requests (PRs) from the AIDev dataset. Temporal trend analysis reveals heterogeneous evolution patterns: Devin exhibits the only consistent positive trend in acceptance rate (+0.77% per week over 32 weeks), whereas other agents remain largely stable. Our analysis suggests that the PR task type is a dominant factor influencing acceptance rates: documentation tasks achieve 82.1% acceptance compared to 66.1% for new features - a 16 percentage point gap that exceeds typical inter-agent variance for most tasks. OpenAI Codex achieves consistently high acceptance rates across all nine task categories (59.6%-88.6%), with stratified Chi-square tests confirming statistically significant advantages over other agents in several task categories. However, no single agent performs best across all task types: Claude Code leads in documentation (92.3%) and features (72.6%), while Cursor excels in fix tasks (80.4%).",
    "title_zh": "比较AI编程代理：基于任务分层的拉取请求接受分析",
    "abstract_zh": "人工智能驱动的代码助手迅速普及，正在深刻改变软件开发实践，但针对其在不同任务类型及随时间演变中的有效性进行系统性比较的研究仍较为有限。本文通过一项实证研究，对比了五种主流代码助手（OpenAI Codex、GitHub Copilot、Devin、Cursor 和 Claude Code），分析了来自 AIDev 数据集的 7,156 个拉取请求（PR）。时间趋势分析揭示了各异的演进模式：Devin 是唯一在接受率上呈现持续上升趋势的工具（32 周内每周提升 0.77%），而其他工具的接受率则基本保持稳定。我们的分析表明，PR 任务类型是影响接受率的主导因素：文档类任务的接受率高达 82.1%，而新功能类任务仅为 66.1%，两者相差 16 个百分点，这一差距甚至超过了大多数任务类型中不同工具之间的典型差异。OpenAI Codex 在全部九类任务中均表现出稳定的高接受率（59.6%–88.6%），分层卡方检验进一步证实其在多个任务类别中相较于其他工具具有统计学显著优势。然而，没有单一工具在所有任务类型中均表现最佳：Claude Code 在文档任务（92.3%）和功能开发任务（72.6%）中领先，而 Cursor 在修复类任务中表现突出（80.4%）。"
  },
  {
    "date": "2026-02-09",
    "title": "ArkEval: Benchmarking and Evaluating Automated CodeRepair for ArkTS",
    "authors": "Bang Xie, Senjian Zhang, Zhiyuan Peng, Wei Chen, Chenhao Ying, Yuan Luo",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08866v1",
    "source": "arXiv",
    "abstract": "Large language models have transformed code generation, enabling unprecedented automation in software development. As mobile ecosystems evolve, HarmonyOS has emerged as a critical platform requiring robust development tools. Software development for the HarmonyOS ecosystem relies heavily on ArkTS, a statically typed extension of TypeScript. Despite its growing importance, the ecosystem lacks robust tools for automated code repair, primarily due to the absence of a high-quality benchmark for evaluation. To address this gap, we present ArkEval, a unified framework for ArkTS automated repair workflow evaluation and benchmark construction. It provides the first comprehensive benchmark specifically designed for ArkTS automated program repair. We constructed this benchmark by mining issues from a large-scale official Huawei repository containing over 400 independent ArkTS applications. Through a rigorous multi-stage filtering process, we curated 502 reproducible issues. To ensure testability, we employed a novel LLM-based test generation and voting mechanism involving Claude and other models. Furthermore, we standardized problem statements to facilitate fair evaluation. Finally, we evaluated four state-of-the-art Large Language Models (LLMs) on our benchmark using a retrieval-augmented repair workflow. Our results highlight the current capabilities and limitations of LLMs in repairing ArkTS code, paving the way for future research in this low-resource language domain.",
    "title_zh": "ArkEval：面向ArkTS的自动化代码修复基准测试与评估",
    "abstract_zh": "大型语言模型已彻底改变了代码生成领域，推动了软件开发自动化水平的空前提升。随着移动生态系统的不断发展，鸿蒙系统（HarmonyOS）作为一项关键平台，亟需强大的开发工具支持。HarmonyOS 生态系统的软件开发高度依赖 ArkTS——一种静态类型的 TypeScript 扩展语言。尽管 ArkTS 的重要性日益凸显，但其生态系统仍缺乏可靠的自动化代码修复工具，主要原因在于缺乏高质量的评估基准。为填补这一空白，我们提出了 ArkEval，一个专为 ArkTS 自动化修复流程评估与基准构建而设计的统一框架。该框架提供了首个专门针对 ArkTS 自动化程序修复的综合性基准。我们通过从包含 400 多个独立 ArkTS 应用的大型官方华为代码仓库中挖掘问题，构建了这一基准。经过严格的多阶段筛选流程，最终筛选出 502 个可复现的问题。为确保问题的可测试性，我们引入了一种基于大语言模型（LLM）的新型测试生成与投票机制，结合 Claude 等多个模型协同工作。此外，我们对问题描述进行了标准化处理，以保障评估的公平性。最后，我们采用检索增强型修复流程，在该基准上对四种最先进的大语言模型进行了评估。实验结果揭示了当前大语言模型在修复 ArkTS 代码方面的实际能力与局限性，为这一低资源语言领域的未来研究奠定了基础。"
  },
  {
    "date": "2026-02-09",
    "title": "How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs",
    "authors": "Yapei Chang, Kyle Lo, Mohit Iyyer, Luca Soldaini",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08808v1",
    "source": "arXiv",
    "abstract": "Generating step-by-step \"how-to\" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale.",
    "title_zh": "How2Everything：从网络中挖掘“如何做”程序以评估和改进大语言模型",
    "abstract_zh": "生成分步“如何做”指南是大型语言模型（LLM）的一项关键能力：用户在聊天机器人中常请求此类指导，而分步规划对于解决复杂任务的推理至关重要。然而，在真实世界任务中，大规模地衡量和提升程序性有效性的方法仍面临挑战，且研究不足。为应对这一问题，我们提出了 How2Everything——一个可扩展的框架，用于评估和改进目标导向的程序生成。该框架包含 How2Mine，它从跨14个主题的980万网页中挖掘出35.1万条程序，并可轻松扩展至更大语料库。基于此数据池，我们构建了 How2Bench，一个包含7000个样本的评估集，各主题间分布均衡。为实现可靠的结果评分，我们开发了 How2Score，一种评估协议，利用大语言模型作为评判者，检测生成内容中是否存在任何关键性错误，这些错误将导致无法达成目标。为实现低成本、可复现的评估，我们将前沿模型蒸馏为一个开源的80亿参数模型，在与人工标注者的一致性上达到80.5%。How2Bench揭示了模型规模和训练阶段之间的清晰扩展趋势，甚至在预训练早期阶段即可提供有效信号。最后，使用 How2Score 作为奖励信号进行强化学习，使三个模型在 How2Bench 上的性能提升超过10分，且在标准基准测试上未出现系统性退化，提升效果对表面文档记忆或格式合规性等干扰因素也具有鲁棒性。综上所述，How2Everything 展示了如何利用预训练的网络数据，实现大规模能力评估与持续改进的闭环。"
  },
  {
    "date": "2026-02-09",
    "title": "Trapped by simplicity: When Transformers fail to learn from noisy features",
    "authors": "Evan Peters, Ando Deng, Matheus H. Zambianco, Devin Blankespoor, Achim Kempf",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08695v1",
    "source": "arXiv",
    "abstract": "Noise is ubiquitous in data used to train large language models, but it is not well understood whether these models are able to correctly generalize to inputs generated without noise. Here, we study noise-robust learning: are transformers trained on data with noisy features able to find a target function that correctly predicts labels for noiseless features? We show that transformers succeed at noise-robust learning for a selection of $k$-sparse parity and majority functions, compared to LSTMs which fail at this task for even modest feature noise. However, we find that transformers typically fail at noise-robust learning of random $k$-juntas, especially when the boolean sensitivity of the optimal solution is smaller than that of the target function. We argue that this failure is due to a combination of two factors: transformers' bias toward simpler functions, combined with an observation that the optimal function for noise-robust learning typically has lower sensitivity than the target function for random boolean functions. We test this hypothesis by exploiting transformers' simplicity bias to trap them in an incorrect solution, but show that transformers can escape this trap by training with an additional loss term penalizing high-sensitivity solutions. Overall, we find that transformers are particularly ineffective for learning boolean functions in the presence of feature noise.",
    "title_zh": "被简单性所困：当Transformer无法从噪声特征中学习时",
    "abstract_zh": "在用于训练大型语言模型的数据中，噪声无处不在，但目前尚不清楚这些模型是否能够正确地泛化到无噪声输入。本文研究了噪声鲁棒学习：在包含噪声特征的数据上训练的Transformer模型，能否找到一个目标函数，从而对无噪声特征进行正确预测？我们发现，与LSTM相比，Transformer在若干$k$-稀疏奇偶校验函数和多数函数上能够成功实现噪声鲁棒学习，而LSTM即使在适度的特征噪声下也难以完成该任务。然而，我们发现Transformer在学习随机$k$-junta函数时通常表现不佳，尤其是当最优解的布尔敏感度低于目标函数时。我们认为，这种失败源于两个因素的结合：Transformer对更简单函数的偏好，以及一个观察结果——对于随机布尔函数，噪声鲁棒学习的最优解通常比目标函数具有更低的敏感度。我们通过利用Transformer的简单性偏好，使其陷入错误解，但进一步证明，通过引入一个惩罚高敏感度解的附加损失项，Transformer能够摆脱这一陷阱。总体而言，我们发现Transformer在特征噪声存在的情况下，对布尔函数的学习尤其低效。"
  },
  {
    "date": "2026-02-09",
    "title": "PBLean: Pseudo-Boolean Proof Certificates for Lean 4",
    "authors": "Stefan Szeider",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08692v1",
    "source": "arXiv",
    "abstract": "We present PBLean, a method for importing VeriPB pseudo-Boolean (PB) proof certificates into Lean 4. Key to our approach is reflection: a Boolean checker function whose soundness is fully proved in Lean and executed as compiled native code. Our method scales to proofs with tens of thousands of steps that would exhaust memory under explicit proof-term construction. Our checker supports all VeriPB kernel rules, including cutting-plane derivations and proof-by-contradiction subproofs. In contrast to external verified checkers that produce verdicts, our integration yields Lean theorems that can serve as composable lemmas in larger formal developments. To derive theorems about the original combinatorial problems rather than about PB constraints alone, we support verified encodings. This closes the trust gap between solver output and problem semantics since the constraint translation and its correctness proof are both formalized in Lean. We demonstrate the approach on various combinatorial problems.",
    "title_zh": "PBLean：用于 Lean 4 的伪布尔证明证书",
    "abstract_zh": "我们提出 PBLean，一种将 VeriPB 伪布尔（PB）证明证书导入 Lean 4 的方法。本方法的核心在于“反射”：一个在 Lean 中完全形式化证明其正确性的布尔检查函数，并以编译后的原生代码执行。我们的方法可扩展至包含数万步的证明，而这些证明在显式构造证明项时会耗尽内存。我们的检查器支持 VeriPB 内核的所有规则，包括割平面推导和反证法子证明。与仅输出结论的外部已验证检查器不同，我们的集成方式生成的是可在更大形式化体系中组合使用的 Lean 定理。为了推导关于原始组合问题的定理，而非仅限于 PB 约束本身，我们还支持已验证的编码机制。这消除了求解器输出与问题语义之间的信任鸿沟，因为约束转换及其正确性证明均已在 Lean 中形式化。我们在多种组合问题上展示了该方法的有效性。"
  },
  {
    "date": "2026-02-09",
    "title": "LLaDA2.1: Speeding Up Text Diffusion via Token Editing",
    "authors": "Tiwei Bie, Maosong Cao, Xiang Cao, Bingsen Chen, Fuyuan Chen, Kun Chen, Lun Du, Daozhuo Feng, Haibo Feng, Mingliang Gong, Zhuocheng Gong, Yanmei Gu, Jian Guan, Kaiyuan Guan, Hongliang He, Zenan Huang, Juyong Jiang, Zhonghui Jiang, Zhenzhong Lan, Chengxi Li, Jianguo Li, Zehuan Li, Huabin Liu, Lin Liu, Guoshan Lu, Yuan Lu, Yuxin Ma, Xingyu Mou, Zhenxuan Pan, Kaida Qiu, Yuji Ren, Jianfeng Tan, Yiding Tian, Zian Wang, Lanning Wei, Tao Wu, Yipeng Xing, Wentao Ye, Liangyu Zha, Tianze Zhang, Xiaolu Zhang, Junbo Zhao, Da Zheng, Hao Zhong, Wanli Zhong, Jun Zhou, Junlin Zhou, Liwang Zhu, Muzhi Zhu, Yihong Zhuang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08676v1",
    "source": "arXiv",
    "abstract": "While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.",
    "title_zh": "LLaDA2.1：通过令牌编辑加速文本扩散",
    "abstract_zh": "尽管LLaDA2.0展示了1000亿级块扩散模型的可扩展潜力及其固有的并行化能力，但解码速度与生成质量之间的微妙平衡始终是一个难以突破的瓶颈。如今，我们隆重推出LLaDA2.1，这是一次范式革新，旨在彻底超越这一权衡困境。通过将Token-to-Token（T2T）编辑无缝融入传统的Mask-to-Token（M2T）机制，我们提出了一种联合且可配置的阈值解码方案。这一结构创新催生出两种截然不同的运行模式：速模式（S模式），大胆降低M2T阈值以突破传统限制，同时依赖T2T机制对输出进行精细化修正；质模式（Q模式），采用更为保守的阈值策略，在可接受的效率损失下实现卓越的基准性能。在此演进基础上，依托超大上下文窗口，我们首次构建了专为扩散语言模型（dLLMs）量身定制的大规模强化学习（RL）框架，其核心融合了稳定梯度估计的专项技术。这一对齐机制不仅显著提升了推理的精确性，更大幅增强了指令遵循的忠实度，弥合了扩散动态与复杂人类意图之间的鸿沟。最后，我们正式发布LLaDA2.1-Mini（160亿参数）与LLaDA2.1-Flash（1000亿参数）两个版本。在33项严格基准测试中，LLaDA2.1展现出卓越的任务表现与闪电般的解码速度。即便在1000亿参数规模下，其在编码任务中仍实现了惊人的性能：HumanEval+上达到892 TPS，BigCodeBench上达801 TPS，LiveCodeBench上达663 TPS。"
  },
  {
    "date": "2026-02-09",
    "title": "LLMs + Security = Trouble",
    "authors": "Benjamin Livshits",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08422v1",
    "source": "arXiv",
    "abstract": "We argue that when it comes to producing secure code with AI, the prevailing \"fighting fire with fire\" approach -- using probabilistic AI-based checkers or attackers to secure probabilistically generated code -- fails to address the long tail of security bugs. As a result, systems may remain exposed to zero-day vulnerabilities that can be discovered by better-resourced or more persistent adversaries. While neurosymbolic approaches that combine LLMs with formal methods are attractive in principle, we argue that they are difficult to reconcile with the \"vibe coding\" workflow common in LLM-assisted development: unless the end-to-end verification pipeline is fully automated, developers are repeatedly asked to validate specifications, resolve ambiguities, and adjudicate failures, making the human-in-the-loop a likely point of weakness, compromising secure-by-construction guarantees. In this paper we argue that stronger security guarantees can be obtained by enforcing security constraints during code generation (e.g., via constrained decoding), rather than relying solely on post-hoc detection and repair. This direction is particularly promising for diffusion-style code models, whose approach provides a natural elegant opportunity for modular, hierarchical security enforcement, allowing us to combine lower-latency generation techniques with generating secure-by-construction code.",
    "title_zh": "大语言模型 + 安全 = 麻烦",
    "abstract_zh": "我们认为，在利用人工智能生成安全代码时，当前普遍采用的“以火攻火”策略——即使用基于概率的AI检查器或攻击工具来保障由概率生成代码的安全性——无法有效应对安全漏洞的长尾问题。因此，系统仍可能暴露于零日漏洞之下，这些漏洞可能被资源更充足或更具持续性的攻击者发现。尽管将大语言模型（LLM）与形式化方法相结合的神经符号方法在理论上颇具吸引力，但我们认为，这类方法难以与当前主流的“直觉编程”（vibe coding）工作流相兼容：除非端到端的验证流程实现完全自动化，否则开发者将反复被要求验证规范、解决歧义并裁定失败情况，这使得“人在回路”环节成为潜在的薄弱点，从而破坏了“构建即安全”的保障承诺。本文主张，通过在代码生成阶段强制实施安全约束（例如通过约束解码），而非仅依赖事后的检测与修复，可以获得更强的安全保障。这一方向对扩散风格的代码模型尤为有前景，因为其生成机制天然提供了模块化、分层式安全控制的优雅机会，使我们能够结合低延迟生成技术，同时生成“构建即安全”的代码。"
  },
  {
    "date": "2026-02-09",
    "title": "SWE Context Bench: A Benchmark for Context Learning in Coding",
    "authors": "Jared Zhu, Minhao Hu, Junde Wu",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08316v1",
    "source": "arXiv",
    "abstract": "Large language models are increasingly used as programming agents for repository level software engineering tasks. While recent benchmarks evaluate correctness in realistic codebases, they largely treat tasks as independent and do not assess whether agents can reuse experience across related problems. As a result, the ability of agents to accumulate, retrieve, and apply prior experience, as well as the efficiency gains from such reuse, remains difficult to measure. We introduce SWE-ContextBench, a benchmark designed to explicitly evaluate experience reuse in programming agents. Built on SWE-Bench Lite, SWE-ContextBench augments 300 base tasks with 99 related tasks derived from real dependency and reference relationships among GitHub issues and pull requests, forming task sequences with shared context. The benchmark evaluates agents along three complementary dimensions: prediction accuracy, time efficiency, and cost efficiency. Using SWE-ContextBench, we study multiple experience reuse settings, including oracle guided and autonomous retrieval, as well as full execution trajectories and compact summaries. Our results show that correctly selected summarized experience improves resolution accuracy and substantially reduces runtime and token cost, particularly on harder tasks. In contrast, unfiltered or incorrectly selected experience provides limited or negative benefits. These findings highlight the importance of experience representation and retrieval quality, and position SWE-ContextBench as a principled benchmark for studying experience reuse in programming agents.",
    "title_zh": "SWE Context Bench：编码上下文学习的基准测试",
    "abstract_zh": "大型语言模型正越来越多地被用作编程代理，以完成代码库级别的软件工程任务。尽管近期的基准测试在真实代码库中评估了任务的正确性，但它们大多将任务视为独立事件，未考察代理在相关问题之间复用经验的能力。因此，代理积累、检索并应用先前经验的能力，以及由此带来的效率提升，仍难以衡量。为此，我们提出了 SWE-ContextBench，一个专门用于显式评估编程代理中经验复用能力的基准测试。该基准基于 SWE-Bench Lite，通过引入 99 个源自 GitHub 问题与拉取请求之间真实依赖关系和引用关系的相关任务，对 300 个基础任务进行扩展，形成具有共享上下文的任务序列。SWE-ContextBench 从三个互补维度评估代理的表现：预测准确性、时间效率和成本效率。利用 SWE-ContextBench，我们研究了多种经验复用场景，包括受引导的（oracle-guided）与自主检索，以及完整执行轨迹与紧凑摘要两种形式。实验结果表明，经过正确选择的经验摘要能够显著提高问题解决的准确率，并大幅降低运行时间和 token 消耗，尤其在较难任务上效果更为明显。相反，未经筛选或错误选择的经验带来的收益有限，甚至可能产生负面影响。这些发现凸显了经验表示与检索质量的重要性，也使 SWE-ContextBench 成为研究编程代理中经验复用问题的一个系统化基准。"
  },
  {
    "date": "2026-02-09",
    "title": "Toward Formalizing LLM-Based Agent Designs through Structural Context Modeling and Semantic Dynamics Analysis",
    "authors": "Haoyu Jia, Kento Kawaharazuka, Kei Okada",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08276v1",
    "source": "arXiv",
    "abstract": "Current research on large language model (LLM) agents is fragmented: discussions of conceptual frameworks and methodological principles are frequently intertwined with low-level implementation details, causing both readers and authors to lose track amid a proliferation of superficially distinct concepts. We argue that this fragmentation largely stems from the absence of an analyzable, self-consistent formal model that enables implementation-independent characterization and comparison of LLM agents. To address this gap, we propose the \\texttt{Structural Context Model}, a formal model for analyzing and comparing LLM agents from the perspective of context structure. Building upon this foundation, we introduce two complementary components that together span the full lifecycle of LLM agent research and development: (1) a declarative implementation framework; and (2) a sustainable agent engineering workflow, \\texttt{Semantic Dynamics Analysis}. The proposed workflow provides principled insights into agent mechanisms and supports rapid, systematic design iteration. We demonstrate the effectiveness of the complete framework on dynamic variants of the monkey-banana problem, where agents engineered using our approach achieve up to a 32 percentage points improvement in success rate on the most challenging setting.",
    "title_zh": "通过结构化上下文建模与语义动态分析对基于大语言模型的智能体设计进行形式化",
    "abstract_zh": "当前关于大语言模型（LLM）智能体的研究呈现碎片化状态：概念框架与方法论原则的讨论常常与底层实现细节混杂在一起，导致读者和作者在大量表面看似不同的概念中迷失方向。我们认为，这种碎片化现象在很大程度上源于缺乏一个可分析、自洽的形式化模型，而该模型能够实现对LLM智能体的与实现无关的表征与比较。为弥补这一空白，我们提出了**结构化上下文模型**（\\texttt{Structural Context Model}），一个从上下文结构角度分析和比较LLM智能体的形式化模型。在此基础上，我们引入了两个互补的组成部分，共同覆盖LLM智能体研究与开发的全生命周期：（1）一种声明式实现框架；以及（2）一种可持续的智能体工程工作流——\\texttt{语义动态分析}（Semantic Dynamics Analysis）。所提出的流程为智能体机制提供了原则性洞见，并支持快速、系统的迭代设计。我们在动态变体的“猴子-香蕉”问题上验证了该完整框架的有效性，使用本方法设计的智能体在最具挑战性的场景中，成功率最高提升了32个百分点。"
  },
  {
    "date": "2026-02-09",
    "title": "Pretraining with Token-Level Adaptive Latent Chain-of-Thought",
    "authors": "Boyi Zeng, Yiqin Hao, He Li, Shixiang Song, Feichen Song, Zitong Wang, Siyuan Huang, Yi Xu, ZiWei He, Xinbing Wang, Zhouhan Lin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08220v1",
    "source": "arXiv",
    "abstract": "Scaling large language models by increasing parameters and training data is increasingly constrained by limited high-quality corpora and rising communication costs. This work explores an alternative axis: increasing per-token computation without expanding parameters, by internalizing latent Chain-of-Thought (CoT) into pretraining. We propose Pretraining with Token-Level Adaptive Latent CoT (adaptive latent CoT), where the model generates a variable-length latent CoT trajectory before emitting each token -- allocating longer trajectories to difficult tokens and shorter (or even zero) trajectories to easy ones. Importantly, this behavior emerges naturally from one-stage pretraining on general text and reduces computation in both training and inference via token-wise adaptive halting. Experiments with Llama architectures show that adaptive latent CoT consistently improves language modeling perplexity and broad downstream accuracy, even with fewer training FLOPs than prior recurrent baselines.",
    "title_zh": "基于标记级自适应潜在思维链的预训练",
    "abstract_zh": "通过增加参数量和训练数据来扩展大型语言模型，正日益受到高质量语料库有限以及通信成本上升的制约。本文探索了一条替代路径：在不增加参数量的前提下，通过提升每个标记（token）的计算量来实现模型扩展，方法是将隐式思维链（Chain-of-Thought, CoT）内化到预训练过程中。我们提出了“基于标记级自适应隐式思维链”的预训练方法（adaptive latent CoT），该方法在生成每个标记之前，会自动生成一个长度可变的隐式思维链轨迹——对困难的标记分配更长的轨迹，而对简单的标记则分配较短甚至为零的轨迹。重要的是，这种行为能够通过在通用文本上进行单阶段预训练自然涌现，并通过标记级别的自适应终止机制，在训练和推理阶段均有效降低计算开销。在 Llama 架构上的实验表明，adaptive latent CoT 在语言建模困惑度和广泛下游任务的准确率方面均持续提升，且所需的训练浮点运算量（FLOPs）甚至少于以往的循环基线方法。"
  },
  {
    "date": "2026-02-09",
    "title": "Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models",
    "authors": "Mingzi Cao, Xingwei Tan, Mahmud Akhter, Marco Valentino, Maria Liakata, Xi Wang, Nikolaos Aletras",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08658v1",
    "source": "arXiv",
    "abstract": "Deduction, induction, and abduction are fundamental reasoning paradigms, core for human logical thinking. Although improving Large Language Model (LLM) reasoning has attracted significant research efforts, the extent to which the fundamental paradigms induce generalization has yet to be systematically explored. In this study, we shed light on how the interplay between these core paradigms influences LLMs' reasoning behavior. To this end, we first collect a new dataset of reasoning trajectories from symbolic tasks, each targeting one of the three fundamental paradigms, to abstract from concrete world knowledge. Then, we investigate effective ways for inducing these skills into LLMs. We experiment with a battery of methods including simple fine-tuning, and more complex approaches to increase model depth, or transform a dense model to a mixture-of-experts. We comprehensively evaluate induced models on realistic out-of-domain tasks, that are entirely formulated in natural language and contain real-world knowledge. Our results reveal that our approach yields strong generalizability with substantial performance gains (up to $14.60$) across realistic tasks.",
    "title_zh": "基础推理范式促进语言模型的域外泛化",
    "abstract_zh": "演绎、归纳和溯因是基本的推理范式，构成了人类逻辑思维的核心。尽管提升大型语言模型（LLM）的推理能力已吸引大量研究关注，但这些基础推理范式在促进模型泛化方面的作用尚未得到系统性探索。在本研究中，我们深入探讨了这三种核心推理范式之间的相互作用如何影响LLM的推理行为。为此，我们首先从符号任务中收集了一个新的推理轨迹数据集，每个任务均针对三种基本推理范式之一，以抽象出具体的现实世界知识。随后，我们探究了有效将这些推理技能引入LLM的方法。我们实验了多种方法，包括简单的微调，以及更复杂的策略，如增强模型深度，或把密集模型转换为专家混合（Mixture-of-Experts）结构。我们在完全以自然语言表述且包含真实世界知识的现实域外任务上，对所诱导的模型进行了全面评估。结果表明，我们的方法在各类真实任务中展现出强大的泛化能力，并带来了显著的性能提升（最高达14.60分）。"
  },
  {
    "date": "2026-02-09",
    "title": "Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning",
    "authors": "Zhuoen Chen, Dongfang Li, Meishan Zhang, Baotian Hu, Min Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08382v1",
    "source": "arXiv",
    "abstract": "Large Language Models (LLMs) face significant challenges in long-context processing, including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context inference based on chunk-wise compression and selective memory recall, rather than processing all raw tokens. The framework segments long inputs into chunks and encodes each chunk into compressed memory representations using a learned compressor. A gating module dynamically selects relevant memory blocks, which are then iteratively processed by a reasoning module with an evolving working memory to solve downstream tasks. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning, while the gating module is trained separately as a classifier. Experimental results show that the proposed method achieves competitive accuracy on multi-hop reasoning benchmarks such as RULER-HQA, extrapolates context length from 7K to 1.75M tokens, and offers a favorable accuracy-efficiency trade-off compared to strong long-context baselines. In particular, it achieves up to a 2 times reduction in peak GPU memory usage and a 6 times inference speedup over MemAgent.",
    "title_zh": "通过端到端强化学习实现压缩记忆上的动态长上下文推理",
    "abstract_zh": "大型语言模型（LLMs）在处理长上下文时面临诸多挑战，包括二次方级的计算开销、信息遗忘问题，以及检索增强生成（RAG）中固有的上下文碎片化问题。为此，我们提出了一种受认知启发的高效长上下文推理框架，该框架基于分块压缩与选择性记忆召回机制，而非处理所有原始token。该框架将长输入分割为多个块，利用一个可学习的压缩器将每个块编码为压缩的记忆表示。一个门控模块动态选择相关记忆块，随后由具备动态演化工作记忆的推理模块对这些记忆块进行迭代处理，以完成下游任务。压缩器与推理器通过端到端强化学习进行联合优化，而门控模块则作为分类器独立训练。实验结果表明，所提方法在多跳推理基准（如RULER-HQA）上取得了具有竞争力的准确率，能够将上下文长度从7K扩展至175万token，并在准确率与效率之间实现了优于强基线模型的权衡。特别地，该方法相比MemAgent，峰值GPU内存使用量最多降低2倍，推理速度提升达6倍。"
  },
  {
    "date": "2026-02-09",
    "title": "Reinforcement Learning with Backtracking Feedback",
    "authors": "Bilgehan Sel, Vaishakh Keshava, Phillip Wallis, Lukas Rutishauser, Ming Jin, Dingcheng Li",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08377v1",
    "source": "arXiv",
    "abstract": "Addressing the critical need for robust safety in Large Language Models (LLMs), particularly against adversarial attacks and in-distribution errors, we introduce Reinforcement Learning with Backtracking Feedback (RLBF). This framework advances upon prior methods, such as BSAFE, by primarily leveraging a Reinforcement Learning (RL) stage where models learn to dynamically correct their own generation errors. Through RL with critic feedback on the model's live outputs, LLMs are trained to identify and recover from their actual, emergent safety violations by emitting an efficient \"backtrack by x tokens\" signal, then continuing generation autoregressively. This RL process is crucial for instilling resilience against sophisticated adversarial strategies, including middle filling, Greedy Coordinate Gradient (GCG) attacks, and decoding parameter manipulations. To further support the acquisition of this backtracking capability, we also propose an enhanced Supervised Fine-Tuning (SFT) data generation strategy (BSAFE+). This method improves upon previous data creation techniques by injecting violations into coherent, originally safe text, providing more effective initial training for the backtracking mechanism. Comprehensive empirical evaluations demonstrate that RLBF significantly reduces attack success rates across diverse benchmarks and model scales, achieving superior safety outcomes while critically preserving foundational model utility.",
    "title_zh": "带有回溯反馈的强化学习",
    "abstract_zh": "针对大型语言模型（LLMs）在对抗性攻击和分布内错误方面所面临的严峻安全挑战，我们提出了基于回溯反馈的强化学习（Reinforcement Learning with Backtracking Feedback, RLBF）框架。该框架在先前方法（如BSAFE）的基础上实现重要突破，其核心在于引入一个强化学习（RL）阶段，使模型能够动态识别并修正自身生成过程中的错误。通过利用对模型实时输出的评判者（critic）反馈，LLMs被训练为在发现实际出现的安全违规时，发出一个高效的“回溯x个标记”信号，随后以自回归方式继续生成。这一强化学习过程对于提升模型抵御复杂对抗策略（包括中间填充攻击、贪婪坐标梯度攻击GCG以及解码参数篡改）的韧性至关重要。\n\n为进一步支持该回溯能力的学习，我们还提出了一种改进的监督微调（SFT）数据生成策略（BSAFE+）。该方法通过在原本安全且连贯的文本中注入安全违规，相较于以往的数据生成技术，能够更有效地为回溯机制提供初始训练信号。全面的实证评估表明，RLBF在多种基准测试和不同规模的模型上均显著降低了攻击成功率，不仅实现了更优的安全性能，同时有效保持了模型的基础能力与实用性。"
  },
  {
    "date": "2026-02-09",
    "title": "Latent Reasoning with Supervised Thinking States",
    "authors": "Ido Amos, Avi Caciularu, Mor Geva, Amir Globerson, Jonathan Herzig, Lior Shani, Idan Szpektor",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08332v1",
    "source": "arXiv",
    "abstract": "Reasoning with a chain-of-thought (CoT) enables Large Language Models (LLMs) to solve complex tasks but incurs significant inference costs due to the generation of long rationales. We propose Thinking States, a method that performs reasoning {\\em while} the input is processing. Specifically, Thinking States generates sequences of thinking tokens every few input tokens, transforms the thoughts back into embedding space, and adds them to the following input tokens. This has two key advantages. First, it captures the recurrent nature of CoT, but where the thought tokens are generated as input is processing. Second, since the thoughts are represented as tokens, they can be learned from natural language supervision, and using teacher-forcing, which is parallelizable. Empirically, Thinking States outperforms other latent reasoning methods on multiple reasoning tasks, narrowing the gap to CoT on math problems, and matching its performance on 2-Hop QA with improved latency. On state-tracking tasks, we show Thinking States leads to stronger reasoning behavior than CoT, successfully extrapolating to longer sequences than seen during training.",
    "title_zh": "带有监督思维状态的潜在推理",
    "abstract_zh": "思维状态（Thinking States）是一种新型方法，能够在输入处理过程中同步进行推理，从而克服传统链式思维（Chain-of-Thought, CoT）在推理时产生的高昂计算开销。传统的CoT需要生成冗长的推理过程，导致推理成本显著增加。而Thinking States通过在每处理若干输入token后，即时生成一系列“思考token”，将这些思考内容转换回嵌入空间，并将其融入后续的输入token中，实现推理与输入处理的并行化。\n\n该方法具有两大优势：其一，它捕捉了CoT中反复出现的思维模式，但思考token的生成是随着输入逐步进行的，而非在输入全部完成后才开始；其二，由于思考内容以token形式表示，因此可以利用自然语言监督信号进行学习，并支持教师强制（teacher-forcing）训练，这种训练方式具备高度并行性，显著提升效率。\n\n实证结果表明，Thinking States在多个推理任务上优于其他隐式推理方法。在数学问题求解方面，其性能接近CoT水平，显著缩小了差距；在双跳问答（2-Hop QA）任务中，不仅达到与CoT相当的准确率，还实现了更低的延迟。在状态追踪任务中，Thinking States展现出比CoT更强的推理能力，能够成功外推至训练阶段未见过的更长序列，表现出更优的泛化性能。"
  },
  {
    "date": "2026-02-09",
    "title": "Specification Vibing for Automated Program Repair",
    "authors": "Taohong Zhu, Lucas C. Cordeiro, Mustafa A. Mustafa, Youcheng Sun",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08263v1",
    "source": "arXiv",
    "abstract": "Large language model (LLM)-driven automated program repair (APR) has advanced rapidly, but most methods remain code-centric: they directly rewrite source code and thereby risk hallucinated, behaviorally inconsistent fixes. This limitation suggests the need for an alternative repair paradigm that relies on a representation more accessible to LLMs than raw code, enabling more accurate understanding, analysis, and alignment during repair. To address this gap, we propose VibeRepair, a specification-centric APR technique that treats repair as behavior-specification repair rather than ad-hoc code editing. VibeRepair first translates buggy code into a structured behavior specification that captures the program's intended runtime behavior, then infers and repairs specification misalignments, and finally synthesizes code strictly guided by the corrected behavior specification. An on-demand reasoning component enriches hard cases with program analysis and historical bug-fix evidence while controlling cost. Across Defects4J and real-world benchmarks and multiple LLMs, VibeRepair demonstrates consistently strong repair effectiveness with a significantly smaller patch space. On Defects4J v1.2, VibeRepair correctly repairs 174 bugs, exceeding the strongest state-of-the-art baseline by 28 bugs, which corresponds to a 19% improvement. On Defects4J v2.0, it repairs 178 bugs, outperforming prior approaches by 33 bugs, representing a 23% improvement. Evaluations on real-world benchmarks collected after the training period of selected LLMs further confirm its effectiveness and generalizability. By centering repair on explicit behavioral intent, VibeRepair reframes APR for the era of \"vibe\" coding: make the behavior sing, and the code will follow.",
    "title_zh": "自动化程序修复的规范振动",
    "abstract_zh": "基于大语言模型（LLM）的自动化程序修复（APR）技术发展迅速，但大多数方法仍以代码为中心：直接修改源代码，因而存在生成幻觉性、行为不一致修复方案的风险。这一局限性表明，亟需一种替代性的修复范式，其采用的表示形式应比原始代码更易于LLM理解，从而在修复过程中实现更准确的理解、分析与对齐。为填补这一空白，我们提出VibeRepair——一种以规范为中心的APR技术，将修复视为行为规范的修复，而非随意的代码编辑。VibeRepair首先将存在缺陷的代码转化为结构化的行为规范，以捕捉程序预期的运行时行为；随后推断并修复规范中的不一致之处；最后严格依据修正后的行为规范生成代码。一个按需调用的推理组件在处理复杂案例时，结合程序分析与历史修复证据，同时控制计算成本。在Defects4J和真实世界基准测试中，VibeRepair在多个LLM上均展现出一致且出色的修复效果，且其补丁搜索空间显著更小。在Defects4J v1.2上，VibeRepair成功修复了174个缺陷，比最强的现有基线多修复28个，提升达19%；在Defects4J v2.0上，修复了178个缺陷，比先前方法多出33个，提升23%。在选定LLM训练期之后收集的真实世界基准测试中，评估结果进一步验证了其有效性与泛化能力。通过聚焦于显式的运行意图，VibeRepair重新定义了“氛围式编程”（vibe coding）时代的APR：让行为“唱响”，代码自然随之而至。"
  },
  {
    "date": "2026-02-09",
    "title": "Weak-Driven Learning: How Weak Agents make Strong Agents Stronger",
    "authors": "Zehao Chen, Gongxun Li, Tianxiang Ai, Yifei Li, Zixuan Huang, Wang Zhou, Fuzhen Zhuang, Xianglong Liu, Jianxin Li, Deqing Wang, Yikun Ban",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08222v1",
    "source": "arXiv",
    "abstract": "As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.",
    "title_zh": "弱驱动学习：弱智能体如何让强智能体更强大",
    "abstract_zh": "随着后训练优化在提升大语言模型性能方面日益重要，我们观察到一个持续存在的饱和瓶颈：当模型达到高度自信状态后，进一步训练带来的收益逐渐衰减。现有方法通常继续强化目标预测，但我们发现，模型自身历史中的弱状态仍蕴含着丰富的监督信号。基于这一观察，我们提出了WMSS（弱代理可使强代理更强大）这一后训练范式，利用弱检查点来引导持续优化。通过熵动态分析识别可恢复的学习差距，并借助补偿性学习加以强化，WMSS使强代理能够突破传统后训练的饱和限制。在数学推理和代码生成数据集上的实验表明，采用该方法训练的代理实现了显著的性能提升，且不增加任何额外的推理成本。"
  },
  {
    "date": "2026-02-09",
    "title": "Evasion of IoT Malware Detection via Dummy Code Injection",
    "authors": "Sahar Zargarzadeh, Mohammad Islam",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08170v1",
    "source": "arXiv",
    "abstract": "The Internet of Things (IoT) has revolutionized connectivity by linking billions of devices worldwide. However, this rapid expansion has also introduced severe security vulnerabilities, making IoT devices attractive targets for malware such as the Mirai botnet. Power side-channel analysis has recently emerged as a promising technique for detecting malware activity based on device power consumption patterns. However, the resilience of such detection systems under adversarial manipulation remains underexplored. This work presents a novel adversarial strategy against power side-channel-based malware detection. By injecting structured dummy code into the scanning phase of the Mirai botnet, we dynamically perturb power signatures to evade AI/ML-based anomaly detection without disrupting core functionality. Our approach systematically analyzes the trade-offs between stealthiness, execution overhead, and evasion effectiveness across multiple state-of-the-art models for side-channel analysis, using a custom dataset collected from smartphones of diverse manufacturers. Experimental results show that our adversarial modifications achieve an average attack success rate of 75.2\\%, revealing practical vulnerabilities in power-based intrusion detection frameworks.",
    "title_zh": "通过注入虚假代码规避物联网恶意软件检测",
    "abstract_zh": "物联网（IoT）通过连接全球数十亿台设备，彻底改变了互联互通的方式。然而，这种快速扩张也带来了严重的安全漏洞，使得IoT设备成为恶意软件（如Mirai僵尸网络）的诱人目标。近年来，功耗侧信道分析作为一种新兴技术，已展现出基于设备功耗模式检测恶意活动的潜力。然而，此类检测系统在面对对抗性干扰时的鲁棒性仍缺乏深入研究。本文提出了一种针对基于功耗侧信道的恶意软件检测的新颖对抗策略。通过在Mirai僵尸网络的扫描阶段注入结构化的虚假代码，我们动态地扰动设备的功耗特征，从而在不破坏核心功能的前提下规避基于人工智能/机器学习的异常检测。我们的方法系统性地分析了在多种先进的侧信道分析模型中，隐蔽性、执行开销与逃逸效果之间的权衡关系，并基于来自多个不同制造商智能手机的自定义数据集进行实验验证。实验结果表明，所提出的对抗性修改平均攻击成功率高达75.2%，揭示了基于功耗的入侵检测框架在实际应用中存在的显著漏洞。"
  },
  {
    "date": "2026-02-09",
    "title": "DirMoE: Dirichlet-routed Mixture of Experts",
    "authors": "Amirhossein Vahidi, Hesam Asadollahzadeh, Navid Akhavan Attar, Marie Moullet, Kevin Ly, Xingyi Yang, Mohammad Lotfollahi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.09001v1",
    "source": "arXiv",
    "abstract": "Mixture-of-Experts (MoE) models have demonstrated exceptional performance in large-scale language models. Existing routers typically rely on non-differentiable Top-$k$+Softmax, limiting their performance and scalability. We argue that two distinct decisions, which experts to activate and how to distribute expert contributions among them, are conflated in standard Top-$k$+Softmax. We introduce Dirichlet-Routed MoE (DirMoE), a novel end-to-end differentiable routing mechanism built on a Dirichlet variational autoencoder framework. This design fundamentally disentangles the core routing problems: expert selection, modeled by a Bernoulli component, and expert contribution among chosen experts, handled by a Dirichlet component. The entire forward pass remains fully differentiable through the use of Gumbel-Sigmoid relaxation for the expert selection and implicit reparameterization for the Dirichlet distribution. Our training objective, a variational ELBO, includes a direct sparsity penalty that precisely controls the number of active experts in expectation, alongside a schedule for key hyperparameters that guides the model from an exploratory to a definitive routing state. Moreover, our DirMoE router matches or exceeds other methods while improving expert specialization.",
    "title_zh": "DirMoE：基于狄利克雷路由的专家混合模型",
    "abstract_zh": "混合专家（Mixture-of-Experts, MoE）模型在大规模语言模型中展现了卓越的性能。现有的路由机制通常依赖于非可微的Top-$k$+Softmax，这限制了其性能和可扩展性。我们认为，在标准的Top-$k$+Softmax中，两个关键决策——激活哪些专家，以及如何在这些专家之间分配贡献——被混淆在一起。为此，我们提出了Dirichlet-Routed MoE（DirMoE），一种基于Dirichlet变分自编码器框架的新型端到端可微路由机制。该设计从根本上解耦了核心路由问题：专家选择由伯努利分量建模，而所选专家之间的贡献分配则由狄利克雷分量处理。通过使用Gumbel-Sigmoid松弛进行专家选择，以及隐式重参数化处理狄利克雷分布，整个前向传播过程保持完全可微。我们的训练目标为变分ELBO，其中包含一个直接的稀疏性惩罚项，可精确控制期望激活专家的数量，同时辅以关键超参数的调度策略，引导模型从探索性路由状态逐步过渡到确定性路由状态。此外，我们的DirMoE路由机制在性能上达到或超越其他方法，同时显著提升了专家的专业化程度。"
  },
  {
    "date": "2026-02-09",
    "title": "InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery",
    "authors": "Shiyang Feng, Runmin Ma, Xiangchao Yan, Yue Fan, Yusong Hu, Songtao Huang, Shuaiyu Zhang, Zongsheng Cao, Tianshuo Peng, Jiakang Yuan, Zijie Guo, Zhijie Zhong, Shangheng Du, Weida Wang, Jinxin Shi, Yuhao Zhou, Xiaohan He, Zhiyin Yu, Fangchen Yu, Qihao Zheng, Jiamin Wu, Mianxin Liu, Chi Zhang, Shaowei Hou, Shuya Li, Yankai Jiang, Wenjie Lou, Lilong Wang, Zifu Wang, Jiong Wang, Wanghan Xu, Yue Deng, Dongrui Liu, Yiheng Wang, Wenlong Zhang, Fenghua Ling, Shufei Zhang, Xiaosong Wang, Shuangjia Zheng, Xun Huang, Siqi Sun, Shuyue Hu, Peng Ye, Chunfeng Song, Bin Wang, Conghui He, Yihao Liu, Xin Li, Qibin Hou, Tao Chen, Xiangyu Yue, Bin Wang, Liang He, Dahua Lin, Bowen Zhou, Bo Zhang, Lei Bai",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08990v1",
    "source": "arXiv",
    "abstract": "We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.",
    "title_zh": "InternAgent-1.5：面向长时程自主科学发现的统一智能体框架",
    "abstract_zh": "我们介绍了InternAgent-1.5，这是一个统一的系统，旨在实现计算与实证领域中的端到端科学发现。该系统基于一个结构化的架构，由三个协同工作的子系统组成：生成、验证与演化。这些子系统由深度研究、解决方案优化以及长时程记忆等基础能力支持。该架构使InternAgent-1.5能够在长时间的发现周期中持续运行，同时保持行为的一致性与持续改进。此外，系统还实现了计算建模与实验室实验在单一统一框架内的协调运作。我们在GAIA、HLE、GPQA和FrontierScience等科学推理基准上对InternAgent-1.5进行了评估，结果表明其性能处于领先地位，展现出强大的基础能力。除了基准测试外，我们进一步评估了两类发现任务：在算法发现任务中，InternAgent-1.5能够自主设计出在核心机器学习问题上具有竞争力的方法；在实证发现任务中，它能够独立执行完整的计算实验或湿实验，并在地球科学、生命科学、生物科学及物理科学等领域产出科学发现。总体而言，这些结果表明，InternAgent-1.5为自主科学发现提供了一个通用且可扩展的框架。"
  },
  {
    "date": "2026-02-09",
    "title": "Root Cause Analysis Method Based on Large Language Models with Residual Connection Structures",
    "authors": "Liming Zhou, Ailing Liu, Hongwei Liu, Min He, Heng Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.08804v1",
    "source": "arXiv",
    "abstract": "Root cause localization remain challenging in complex and large-scale microservice architectures. The complex fault propagation among microservices and the high dimensionality of telemetry data, including metrics, logs, and traces, limit the effectiveness of existing root cause analysis (RCA) methods. In this paper, a residual-connection-based RCA method using large language model (LLM), named RC-LLM, is proposed. A residual-like hierarchical fusion structure is designed to integrate multi-source telemetry data, while the contextual reasoning capability of large language models is leveraged to model temporal and cross-microservice causal dependencies. Experimental results on CCF-AIOps microservice datasets demonstrate that RC-LLM achieves strong accuracy and efficiency in root cause analysis.",
    "title_zh": "基于残差连接结构的大语言模型根因分析方法",
    "abstract_zh": "在复杂且大规模的微服务架构中，根因定位仍然面临巨大挑战。微服务之间复杂的故障传播机制以及包括指标、日志和链路追踪在内的高维遥测数据，限制了现有根因分析（RCA）方法的有效性。本文提出了一种基于残差连接的、利用大语言模型（LLM）的根因分析方法，命名为RC-LLM。该方法设计了一种类残差的分层融合结构，以整合多源遥测数据，同时利用大语言模型的上下文推理能力，建模时间序列上的动态变化及跨微服务的因果依赖关系。在CCF-AIOps微服务数据集上的实验结果表明，RC-LLM在根因分析任务中表现出优异的准确率与效率。"
  },
  {
    "date": "2026-2-9",
    "title": "MTF: an Open-Source Metamorphic Testing Framework for LLM-based systems",
    "authors": "Theis Henry, Sian Savourat, Lydie du Bousquet, Masahide Nakamura",
    "publish": "Proceedings of the 2025 5th International Conference on Artificial Intelligence and Application Technologies",
    "url": "https://doi.org/10.1145/3787120.3787123",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "MTF：一个用于基于大语言模型系统的开源变形测试框架",
    "abstract_zh": "None"
  },
  {
    "date": "2026-2-9",
    "title": "MetaML-Pro: Cross-Stage Design Flow Automation for Efficient Deep Learning Acceleration",
    "authors": "Zhiqiang Que, Jose G. F. Coutinho, Ce Guo, Hongxiang Fan, Wayne Luk",
    "publish": "ACM Transactions on Reconfigurable Technology and Systems",
    "url": "https://doi.org/10.1145/3795794",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "MetaML-Pro：面向高效深度学习加速的跨阶段设计流程自动化",
    "abstract_zh": "None"
  },
  {
    "date": "2026-2-9",
    "title": "Fix Pattern-Aware Vulnerability Patch Generation via In-Context Learning",
    "authors": "Miaomiao Shao, Yuxin Ding, Cuiyun Gao, Junru Wang, Guoqing Zhu",
    "publish": "ACM Transactions on Software Engineering and Methodology",
    "url": "https://doi.org/10.1145/3796512",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "通过上下文学习实现模式感知的漏洞补丁生成",
    "abstract_zh": "None"
  }
]