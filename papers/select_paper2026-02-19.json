[
  {
    "date": "2026-02-19",
    "title": "AgentConductor: Topology Evolution for Multi-Agent Competition-Level Code Generation",
    "authors": "Siyu Wang, Ruotian Lu, Zhihao Yang, Yuchao Wang, Yanzhou Zhang, Lei Xu, Qimin Xu, Guojun Yin, Cailian Chen, Xinping Guan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17100v1",
    "source": "arXiv",
    "abstract": "Large language model(LLM)-driven multi-agent systems(MAS) coordinate specialized agents through predefined interaction topologies and have shown promise for complex tasks such as competition-level code generation. Recent studies demonstrate that carefully designed multi-agent workflows and communication graphs can significantly improve code generation performance by leveraging collaborative reasoning. However, existing methods neither adapt topology density to task difficulty nor iteratively refine the topology within an instance using execution feedback, which leads to redundant communication and performance bottlenecks. To address these issues, we propose AgentConductor: a reinforcement learning-optimized MAS with an LLM-based orchestrator agent as its core, which enables end-to-end feedback-driven dynamic generation of interaction topologies. For each query, AgentConductor infers agent roles and task difficulty, then constructs a task-adapted, density-aware layered directed acyclic graph (DAG) topology, underpinned by two key innovations. First, we design a novel topological density function that captures communication-aware mathematical characterizations of multi-agent interactions. Second, we adopt difficulty interval partitioning to avoid excessive pruning for precise topological density upper bound measurement per difficulty level and finer-grained control. Empirically, across three competition-level and two foundational code datasets, AgentConductor achieves state-of-the-art accuracy, outperforming the strongest baseline by up to 14.6% in pass@1 accuracy, 13% in density reduction, and 68% in token cost reduction.",
    "title_zh": "AgentConductor：面向多智能体竞赛级代码生成的拓扑演化",
    "abstract_zh": "基于大语言模型（LLM）的多智能体系统（MAS）通过预定义的交互拓扑结构协调专业化智能体，在复杂任务（如竞赛级代码生成）中展现出巨大潜力。近期研究显示，精心设计的多智能体工作流与通信图能够通过协同推理显著提升代码生成性能。然而，现有方法既无法根据任务难度自适应调整拓扑密度，也无法在单个任务实例中利用执行反馈迭代优化拓扑结构，导致通信冗余和性能瓶颈。为解决上述问题，我们提出 AgentConductor：一种由 LLM 驱动的强化学习优化型多智能体系统，其核心为一个基于 LLM 的协调智能体，能够实现端到端、反馈驱动的动态交互拓扑生成。针对每个查询，AgentConductor 首先推断智能体角色与任务难度，随后构建任务自适应、密度感知的分层有向无环图（DAG）拓扑结构，其背后依托两项关键创新：第一，我们设计了一种新颖的拓扑密度函数，能够捕捉多智能体交互中的通信感知数学特征；第二，我们引入难度区间划分机制，避免在不同难度层级上过度剪枝，从而实现对拓扑密度上限的精确测量与更细粒度的控制。实验结果表明，在三个竞赛级代码数据集和两个基础代码数据集上，AgentConductor 达到了当前最优的准确率，相较于最强基线，pass@1 准确率最高提升 14.6%，拓扑密度降低 13%，令牌（token）消耗减少 68%。"
  },
  {
    "date": "2026-02-19",
    "title": "FLoRG: Federated Fine-tuning with Low-rank Gram Matrices and Procrustes Alignment",
    "authors": "Chuiyang Meng, Ming Tang, Vincent W. S. Wong",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17095v1",
    "source": "arXiv",
    "abstract": "Parameter-efficient fine-tuning techniques such as low-rank adaptation (LoRA) enable large language models (LLMs) to adapt to downstream tasks efficiently. Federated learning (FL) further facilitates this process by enabling collaborative fine-tuning across distributed clients without sharing private data. However, the use of two separate low-rank matrices in LoRA for federated fine-tuning introduces two types of challenges. The first challenge arises from the error induced by separately aggregating those two low-rank matrices. The second challenge occurs even when the product of two low-rank matrices is aggregated. The server needs to recover factors via matrix decomposition, which is non-unique and can introduce decomposition drift. To tackle the aforementioned challenges, we propose FLoRG, a federated fine-tuning framework which employs a single low-rank matrix for fine-tuning and aggregates its Gram matrix (i.e., the matrix of inner products of its column vectors), eliminating the aggregation error while also reducing the communication overhead. FLoRG minimizes the decomposition drift by introducing a Procrustes alignment approach which aligns the decomposed matrix between consecutive fine-tuning rounds for consistent updates. We theoretically analyze the convergence of FLoRG and prove that adopting the Procrustes alignment results in a tighter convergence bound. Experimental results across multiple LLM fine-tuning benchmarks demonstrate that FLoRG outperforms five state-of-the-art baseline schemes in the downstream task accuracy and can reduce the communication overhead by up to 2041$\\times$.",
    "title_zh": "FLoRG：基于低秩格拉姆矩阵与普鲁克斯特对齐的联邦微调",
    "abstract_zh": "参数高效微调技术（如低秩适应，LoRA）使得大型语言模型（LLMs）能够高效地适应下游任务。联邦学习（FL）进一步促进了这一过程，使分布在不同客户端的模型能够在不共享私有数据的情况下协同微调。然而，在联邦微调中使用LoRA的两个独立低秩矩阵会引入两类挑战。第一类挑战源于对这两个低秩矩阵分别聚合所导致的误差；第二类挑战即使在对两个低秩矩阵的乘积进行聚合时依然存在：服务器需要通过矩阵分解恢复因子，而这种分解是非唯一的，容易引发分解漂移。为解决上述问题，我们提出FLoRG——一种联邦微调框架，该框架采用单一低秩矩阵进行微调，并聚合其格拉姆矩阵（即其列向量内积构成的矩阵），从而消除聚合误差，同时显著降低通信开销。FLoRG通过引入普罗克鲁斯特斯对齐（Procrustes alignment）方法，对连续微调轮次中分解出的矩阵进行对齐，有效减小了分解漂移。我们对FLoRG的收敛性进行了理论分析，证明采用普罗克鲁斯特斯对齐可获得更紧的收敛界。在多个大型语言模型微调基准上的实验结果表明，FLoRG在下游任务准确率上优于五种最先进的基线方法，同时通信开销最高可降低2041倍。"
  },
  {
    "date": "2026-02-19",
    "title": "What to Cut? Predicting Unnecessary Methods in Agentic Code Generation",
    "authors": "Kan Watanabe, Tatsuya Shirai, Yutaro Kashiwa, Hajimu Iida",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17091v1",
    "source": "arXiv",
    "abstract": "Agentic Coding, powered by autonomous agents such as GitHub Copilot and Cursor, enables developers to generate code, tests, and pull requests from natural language instructions alone. While this accelerates implementation, it produces larger volumes of code per pull request, shifting the burden from implementers to reviewers. In practice, a notable portion of AI-generated code is eventually deleted during review, yet reviewers must still examine such code before deciding to remove it. No prior work has explored methods to help reviewers efficiently identify code that will be removed.In this paper, we propose a prediction model that identifies functions likely to be deleted during PR review. Our results show that functions deleted for different reasons exhibit distinct characteristics, and our model achieves an AUC of 87.1%. These findings suggest that predictive approaches can help reviewers prioritize their efforts on essential code.",
    "title_zh": "要剪裁什么？预测代理代码生成中的冗余方法",
    "abstract_zh": "由自主代理（如 GitHub Copilot 和 Cursor）驱动的智能编程（Agentic Coding）使开发者仅通过自然语言指令即可生成代码、测试用例和拉取请求（Pull Requests）。尽管这显著加快了开发实现的速度，但也导致每个拉取请求中产生的代码量大幅增加，从而将审查负担从实现者转移到了审查者。实际上，相当一部分由 AI 生成的代码在审查过程中最终被删除，而审查者仍需先审阅这些代码，才能决定是否删除。然而，此前尚无研究探索如何帮助审查者高效识别那些将被删除的代码。本文提出了一种预测模型，用于识别在拉取请求审查过程中可能被删除的函数。实验结果表明，因不同原因被删除的函数具有显著不同的特征，而我们的模型在预测性能上达到了 87.1% 的 AUC 值。这些发现表明，预测性方法能够帮助审查者更有效地聚焦于关键代码，从而提升审查效率。"
  },
  {
    "date": "2026-02-19",
    "title": "ReIn: Conversational Error Recovery with Reasoning Inception",
    "authors": "Takyoung Kim, Jinseok Nam, Chandrayee Basu, Xing Fan, Chengyuan Ma, Heng Ji, Gokhan Tur, Dilek Hakkani-Tür",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17022v1",
    "source": "arXiv",
    "abstract": "Conversational agents powered by large language models (LLMs) with tool integration achieve strong performance on fixed task-oriented dialogue datasets but remain vulnerable to unanticipated, user-induced errors. Rather than focusing on error prevention, this work focuses on error recovery, which necessitates the accurate diagnosis of erroneous dialogue contexts and execution of proper recovery plans. Under realistic constraints precluding model fine-tuning or prompt modification due to significant cost and time requirements, we explore whether agents can recover from contextually flawed interactions and how their behavior can be adapted without altering model parameters and prompts. To this end, we propose Reasoning Inception (ReIn), a test-time intervention method that plants an initial reasoning into the agent's decision-making process. Specifically, an external inception module identifies predefined errors within the dialogue context and generates recovery plans, which are subsequently integrated into the agent's internal reasoning process to guide corrective actions, without modifying its parameters or system prompts. We evaluate ReIn by systematically simulating conversational failure scenarios that directly hinder successful completion of user goals: user's ambiguous and unsupported requests. Across diverse combinations of agent models and inception modules, ReIn substantially improves task success and generalizes to unseen error types. Moreover, it consistently outperforms explicit prompt-modification approaches, underscoring its utility as an efficient, on-the-fly method. In-depth analysis of its operational mechanism, particularly in relation to instruction hierarchy, indicates that jointly defining recovery tools with ReIn can serve as a safe and effective strategy for improving the resilience of conversational agents without modifying the backbone models or system prompts.",
    "title_zh": "ReIn：基于推理启动的对话错误恢复",
    "abstract_zh": "由大型语言模型（LLMs）驱动并集成工具的对话代理在固定任务导向对话数据集上表现出色，但仍容易受到用户引发的意外错误影响。与以往侧重于错误预防的研究不同，本文聚焦于错误恢复，这要求对错误的对话上下文进行准确诊断，并执行恰当的恢复策略。在现实场景中，由于微调模型或修改提示（prompt）成本高昂且耗时，往往难以实施。因此，本文探讨了代理是否能够在不修改模型参数或系统提示的前提下，从上下文错误的交互中恢复，并实现行为适应。为此，我们提出了一种名为“推理启始”（Reasoning Inception, ReIn）的测试时干预方法，该方法在代理决策过程中植入初始推理机制。具体而言，一个外部的启始模块能够识别对话上下文中的预定义错误，并生成相应的恢复计划，这些计划随后被整合进代理的内部推理流程中，以引导其采取纠正措施，而无需修改模型参数或系统提示。我们通过系统性地模拟直接阻碍用户目标成功完成的对话失败场景——即用户提出模糊且缺乏支持的请求——对ReIn进行了评估。在多种代理模型与启始模块的组合下，ReIn显著提升了任务成功率，并展现出对未见过的错误类型的泛化能力。此外，其性能持续优于显式修改提示的方法，凸显了其作为高效、实时干预手段的实用价值。深入分析其工作机制，特别是与指令层级的关系表明，结合ReIn共同定义恢复工具，可作为一种安全且有效的策略，无需修改基础模型或系统提示，即可增强对话代理的鲁棒性。"
  },
  {
    "date": "2026-02-19",
    "title": "Mason: Type- and Name-Guided Program Synthesis",
    "authors": "Jasper Geer, Fox Huston, Jeffrey S. Foster",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.16981v1",
    "source": "arXiv",
    "abstract": "Object-oriented programs tend to be written using many common coding idioms, such as those captured by design patterns. While design patterns are useful, implementing them is often tedious and repetitive, requiring boilerplate code that distracts the programmer from more essential details. In this paper, we introduce Mason, a tool that synthesizes object-oriented programs from partial program pieces, and we apply it to automatically insert design patterns into programs. At the core of Mason is a novel technique we call type- and name-guided synthesis, in which an enumerative solver traverses a partial program to generate typing constraints; discharges constraints via program transformations guided by the names of constrained types and members; and backtracks when a constraint is violated or a candidate program fails unit tests. We also introduce two extensions to Mason: a non-local backtracking heuristic that uses execution traces, and a language of patterns that impose syntactic restrictions on missing names. We evaluate Mason on a suite of benchmarks to which Mason must add various well-known design patterns implemented as a library of program pieces. We find that Mason performs well when very few candidate programs satisfy its typing constraints and that our extensions can improve Mason's performance significantly when this is not the case. We believe that Mason takes an important step forward in synthesizing multi-class object-oriented programs using design patterns.",
    "title_zh": "梅森：类型与名称引导的程序合成",
    "abstract_zh": "面向对象的程序通常会使用许多常见的编码惯用法，例如设计模式所描述的那些。虽然设计模式非常有用，但实现它们往往繁琐且重复，需要编写大量样板代码，这会使程序员难以专注于更关键的细节。在本文中，我们介绍了Mason——一种从部分程序片段自动生成面向对象程序的工具，并将其应用于自动向程序中插入设计模式。Mason的核心是一种新颖的技术，称为“类型与名称引导的合成”：该技术利用枚举求解器遍历部分程序以生成类型约束；通过受受限类型和成员名称指导的程序变换来消除这些约束；当某个约束被违反或候选程序未能通过单元测试时，则进行回溯。我们还为Mason引入了两个扩展：一种基于执行轨迹的非局部回溯启发式方法，以及一种用于对缺失名称施加语法限制的模式语言。我们在一组基准测试上评估了Mason，这些测试要求Mason将各种著名的、以程序片段库形式实现的设计模式添加到程序中。实验结果表明，当满足类型约束的候选程序数量很少时，Mason表现良好；而当这种情况不成立时，我们的扩展能够显著提升Mason的性能。我们认为，Mason在利用设计模式自动生成多类面向对象程序方面迈出了重要的一步。"
  },
  {
    "date": "2026-02-19",
    "title": "DAVE: A Policy-Enforcing LLM Spokesperson for Secure Multi-Document Data Sharing",
    "authors": "René Brinkhege, Prahlad Menon",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17413v1",
    "source": "arXiv",
    "abstract": "In current inter-organizational data spaces, usage policies are enforced mainly at the asset level: a whole document or dataset is either shared or withheld. When only parts of a document are sensitive, providers who want to avoid leaking protected information typically must manually redact documents before sharing them, which is costly, coarse-grained, and hard to maintain as policies or partners change. We present DAVE, a usage policy-enforcing LLM spokesperson that answers questions over private documents on behalf of a data provider. Instead of releasing documents, the provider exposes a natural language interface whose responses are constrained by machine-readable usage policies. We formalize policy-violating information disclosure in this setting, drawing on usage control and information flow security, and introduce virtual redaction: suppressing sensitive information at query time without modifying source documents. We describe an architecture for integrating such a spokesperson with Eclipse Dataspace Components and ODRL-style policies, and outline an initial provider-side integration prototype in which QA requests are routed through a spokesperson service instead of triggering raw document transfer. Our contribution is primarily architectural: we do not yet implement or empirically evaluate the full enforcement pipeline. We therefore outline an evaluation methodology to assess security, utility, and performance trade-offs under benign and adversarial querying as a basis for future empirical work on systematically governed LLM access to multi-party data spaces.",
    "title_zh": "DAVE：用于安全多文档数据共享的策略强制型大语言模型发言人",
    "abstract_zh": "在当前的组织间数据空间中，使用策略主要在资产层面执行：整个文档或数据集要么被共享，要么被保留。当文档仅部分涉及敏感信息时，为避免泄露受保护信息，数据提供方通常必须在共享前手动对文档进行删减处理，这不仅成本高昂、粒度粗略，而且随着策略或合作方的变化，维护起来也十分困难。我们提出了DAVE——一种基于大语言模型（LLM）的使用策略执行代理，该代理代表数据提供方回答关于私有文档的问题。数据提供方不直接发布文档，而是暴露一个自然语言接口，其响应受到机器可读使用策略的约束。我们在此场景下形式化了违反策略的信息披露问题，借鉴了使用控制和信息流安全的相关理论，并引入了“虚拟删减”机制：在查询时动态抑制敏感信息，而无需修改原始文档。我们描述了一种将此类代理与Eclipse Dataspace Components及ODRL风格策略集成的架构，并概述了一个初步的提供方侧集成原型，其中问答请求通过代理服务进行处理，而非触发原始文档的传输。我们的主要贡献在于架构设计：目前尚未实现或实证评估完整的策略执行流程。因此，我们提出了一种评估方法论，用于在良性与对抗性查询场景下，系统性地评估安全、可用性与性能之间的权衡，为未来关于多参与方数据空间中受控LLM访问的实证研究奠定基础。"
  },
  {
    "date": "2026-02-19",
    "title": "Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval",
    "authors": "Adrià Molina, Oriol Ramos Terrades, Josep Lladós",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17386v1",
    "source": "arXiv",
    "abstract": "Information retrieval lies at the foundation of the modern digital industry. While natural language search has seen dramatic progress in recent years largely driven by embedding-based models and large-scale pretraining, the field still faces significant challenges. Specifically, queries that involve complex relationships, object compositions, or precise constraints such as identities, counts and proportions often remain unresolved or unreliable within current frameworks. In this paper, we propose a novel framework that integrates formal verification into deep learning-based image retrieval through a synergistic combination of graph-based verification methods and neural code generation. Our approach aims to support open-vocabulary natural language queries while producing results that are both trustworthy and verifiable. By grounding retrieval results in a system of formal reasoning, we move beyond the ambiguity and approximation that often characterize vector representations. Instead of accepting uncertainty as a given, our framework explicitly verifies each atomic truth in the user query against the retrieved content. This allows us to not only return matching results, but also to identify and mark which specific constraints are satisfied and which remain unmet, thereby offering a more transparent and accountable retrieval process while boosting the results of the most popular embedding-based approaches.",
    "title_zh": "视觉模型检测：基于图的视觉程序推断用于图像检索",
    "abstract_zh": "信息检索构成了现代数字产业的基础。尽管近年来自然语言搜索取得了显著进展，主要得益于基于嵌入的模型和大规模预训练技术，但该领域仍面临诸多挑战。具体而言，涉及复杂关系、对象组合或精确约束（如身份、数量和比例）的查询，在当前框架下往往难以解决或结果不可靠。本文提出了一种新颖的框架，通过图式验证方法与神经代码生成的协同结合，将形式化验证融入基于深度学习的图像检索中。我们的方法旨在支持开放词汇的自然语言查询，同时生成可信且可验证的结果。通过将检索结果建立在形式化推理体系之上，我们超越了向量表示常有的模糊性和近似性。与接受不确定性为必然不同，我们的框架会显式地将用户查询中的每一个基本事实与检索内容进行验证。这不仅使我们能够返回匹配结果，还能明确标识出哪些具体约束得到满足，哪些尚未满足，从而实现更透明、更具问责性的检索过程，同时显著提升当前最主流的嵌入式方法的性能。"
  },
  {
    "date": "2026-02-19",
    "title": "What Makes a Good LLM Agent for Real-world Penetration Testing?",
    "authors": "Gelei Deng, Yi Liu, Yuekang Li, Ruozhao Yang, Xiaofei Xie, Jie Zhang, Han Qiu, Tianwei Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17622v1",
    "source": "arXiv",
    "abstract": "LLM-based agents show promise for automating penetration testing, yet reported performance varies widely across systems and benchmarks. We analyze 28 LLM-based penetration testing systems and evaluate five representative implementations across three benchmarks of increasing complexity. Our analysis reveals two distinct failure modes: Type A failures stem from capability gaps (missing tools, inadequate prompts) that engineering readily addresses, while Type B failures persist regardless of tooling due to planning and state management limitations. We show that Type B failures share a root cause that is largely invariant to the underlying LLM: agents lack real-time task difficulty estimation. As a result, agents misallocate effort, over-commit to low-value branches, and exhaust context before completing attack chains. Based on this insight, we present Excalibur, a penetration testing agent that couples strong tooling with difficulty-aware planning. A Tool and Skill Layer eliminates Type A failures through typed interfaces and retrieval-augmented knowledge. A Task Difficulty Assessment (TDA) mechanism addresses Type B failures by estimating tractability through four measurable dimensions (horizon estimation, evidence confidence, context load, and historical success) and uses these estimates to guide exploration-exploitation decisions within an Evidence-Guided Attack Tree Search (EGATS) framework. Excalibur achieves up to 91% task completion on CTF benchmarks with frontier models (39 to 49% relative improvement over baselines) and compromises 4 of 5 hosts on the GOAD Active Directory environment versus 2 by prior systems. These results show that difficulty-aware planning yields consistent end-to-end gains across models and addresses a limitation that model scaling alone does not eliminate.",
    "title_zh": "什么造就了适用于现实世界渗透测试的优秀大语言模型代理？",
    "abstract_zh": "基于大语言模型（LLM）的智能体在自动化渗透测试方面展现出巨大潜力，但不同系统和基准测试中的表现差异显著。我们分析了28个基于LLM的渗透测试系统，并在三个复杂度逐步提升的基准上评估了五个代表性实现。分析揭示出两种截然不同的失败模式：A类失败源于能力缺口（如缺少工具、提示设计不足），这类问题通过工程手段可轻松解决；而B类失败则无论工具如何改进都持续存在，其根源在于规划与状态管理能力的局限。我们发现，B类失败的根本原因在很大程度上与底层LLM无关——智能体缺乏对任务实时难度的评估能力。因此，智能体在任务执行中出现资源错配，过度投入低价值分支，最终在完成攻击链之前耗尽上下文容量。\n\n基于这一洞察，我们提出了Excalibur——一种融合强大工具链与难度感知规划的渗透测试智能体。其“工具与技能层”通过类型化接口和检索增强知识，有效消除A类失败；而“任务难度评估”（Task Difficulty Assessment, TDA）机制则通过四个可量化的维度（未来展望估计、证据置信度、上下文负载和历史成功率）来评估任务的可解性，并据此在“证据引导的攻击树搜索”（Evidence-Guided Attack Tree Search, EGATS）框架内指导探索与利用的权衡决策。\n\n实验结果表明，Excalibur在CTF基准测试中，使用前沿大模型时任务完成率最高可达91%，相比基线系统提升39%至49%。在GOAD Active Directory环境中，Excalibur成功攻陷5个主机中的4个，而此前系统仅能攻陷2个。这些结果表明，难度感知规划能够带来跨模型的一致性端到端性能提升，解决了仅靠模型规模扩展无法克服的关键瓶颈。"
  },
  {
    "date": "2026-02-19",
    "title": "Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation",
    "authors": "Dun Yuan, Hao Zhou, Xue Liu, Hao Chen, Yan Xin, Jianzhong, Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17529v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model's outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG's effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios.",
    "title_zh": "利用动态知识图谱与可解释的检索增强生成技术提升大型语言模型在电信领域的应用",
    "abstract_zh": "大型语言模型（LLMs）在多种任务中展现出强大的潜力，但在电信领域的应用仍面临挑战，主要源于领域复杂性、标准不断演进以及专业术语的使用。因此，通用领域的大模型在该场景下可能难以提供准确可靠的结果，导致幻觉现象增多，降低其在电信运营中的实际效用。为解决上述局限性，本文提出一种名为KG-RAG的新框架，通过将知识图谱（KG）与检索增强生成（RAG）相结合，提升大模型在电信特定任务中的表现。具体而言，知识图谱从电信标准和技术文档中提取并结构化领域知识，而RAG则支持动态检索相关事实，以确保模型输出的准确性与可解释性。这种融合方式显著提升了事实准确性，减少了幻觉，同时保障了输出结果符合电信规范。在多个基准数据集上的实验结果表明，KG-RAG在性能上优于仅使用LLM或标准RAG的基线方法，例如，相较于RAG，KG-RAG平均准确率提升14.3%；相较于纯LLM模型，提升达21.6%。这些结果充分证明了KG-RAG在复杂电信场景中生成准确、可靠且可解释输出的有效性。"
  },
  {
    "date": "2026-02-19",
    "title": "When Models Ignore Definitions: Measuring Semantic Override Hallucinations in LLM Reasoning",
    "authors": "Yogeswar Reddy Thota, Setareh Rafatirad, Homayoun Houman, Tooraj Nikoubin",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17520v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) demonstrate strong performance on standard digital logic and Boolean reasoning tasks, yet their reliability under locally redefined semantics remains poorly understood. In many formal settings, such as circuit specifications, examinations, and hardware documentation, operators and components are explicitly redefined within narrow scope. Correct reasoning in these contexts requires models to temporarily suppress globally learned conventions in favor of prompt-local definitions. In this work, we study a systematic failure mode we term semantic override, in which an LLM reverts to its pretrained default interpretation of operators or gate behavior despite explicit redefinition in the prompt. We also identify a related class of errors, assumption injection, where models commit to unstated hardware semantics when critical details are underspecified, rather than requesting clarification. We introduce a compact micro-benchmark of 30 logic and digital-circuit reasoning tasks designed as verifier-style traps, spanning Boolean algebra, operator overloading, redefined gates, and circuit-level semantics. Evaluating three frontier LLMs, we observe persistent noncompliance with local specifications, confident but incompatible assumptions, and dropped constraints even in elementary settings. Our findings highlight a gap between surface-level correctness and specification-faithful reasoning, motivating evaluation protocols that explicitly test local unlearning and semantic compliance in formal domains.",
    "title_zh": "当模型忽略定义时：衡量大语言模型推理中的语义覆盖幻觉",
    "abstract_zh": "大型语言模型（LLMs）在标准的数字逻辑与布尔推理任务中表现出色，但其在局部语义重新定义情境下的可靠性仍缺乏深入理解。在许多形式化场景中，如电路规格说明、考试题目和硬件文档中，操作符与组件常在有限范围内被显式重新定义。在这些上下文中进行正确推理，要求模型能够暂时摒弃全局预训练所习得的惯例，转而遵循提示中的局部定义。本文研究了一种系统性失效模式，称为“语义覆盖”（semantic override），即尽管提示中已明确重新定义，LLM 仍会回归其预训练时默认的操作符或门电路行为。我们还识别出另一类相关错误，称为“假设注入”（assumption injection），即当关键细节未充分说明时，模型会自行假设未明示的硬件语义，而非主动请求澄清。为此，我们设计了一个包含30个逻辑与数字电路推理任务的紧凑型微基准测试，作为验证器风格的陷阱，涵盖布尔代数、操作符重载、重新定义的门电路以及电路级语义等维度。对三款前沿LLM的评估显示，它们在面对局部规范时仍存在持续的不合规现象，表现出自信但与规范相冲突的假设，甚至在基础场景中也出现了约束丢失的情况。我们的研究揭示了表面正确性与规范忠实推理之间的差距，强调了在形式化领域中需要引入明确测试局部“遗忘”与语义合规性的评估协议。"
  },
  {
    "date": "2026-02-19",
    "title": "ShadAR: LLM-driven shader generation to transform visual perception in Augmented Reality",
    "authors": "Yanni Mei, Samuel Wendt, Florian Mueller, Jan Gugenheimer",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17481v1",
    "source": "arXiv",
    "abstract": "Augmented Reality (AR) can simulate various visual perceptions, such as how individuals with colorblindness see the world. However, these simulations require developers to predefine each visual effect, limiting flexibility. We present ShadAR, an AR application enabling real-time transformation of visual perception through shader generation using large language models (LLMs). ShadAR allows users to express their visual intent via natural language, which is interpreted by an LLM to generate corresponding shader code. This shader is then compiled real-time to modify the AR headset viewport. We present our LLM-driven shader generation pipeline and demonstrate its ability to transform visual perception for inclusiveness and creativity.",
    "title_zh": "ShadAR：基于大语言模型的着色器生成，以增强现实中的视觉感知",
    "abstract_zh": "增强现实（AR）能够模拟各种视觉感知，例如展示色盲人士眼中的世界。然而，这些模拟通常需要开发者预先定义每种视觉效果，限制了灵活性。我们提出了ShadAR，一款基于大语言模型（LLMs）实时生成着色器的AR应用，使视觉感知的实时转换成为可能。用户可通过自然语言表达其视觉意图，大语言模型将理解该意图并生成相应的着色器代码，随后该代码被实时编译，用于修改AR头显的视口。我们展示了由大语言模型驱动的着色器生成流程，并验证了其在促进视觉包容性与激发创造力方面的强大能力。"
  },
  {
    "date": "2026-02-19",
    "title": "Privacy in Theory, Bugs in Practice: Grey-Box Auditing of Differential Privacy Libraries",
    "authors": "Tudor Cebere, David Erb, Damien Desfontaines, Aurélien Bellet, Jack Fitzsimons",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17454v1",
    "source": "arXiv",
    "abstract": "Differential privacy (DP) implementations are notoriously prone to errors, with subtle bugs frequently invalidating theoretical guarantees. Existing verification methods are often impractical: formal tools are too restrictive, while black-box statistical auditing is intractable for complex pipelines and fails to pinpoint the source of the bug. This paper introduces Re:cord-play, a gray-box auditing paradigm that inspects the internal state of DP algorithms. By running an instrumented algorithm on neighboring datasets with identical randomness, Re:cord-play directly checks for data-dependent control flow and provides concrete falsification of sensitivity violations by comparing declared sensitivity against the empirically measured distance between internal inputs. We generalize this to Re:cord-play-sample, a full statistical audit that isolates and tests each component, including untrusted ones. We show that our novel testing approach is both effective and necessary by auditing 12 open-source libraries, including SmartNoise SDK, Opacus, and Diffprivlib, and uncovering 13 privacy violations that impact their theoretical guarantees. We release our framework as an open-source Python package, thereby making it easy for DP developers to integrate effective, computationally inexpensive, and seamless privacy testing as part of their software development lifecycle.",
    "title_zh": "理论上的隐私，实践中的漏洞：差分隐私库的灰盒审计",
    "abstract_zh": "差分隐私（DP）实现极易出错，细微的漏洞常常会破坏其理论保证。现有的验证方法往往不切实际：形式化工具过于严格，而黑盒统计审计对于复杂的数据处理流程难以实施，且无法精确定位错误来源。本文提出 Re:cord-play，一种灰盒审计范式，通过检查差分隐私算法的内部状态来实现审计。通过在具有相同随机性的相邻数据集上运行经过插桩的算法，Re:cord-play 能够直接检测依赖于数据的控制流，并通过比较声明的敏感度与内部输入之间实测距离，提供对敏感度违规的明确反例验证。我们进一步将其推广为 Re:cord-play-sample，一种完整的统计审计方法，能够隔离并测试每个组件（包括不可信组件）。通过审计12个开源库（包括 SmartNoise SDK、Opacus 和 Diffprivlib），我们展示了该新型测试方法的有效性与必要性，共发现13个影响理论保证的隐私漏洞。我们已将该框架以开源 Python 包的形式发布，使 DP 开发者能够轻松地将高效、计算开销低且无缝集成的隐私测试纳入其软件开发流程中。"
  },
  {
    "date": "2026-02-19",
    "title": "SimulatorCoder: DNN Accelerator Simulator Code Generation and Optimization via Large Language Models",
    "authors": "Yuhuan Xia, Tun Li, Hongji Zhou, Xianfa Zhou, Chong Chen, Ruiyu Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17169v1",
    "source": "arXiv",
    "abstract": "This paper presents SimulatorCoder, an agent powered by large language models (LLMs), designed to generate and optimize deep neural network (DNN) accelerator simulators based on natural language descriptions. By integrating domain-specific prompt engineering including In-Context Learning (ICL), Chain-of-Thought (CoT) reasoning, and a multi-round feedback-verification flow, SimulatorCoder systematically transforms high-level functional requirements into efficient, executable, and architecture-aligned simulator code. Experiments based on the customized SCALE-Sim benchmark demonstrate that structured prompting and feedback mechanisms substantially improve both code generation accuracy and simulator performance. The resulting simulators not only maintain cycle-level fidelity with less than 1% error compared to manually implemented counterparts, but also consistently achieve lower simulation runtimes, highlighting the effectiveness of LLM-based methods in accelerating simulator development. Our code is available at https://github.com/xiayuhuan/SimulatorCoder.",
    "title_zh": "SimulatorCoder：基于大语言模型的DNN加速器模拟器代码生成与优化",
    "abstract_zh": "本文提出了SimulatorCoder，一个由大型语言模型（LLMs）驱动的智能体，旨在根据自然语言描述自动生成并优化深度神经网络（DNN）加速器模拟器。通过整合领域特定的提示工程方法，包括上下文学习（In-Context Learning, ICL）、思维链（Chain-of-Thought, CoT）推理以及多轮反馈验证流程，SimulatorCoder能够系统地将高层次的功能需求转化为高效、可执行且与架构一致的模拟器代码。基于定制化的SCALE-Sim基准测试的实验表明，结构化提示与反馈机制显著提升了代码生成的准确率和模拟器性能。生成的模拟器不仅在周期级精度上与人工实现的版本保持高度一致（误差低于1%），而且在模拟运行时间上始终更优，充分展现了基于LLM的方法在加速模拟器开发方面的有效性。我们的代码已开源，地址为：https://github.com/xiayuhuan/SimulatorCoder。"
  },
  {
    "date": "2026-02-19",
    "title": "Computer-Using World Model",
    "authors": "Yiming Guan, Rui Yu, John Zhang, Lu Wang, Chaoyun Zhang, Liqun Li, Bo Qiao, Si Qin, He Huang, Fangkai Yang, Pu Zhao, Lukas Wutschitz, Samuel Kessler, Huseyin A Inan, Robert Sim, Saravan Rajmohan, Qingwei Lin, Dongmei Zhang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17365v1",
    "source": "arXiv",
    "abstract": "Agents operating in complex software environments benefit from reasoning about the consequences of their actions, as even a single incorrect user interface (UI) operation can derail long, artifact-preserving workflows. This challenge is particularly acute for computer-using scenarios, where real execution does not support counterfactual exploration, making large-scale trial-and-error learning and planning impractical despite the environment being fully digital and deterministic. We introduce the Computer-Using World Model (CUWM), a world model for desktop software that predicts the next UI state given the current state and a candidate action. CUWM adopts a two-stage factorization of UI dynamics: it first predicts a textual description of agent-relevant state changes, and then realizes these changes visually to synthesize the next screenshot. CUWM is trained on offline UI transitions collected from agents interacting with real Microsoft Office applications, and further refined with a lightweight reinforcement learning stage that aligns textual transition predictions with the structural requirements of computer-using environments. We evaluate CUWM via test-time action search, where a frozen agent uses the world model to simulate and compare candidate actions before execution. Across a range of Office tasks, world-model-guided test-time scaling improves decision quality and execution robustness.",
    "title_zh": "计算机使用世界模型",
    "abstract_zh": "在复杂的软件环境中，代理若能推理自身行为的后果，将显著提升效率。因为即使一次微小的用户界面（UI）操作失误，也可能导致长期且依赖于成果保持的工作流中断。这一挑战在计算机使用场景中尤为突出：由于真实执行环境无法支持反事实探索，尽管环境本身是完全数字化且确定性的，大规模的试错学习与规划仍不切实际。为此，我们提出了“计算机使用世界模型”（Computer-Using World Model, CUWM），这是一种针对桌面软件的世界模型，能够根据当前状态和候选动作预测下一UI状态。CUWM采用两阶段的UI动态分解机制：首先预测与代理相关的状态变化的文本描述，然后基于该描述在视觉上实现这些变化，生成下一帧截图。CUWM基于真实用户与微软Office应用交互过程中收集的离线UI状态转移数据进行训练，并通过一个轻量级强化学习阶段进一步优化，使文本过渡预测与计算机使用环境的结构化需求保持一致。我们在测试时采用动作搜索策略对CUWM进行评估，即冻结的代理利用世界模型在实际执行前模拟并比较多个候选动作。在一系列Office任务中，基于世界模型的测试时扩展显著提升了决策质量与执行鲁棒性。"
  },
  {
    "date": "2026-02-19",
    "title": "Socio-Technical Well-Being of Quantum Software Communities: An Overview on Community Smells",
    "authors": "Stefano Lambiase, Manuel De Stefano, Fabio Palomba, Filomena Ferrucci, Andrea De Lucia",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17320v1",
    "source": "arXiv",
    "abstract": "Quantum computing has gained significant attention due to its potential to solve computational problems beyond the capabilities of classical computers. With major corporations and academic institutions investing in quantum hardware and software, there has been a rise in the development of quantum-enabled systems, particularly within open-source communities. However, despite the promising nature of quantum technologies, these communities face critical socio-technical challenges, including the emergence of socio-technical anti-patterns known as community smells. These anti-patterns, prevalent in open-source environments, have the potential to negatively impact both product quality and community health by introducing technical debt and amplifying architectural and code smells. Despite the importance of these socio-technical factors, there remains a scarcity of research investigating their influence within quantum open-source communities. This work aims to address this gap by providing a first step in analyzing the socio-technical well-being of quantum communities through a cross-sectional study. By understanding the socio-technical dynamics at play, it is expected that foundational knowledge can be established to mitigate the risks associated with community smells and ensure the long-term sustainability of open-source quantum initiatives.",
    "title_zh": "量子软件社区的社会技术福祉：社区异味的综述",
    "abstract_zh": "量子计算因其有望解决经典计算机无法处理的计算难题而受到广泛关注。随着大型企业与学术机构在量子硬件和软件上的持续投入，量子赋能系统的开发迅速发展，尤其是在开源社区中表现尤为突出。然而，尽管量子技术前景广阔，这些社区仍面临一系列关键的社会技术挑战，其中尤为突出的是被称为“社区异味”的社会技术反模式。这些反模式在开源环境中普遍存在，可能通过引入技术债务，加剧架构与代码异味，从而对产品质量和社区健康造成负面影响。尽管这些社会技术因素至关重要，但目前针对其在量子开源社区中影响的研究仍十分匮乏。本文旨在填补这一空白，通过一项横断面研究，迈出分析量子社区社会技术健康状况的第一步。通过深入理解其中的社会技术动态，有望建立基础性知识，以应对社区异味带来的风险，确保开源量子项目的长期可持续发展。"
  },
  {
    "date": "2026-02-19",
    "title": "ArXiv-to-Model: A Practical Study of Scientific LM Training",
    "authors": "Anuj Gupta",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17288v1",
    "source": "arXiv",
    "abstract": "While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.",
    "title_zh": "ArXiv-to-Model：科学语言模型训练的实践研究",
    "abstract_zh": "尽管前沿的大语言模型展现出强大的推理与数学能力，但从原始数据源训练领域专用科学语言模型的实际过程仍缺乏充分的文档记录。在本研究中，我们详细阐述了一个从原始arXiv LaTeX源码训练13.6亿参数科学语言模型的案例，覆盖数学、计算机科学和理论物理三个领域。我们构建了一个端到端的处理流程，包括元数据过滤、归档验证、LaTeX提取、文本规范化、领域感知的分词处理，以及在有限算力（2块A100 GPU）条件下的密集Transformer训练。通过24次实验运行，我们分析了训练稳定性、扩展性行为、数据产出损失以及基础设施瓶颈。研究结果表明，预处理决策显著影响可用的token总量，分词方式对符号稳定性具有重要影响，而存储与I/O限制可能与计算资源一样成为关键制约因素。我们进一步分析了模型收敛动态，并展示了在数据丰富场景下（520亿预训练token）的稳定训练表现。本文并未提出新颖的模型架构，而是基于工程实践，提供了一次从零开始训练小型科学语言模型的透明、可复现的完整记录。我们希望这些见解能为在中等算力预算下开展研究、致力于构建领域专用模型的学者提供切实支持。"
  },
  {
    "date": "2026-02-19",
    "title": "Efficient Parallel Algorithm for Decomposing Hard CircuitSAT Instances",
    "authors": "Victor Kondratiev, Irina Gribanova, Alexander Semenov",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17130v1",
    "source": "arXiv",
    "abstract": "We propose a novel parallel algorithm for decomposing hard CircuitSAT instances. The technique employs specialized constraints to partition an original SAT instance into a family of weakened formulas. Our approach is implemented as a parameterized parallel algorithm, where adjusting the parameters allows efficient identification of high-quality decompositions, guided by hardness estimations computed in parallel. We demonstrate the algorithm's practical efficacy on challenging CircuitSAT instances, including those encoding Logical Equivalence Checking of Boolean circuits and preimage attacks on cryptographic hash functions.",
    "title_zh": "高效并行算法分解困难的电路SAT实例",
    "abstract_zh": "我们提出了一种新颖的并行算法，用于分解复杂的 CircuitSAT 实例。该技术通过引入专门的约束，将原始的 SAT 实例划分为一系列弱化后的公式族。我们的方法实现为一种参数化并行算法，通过调整参数，能够高效地识别出高质量的分解方案，其过程由并行计算得到的难度估计进行引导。我们在具有挑战性的 CircuitSAT 实例上验证了该算法的实际有效性，包括用于布尔电路逻辑等价性检查以及对密码哈希函数的原像攻击等场景。"
  },
  {
    "date": "2026-02-19",
    "title": "How AI Coding Agents Communicate: A Study of Pull Request Description Characteristics and Human Review Responses",
    "authors": "Kan Watanabe, Rikuto Tsuchida, Takahiro Monno, Bin Huang, Kazuma Yamasaki, Youmei Fan, Kazumasa Shimari, Kenichi Matsumoto",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17084v1",
    "source": "arXiv",
    "abstract": "The rapid adoption of large language models has led to the emergence of AI coding agents that autonomously create pull requests on GitHub. However, how these agents differ in their pull request description characteristics, and how human reviewers respond to them, remains underexplored. In this study, we conduct an empirical analysis of pull requests created by five AI coding agents using the AIDev dataset. We analyze agent differences in pull request description characteristics, including structural features, and examine human reviewer response in terms of review activity, response timing, sentiment, and merge outcomes. We find that AI coding agents exhibit distinct PR description styles, which are associated with differences in reviewer engagement, response time, and merge outcomes. We observe notable variation across agents in both reviewer interaction metrics and merge rates. These findings highlight the role of pull request presentation and reviewer interaction dynamics in human-AI collaborative software development.",
    "title_zh": "AI编程代理的沟通方式：基于拉取请求描述特征与人类评审响应的研究",
    "abstract_zh": "大型语言模型的快速普及催生了能够自主在 GitHub 上创建拉取请求（Pull Requests）的 AI 编码代理。然而，这些代理在拉取请求描述特征上的差异，以及人类评审者对其的响应方式，仍缺乏深入研究。在本研究中，我们基于 AIDev 数据集，对五种 AI 编码代理生成的拉取请求进行了实证分析。我们探讨了不同代理在拉取请求描述特征上的差异，包括结构化特征，并从评审活动、响应时间、情感倾向及合并结果等方面考察了人类评审者的反馈。研究发现，AI 编码代理呈现出显著不同的 PR 描述风格，这些风格与评审者参与度、响应速度及合并结果存在关联。不同代理在评审互动指标和合并率方面均表现出明显差异。这些发现凸显了拉取请求的呈现方式以及人机协作开发中评审互动动态的重要性。"
  },
  {
    "date": "2026-02-19",
    "title": "Wink: Recovering from Misbehaviors in Coding Agents",
    "authors": "Rahul Nanda, Chandra Maddila, Smriti Jha, Euna Mehnaz Khan, Matteo Paltenghi, Satish Chandra",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17037v1",
    "source": "arXiv",
    "abstract": "Autonomous coding agents, powered by large language models (LLMs), are increasingly being adopted in the software industry to automate complex engineering tasks. However, these agents are prone to a wide range of misbehaviors, such as deviating from the user's instructions, getting stuck in repetitive loops, or failing to use tools correctly. These failures disrupt the development workflow and often require resource-intensive manual intervention. In this paper, we present a system for automatically recovering from agentic misbehaviors at scale. We first introduce a taxonomy of misbehaviors grounded in an analysis of production traffic, identifying three primary categories: Specification Drift, Reasoning Problems, and Tool Call Failures, which we find occur in about 30% of all agent trajectories. To address these issues, we developed a lightweight, asynchronous self-intervention system named Wink. Wink observes agent trajectories and provides targeted course-correction guidance to nudge the agent back to a productive path. We evaluated our system on over 10,000 real world agent trajectories and found that it successfully resolves 90% of the misbehaviors that require a single intervention. Furthermore, a live A/B test in our production environment demonstrated that our system leads to a statistically significant reduction in Tool Call Failures, Tokens per Session and Engineer Interventions per Session. We present our experience designing and deploying this system, offering insights into the challenges of building resilient agentic systems at scale.",
    "title_zh": "眨眼：编码智能体行为失当后的恢复机制",
    "abstract_zh": "由大型语言模型（LLMs）驱动的自主编码代理在软件行业中正被越来越多地采用，以自动化复杂的工程任务。然而，这些代理容易出现各种异常行为，例如偏离用户指令、陷入重复循环，或未能正确使用工具。这些失败会打断开发流程，通常需要耗费大量资源的手动干预。本文提出了一种可大规模自动恢复代理异常行为的系统。我们首先基于对生产流量的分析，构建了一个异常行为的分类体系，识别出三大主要类别：规范漂移（Specification Drift）、推理问题（Reasoning Problems）和工具调用失败（Tool Call Failures），发现这三类问题约占所有代理轨迹的30%。为应对这些问题，我们开发了一种轻量级、异步的自我干预系统，命名为Wink。Wink能够监控代理的执行轨迹，并提供有针对性的纠正指导，帮助代理重新回到高效的工作路径上。我们在超过10,000条真实世界中的代理轨迹上评估了该系统，结果表明，对于仅需一次干预即可修复的异常行为，Wink成功解决了其中90%。此外，在生产环境中进行的实时A/B测试显示，该系统显著降低了工具调用失败率、每会话的token消耗量以及每会话的工程师干预次数。本文分享了我们在设计与部署该系统过程中的实践经验，为构建可大规模运行的高韧性代理系统提供了重要洞见。"
  },
  {
    "date": "2026-02-19",
    "title": "M2F: Automated Formalization of Mathematical Literature at Scale",
    "authors": "Zichen Wang, Wanli Ma, Zhenyu Ming, Gong Zhang, Kun Yuan, Zaiwen Wen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17016v1",
    "source": "arXiv",
    "abstract": "Automated formalization of mathematics enables mechanical verification but remains limited to isolated theorems and short snippets. Scaling to textbooks and research papers is largely unaddressed, as it requires managing cross-file dependencies, resolving imports, and ensuring that entire projects compile end-to-end. We present M2F (Math-to-Formal), the first agentic framework for end-to-end, project-scale autoformalization in Lean. The framework operates in two stages. The statement compilation stage splits the document into atomic blocks, orders them via inferred dependencies, and repairs declaration skeletons until the project compiles, allowing placeholders in proofs. The proof repair stage closes these holes under fixed signatures using goal-conditioned local edits. Throughout both stages, M2F keeps the verifier in the loop, committing edits only when toolchain feedback confirms improvement. In approximately three weeks, M2F converts long-form mathematical sources into a project-scale Lean library of 153,853 lines from 479 pages textbooks on real analysis and convex analysis, fully formalized as Lean declarations with accompanying proofs. This represents textbook-scale formalization at a pace that would typically require months or years of expert effort. On FATE-H, we achieve $96\\%$ proof success (vs.\\ $80\\%$ for a strong baseline). Together, these results demonstrate that practical, large-scale automated formalization of mathematical literature is within reach. The full generated Lean code from our runs is available at https://github.com/optsuite/ReasBook.git.",
    "title_zh": "M2F：大规模数学文献的自动化形式化",
    "abstract_zh": "自动化数学形式化能够实现机械验证，但目前仍局限于孤立定理和简短片段。将这一技术扩展到教材和研究论文层面尚未得到充分解决，因为这需要处理跨文件依赖关系、解析导入语句，并确保整个项目能够端到端地成功编译。我们提出了 M2F（Math-to-Formal），这是首个面向 Lean 的代理式框架，实现了端到端、项目规模的自动化形式化。该框架分为两个阶段：在声明编译阶段，系统将文档拆分为原子块，通过推断依赖关系进行排序，并不断修复声明骨架，直至项目能够成功编译，同时允许在证明中保留占位符；在证明修复阶段，系统在固定签名下，利用目标导向的局部修改来填补这些证明空缺。在整个过程中，M2F 始终将验证器置于反馈回路中，仅在工具链反馈表明改进时才提交修改。在约三周时间内，M2F 成功将长篇数学文献转化为一个包含 153,853 行代码的 Lean 库，涵盖 479 页关于实分析与凸分析的教材内容，所有定理均以 Lean 声明形式完整形式化，并附带完整证明。这一成果实现了教材级别的形式化，其效率相当于传统专家工作数月甚至数年才能完成的规模。在 FATE-H 基准测试中，M2F 达到了 96% 的证明成功率（相比之下，强基线方法仅为 80%）。这些结果共同表明，实用且大规模的数学文献自动化形式化已近在咫尺。我们所有运行生成的完整 Lean 代码已公开，可访问 https://github.com/optsuite/ReasBook.git。"
  },
  {
    "date": "2026-02-19",
    "title": "Arcee Trinity Large Technical Report",
    "authors": "Varun Singh, Lucas Krauss, Sami Jaghouar, Matej Sirovatka, Charles Goddard, Fares Obied, Jack Min Ong, Jannik Straube, Fern, Aria Harley, Conner Stewart, Colin Kealty, Maziyar Panahi, Simon Kirsten, Anushka Deshpande, Anneketh Vij, Arthur Bresnu, Pranav Veldurthi, Raghav Ravishankar, Hardik Bishnoi, DatologyAI Team, Arcee AI Team, Prime Intellect Team, Mark McQuade, Johannes Hagemann, Lucas Atkins",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17004v1",
    "source": "arXiv",
    "abstract": "We present the technical report for Arcee Trinity Large, a sparse Mixture-of-Experts model with 400B total parameters and 13B activated per token. Additionally, we report on Trinity Nano and Trinity Mini, with Trinity Nano having 6B total parameters with 1B activated per token, Trinity Mini having 26B total parameters with 3B activated per token. The models' modern architecture includes interleaved local and global attention, gated attention, depth-scaled sandwich norm, and sigmoid routing for Mixture-of-Experts. For Trinity Large, we also introduce a new MoE load balancing strategy titled Soft-clamped Momentum Expert Bias Updates (SMEBU). We train the models using the Muon optimizer. All three models completed training with zero loss spikes. Trinity Nano and Trinity Mini were pre-trained on 10 trillion tokens, and Trinity Large was pre-trained on 17 trillion tokens. The model checkpoints are available at https://huggingface.co/arcee-ai.",
    "title_zh": "阿瑞斯·特里尼蒂大型技术报告",
    "abstract_zh": "我们发布了Arcee Trinity Large的技術報告，這是一款總參數量為4000億、每token激活130億參數的稀疏專家混合模型（Mixture-of-Experts）。此外，我們還介紹了Trinity Nano和Trinity Mini兩款模型：Trinity Nano總參數為60億，每token激活10億參數；Trinity Mini總參數為260億，每token激活30億參數。這些模型採用現代化架構，包含交錯的局部與全局注意力機制、門控注意力、深度縮放的沙灘歸一化（sandwich norm）以及用於專家混合的Sigmoid路由機制。針對Trinity Large，我們還提出了一種新的MoE負載均衡策略，稱為「軟夾持動量專家偏置更新」（Soft-clamped Momentum Expert Bias Updates, SMEBU）。所有模型均使用Muon優化器進行訓練，且在訓練過程中均未出現任何損失突增。Trinity Nano和Trinity Mini在10萬億個token上進行預訓練，而Trinity Large則在17萬億個token上完成預訓練。模型檢查點可於 https://huggingface.co/arcee-ai 下載。"
  },
  {
    "date": "2026-02-19",
    "title": "Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web",
    "authors": "Linxi Jiang, Rui Xi, Zhijie Liu, Shuo Chen, Zhiqiang Lin, Suman Nath",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17245v1",
    "source": "arXiv",
    "abstract": "The Web is evolving from a medium that humans browse to an environment where software agents act on behalf of users. Advances in large language models (LLMs) make natural language a practical interface for goal-directed tasks, yet most current web agents operate on low-level primitives such as clicks and keystrokes. These operations are brittle, inefficient, and difficult to verify. Complementing content-oriented efforts such as NLWeb's semantic layer for retrieval, we argue that the agentic web also requires a semantic layer for web actions. We propose \\textbf{Web Verbs}, a web-scale set of typed, semantically documented functions that expose site capabilities through a uniform interface, whether implemented through APIs or robust client-side workflows. These verbs serve as stable and composable units that agents can discover, select, and synthesize into concise programs. This abstraction unifies API-based and browser-based paradigms, enabling LLMs to synthesize reliable and auditable workflows with explicit control and data flow. Verbs can carry preconditions, postconditions, policy tags, and logging support, which improves \\textbf{reliability} by providing stable interfaces, \\textbf{efficiency} by reducing dozens of steps into a few function calls, and \\textbf{verifiability} through typed contracts and checkable traces. We present our vision, a proof-of-concept implementation, and representative case studies that demonstrate concise and robust execution compared to existing agents. Finally, we outline a roadmap for standardization to make verbs deployable and trustworthy at web scale.",
    "title_zh": "网络动词：用于代理网络上可靠任务组合的类型化抽象",
    "abstract_zh": "网络正在从人类浏览的媒介，演变为软件代理代表用户执行任务的环境。大型语言模型（LLMs）的发展使得自然语言成为实现目标导向任务的实用接口，然而目前大多数网络代理仍依赖于点击、输入等低层次操作。这些操作脆弱、低效且难以验证。在诸如NLWeb的语义检索层等以内容为中心的努力之外，我们认为，智能体网络同样需要一个用于网络操作的语义层。我们提出了**Web Verbs**——一个大规模、类型化、语义化注释的函数集合，通过统一接口暴露网站功能，无论这些功能是通过API实现，还是通过稳健的客户端工作流完成。这些“动词”作为稳定且可组合的单元，使智能体能够发现、选择并合成简洁的程序。这一抽象统一了基于API与基于浏览器的范式，使大型语言模型能够合成可靠且可审计的工作流，具备明确的控制流与数据流。这些动词可携带前置条件、后置条件、策略标签和日志支持，从而在三个方面显著提升系统能力：**可靠性**——通过提供稳定的接口；**效率**——将数十个步骤简化为少数函数调用；**可验证性**——通过类型化契约和可检查的执行轨迹。我们阐述了这一愿景，展示了概念验证实现，并通过代表性案例研究证明，与现有代理相比，Web Verbs能够实现更简洁、更稳健的执行。最后，我们提出了标准化路线图，旨在使Web Verbs在全网范围内可部署、可信赖。"
  },
  {
    "date": "2026-02-19",
    "title": "A Theoretical Framework for Modular Learning of Robust Generative Models",
    "authors": "Corinna Cortes, Mehryar Mohri, Yutao Zhong",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17554v1",
    "source": "arXiv",
    "abstract": "Training large-scale generative models is resource-intensive and relies heavily on heuristic dataset weighting. We address two fundamental questions: Can we train Large Language Models (LLMs) modularly-combining small, domain-specific experts to match monolithic performance-and can we do so robustly for any data mixture, eliminating heuristic tuning? We present a theoretical framework for modular generative modeling where a set of pre-trained experts are combined via a gating mechanism. We define the space of normalized gating functions, $G_{1}$, and formulate the problem as a minimax game to find a single robust gate that minimizes divergence to the worst-case data mixture. We prove the existence of such a robust gate using Kakutani's fixed-point theorem and show that modularity acts as a strong regularizer, with generalization bounds scaling with the lightweight gate's complexity. Furthermore, we prove that this modular approach can theoretically outperform models retrained on aggregate data, with the gap characterized by the Jensen-Shannon Divergence. Finally, we introduce a scalable Stochastic Primal-Dual algorithm and a Structural Distillation method for efficient inference. Empirical results on synthetic and real-world datasets confirm that our modular architecture effectively mitigates gradient conflict and can robustly outperform monolithic baselines.",
    "title_zh": "模块化学习鲁棒生成模型的理论框架",
    "abstract_zh": "训练大规模生成模型资源消耗巨大，且严重依赖启发式数据集加权。我们针对两个根本性问题展开研究：能否通过模块化方式——将多个小型、领域特定的专家模型组合起来——实现与单体模型相当的性能？以及能否在任何数据混合情况下稳健地实现这一目标，从而消除对启发式调参的依赖？我们提出了一种模块化生成建模的理论框架，其中一组预训练专家通过门控机制进行组合。我们定义了归一化门控函数的空间 $G_{1}$，并将该问题建模为一个极小极大博弈，以寻找一个单一的鲁棒门控函数，使其对最坏情况下的数据混合具有最小的偏离。我们利用 Kakutani 不动点定理证明了此类鲁棒门控的存在性，并表明模块化结构本身起到了强大的正则化作用，其泛化误差界与轻量级门控函数的复杂度成正比。此外，我们证明了这种模块化方法在理论上可以超越在聚合数据上重新训练的模型，二者之间的性能差距由 Jensen-Shannon 散度所刻画。最后，我们提出一种可扩展的随机原始-对偶算法，以及一种用于高效推理的结构蒸馏方法。在合成数据和真实世界数据集上的实验结果表明，我们的模块化架构能够有效缓解梯度冲突，并在各种场景下稳健地超越单体基线模型的表现。"
  },
  {
    "date": "2026-02-19",
    "title": "Using LLMs for Knowledge Component-level Correctness Labeling in Open-ended Coding Problems",
    "authors": "Zhangqi Duan, Arnav Kankaria, Dhruv Kartik, Andrew Lan",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17542v1",
    "source": "arXiv",
    "abstract": "Fine-grained skill representations, commonly referred to as knowledge components (KCs), are fundamental to many approaches in student modeling and learning analytics. However, KC-level correctness labels are rarely available in real-world datasets, especially for open-ended programming tasks where solutions typically involve multiple KCs simultaneously. Simply propagating problem-level correctness to all associated KCs obscures partial mastery and often leads to poorly fitted learning curves. To address this challenge, we propose an automated framework that leverages large language models (LLMs) to label KC-level correctness directly from student-written code. Our method assesses whether each KC is correctly applied and further introduces a temporal context-aware Code-KC mapping mechanism to better align KCs with individual student code. We evaluate the resulting KC-level correctness labels in terms of learning curve fit and predictive performance using the power law of practice and the Additive Factors Model. Experimental results show that our framework leads to learning curves that are more consistent with cognitive theory and improves predictive performance, compared to baselines. Human evaluation further demonstrates substantial agreement between LLM and expert annotations.",
    "title_zh": "使用大语言模型进行开放性编程问题的知识组件级正确性标注",
    "abstract_zh": "细粒度技能表示，通常称为知识成分（Knowledge Components, KCs），是学生建模与学习分析中诸多方法的基础。然而，在真实世界的数据集中，KC级别的正确性标签却很少见，尤其是在开放式的编程任务中，学生的解决方案通常同时涉及多个KC。简单地将题目级别的正确性传播到所有相关KC上，会掩盖部分掌握的情况，往往导致学习曲线拟合效果不佳。为应对这一挑战，我们提出了一种自动化框架，利用大型语言模型（LLMs）直接从学生编写的代码中标注KC级别的正确性。我们的方法评估每个KC是否被正确应用，并进一步引入一种考虑时间上下文的Code-KC映射机制，以更准确地将KC与个体学生的代码对齐。我们通过幂律练习和加性因子模型，从学习曲线拟合度和预测性能两个方面评估了所得KC级别正确性标签的效果。实验结果表明，与基线方法相比，我们的框架生成的学习曲线更符合认知理论，且预测性能显著提升。人工评估进一步证明，LLM标注与专家标注之间具有高度一致性。"
  },
  {
    "date": "2026-02-19",
    "title": "NotebookRAG: Retrieving Multiple Notebooks to Augment the Generation of EDA Notebooks for Crowd-Wisdom",
    "authors": "Yi Shan, Yixuan He, Zekai Shao, Kai Xu, Siming Chen",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17215v1",
    "source": "arXiv",
    "abstract": "High-quality exploratory data analysis (EDA) is essential in the data science pipeline, but remains highly dependent on analysts' expertise and effort. While recent LLM-based approaches partially reduce this burden, they struggle to generate effective analysis plans and appropriate insights and visualizations when user intent is abstract. Meanwhile, a vast collection of analysis notebooks produced across platforms and organizations contains rich analytical knowledge that can potentially guide automated EDA. Retrieval-augmented generation (RAG) provides a natural way to leverage such corpora, but general methods often treat notebooks as static documents and fail to fully exploit their potential knowledge for automating EDA. To address these limitations, we propose NotebookRAG, a method that takes user intent, datasets, and existing notebooks as input to retrieve, enhance, and reuse relevant notebook content for automated EDA generation. For retrieval, we transform code cells into context-enriched executable components, which improve retrieval quality and enable rerun with new data to generate updated visualizations and reliable insights. For generation, an agent leverages enhanced retrieval content to construct effective EDA plans, derive insights, and produce appropriate visualizations. Evidence from a user study with 24 participants confirms the superiority of our method in producing high-quality and intent-aligned EDA notebooks.",
    "title_zh": "NotebookRAG：检索多个笔记本以增强众包智慧的EDA笔记本生成",
    "abstract_zh": "高质量的探索性数据分析（EDA）在数据科学流程中至关重要，但目前仍高度依赖分析人员的专业知识和投入。尽管近期基于大语言模型（LLM）的方法在一定程度上减轻了这一负担，但在用户意图较为抽象时，仍难以生成有效的分析计划以及恰当的洞察与可视化结果。与此同时，跨平台和组织产生的大量分析笔记本文档中蕴含着丰富的分析知识，这些知识有望为自动化EDA提供指导。检索增强生成（RAG）为利用此类知识库提供了自然途径，但现有的通用方法通常将笔记本文档视为静态文档，未能充分挖掘其在自动化EDA中的潜在价值。为解决上述局限，我们提出NotebookRAG，一种以用户意图、数据集及现有笔记本文档为输入，通过检索、增强和复用相关笔记内容来实现自动化EDA生成的方法。在检索阶段，我们将代码单元格转化为富含上下文信息的可执行组件，从而提升检索质量，并支持使用新数据重新运行，生成更新的可视化结果和可靠的分析洞察。在生成阶段，一个智能代理利用增强后的检索内容，构建高效的EDA计划，推导出有意义的洞察，并生成合适的可视化图表。一项包含24名参与者的用户研究结果表明，我们的方法在生成高质量且与用户意图高度一致的EDA笔记本文档方面具有显著优势。"
  },
  {
    "date": "2026-02-19",
    "title": "Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering",
    "authors": "Kishan Maharaj, Nandakishore Menon, Ashita Saxena, Srikanth Tamilselvam",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17183v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs) increasingly assist software engineering tasks that require reasoning over long code contexts, yet their robustness under varying input conditions remains unclear. We conduct a systematic study of long-context code question answering using controlled ablations that test sensitivity to answer format, distractors, and context scale. Extending LongCodeBench Python dataset with new COBOL and Java question-answer sets, we evaluate state-of-the-art models under three settings: (i) shuffled multiple-choice options, (ii) open-ended questions and (iii) needle-in-a-haystack contexts containing relevant and adversarially irrelevant information. Results show substantial performance drops in both shuffled multiple-choice options and open-ended questions, and brittle behavior in the presence of irrelevant cues. Our findings highlight limitations of current long-context evaluations and provide a broader benchmark for assessing code reasoning in both legacy and modern systems.",
    "title_zh": "大语言模型在长上下文代码问答中的鲁棒性与推理一致性",
    "abstract_zh": "大型语言模型（LLMs）在需要对长代码上下文进行推理的软件工程任务中正发挥越来越重要的作用，但其在不同输入条件下的鲁棒性仍不明确。本文通过受控消融实验，系统性地研究了长上下文代码问答任务，重点考察了答案格式、干扰项以及上下文规模对模型性能的影响。我们扩展了LongCodeBench Python数据集，新增了COBOL和Java的问答数据集，并在三种设置下评估了当前最先进的模型：（i）选项顺序打乱的多项选择题，（ii）开放式问题，以及（iii）“针尖对 haystack”式上下文，其中包含相关和对抗性无关信息。实验结果表明，模型在选项打乱的多项选择题和开放式问题中均出现显著性能下降，在无关线索存在时表现出脆弱行为。研究结果揭示了当前长上下文评估方法的局限性，并为评估传统系统与现代系统中的代码推理能力提供了更全面的基准。"
  },
  {
    "date": "2026-02-19",
    "title": "When to Trust the Cheap Check: Weak and Strong Verification for Reasoning",
    "authors": "Shayan Kiyani, Sima Noorani, George Pappas, Hamed Hassani",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17633v1",
    "source": "arXiv",
    "abstract": "Reasoning with LLMs increasingly unfolds inside a broader verification loop. Internally, systems use cheap checks, such as self-consistency or proxy rewards, which we call weak verification. Externally, users inspect outputs and steer the model through feedback until results are trustworthy, which we call strong verification. These signals differ sharply in cost and reliability: strong verification can establish trust but is resource-intensive, while weak verification is fast and scalable but noisy and imperfect. We formalize this tension through weak--strong verification policies, which decide when to accept or reject based on weak verification and when to defer to strong verification. We introduce metrics capturing incorrect acceptance, incorrect rejection, and strong-verification frequency. Over population, we show that optimal policies admit a two-threshold structure and that calibration and sharpness govern the value of weak verifiers. Building on this, we develop an online algorithm that provably controls acceptance and rejection errors without assumptions on the query stream, the language model, or the weak verifier.",
    "title_zh": "何时信任廉价检查：推理中的弱验证与强验证",
    "abstract_zh": "随着大语言模型（LLM）推理过程的不断演进，其内部逐渐形成一个更广泛的验证循环。在系统内部，通过低成本的检查手段（如自一致性检验或代理奖励）进行初步验证，我们称之为“弱验证”；而在外部，用户则通过审查输出结果，并通过反馈不断引导模型，直到获得可信结果，这一过程被称为“强验证”。这两种验证信号在成本和可靠性方面存在显著差异：强验证能够建立信任，但代价高昂且资源消耗大；而弱验证速度快、可扩展性强，但噪声大且不完美。为此，我们提出了“弱—强验证策略”的形式化框架，用以决定在何种情况下基于弱验证接受或拒绝结果，以及在何种情况下应转而依赖强验证。我们引入了若干度量指标，用于衡量错误接受、错误拒绝以及强验证的频率。在群体层面，我们证明了最优策略具有双阈值结构，且弱验证器的校准性与锐度决定了其价值。基于此，我们设计了一种在线算法，该算法在无需对查询流、语言模型或弱验证器做出任何假设的前提下，能够严格控制接受与拒绝的错误率。"
  },
  {
    "date": "2026-02-19",
    "title": "AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing",
    "authors": "Jianda Du, Youran Sun, Haizhao Yang",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17607v1",
    "source": "arXiv",
    "abstract": "PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \\texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions. Unlike black-box neural solvers, our framework generates transparent solvers grounded in classical numerical analysis. We introduce a coarse-to-fine execution strategy and a residual-based self-verification mechanism. Experiments on 24 canonical and real-world PDE problems demonstrate that \\texttt{AutoNumerics} achieves competitive or superior accuracy compared to existing neural and LLM-based baselines, and correctly selects numerical schemes based on PDE structural properties, suggesting its viability as an accessible paradigm for automated PDE solving.",
    "title_zh": "AutoNumerics：一种自主的、与偏微分方程无关的多智能体科学计算流水线",
    "abstract_zh": "偏微分方程（PDEs）在科学与工程建模中占据核心地位，然而设计高精度的数值求解器通常需要深厚的数学知识和大量的人工调参。近年来基于神经网络的方法虽提升了灵活性，但往往计算成本高昂，且可解释性有限。我们提出了 \\texttt{AutoNumerics}，一个基于多智能体的框架，能够直接从自然语言描述中自主设计、实现、调试并验证通用偏微分方程的数值求解器。与黑箱神经求解器不同，我们的框架生成的求解器具有透明性，其基础源于经典的数值分析理论。我们引入了一种“粗到细”的执行策略以及基于残差的自我验证机制。在24个经典及实际应用中的偏微分方程问题上的实验表明，\\texttt{AutoNumerics} 在精度上达到或超越现有神经网络与大语言模型（LLM）基线方法，并能根据偏微分方程的结构特性正确选择数值格式，展现出其作为自动化偏微分方程求解新范式的重要潜力。"
  },
  {
    "date": "2026-02-19",
    "title": "KLong: Training LLM Agent for Extremely Long-horizon Tasks",
    "authors": "Yue Liu, Zhiyuan Hu, Flood Sung, Jiaheng Zhang, Bryan Hooi",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17547v1",
    "source": "arXiv",
    "abstract": "This paper introduces KLong, an open-source LLM agent trained to solve extremely long-horizon tasks. The principle is to first cold-start the model via trajectory-splitting SFT, then scale it via progressive RL training. Specifically, we first activate basic agentic abilities of a base model with a comprehensive SFT recipe. Then, we introduce Research-Factory, an automated pipeline that generates high-quality training data by collecting research papers and constructing evaluation rubrics. Using this pipeline, we build thousands of long-horizon trajectories distilled from Claude 4.5 Sonnet (Thinking). To train with these extremely long trajectories, we propose a new trajectory-splitting SFT, which preserves early context, progressively truncates later context, and maintains overlap between sub-trajectories. In addition, to further improve long-horizon task-solving capability, we propose a novel progressive RL, which schedules training into multiple stages with progressively extended timeouts. Experiments demonstrate the superiority and generalization of KLong, as shown in Figure 1. Notably, our proposed KLong (106B) surpasses Kimi K2 Thinking (1T) by 11.28% on PaperBench, and the performance improvement generalizes to other coding benchmarks like SWE-bench Verified and MLE-bench.",
    "title_zh": "KLong：用于超长时序任务的大型语言模型代理训练",
    "abstract_zh": "本文介绍了KLong，一个开源的大型语言模型（LLM）代理，专为解决极长时序任务而训练。其核心方法是：首先通过轨迹分割的监督微调（trajectory-splitting SFT）对模型进行冷启动，随后通过渐进式强化学习（progressive RL）进行扩展训练。具体而言，我们首先采用全面的监督微调方案，激活基础模型的基本智能体能力。接着，我们引入了Research-Factory——一个自动化数据生成流水线，通过收集研究论文并构建评估标准，生成高质量的训练数据。利用该流水线，我们从Claude 4.5 Sonnet（Thinking）中提炼出数千条长时序轨迹。为训练这些极长轨迹，我们提出了一种新型的轨迹分割监督微调方法，该方法保留早期上下文，逐步截断后期上下文，并保持子轨迹之间的重叠。此外，为进一步提升长时序任务求解能力，我们提出了一种新颖的渐进式强化学习方法，将训练过程划分为多个阶段，每个阶段的超时时间逐步延长。实验结果表明，KLong在性能和泛化能力方面均表现出显著优势，如图1所示。值得注意的是，我们提出的KLong（106B）在PaperBench上的表现比Kimi K2 Thinking（1T）高出11.28%，且性能提升在其他编码基准（如SWE-bench Verified和MLE-bench）上也具有良好的泛化能力。"
  },
  {
    "date": "2026-02-19",
    "title": "Multi-Round Human-AI Collaboration with User-Specified Requirements",
    "authors": "Sima Noorani, Shayan Kiyani, Hamed Hassani, George Pappas",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17646v1",
    "source": "arXiv",
    "abstract": "As humans increasingly rely on multiround conversational AI for high stakes decisions, principled frameworks are needed to ensure such interactions reliably improve decision quality. We adopt a human centric view governed by two principles: counterfactual harm, ensuring the AI does not undermine human strengths, and complementarity, ensuring it adds value where the human is prone to err. We formalize these concepts via user defined rules, allowing users to specify exactly what harm and complementarity mean for their specific task. We then introduce an online, distribution free algorithm with finite sample guarantees that enforces the user-specified constraints over the collaboration dynamics. We evaluate our framework across two interactive settings: LLM simulated collaboration on a medical diagnostic task and a human crowdsourcing study on a pictorial reasoning task. We show that our online procedure maintains prescribed counterfactual harm and complementarity violation rates even under nonstationary interaction dynamics. Moreover, tightening or loosening these constraints produces predictable shifts in downstream human accuracy, confirming that the two principles serve as practical levers for steering multi-round collaboration toward better decision quality without the need to model or constrain human behavior.",
    "title_zh": "多轮人机协作，满足用户指定需求",
    "abstract_zh": "随着人类在高风险决策中越来越多地依赖多轮对话式人工智能，亟需建立有原则的框架，以确保此类交互能够可靠地提升决策质量。我们采用以人类为中心的视角，遵循两项核心原则：反事实伤害（counterfactual harm），即确保人工智能不会削弱人类的优势；互补性（complementarity），即确保人工智能在人类容易出错的领域提供价值。我们通过用户自定义规则的形式化表达，使用户能够明确界定其特定任务中“伤害”与“互补性”的具体含义。随后，我们提出一种在线、无需分布假设的算法，具备有限样本保证，能够确保在协作过程中满足用户设定的约束条件。我们在两个交互场景中评估了该框架：一是大型语言模型模拟的医学诊断协作任务，二是人类众包参与的图像推理任务。实验结果表明，即使在非平稳的交互动态下，我们的在线方法仍能有效控制预设的反事实伤害和互补性违规率。此外，收紧或放宽这些约束条件会带来可预测的人类下游准确率变化，证实这两项原则可作为实际可控的调节杠杆，引导多轮协作向更高决策质量演进，而无需对人类行为进行建模或施加限制。"
  },
  {
    "date": "2026-02-19",
    "title": "Generating Rely-Guarantee Conditions with the Conditional-Writes Domain",
    "authors": "James Tobler, Graeme Smith",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17142v1",
    "source": "arXiv",
    "abstract": "Abstract interpretation has been shown to be a promising technique for the thread-modular verification of concurrent programs. Central to this is the generation of interferences, in the form of rely-guarantee conditions, conforming to a user-chosen structure. In this work, we introduce one such structure called the conditional-writes domain, designed for programs where it suffices to establish only the conditions under which particular variables are written to by each thread. We formalise our analysis within a novel abstract interpretation framework that is highly modular and can be easily extended to capture other structures for rely-guarantee conditions. We formalise two versions of our approach and evaluate their implementations on a simple programming language.",
    "title_zh": "使用条件写入域生成依赖-保证条件",
    "abstract_zh": "抽象解释已被证明是实现并发程序线程模块化验证的一种有前景的技术。其核心在于生成符合用户所选结构的干扰信息，以依赖-保证条件的形式呈现。在本工作中，我们引入了一种称为“条件写入域”的结构，适用于那些只需确定每个线程在何种条件下会写入特定变量的程序。我们在一个新颖的抽象解释框架内形式化了该分析方法，该框架具有高度模块化特性，可轻松扩展以支持其他依赖-保证条件结构。我们形式化了该方法的两个版本，并在一种简单编程语言上对其进行了实现与评估。"
  },
  {
    "date": "2026-02-19",
    "title": "Operationalization of Machine Learning with Serverless Architecture: An Industrial Operationalization of Machine Learning with Serverless Architecture: An Industrial Implementation for Harmonized System Code Prediction",
    "authors": "Sai Vineeth Kandappareddigari, Santhoshkumar Jagadish, Gauri Verma, Ilhuicamina Contreras, Christopher Dignam, Anmol Srivastava, Benjamin Demers",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17102v1",
    "source": "arXiv",
    "abstract": "This paper presents a serverless MLOps framework orchestrating the complete ML lifecycle from data ingestion, training, deployment, monitoring, and retraining to using event-driven pipelines and managed services. The architecture is model-agnostic, supporting diverse inference patterns through standardized interfaces, enabling rapid adaptation without infrastructure overhead. We demonstrate practical applicability through an industrial implementation for Harmonized System (HS) code prediction, a compliance-critical task where short, unstructured product descriptions are mapped to standardized codes used by customs authorities in global trade. Frequent updates and ambiguous descriptions make classification challenging, with errors causing shipment delays and financial losses. Our solution uses a custom text embedding encoder and multiple deep learning architectures, with Text-CNN achieving 98 percent accuracy on ground truth data. Beyond accuracy, the pipeline ensures reproducibility, auditability, and SLA adherence under variable loads via auto-scaling. A key feature is automated A/B testing, enabling dynamic model selection and safe promotion in production. Cost-efficiency drives model choice; while transformers may achieve similar accuracy, their long-term operational costs are significantly higher. Deterministic classification with predictable latency and explainability is prioritized, though the architecture remains extensible to transformer variants and LLM-based inference. The paper first introduces the deep learning architectures with simulations and model comparisons, then discusses industrialization through serverless architecture, demonstrating automated retraining, prediction, and validation of HS codes. This work provides a replicable blueprint for operationalizing ML using serverless architecture, enabling enterprises to scale while optimizing performance and economics.",
    "title_zh": "基于无服务器架构的机器学习操作化：一种用于协调系统编码预测的工业级实现",
    "abstract_zh": "本文提出了一种无服务器的MLOps框架，通过事件驱动的流水线和托管服务，实现从数据摄入、训练、部署、监控到重训练的完整机器学习生命周期管理。该架构具有模型无关性，通过标准化接口支持多种推理模式，能够在无需额外基础设施投入的情况下实现快速适应。我们通过一个工业级应用实例——协调制度（HS）编码预测，展示了该方案的实际可行性。HS编码预测是一项合规性要求极高的任务，需将简短且非结构化的商品描述映射为全球贸易中海关机构使用的标准化编码。由于描述信息频繁更新且语义模糊，分类任务极具挑战性，错误可能导致货物运输延误和经济损失。\n\n我们的解决方案采用自定义文本嵌入编码器和多种深度学习架构，其中Text-CNN在真实数据上达到了98%的准确率。除了高精度外，该流水线还通过自动伸缩机制，在不同负载条件下确保了可复现性、可审计性以及服务等级协议（SLA）的遵守。一个关键特性是自动化A/B测试，支持动态模型选择，并安全地将新模型推送到生产环境。在模型选型上，成本效益是核心考量因素：尽管Transformer模型可能达到相近的准确率，但其长期运营成本显著更高。因此，本方案优先选择具有确定性分类、可预测延迟和可解释性的模型，同时架构本身仍具备扩展性，可兼容Transformer变体及基于大语言模型（LLM）的推理。\n\n本文首先介绍了深度学习架构的设计与仿真结果，对比了不同模型的性能表现；随后探讨了通过无服务器架构实现工业化的路径，展示了HS编码的自动化重训练、预测与验证流程。本研究为利用无服务器架构实现机器学习的工程化落地提供了一个可复现的范本，助力企业实现规模化发展的同时，优化性能与经济性。"
  },
  {
    "date": "2026-02-19",
    "title": "BankMathBench: A Benchmark for Numerical Reasoning in Banking Scenarios",
    "authors": "Yunseung Lee, Subin Kim, Youngjun Kwak, Jaegul Choo",
    "publish": "arXiv",
    "url": "https://arxiv.org/abs/2602.17072v1",
    "source": "arXiv",
    "abstract": "Large language models (LLMs)-based chatbots are increasingly being adopted in the financial domain, particularly in digital banking, to handle customer inquiries about products such as deposits, savings, and loans. However, these models still exhibit low accuracy in core banking computations-including total payout estimation, comparison of products with varying interest rates, and interest calculation under early repayment conditions. Such tasks require multi-step numerical reasoning and contextual understanding of banking products, yet existing LLMs often make systematic errors-misinterpreting product types, applying conditions incorrectly, or failing basic calculations involving exponents and geometric progressions. However, such errors have rarely been captured by existing benchmarks. Mathematical datasets focus on fundamental math problems, whereas financial benchmarks primarily target financial documents, leaving everyday banking scenarios underexplored. To address this limitation, we propose BankMathBench, a domain-specific dataset that reflects realistic banking tasks. BankMathBench is organized in three levels of difficulty-basic, intermediate, and advanced-corresponding to single-product reasoning, multi-product comparison, and multi-condition scenarios, respectively. When trained on BankMathBench, open-source LLMs exhibited notable improvements in both formula generation and numerical reasoning accuracy, demonstrating the dataset's effectiveness in enhancing domain-specific reasoning. With tool-augmented fine-tuning, the models achieved average accuracy increases of 57.6%p (basic), 75.1%p (intermediate), and 62.9%p (advanced), representing significant gains over zero-shot baselines. These findings highlight BankMathBench as a reliable benchmark for evaluating and advancing LLMs' numerical reasoning in real-world banking scenarios.",
    "title_zh": "BankMathBench：银行场景下的数值推理基准测试",
    "abstract_zh": "基于大语言模型（LLMs）的聊天机器人在金融领域，尤其是数字银行中正得到越来越广泛的应用，用于处理客户关于存款、储蓄和贷款等产品的问题。然而，这些模型在核心银行业务计算任务中仍表现出较低的准确性，包括总收益估算、不同利率产品间的比较，以及提前还款条件下的利息计算。这些任务需要多步数值推理和对银行产品上下文的深入理解，但现有LLMs常常出现系统性错误——如误解产品类型、错误应用条件，或在涉及指数运算和等比数列的基本计算中出错。然而，这些错误在现有基准测试中却很少被捕捉到。数学数据集主要关注基础数学问题，而金融基准测试则侧重于金融文档分析，导致日常银行业务场景长期缺乏系统性研究。为解决这一局限，我们提出了BankMathBench，一个面向特定领域的数据集，真实反映日常银行业务任务。BankMathBench分为三个难度层级：基础、中级和高级，分别对应单一产品推理、多产品比较以及多条件复杂场景。当在BankMathBench上进行训练时，开源LLMs在公式生成和数值推理准确性方面均表现出显著提升，充分证明了该数据集在增强领域特定推理能力方面的有效性。通过工具增强的微调方法，模型在各层级上的平均准确率分别提升了57.6个百分点（基础）、75.1个百分点（中级）和62.9个百分点（高级），相较于零样本基线实现了显著进步。这些结果表明，BankMathBench是一个可靠且具有实际意义的基准，可用于评估和推动LLMs在真实银行业务场景中数值推理能力的发展。"
  },
  {
    "date": "2026-2-19",
    "title": "Relevance-Guided Retrieval under Continually Expanding Knowledge Bases",
    "authors": "Hyundong Jin, Heayoun Choi, Eunwoo Kim",
    "publish": "2025 16th International Conference on Information and Communication Technology Convergence (ICTC)",
    "url": "https://doi.org/10.1109/ictc66702.2025.11388632",
    "source": "IEEE",
    "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models by incorporating external knowledge to support responses in information-intensive tasks. As knowledge bases expand to accommodate emerging concepts and domains, existing approaches that apply flat retrieval over the entire corpus encounter substantial limitations in efficiency and relevance. To address these challenges, we propose a retrieval method that structures the knowledge base into semantically coherent subsets and confines retrieval to the most relevant subset for each query. This hierarchical procedure enables scalable retrieval under continual expansion of the knowledge base by reducing search space and mitigating irrelevant matches. Experimental results on knowledge-intensive benchmarks show that the proposed method consistently improves both retrieval accuracy and computational efficiency, providing a practical approach to retrieval-augmented generation in expanding knowledge bases.",
    "title_zh": "持续扩展知识库下的相关性引导检索",
    "abstract_zh": "检索增强生成（RAG）通过引入外部知识来增强大型语言模型，从而在信息密集型任务中支持更准确的响应。随着知识库不断扩展以涵盖新兴概念和领域，现有方法在整篇语料库上进行扁平化检索，面临效率和相关性方面的显著局限。为应对这些挑战，我们提出一种新的检索方法：将知识库结构化为语义连贯的子集，并将每次查询的检索范围限制在最相关的子集中。这种分层检索机制通过缩小搜索空间并减少无关匹配，实现了知识库持续扩展下的可扩展检索。在信息密集型基准测试中的实验结果表明，所提方法在检索准确率和计算效率方面均表现出持续提升，为不断扩展的知识库提供了实用的检索增强生成解决方案。"
  },
  {
    "date": "2026-2-19",
    "title": "Boosting Compiler Fault Localization: Getting the Best of Both Worlds by Fusing Dynamic and Historical Data",
    "authors": "Qingyang Li, Yibiao Yang, Maolin Sun, Jiangchang Wu, Qingkai Shi, Yuming Zhou, Baowen Xu",
    "publish": "IEEE Transactions on Software Engineering",
    "url": "https://doi.org/10.1109/tse.2026.3666208",
    "source": "IEEE",
    "abstract": "Compilers are prone to bugs that can have severe consequences for downstream applications. Accurately identifying and localizing compiler faults poses unique challenges due to the inherent complexity and large scale of modern compiler infrastructures. Existing studies have proposed various techniques to construct passing and failing executions by generating witness test programs from bug-inducing test cases or by producing adversarial compilation configurations for the same test program. These executions are then leveraged to apply spectrum-based fault localization (SBFL) techniques for isolating compiler faults, yielding promising results. Recently, Yang et al. revisited SBFL-based techniques and showed that a simple yet widely adopted debugging practice—treating files modified in bug-inducing commits (BICs) as potential fault candidates—can surprisingly outperform SBFL-based techniques on the most critical localization metrics. Moreover, they further demonstrated that BIC-based and SBFL-based techniques are highly complementary, as they tend to localize different subsets of compiler faults. Consequently, effectively integrating these two sources of information to improve compiler fault localization remains an open and largely unexplored challenge. To address this problem, we propose D<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">UAL</small>T<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">RACK</small>, a hybrid approach that integrates dynamic execution information from SBFL with historical information derived from BICs. D<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">UAL</small>T<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">RACK</small> employs a two-layer framework that first prioritizes files modified in bug-inducing commits and then refines their rankings using suspiciousness scores computed by SBFL formulas. An evaluation on 120 real-world compiler bugs from GCC and LLVM shows that DUALTRACK successfully identifies 52% of faulty files at the Top-1 rank, demonstrating a substantial improvement over existing state-of-the-art compiler fault localization techniques.",
    "title_zh": "提升编译器故障定位：通过融合动态与历史数据实现最佳效果",
    "abstract_zh": "编译器容易出现可能导致下游应用程序产生严重后果的错误。由于现代编译器基础设施固有的复杂性和规模庞大，准确识别并定位编译器故障面临着独特挑战。现有研究提出了多种技术，通过从引发错误的测试用例生成“见证”测试程序，或为同一测试程序生成对抗性编译配置，来构建通过和失败的执行路径。这些执行路径随后被用于应用基于谱的故障定位（SBFL）技术，以隔离编译器故障，取得了令人鼓舞的结果。最近，Yang 等人重新审视了基于 SBFL 的技术，发现一种简单但广泛采用的调试实践——将引发错误的提交（BIC）中修改的文件视为潜在故障候选——在最关键的定位指标上，竟意外地优于基于 SBFL 的方法。此外，他们进一步证明，基于 BIC 的方法与基于 SBFL 的方法具有高度互补性，因为它们倾向于定位不同子集的编译器故障。因此，如何有效整合这两种信息来源以提升编译器故障定位效果，仍是一个开放且尚未充分探索的挑战。\n\n为应对这一问题，我们提出了 DUALTRACK，一种融合动态执行信息（来自 SBFL）与源自 BIC 的历史信息的混合方法。DUALTRACK 采用两层框架：首先对 BIC 中修改的文件进行优先排序，然后利用 SBFL 公式计算的可疑度得分对这些文件的排名进行精细化调整。在 GCC 和 LLVM 中 120 个真实世界编译器漏洞上的评估表明，DUALTRACK 能在 Top-1 排名中成功定位 52% 的故障文件，显著优于现有的最先进编译器故障定位技术。"
  },
  {
    "date": "2026-2-19",
    "title": "Action Model-Based Natural Language Control System for Digital Twins",
    "authors": "Younghwan Jeong, Won Gi Choi, Taemin Hwang, Jin-Young Lee, Sang-Shin Lee",
    "publish": "2025 16th International Conference on Information and Communication Technology Convergence (ICTC)",
    "url": "https://doi.org/10.1109/ictc66702.2025.11388920",
    "source": "IEEE",
    "abstract": "Digital twins, which enable seamless interaction between the physical and virtual worlds for simulation and control, have garnered significant attention across various domains. However, operating and managing digital twins beyond mere monitoring typically requires substantial expertise and effort, presenting a high barrier to entry for non-experts. In this study, we propose an Action Model-Based Digital twin control system that allows non-specialists to intuitively operate digital twins. By leveraging a large action model, the system interprets user intent from natural language commands and dynamically controls key functions of the digital twin without the need for domain-specific knowledge.",
    "title_zh": "基于动作模型的数字孪生自然语言控制系统",
    "abstract_zh": "数字孪生技术通过实现物理世界与虚拟世界之间的无缝交互，为仿真与控制提供了强大支持，在多个领域引起了广泛关注。然而，除了简单的监控之外，对数字孪生进行操作和管理通常需要大量的专业知识和精力，这对非专业人士构成了较高的使用门槛。在本研究中，我们提出了一种基于动作模型的数字孪生控制系统，使非专业人士能够直观地操作数字孪生。该系统利用大规模动作模型，从自然语言指令中理解用户意图，并动态控制数字孪生的关键功能，而无需具备特定领域的专业知识。"
  },
  {
    "date": "2026-2-19",
    "title": "Enhancing the Reliability of Large Language Models in Specialized Domains by Balancing Internal and External Knowledge",
    "authors": "Yuran Li, Di Wu, Benoit Boulet",
    "publish": "2025 3rd International Conference on Foundation and Large Language Models (FLLM)",
    "url": "https://doi.org/10.1109/fllm67465.2025.11391199",
    "source": "IEEE",
    "abstract": "With the widespread application of large language models, numerous approaches have been proposed to enhance their performance. Most existing works focus on general-purpose tasks. Although some domain-specific approaches have emerged, they typically rely on enriching LLMs’ knowledge through additional training or retrieving external information, without effectively leveraging internal knowledge or balancing it with external sources. To address this, we propose a framework that operates effectively with only a few representative training samples (no more than one order of magnitude) to adapt to specialized tasks. This method is well-suited for scenarios with limited data, where fine-tuning is infeasible. Our method first adopts LLM-generated few-shot examples from training set, involving Chain-of-Thought, to maximize the utilization of internal knowledge. Then, uncertainty-guided retrieval-augmented generation is employed to selectively incorporate external information. This method balances internal and external knowledge, thereby reducing hallucinations arising from knowledge gap and errors caused by inaccurate retrieved context. Our method is compared with several representative, general-purpose methods on domain-specific datasets (law and finance). Experimental results show that our approach outperforms existing training-free, general-purpose methods across most tasks.",
    "title_zh": "通过平衡内部与外部知识提升大型语言模型在专业领域中的可靠性",
    "abstract_zh": "随着大型语言模型的广泛应用，已有大量方法被提出以提升其性能。现有大多数研究集中于通用任务，尽管一些面向特定领域的 approaches 已出现，但它们通常依赖于通过额外训练或检索外部信息来增强 LLM 的知识，而未能有效利用模型内部知识，也未能在内部与外部信息之间实现良好平衡。为解决这一问题，我们提出了一种新框架，仅需少量代表性训练样本（不超过一个数量级）即可有效适应专业任务，特别适用于数据有限、微调不可行的场景。该方法首先利用 LLM 从训练集中生成包含思维链（Chain-of-Thought）的少样本示例，以最大化内部知识的利用效率；随后采用基于不确定性的检索增强生成机制，有选择性地引入外部信息。该方法在内部与外部知识之间实现了有效平衡，从而减少了因知识缺失导致的幻觉现象，以及因检索上下文不准确引发的错误。我们在法律和金融等领域的特定数据集上，将该方法与多种代表性通用方法进行了对比。实验结果表明，我们的方法在大多数任务上均优于现有的无需训练的通用方法。"
  },
  {
    "date": "2026-2-19",
    "title": "Cluster-Based Resampling for Imbalanced Datasets in Software Defect Prediction",
    "authors": "Harsih Rianto, Omar Pahlevi, Demulyati, Amrin, Budi Supriyadi, Arfan Sansprayada",
    "publish": "2025 IEEE International Conference on Advanced Information Scientific Development (ICAISD)",
    "url": "https://doi.org/10.1109/icaisd68166.2025.11385612",
    "source": "IEEE",
    "abstract": "Software defect prediction is an important task in software engineering because it helps identify defective modules early and reduces maintenance costs. However, the imbalanced nature of datasets often causes classification models to be biased toward the majority class while neglecting the minority class. This study aims to address this issue by applying Cluster-Based Resampling (CBR) and evaluating its effectiveness compared to baseline models. Secondary data were obtained from the NASA Metrics Data Program (MDP). Three classification algorithms were employed, namely Logistic Regression (LR), Random Forest (RF), and Gradient Boosting (GB), evaluated using Accuracy, Area Under the Curve (AUC), Recall, F1-Score, and G-Mean with the <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathbf{1 0}$</tex>-fold cross-validation technique. The results show that before applying CBR, the models had high accuracy and AUC, for example LR achieved an accuracy of 0.99 on MC1 and GB achieved an AUC of 0.97 on PC5, but the recall and G-Mean values were very low, such as LR recall of only 0.02 and G-Mean of 0.15 on CM1. After applying CBR, although accuracy slightly decreased (e.g., CBR-LR only 0.84 on MC1), model performance significantly improved in other metrics, such as CBR-GB with a recall of 0.74 on PC1, an F1-score of 0.87 on MC1, and a G-Mean of up to 0.91 on PC5, while AUC remained stable in the range of <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$0.80-0.94$</tex>. These findings confirm that CBR is effective in reducing class imbalance and producing fairer and more representative evaluation. In practical terms, CBR can be integrated into software development pipelines or continuous integration/continuous deployment (CI/CD) workflows to enhance defect prediction models, enabling teams to detect high-risk modules earlier, automate quality checks, and optimize testing efforts in real-world industrial environments.",
    "title_zh": "基于聚类的重采样方法在软件缺陷预测中的不平衡数据集应用",
    "abstract_zh": "软件缺陷预测是软件工程中的重要任务，因为它有助于早期识别存在缺陷的模块，从而降低维护成本。然而，数据集的不平衡性常常导致分类模型偏向多数类，而忽视少数类。本研究旨在通过应用基于聚类的重采样（Cluster-Based Resampling, CBR）方法来解决这一问题，并评估其相对于基准模型的有效性。研究使用的二次数据来自NASA度量数据计划（Metrics Data Program, MDP）。采用三种分类算法：逻辑回归（Logistic Regression, LR）、随机森林（Random Forest, RF）和梯度提升（Gradient Boosting, GB），并使用10折交叉验证技术，通过准确率（Accuracy）、曲线下面积（AUC）、召回率（Recall）、F1分数（F1-Score）和G-Mean五个指标进行评估。\n\n结果表明，在应用CBR之前，各模型虽然具有较高的准确率和AUC，例如LR在MC1数据集上准确率达到0.99，GB在PC5数据集上AUC达到0.97，但其召回率和G-Mean值却极低，如LR在CM1上的召回率仅为0.02，G-Mean仅为0.15。在应用CBR后，尽管准确率略有下降（例如CBR-LR在MC1上的准确率仅为0.84），但模型在其他指标上的表现显著提升：CBR-GB在PC1上的召回率达到0.74，CBR-LR在MC1上的F1分数达到0.87，CBR-GB在PC5上的G-Mean高达0.91，同时AUC保持在0.80–0.94的稳定范围内。\n\n这些结果证实，CBR能够有效缓解类别不平衡问题，使评估结果更加公平且更具代表性。在实际应用中，CBR可集成到软件开发流程或持续集成/持续部署（CI/CD）工作流中，以增强缺陷预测模型的能力，帮助开发团队更早发现高风险模块，实现质量检查的自动化，并在真实工业环境中优化测试资源分配。"
  },
  {
    "date": "2026-2-19",
    "title": "AI Feels Time: Augmenting LLMs with a Bio-Inspired Temporal Cognitive Notebook : FLLM2025",
    "authors": "Naba Alghudran",
    "publish": "2025 3rd International Conference on Foundation and Large Language Models (FLLM)",
    "url": "https://doi.org/10.1109/fllm67465.2025.11390929",
    "source": "IEEE",
    "abstract": "Large Language Models (LLMs) exhibit remarkable linguistic capabilities but often struggle with complex, multi-step reasoning tasks. We hypothesize that this limitation stems, in part, from the absence of temporal cognition—the capacity to represent, track, and reason over time. Inspired by neuroscience, particularly the role of \"time cells\" in the hippocampus and entorhinal cortex, we introduce the Temporal Cognitive Notebook (TCN): a novel architecture that integrates adaptive computation, temporally structured memory, and entropy-based metacognitive control. The TCN enables LLMs to dynamically allocate reasoning resources and monitor progress over time, promoting more patient and context-aware reasoning. Theoretical analyses and preliminary evaluations suggest that embedding temporal cognition could bridge a fundamental gap between human and artificial reasoning.",
    "title_zh": "AI 感知时间：基于生物启发的时空认知笔记增强大语言模型：FLLM2025",
    "abstract_zh": "大型语言模型（LLMs）展现出卓越的语言能力，但在处理复杂、多步骤的推理任务时往往表现不佳。我们假设，这一局限性部分源于缺乏时间认知——即对时间进行表征、追踪和推理的能力。受神经科学的启发，特别是海马体和内嗅皮层中“时间细胞”的作用，我们提出了时间认知笔记（Temporal Cognitive Notebook, TCN）：一种新型架构，融合了自适应计算、时序结构化记忆以及基于熵的元认知控制。TCN使大型语言模型能够动态分配推理资源，并随时间监控推理进展，从而促进更耐心、更具情境意识的推理过程。理论分析与初步评估表明，嵌入时间认知可能弥合人类与人工智能推理之间的一个根本性差距。"
  },
  {
    "date": "2026-2-19",
    "title": "A Rank-Aware Retrieval Precision Modeling and Calculation Method for Retrieval Augmented Generation",
    "authors": "Binghao Liang, Chuangang Zhang, Mingming Yuan",
    "publish": "2025 6th International Conference on Information Science and Education (ICISE-IE)",
    "url": "https://doi.org/10.1109/icise-ie68873.2025.11379172",
    "source": "IEEE",
    "abstract": "Retrieval Augmented Generation (RAG) is a key paradigm for enhancing the capability of Large Language Models (LLMs) in educational applications, such as intelligent tutoring systems and knowledge-based Q&A platforms. However, existing retrieval precision models often fail to adequately account for the ranking of retrieved knowledge, which critically influences the quality and factual consistency of educational content generated by LLMs. To address this gap, this paper proposes a Rank-aware Retrieval Precision Modeling and Calculation Method. We introduce the Weighted Context Precision (WCP) and its computationally efficient variant, Simplified Weighted Context Precision (SWCP), which incorporate rank-sensitive weighting and multi-level relevance scoring to better reflect the impact of retrieval ranking on educational content generation. A multi-agent experimental environment, including a virtual educational data generation system and an LLM-based scoring framework, was designed to systematically evaluate the proposed models while mitigating the influence of LLMs' parametric knowledge. Experimental results demonstrate that SWCP achieves a significantly stronger correlation with LLMs response quality compared to traditional models like IDCP, reducing Mean Squared Error (MSE) by 61.8% and Mean Absolute Error (MAE) by 38.7% on average. The proposed method provides a practical, automated evaluation tool for optimizing retrieval and re-ranking strategies in educational RAG systems, thereby enhancing the accuracy and efficiency of knowledge delivery in digital learning environments.",
    "title_zh": "一种面向检索增强生成的等级感知检索精确度建模与计算方法",
    "abstract_zh": "检索增强生成（Retrieval Augmented Generation, RAG）是提升大型语言模型（LLMs）在教育领域应用能力的关键范式，例如智能辅导系统和基于知识的问答平台。然而，现有的检索精确度评估模型往往未能充分考虑检索结果的排序影响，而这一因素对LLMs生成的教育内容质量与事实一致性具有决定性作用。为弥补这一不足，本文提出一种面向排序感知的检索精确度建模与计算方法。我们引入了加权上下文精确度（Weighted Context Precision, WCP）及其计算高效的简化版本——简化加权上下文精确度（Simplified Weighted Context Precision, SWCP），通过引入对排序敏感的加权机制和多层次相关性评分，更准确地反映检索排序对教育内容生成的影响。为系统评估所提模型，我们构建了一个多智能体实验环境，包含虚拟教育数据生成系统和基于LLM的评分框架，以有效降低LLM参数化知识带来的干扰。实验结果表明，SWCP相较于传统模型（如IDCP）与LLM生成内容质量之间的相关性显著更强，平均降低了61.8%的均方误差（MSE）和38.7%的平均绝对误差（MAE）。所提出的方法为教育领域RAG系统的检索与重排序策略优化提供了一种实用且自动化的评估工具，从而有效提升数字学习环境中知识传递的准确性与效率。"
  },
  {
    "date": "2026-2-19",
    "title": "Improving Edge-Case Reasoning in GPT-4.1 through Adaptive Fine-Tuning and Context-Aware Prompt Engineering",
    "authors": "T. Ratha Jeyalakshmi, Smitha GV, Hrudai Kumar K, Ijaz Ahmad S",
    "publish": "2025 International Conference on Electronics and Computing, Communication Networking Automation Technologies (ICEC2NT)",
    "url": "https://doi.org/10.1109/icec2nt65402.2025.11380091",
    "source": "IEEE",
    "abstract": "This paper presents RECAP (Reasoning Enhancement through Context-Aware Prompting), a dual-strategy framework that combines sophisticated prompt engineering techniques that restructure inputs to emphasize disambiguation, analogical examples, and counterfactual reasoning, and parameter-efficient fine-tuning using synthetic edge-case datasets tailored to particular domains in order to address this shortcoming. Over the GPT-4.1 API, RECAP functions as a thin, modular layer that allows for enhancements without changing the foundational model. Comprehensive tests on five domain-specific benchmarks show that RECAP greatly increases the interpretability of generated outputs, lowers the rate of contradiction, and improves logical consistency. In addition to offering a methodological contribution, this paper offers a useful road map for implementing LLMs in delicate, high-stakes situations.",
    "title_zh": "通过自适应微调与上下文感知的提示工程提升GPT-4.1在边缘情况下的推理能力",
    "abstract_zh": "本文提出了RECAP（通过上下文感知提示进行推理增强）这一双策略框架，结合了先进的提示工程技巧，通过重构输入以突出消歧、类比示例和反事实推理，并采用基于合成边缘案例数据集的参数高效微调方法，针对特定领域解决现有问题。RECAP在GPT-4.1 API上作为一个轻量级、模块化的层运行，能够在不改变基础模型的前提下实现性能提升。在五个特定领域的基准测试中，RECAP显著提高了生成结果的可解释性，降低了矛盾率，并增强了逻辑一致性。除了方法论上的贡献，本文还为在敏感且高风险场景中部署大语言模型提供了实用的实施路线图。"
  },
  {
    "date": "2026-2-19",
    "title": "LLM Reproducibility using Three-Way Caching",
    "authors": "Abhijit Nayak, Raj Bhowmik",
    "publish": "2025 3rd International Conference on Foundation and Large Language Models (FLLM)",
    "url": "https://doi.org/10.1109/fllm67465.2025.11390944",
    "source": "IEEE",
    "abstract": "Generative chat systems often return different answers to semantically equivalent queries, increasing latency, cost, and inconsistency [1]. A response-caching layer has been introduced in this work that stores model outputs and serves them on future requests when the incoming query matches a past query with a user-driven semantic similarity, for instance: ≥90%. Queries are embedded and normalized (lower-casing, stop-word handling, punctuation and whitespace canonicalization) to produce cache keys; nearest-neighbor search retrieves candidates, which are re-ranked by cosine similarity and lightweight intent features. If the top candidate exceeds the 90% or any set threshold score, and passes validators, its stored answer from the cache repository are returned; otherwise the request falls back to live generation and the cache is updated. We outline an evaluation protocol measuring latency reduction, cost savings, and consistency of cached response (exact cache, semantic and causal matches) versus fresh LLM generations (manual and task metrics), and analyze threshold–precision/recall trade-offs. Mitigation of the flaws of simple semantic matches (cosine similarity) are discussed with the introduction of a causal verification in the response caching layer. This simple architecture yields a practical path to faster, cheaper, and more stable chatbot behavior while preserving answer quality through freshness and safety gates.",
    "title_zh": "使用三重缓存实现大语言模型的可复现性",
    "abstract_zh": "生成式聊天系统在面对语义等价的查询时，常常返回不同的答案，这导致延迟增加、成本上升以及结果不一致 [1]。本文提出了一种响应缓存层，用于存储模型输出，并在新请求与历史查询在用户设定的语义相似度（例如：≥90%）下匹配时，直接返回缓存中的答案。查询首先被嵌入并标准化（包括转为小写、处理停用词、标点符号与空格的规范化），以生成缓存键；随后通过最近邻搜索检索候选项，并基于余弦相似度和轻量级意图特征进行重新排序。若排名第一的候选项的相似度超过90%或任何设定的阈值，并通过验证器检查，则直接返回缓存中的答案；否则，请求将回落至实时生成模式，同时更新缓存内容。本文还提出了一套评估协议，用于衡量缓存响应在延迟降低、成本节约以及一致性（精确匹配、语义匹配和因果匹配）方面的表现，并与全新大语言模型生成结果（通过人工评估和任务指标）进行对比，深入分析了阈值设置对精确率与召回率之间的权衡。此外，本文讨论了简单语义匹配（如余弦相似度）存在的缺陷，并引入因果验证机制以增强响应缓存层的可靠性。这一简洁的架构为实现更快、更经济、更稳定的聊天机器人行为提供了切实可行的路径，同时通过引入新鲜度与安全检查机制，有效保障了回答质量。"
  },
  {
    "date": "2026-2-19",
    "title": "VitalSystem: A FauIt-Tolerant System Architecture for Wafer-Scale Integration",
    "authors": "Yiyang Liu, Jinghe Wei, Wenxin Xu, Leran Wang, Huixiang Li, Ying Gao",
    "publish": "2025 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)",
    "url": "https://doi.org/10.1109/apccas67402.2025.11377319",
    "source": "IEEE",
    "abstract": "Wafer-Scale Integration (WSI) technology addresses exponential computational demands, yet hardware failures pose critical challenges. Existing convex fault region (CFR-based) fault-tolerant routing algorithms fail to efficiently utilize healthy resources under scattered fault conditions. This paper presents VitalSystem, a fault-tolerant Network-on-Chip architecture featuring Multi-Tier Health Assessment System (MHAS), Dual Routing Table Router (DRR), and Fault-Tolerant Hierarchical Block-aware Adaptive Optimization (FT-HBAO) routing algorithm. MHAS provides hierarchical health monitoring, DRR enables flexible routing configuration, and FT-HBAO employs two-stage optimization maximizing healthy node utilization. Synthesis results using SMIC 28nm technology yield 20.313mm<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup> total area, 500MHz operating frequency, and 1.004W power consumption while maintaining competitive hardware overhead. Experimental evaluation reveals FT - HBAO achieves superior performance with over 10% improvement in healthy node utilization compared to existing CFR-based approaches under high fault intensity scenarios.",
    "title_zh": "VitalSystem：一种用于晶圆级集成的容错系统架构",
    "abstract_zh": "晶圆级集成（Wafer-Scale Integration, WSI）技术应对着指数级增长的计算需求，然而硬件故障带来了严峻挑战。现有的基于凸故障区域（CFR-based）的容错路由算法在故障分布分散的情况下，难以高效利用健康资源。本文提出VitalSystem，一种具备容错能力的片上网络（Network-on-Chip）架构，包含多级健康评估系统（Multi-Tier Health Assessment System, MHAS）、双路由表路由器（Dual Routing Table Router, DRR）以及容错分层块感知自适应优化（Fault-Tolerant Hierarchical Block-aware Adaptive Optimization, FT-HBAO）路由算法。MHAS实现分层健康监控，DRR支持灵活的路由配置，而FT-HBAO采用两阶段优化策略，最大化健康节点的利用率。基于SMIC 28nm工艺的综合结果表明，该架构总芯片面积为20.313mm²，工作频率达500MHz，功耗为1.004W，同时保持了具有竞争力的硬件开销。实验评估显示，在高故障密度场景下，FT-HBAO相比现有CFR-based方法，健康节点利用率提升超过10%，展现出显著的性能优势。"
  },
  {
    "date": "2026-2-19",
    "title": "Real-Time Visualization of Learning Progress in Online Programming Exercises Using Google Classroom and Colaboratory",
    "authors": "Kazuaki Yoshihara, Nobukazu Iguchi",
    "publish": "2025 International Conference on Education Technology and Computers (ICETC)",
    "url": "https://doi.org/10.1109/icetc66579.2025.11387701",
    "source": "IEEE",
    "abstract": "In online programming education, it is often challenging for instructors to monitor learners’ engagement with coding exercises in real time. In this study, we developed a system that integrates Google Classroom and Colaboratory to automatically collect the execution results and error messages from each learner’s Python notebook. The system is implemented using Google Apps Script, which periodically analyzes the. ipynb files and visualizes the results in a Google Spreadsheet. Correct outputs are highlighted in green, while error outputs are marked in red, enabling instructors to easily track students’ progress during live sessions. A preliminary trial with 11 learners demonstrated that the system effectively supported real-time visualization of learning progress in an online programming class.",
    "title_zh": "使用 Google Classroom 和 Colaboratory 实时可视化在线编程练习中的学习进度",
    "abstract_zh": "在在线编程教育中，教师往往难以实时监控学习者在编程练习中的参与情况。本研究开发了一套系统，将 Google Classroom 与 Colaboratory 相集成，能够自动收集每位学习者 Python 笔记本的执行结果和错误信息。该系统基于 Google Apps Script 实现，可定期分析 .ipynb 文件，并将结果可视化呈现于 Google 电子表格中。正确输出以绿色高亮显示，错误输出则以红色标记，使教师能够轻松跟踪学生在直播课程中的学习进展。对 11 名学习者进行的初步试验表明，该系统能有效支持在线编程课程中学习进度的实时可视化。"
  },
  {
    "date": "2026-2-19",
    "title": "MATDS : Multi-Agent Task Decomposition System based on LLMs",
    "authors": "Joonyoung Jung, Dong-Oh Kang",
    "publish": "2025 16th International Conference on Information and Communication Technology Convergence (ICTC)",
    "url": "https://doi.org/10.1109/ictc66702.2025.11387840",
    "source": "IEEE",
    "abstract": "In multi-agent systems, task decomposition must consider physical constraints, temporal dependencies among tasks, and interactions between agents. Decomposing high-level natural language instructions into executable multi-agent actions is a core component of multi-agent planning systems. In this paper, we propose the multi-agent task decomposition system (MATDS), which leverages the natural language understanding, reasoning, and evaluation capabilities of large language models (LLMs) to perform multi-agent task decomposition. The MATDS takes high-level task instructions provided in natural language as input and decomposes them into executable forms that can be carried out by multiple agents. The MATDS employs LLMs to iteratively decompose high-level tasks through Chain-Of-Thought (CoT) reasoning and evaluates the decomposition results through a critic module to ensure that the generated task plans satisfy feasibility and logical consistency. This enables effective task decomposition that considers multi-agent execution capabilities and parallelism. We conducted experiments on task decomposition using the AI2-THOR simulator and demonstrated that the MATDS can mitigate errors that may occur in few-shot prompting approaches. Overall, MATDS provides a flexible and scalable solution for generating and refining task decompositions for multi-agent systems.",
    "title_zh": "MATDS：基于大语言模型的多智能体任务分解系统",
    "abstract_zh": "在多智能体系统中，任务分解必须考虑物理约束、任务之间的时序依赖关系以及智能体之间的交互。将高层自然语言指令分解为可执行的多智能体动作，是多智能体规划系统的核心组成部分。本文提出了一种多智能体任务分解系统（MATDS），该系统利用大语言模型（LLMs）在自然语言理解、推理和评估方面的能力，实现多智能体任务分解。MATDS以自然语言形式提供的高层任务指令为输入，将其分解为多个智能体可执行的行动形式。MATDS通过链式思维（Chain-of-Thought, CoT）推理机制，利用大语言模型迭代地分解高层任务，并借助一个评判模块对分解结果进行评估，以确保生成的任务计划具备可行性与逻辑一致性。这一机制使得任务分解能够有效考虑多智能体的执行能力与并行性。我们在AI2-THOR模拟器上进行了任务分解实验，结果表明，MATDS能够有效缓解少样本提示方法中可能出现的错误。总体而言，MATDS为多智能体系统生成和优化任务分解提供了一种灵活且可扩展的解决方案。"
  },
  {
    "date": "2026-2-19",
    "title": "AI-vOLT: Multi-Stage Agentic Translation from Operator Intent to Executable PON Procedures",
    "authors": "Chansung Park, Yongwook Ra, Hwan Seok Chung",
    "publish": "2025 16th International Conference on Information and Communication Technology Convergence (ICTC)",
    "url": "https://doi.org/10.1109/ictc66702.2025.11387884",
    "source": "IEEE",
    "abstract": "We propose AI-vOLT, a virtual OLT system integrating SEBA with large language models (LLMs) to enable intent-based PON provisioning through natural language interfaces. Manual provisioning in traditional Passive Optical Networks (PONs) remains resource-intensive and error-prone, limiting their adaptability to next-generation service demands. AI-vOLT addresses this challenge by translating high-level operator intent into low-level executable commands through a multistage agentic workflow of planning, shaping, and execution. We evaluate the framework across four representative provisioning scenarios and multiple LLM backends under varying contextual inputs. Experimental results show that AI-vOLT achieves near-perfect provisioning success (≈ 99%) in simulated environments and is further validated on a 25G physical PON testbed. These findings confirm the practicality of AI-vOLT for reliable, language-driven automation of optical access networks.",
    "title_zh": "AI-vOLT：从操作员意图到可执行PON流程的多阶段智能体翻译",
    "abstract_zh": "我们提出AI-vOLT，一种将软件定义以太网宽带接入（SEBA）与大型语言模型（LLMs）相结合的虚拟光线路终端（OLT）系统，通过自然语言接口实现基于意图的无源光网络（PON）配置。传统无源光网络（PON）中的手动配置仍耗费大量资源且易出错，难以适应下一代业务需求。AI-vOLT通过多阶段代理式工作流——规划、生成与执行——将高层操作员意图自动转化为底层可执行命令，有效应对这一挑战。我们在四种典型配置场景下，结合多种LLM后端及不同上下文输入对框架进行了评估。实验结果表明，AI-vOLT在模拟环境中实现了接近完美的配置成功率（约99%），并在一个25G物理PON测试平台上得到进一步验证。这些成果证实了AI-vOLT在实现可靠、语言驱动的光接入网自动化方面的实际可行性。"
  },
  {
    "date": "2026-2-19",
    "title": "Opportunities and Challenges of Empowering Programming Courses with Large Language Models",
    "authors": "Ke-di Zhang, Ming-fei Wu, Ruo-chun Jin, Jian-jun Xu, Jing Wang",
    "publish": "2025 6th International Conference on Information Science and Education (ICISE-IE)",
    "url": "https://doi.org/10.1109/icise-ie68873.2025.11378500",
    "source": "IEEE",
    "abstract": "This paper presents an empirical study of teaching teams implementing AI-empowered undergraduate programming courses through intelligent teaching assistant platforms. It elaborates on three key aspects of large language models (LLMs) in enhancing teachers' preparation efficiency, assisting students in developing computational thinking skills, and optimizing personalized learning pathways. Furthermore, the study highlights the risks associated with over-reliance on LLMs, the challenges educators encounter when utilizing these models, and the limitations regarding the depth of human-machine interaction. Additionally, it proposes avenues for improvement in AIempowered programming courses across three dimensions: technological application proficiency, effectiveness of teaching practices, and professional development for educators. This provides a theoretical framework for the advanced integration of intelligent technology within foundational computer education.",
    "title_zh": "大型语言模型赋能编程课程的机遇与挑战",
    "abstract_zh": "本文对教学团队通过智能助教平台实施人工智能赋能的本科编程课程进行了实证研究。研究详细阐述了大型语言模型（LLMs）在提升教师备课效率、辅助学生发展计算思维能力以及优化个性化学习路径三个方面的重要作用。同时，研究也指出了过度依赖大型语言模型所带来的风险，教育工作者在使用这些模型时面临的挑战，以及人机交互深度不足的局限性。此外，本文从技术应用能力、教学实践有效性以及教师专业发展三个维度，提出了提升人工智能赋能编程课程的改进路径。该研究为智能技术在基础计算机教育中的深度融合提供了理论框架。"
  },
  {
    "date": "2026-2-19",
    "title": "A Reconfigurable 16KB Cache Architecture for Direct-Mapped and Set-Associative Designs",
    "authors": "V Siva Durga Sankar, M Dheeraj Kumar Reddy, Unais I, S Lingeswar, Saisuriyaa G",
    "publish": "2025 International Conference on Intelligent Innovations in Engineering and Technology (ICIIET)",
    "url": "https://doi.org/10.1109/iciiet65921.2025.11378244",
    "source": "IEEE",
    "abstract": "A reconfigurable <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathbf{1 6 K B}$</tex> cache memory system is designed using Verilog Hardware Description Language to support multiple cache mapping techniques, including direct-mapped and set-associative organizations. The set-associative architecture is configurable as two-way, four-way, or eight-way, offering flexibility in terms of associativity and performance. A mode-selectable control mechanism enables dynamic switching between cache architectures during runtime, based on application-specific requirements or performance trade-offs. The design utilizes Block RAM (BRAM) inference for efficient FPGA implementation, supporting selective activation of cache blocks to optimize access time and minimize power consumption by deactivating unused sets. The modular and parameterized structure makes it highly scalable and well-suited for implementation in embedded processors, FPGA-based accelerators with limited resources, and customizable computing platforms. The proposed cache architecture is novel because it allows dynamic runtime reconfiguration between direct-mapped and multiple set-associative cache structures, which is rarely implemented in FPGA-based designs.",
    "title_zh": "一种适用于直接映射与组相联设计的可重构16KB缓存架构",
    "abstract_zh": "采用Verilog硬件描述语言设计了一种可重构的16 KB缓存内存系统，支持多种缓存映射技术，包括直接映射和组相联组织。组相联架构可配置为两路、四路或八路，从而在关联度和性能之间提供灵活性。通过可选择的模式控制机制，系统能够在运行时根据应用需求或性能权衡动态切换缓存架构。该设计利用块RAM（BRAM）推断技术，实现高效的FPGA部署，支持缓存块的选择性激活，通过关闭未使用的组来优化访问时间并降低功耗。其模块化和参数化结构具有高度可扩展性，非常适合嵌入式处理器、资源受限的FPGA加速器以及可定制计算平台的应用。所提出的缓存架构具有创新性，因为它能够在运行时动态重构直接映射与多种组相联缓存结构之间切换，这种功能在基于FPGA的设计中极为罕见。"
  },
  {
    "date": "2026-2-19",
    "title": "Retrieval-Augmented Reasoning for Offensive Security: Towards Robust LLM Agents for CTF Automation",
    "authors": "Stanislav Elkin, Kirill Anosov, Dmitry Kiselev",
    "publish": "2025 3rd International Conference on Foundation and Large Language Models (FLLM)",
    "url": "https://doi.org/10.1109/fllm67465.2025.11391064",
    "source": "IEEE",
    "abstract": "We present a retrieval-first reasoning-agent architecture for automating offensive cybersecurity tasks in the Capture The Flag (CTF) format. Our core hypothesis is that domain-specific contextual retrieval can stabilize large language model (LLM) reasoning under adversarial, multi-step challenges. To test this, we construct a bespoke retrieval corpus from scratch—excluding benchmark leakage—covering key CTF domains such as binary exploitation, reverse engineering, cryptography, and web vulnerabilities. Using FAISS indexing and Retrieval-Augmented Generation (RAG), our agent integrates this corpus with GPT-4o via a lightweight, tool-agnostic pipeline. Evaluated on NYU CTF benchmark [2], the RAG-enhanced agent achieves a 57.1% improvement in task completion over the vanilla GPT-4o baseline. Unlike prior work, which often depends on fragile tool orchestration, we demonstrate that focused retrieval alone significantly improves reasoning stability, reduces hallucinations, and enhances task-solving accuracy. Our threat model also considers prompt injection and contextual ambiguity, underlining the need for interpretable, corpus-grounded architectures in offensive reasoning.",
    "title_zh": "用于进攻性安全的检索增强推理：迈向鲁棒的大型语言模型智能体以实现CTF自动化",
    "abstract_zh": "我们提出了一种以检索为核心的推理代理架构，用于自动化在夺旗赛（Capture The Flag, CTF）格式下的进攻性网络安全任务。我们的核心假设是：特定领域的上下文检索能够增强大语言模型（LLM）在对抗性、多步骤挑战下的推理稳定性。为验证这一假设，我们从零开始构建了一个专用的检索语料库——排除了基准测试数据泄露，覆盖了CTF中的关键领域，包括二进制漏洞利用、逆向工程、密码学以及Web漏洞。通过FAISS索引与检索增强生成（Retrieval-Augmented Generation, RAG）技术，我们的代理通过一个轻量级、与工具无关的流水线，将该语料库与GPT-4o相结合。在NYU CTF基准测试[2]上的评估表明，经过RAG增强的代理在任务完成率上相比原始GPT-4o基线提升了57.1%。与以往依赖脆弱工具编排的工作不同，我们证明了聚焦于检索本身即可显著提升推理稳定性，减少幻觉现象，并提高任务求解的准确性。此外，我们的威胁模型还考虑了提示注入攻击和上下文模糊性问题，凸显了在进攻性推理中采用可解释、基于语料库的架构的必要性。"
  },
  {
    "date": "2026-2-19",
    "title": "Evaluating the Energy-Efficiency of the Code Generated by LLMs",
    "authors": "Md Arman Islam, Devi Varaprasad Jonnala, Ritika Rekhi, Pratik Pokharel, Siddharth Cilamkoti, Asif Imran, Tevfik Kosar, Bekir Turkkan",
    "publish": "2025 3rd International Conference on Foundation and Large Language Models (FLLM)",
    "url": "https://doi.org/10.1109/fllm67465.2025.11391172",
    "source": "IEEE",
    "abstract": "As the quality of code generated by Large Language Models (LLMs) improves, their adoption in the software industry for automated code generation continues to grow. Researchers primarily focus on enhancing the functional correctness of the generated code while commonly overlooking its energy efficiency and environmental impact. This paper investigates the energy efficiency of the code generated by 20 popular LLMs for 878 programming problems of varying difficulty levels and diverse algorithmic categories selected from the LeetCode platform by comparing them against canonical human-written solutions. Although LLMs can produce functionally correct results in most cases, our findings show that the performance and energy efficiency of LLM-produced solutions are often far below those of human-written solutions. Among the studied LLMs, DeepSeek-v3 and GPT-4o generate the most energy-efficient code, whereas Grok-2 and Gemini-1.5-Pro are among the least energy-efficient models. On average, human-generated canonical solutions are approximately 1.17 times more energy efficient than DeepSeek-v3, 1.21 times more energy efficient than GPT-4o, and over 2 times more energy efficient than Grok-2 and Gemini-1.5-Pro. For specific algorithmic groups such as dynamic programming, backtracking, and bit manipulation, LLM-generated code can consume up to 450 times more energy than human-generated canonical solutions.",
    "title_zh": "评估大语言模型生成代码的能效性",
    "abstract_zh": "随着大型语言模型（LLMs）生成代码质量的不断提升，其在软件行业中用于自动化代码生成的应用持续增长。然而，研究人员主要关注提升生成代码的功能正确性，而往往忽视了代码的能效表现及其对环境的影响。本文针对从LeetCode平台选取的878个不同难度级别和多种算法类别编程问题，系统评估了20种主流LLM生成代码的能效表现，并将其与标准的人工编写解决方案进行对比。尽管LLMs在大多数情况下能够生成功能正确的代码，但我们的研究发现，LLM生成代码的性能和能效通常远低于人工编写的解决方案。在所研究的LLM中，DeepSeek-v3和GPT-4o生成的代码能效最高，而Grok-2和Gemini-1.5-Pro则属于能效最低的模型。平均而言，人工编写的标准解决方案的能效分别比DeepSeek-v3高出约1.17倍、比GPT-4o高出约1.21倍，而比Grok-2和Gemini-1.5-Pro高出超过2倍。在动态规划、回溯算法和位运算等特定算法类别中，LLM生成的代码能耗甚至可高达人工标准解决方案的450倍。"
  },
  {
    "date": "2026-2-19",
    "title": "An Error-Aware Automatic Grading Framework for Python Programs Based on Structural-Semantic Similarity",
    "authors": "Xue Han, Jinwei Wang, Jiayi Che, Jinwei Wang",
    "publish": "2025 6th International Conference on Information Science and Education (ICISE-IE)",
    "url": "https://doi.org/10.1109/icise-ie68873.2025.11378470",
    "source": "IEEE",
    "abstract": "Most automatic grading systems for Python rely on the built-in ast module to construct abstract syntax trees (ASTs), but they fail when programs contain syntax errors, limiting their use in real exams. To overcome this, we propose a static grading method based on the Parso library, which is able to construct syntax trees for programs containing syntax errors by explicitly marking error nodes. Building on this, we introduce an erroraware grading framework that simulates human evaluation: student programs are analyzed to identify and classify syntax errors, critical errors are minimally corrected with penalties assigned by error category, and the corrected programs are assessed against reference solutions using structural and semantic similarity metrics. Experiments on exam data show that the method effectively handles erroneous programs and yields grading results closely aligned with human assessments.",
    "title_zh": "基于结构-语义相似性的错误感知自动评分框架用于Python程序",
    "abstract_zh": "大多数用于Python的自动评分系统依赖于内置的ast模块来构建抽象语法树（AST），但当程序包含语法错误时，这些系统会失效，从而限制了它们在实际考试中的应用。为克服这一问题，我们提出了一种基于Parso库的静态评分方法，该方法能够为包含语法错误的程序构建语法树，并通过显式标记错误节点来处理异常情况。在此基础上，我们设计了一种具有错误感知能力的评分框架，模拟人类评分的逻辑：首先分析学生程序，识别并分类语法错误；对关键错误进行最小程度的修正，并根据错误类型施加相应的扣分；最后，利用结构相似性和语义相似性指标，对修正后的程序与标准答案进行比对评估。在考试数据上的实验表明，该方法能够有效处理含有错误的程序，评分结果与人工评分高度一致。"
  },
  {
    "date": "2026-2-19",
    "title": "Physical Design Implementation and Power Optimization of RV32I RISC-V Processor",
    "authors": "Devang Kailashkumar Vekariya, Gaurav Saini, Ashutosh Raghuvir Manohar",
    "publish": "2025 International Conference on Electronics and Computing, Communication Networking Automation Technologies (ICEC2NT)",
    "url": "https://doi.org/10.1109/icec2nt65402.2025.11380178",
    "source": "IEEE",
    "abstract": "The development of VLSI physical design and technological scaling has allowed for the integration of a high number of transistors on a single chip, improving performance, power efficiency, and area usage. However, when the technology node decreases, other issues arise, such as increasing connection resistance, power dissipation, and timing closure complexity. This study describes the physical design implementation of an RV32I RISC-V processor utilizing a 45 nm open-source PDK, with a focus on the effects of technology scaling and power optimization. The design was implemented using open-source tools like Yosys for synthesis, OpenROAD for placement and routing, KLayout for signoff verification, and OpenTimer for static timing analysis (STA). Clock gating was applied, leading to a 13.84 % reduction in dynamic power consumption, significantly improving power efficiency. A comparative analysis with a 130 nm implementation further demonstrated a 18.9 % reduction in total power, highlighting the benefits of smaller technology nodes. Furthermore, STA results indicated that the design satisfied all timing constraints, allowing for steady operation at 130 MHz with no timing violations.",
    "title_zh": "RV32I RISC-V处理器的物理设计实现与功耗优化",
    "abstract_zh": "VLSI物理设计与技术缩放的发展使得在单个芯片上集成大量晶体管成为可能，从而提升了性能、功耗效率和面积利用率。然而，随着技术节点的缩小，也带来了诸多新问题，例如互连电阻增加、功耗上升以及时序收敛复杂度提高。本研究描述了一款基于45 nm开源PDK的RV32I RISC-V处理器的物理设计实现，重点探讨了技术缩放对设计的影响以及功耗优化策略。设计采用开源工具链完成：使用Yosys进行综合，OpenROAD进行布局布线，KLayout进行签核验证，OpenTimer进行静态时序分析（STA）。通过应用时钟门控技术，动态功耗降低了13.84%，显著提升了功耗效率。与130 nm工艺实现的对比分析表明，总功耗降低了18.9%，进一步验证了更小技术节点的优势。此外，静态时序分析结果表明，设计满足所有时序约束，在130 MHz下可稳定运行且无时序违例。"
  },
  {
    "date": "2026-2-19",
    "title": "A GPT for Ecore Model Building",
    "authors": "Zobia Erum, Issam Al-Azzoni",
    "publish": "2025 3rd International Conference on Foundation and Large Language Models (FLLM)",
    "url": "https://doi.org/10.1109/fllm67465.2025.11391086",
    "source": "IEEE",
    "abstract": "Model-Driven Engineering (MDE) relies on formal models, yet constructing them from natural language specifications remains a manual and error-prone task. Recent advances in large language models (LLMs) such as GPT suggest new opportunities for automating this process. This work presents a GPT-based system fine-tuned to generate valid Ecore models in the sclang smart contract access control modeling language. The approach bridges natural language input and formal metamodel artifacts, reducing effort while improving correctness. A case study on access control models illustrates the effectiveness of the method.",
    "title_zh": "用于生态模型构建的GPT",
    "abstract_zh": "模型驱动工程（MDE）依赖于形式化模型，然而从自然语言规范构建这些模型仍然是一个手动且容易出错的过程。近年来，大型语言模型（LLM）如GPT的进展为自动化这一过程带来了新的机遇。本文提出了一种基于GPT的系统，该系统经过微调，能够生成符合规范的Ecore模型，适用于sclang智能合约访问控制建模语言。该方法实现了自然语言输入与形式化元模型之间的桥梁，显著降低了工作量并提高了正确性。通过访问控制模型的案例研究，验证了该方法的有效性。"
  },
  {
    "date": "2026-2-19",
    "title": "Prompt Injection Detection and Mitigation with AI Multiagent NLP-based Agentic Frameworks",
    "authors": "Diego Gosmar, Deborah A. Dahl, Dario Gosmar",
    "publish": "2025 3rd International Conference on Foundation and Large Language Models (FLLM)",
    "url": "https://doi.org/10.1109/fllm67465.2025.11391215",
    "source": "IEEE",
    "abstract": "Prompt injection is a significant challenge for generative AI systems because it can lead to unintended outputs. We introduce a Multiagent NLP-based experimental framework, specifically designed to address prompt injection vulnerabilities through layered detection and metadata mechanisms. The framework orchestrates specialized AI agents to generate responses, detect vulnerabilities, and mitigate injection effects. An empirical evaluation of 500 engineered injection prompts was conducted, with ten different prompt injection categories properly generated and shuffled (50 prompts for each injection attack category). The experimental results show a significant reduction in the injection score and an increased detection of prompt injection markers, indicating potential applications for mitigation. Novel metrics—including Injection Success Rate (ISR), Policy Override Frequency (POF), Prompt Sanitization Rate (PSR), and Compliance Consistency Score (CCS)—are proposed to derive a composite Total Injection Vulnerability Score (TIVS). The system utilizes the vendor-independent OFP (Open Floor Protocol) framework for agentic AI communication via structured JSON messages. It encapsulates APIs using natural language while also comparing and extending a previously established multiagent experiment on hallucination mitigation to address the specific challenges of prompt injection.",
    "title_zh": "基于AI多智能体NLP的代理框架中的提示注入检测与缓解",
    "abstract_zh": "提示注入是生成式人工智能系统面临的一个重大挑战，因为它可能导致非预期的输出。为此，我们提出了一种基于多智能体自然语言处理（NLP）的实验框架，专门设计用于通过分层检测与元数据机制来应对提示注入漏洞。该框架协调多个专用AI智能体，分别负责生成响应、检测漏洞以及缓解注入影响。我们对500个精心设计的注入提示进行了实证评估，共生成并打乱了十种不同类型的提示注入攻击（每类50个提示）。实验结果表明，注入得分显著降低，提示注入标记的检测率明显提高，显示出该方法在漏洞缓解方面的潜在应用价值。为此，我们提出了四项新指标：注入成功率（ISR）、策略覆盖频率（POF）、提示净化率（PSR）以及合规一致性评分（CCS），并据此构建综合的总提示注入脆弱性评分（TIVS）。系统采用与供应商无关的开放楼层协议（OFP, Open Floor Protocol）框架，通过结构化JSON消息实现智能体间的AI通信。该框架在封装API的同时，使用自然语言进行交互，并在先前已建立的多智能体幻觉缓解实验基础上进行比较与扩展，以应对提示注入所特有的挑战。"
  },
  {
    "date": "2026-2-19",
    "title": "Deep Learning-Based Smart Contract Vulnerability Detection Model",
    "authors": "Rui Wu, Guangfu Wu, Kangjun Li, Lei Liu, Lulu Liu, Weiqiang Wu",
    "publish": "2025 IEEE International Conference on Blockchain Technology and Information Security (ICBCTIS)",
    "url": "https://doi.org/10.1109/icbctis66509.2025.11387182",
    "source": "IEEE",
    "abstract": "As blockchain technology matures, the application scope of smart contracts continues to expand. Security issues in smart contracts have become an important research focus. In view of the limitations of traditional vulnerability detection methods such as limited vulnerability types and high false positive rate, this paper proposes a smart contract vulnerability detection model based on deep learning. By constructing token sequence sets and data flow graphs and inputting them into the pre-training model of this article, we obtained a pre-training model optimized for Solidity code and implemented a multi-label classifier in the downstream detection layer. The proposed model can detect integer overflow vulnerabilities, transaction sequence dependency vulnerabilities, timestamp vulnerabilities, and reentrancy vulnerabilities in the target contract. Experiments show that this model is better than other detection tools in detection effect.",
    "title_zh": "基于深度学习的智能合约漏洞检测模型",
    "abstract_zh": "随着区块链技术的不断成熟，智能合约的应用范围持续扩大。智能合约中的安全问题已成为重要的研究焦点。针对传统漏洞检测方法存在漏洞类型有限、误报率高等局限性，本文提出了一种基于深度学习的智能合约漏洞检测模型。通过构建标记序列集合与数据流图，并将其输入本文提出的预训练模型，获得了针对Solidity代码优化的预训练模型，并在下游检测层实现了多标签分类器。所提出的模型能够有效检测目标合约中的整数溢出漏洞、交易序列依赖漏洞、时间戳漏洞以及重入漏洞。实验结果表明，该模型在检测效果上优于其他现有检测工具。"
  },
  {
    "date": "2026-2-19",
    "title": "Design of Secure Compression Hash Function in RISC-V Based SoC",
    "authors": "Raahul L. S., Sreegayathri K. K., Koushik B., Mohankumar N.",
    "publish": "2025 International Conference on Intelligent Innovations in Engineering and Technology (ICIIET)",
    "url": "https://doi.org/10.1109/iciiet65921.2025.11378962",
    "source": "IEEE",
    "abstract": "In this paper, a next-generation computing architecture that supports for increasing demand on data-intensive applications that demand AI, data science and blockchain. Although SHA-256 offers good cryptographic security, it has to perform many arithmetic and logic operations on resource-limited platforms. As there is increasing concern for security, we need some secure protocols to maintain data confidentiality and integrity. We propose a secure compression hash function where an XOR-based Logic locking is incorporated to enhance security and logical integrity. Since some of the non-critical operations are performed in both software and hardware, we introduce a parallelized cryptographic execution to make sure our design runs with an execution time of <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$2 \\mu \\mathrm{s}$</tex> providing a 99.8% reduction in computation time compared to a purely software-based implementation. The design was analyzed by several features, such as the avalanche effect, the entropy, the Hamming distance, the bit frequency distribution and it has satisfied all conditions, proposing a better secure compression hash function in RISC-V based SoC. The design shows an average Hamming distance of around 50% showing a strong avalanche effect and has an average entropy of 5 bits per byte. The timing and the hardware utilization compare a trade-off between hashing speed and the hardware efficiency ensuring a better throughput implementation. Thus, the proposed design is highly suitable for real-time embedded applications.",
    "title_zh": "基于RISC-V的片上系统中的安全压缩哈希函数设计",
    "abstract_zh": "本文提出了一种下一代计算架构，以满足对数据密集型应用（如人工智能、数据科学和区块链）日益增长的需求。尽管SHA-256提供了良好的密码学安全性，但在资源受限的平台上仍需执行大量算术和逻辑操作。随着安全问题日益受到关注，我们需要一些安全协议来保障数据的机密性和完整性。为此，我们提出了一种安全的压缩哈希函数，通过引入基于异或（XOR）的逻辑锁定技术，以增强安全性和逻辑完整性。由于部分非关键操作在软件和硬件中并行执行，我们设计了一种并行化的密码学执行机制，确保该设计的执行时间低于 $2 \\mu \\mathrm{s}$，相较于纯软件实现，计算时间减少了99.8%。该设计通过多个特性进行了分析，包括雪崩效应、熵值、汉明距离、比特频率分布等，所有指标均满足要求，表明该方案在基于RISC-V的片上系统（SoC）中具有更优的安全性。实验结果表明，该设计的平均汉明距离约为50%，表现出显著的雪崩效应，且每字节的平均熵值达到5比特。时序性能与硬件资源利用率之间实现了良好的权衡，兼顾了哈希速度与硬件效率，从而实现了更高的吞吐量。因此，所提出的架构非常适用于实时嵌入式应用场景。"
  },
  {
    "date": "2026-2-19",
    "title": "Analysis of Novel Curve Fitting-Based Real Number Modeling Technique for High-Switching Applications in Analog Mixed-Signal Systems",
    "authors": "Tamil Surya B, Devadarsan A M, Venkateswaran Padmanabhan, Guha Lakshmanan, Santhanalakshmi Mahalingam, Rajalakshmi Karuppuswamy",
    "publish": "2025 International Conference on Intelligent Innovations in Engineering and Technology (ICIIET)",
    "url": "https://doi.org/10.1109/iciiet65921.2025.11379037",
    "source": "IEEE",
    "abstract": "Increasing demand for complex and high-performance Analog and Mixed-Signal (AMS) designs has created a significant need for efficient verification methodologies. As modern IC designs integrate more functionality, the time and resources required for verification continue to escalate. Ensuring accurate functionality at different abstraction levels is crucial in system design. In addition, simulation tools often involve a trade-off between simulation speed and accuracy. This trade-off is too extreme for analog simulation especially, due to non-linear behaviour and the possibility of wide range of values at each node, making these simulations time-consuming. A black-box model leveraging polynomial regression technique has been proposed to address this issue. Charge Pump and Ring Oscillator circuits are selected for the verification of the proposed technique. Although analog simulator takes significant overhead in simulation time, Spectre has been used for benchmarking the circuits due to its high accuracy. The black-box model proposed is developed using the data collected from Spectre with a focus on output parameters without considering the intermediate nodes. The regression technique relates the input and the output parameters using a multi-degree polynomial to obtain average output values. This is implemented in SystemVerilog using Real Number Modeling (RNM). Then the average output values are assigned to EEnet (U ser Defined N ettype) by behavioral modeling. This novel curve fitting polynomial approach reduces the prolonged simulation time thereby meeting the Time-to-Market requirements.",
    "title_zh": "基于新曲线拟合的实数建模技术在模拟混合信号系统高频开关应用中的分析",
    "abstract_zh": "随着对复杂且高性能模拟与混合信号（AMS）设计需求的不断增长，高效验证方法的需求日益迫切。随着现代集成电路设计集成的功能越来越多，验证所需的时间和资源持续攀升。在系统设计中，确保在不同抽象层次上的功能准确性至关重要。此外，仿真工具通常在仿真速度与精度之间存在权衡，而这种权衡在模拟仿真中尤为突出，因为模拟电路具有非线性特性，且各节点的取值范围可能非常广泛，导致仿真过程耗时较长。为解决这一问题，本文提出了一种基于多项式回归技术的黑箱模型方法。选取电荷泵（Charge Pump）和环形振荡器（Ring Oscillator）电路作为验证该技术的实例。尽管模拟器在仿真时间上存在较大开销，但为保证高精度，仍选用Spectre作为基准仿真工具。所提出的黑箱模型基于Spectre仿真获取的数据构建，重点关注输出参数，而不考虑中间节点。通过多阶多项式回归技术，建立输入参数与输出参数之间的关系，以获得平均输出值。该模型在SystemVerilog中通过实数建模（Real Number Modeling, RNM）实现，随后将计算得到的平均输出值通过行为建模方式赋给EEnet（用户自定义网络类型）。这种新颖的曲线拟合多项式方法显著缩短了仿真时间，从而有效满足了产品上市时间（Time-to-Market）的要求。"
  },
  {
    "date": "2026-2-19",
    "title": "Do LLMs Benefit from Self-Ensembles? A Study of Self-Mixture-of-Agents",
    "authors": "Seungho Lee, Donghyun Lee",
    "publish": "2025 3rd International Conference on Foundation and Large Language Models (FLLM)",
    "url": "https://doi.org/10.1109/fllm67465.2025.11390896",
    "source": "IEEE",
    "abstract": "Large Language Models (LLMs) have become increasingly prevalent in natural language processing applications due to their strong performance on a wide range of language tasks. However, their outputs are still prone to inconsistency and flaws, limiting their reliability. To address these limitations, we explore Mixture-of-Agents (MoA), an existing ensemble method that aggregates responses from multiple LLMs, which has been proven to enhance performance. Building upon this approach, we attempt to evaluate the effectiveness of Self-Mixture-of-Agents (Self-MoA), an innovative ensemble method that aggregates outputs only from a single top-performing LLM. This raises the central question of whether Self-MoA can yield measurable performance improvements. To evaluate the efficacy of Self-MoA, we evaluated Self-MoA across multiple models and datasets to assess its performance. Additionally, we applied supervised fine-tuning (SFT) to assess its impact on performance. Our results show that Self-MoA improves performance only on certain models. We hypothesize that the size of the model, the consistency of the model, and the purpose of the model all contribute to the effectiveness of Self-MoA. However, applying SFT yielded no improvement.",
    "title_zh": "大语言模型是否受益于自集成？——关于自混合智能体的探究",
    "abstract_zh": "大型语言模型（LLMs）由于在多种语言任务上表现出色，已在自然语言处理应用中变得日益普遍。然而，其输出仍容易出现不一致和错误，限制了其可靠性。为解决这些局限性，我们探索了混合代理（Mixture-of-Agents, MoA）这一现有的集成方法，该方法通过聚合多个LLM的响应来提升性能，已被证明有效。在此基础上，我们尝试评估一种创新的集成方法——自混合代理（Self-Mixture-of-Agents, Self-MoA）的有效性，该方法仅聚合单一表现最佳的LLM的输出。这引出了一个核心问题：Self-MoA是否能带来可测量的性能提升？为评估Self-MoA的有效性，我们在多个模型和数据集上进行了测试，以全面评估其性能表现。此外，我们还应用了监督微调（SFT）来考察其对性能的影响。结果表明，Self-MoA仅在某些模型上提升了性能。我们推测，模型规模、模型的一致性以及模型的用途均会影响Self-MoA的有效性。然而，采用SFT并未带来性能提升。"
  },
  {
    "date": "2026-2-19",
    "title": "Design and Implementation of Hybrid Approximate Multiplier (HAX8) for Non-Linear RGB Image Brightness Adjustment",
    "authors": "Radhakrishnan K.R, Ramesh J, Hariharan M, Vanathi P T",
    "publish": "2025 International Conference on Intelligent Innovations in Engineering and Technology (ICIIET)",
    "url": "https://doi.org/10.1109/iciiet65921.2025.11378504",
    "source": "IEEE",
    "abstract": "Multiplication is a critical operation in digital circuit, influencing the performance of arithmetic and signal processing applications. Approximate computing techniques, particularly in compressor-based multipliers, offer a balance struck between computational precision and resource utilization. This work investigates the impact of four different compressor designs (4:2, 5:2, 8:2) on an <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$8\\times 8$</tex> multiplier, evaluating their area, power, and delay characteristics. Error analysis is conducted to assess the trade-offs between computational efficiency and accuracy. The best-performing compressor-based multiplier is then integrated into an image processing application-Non-Linear Brightness Adjustment of an RGB image-where the Red and Green channels undergo transformation while the Blue channel remains unaltered. The proposed design is implemented in Verilog and synthesized using Xilinx Vivado 2018.3. Results demonstrate that the selected multiplier achieves an optimal balance between performance and accuracy, making it suitable for error-resilient applications.",
    "title_zh": "非线性RGB图像亮度调节的混合近似乘法器（HAX8）设计与实现",
    "abstract_zh": "乘法是数字电路中的关键运算，对算术运算和信号处理应用的性能具有重要影响。近似计算技术，尤其是基于压缩器的乘法器，在计算精度与资源利用之间取得了良好平衡。本文研究了四种不同压缩器设计（4:2、5:2、8:2）对一个 $8\\times 8$ 乘法器的影响，评估了其面积、功耗和延迟特性。通过误差分析，进一步评估了计算效率与精度之间的权衡。性能最优的基于压缩器的乘法器被集成到一个图像处理应用——RGB图像的非线性亮度调整中，其中红色和绿色通道经过变换，而蓝色通道保持不变。所提出的电路设计采用Verilog语言实现，并在Xilinx Vivado 2018.3中完成综合。实验结果表明，所选乘法器在性能与精度之间实现了最佳平衡，适用于对误差具有一定容忍度的应用场景。"
  },
  {
    "date": "2026-2-19",
    "title": "Augmented-GDA: LLM In-Loop Gradient Descent Algorithm for Smooth Unconstrained Optimization",
    "authors": "Belgacem Ben Ziada, Thomas Mongaillard, Hang Zou, Samson Lasaulce, Anthony Girardin, Vineeth S. Varma, Mérouane Debbah",
    "publish": "2025 3rd International Conference on Foundation and Large Language Models (FLLM)",
    "url": "https://doi.org/10.1109/fllm67465.2025.11391142",
    "source": "IEEE",
    "abstract": "The problem of exploiting large language models (LLMs) for solving optimization problems is new and existing papers rarely address the enhancement of standard solvers through LLM capabilities. We introduce an approach to augment gradient descent algorithms (GDAs) by adopting an agentic perspective. Indeed, GDAs remain central to optimization and modern machine learning, yet its efficiency often depends on brittle and time-consuming hyperparameter tuning. We investigate whether an LLM can contribute inside a running gradient method for smooth, unconstrained, single-objective standard-form optimization problems (SFOP) with known or estimable gradients. We present Augmented GDA (AGDA), an LLM in-loop controller that monitors progress and adapts learning-rate schedules in real time while keeping the optimizer family fixed within each run. This places the LLM where feedback is most informative, reducing manual trial-and-error without altering the numerical core. Experiments on benchmark functions and two famous deep learning tasks show that in-loop scheduling is competitive with strong static baselines and can yield faster convergence or lower loss.",
    "title_zh": "增强型GDA：用于平滑无约束优化的LLM内循环梯度下降算法",
    "abstract_zh": "利用大型语言模型（LLMs）求解优化问题这一课题尚属新兴领域，现有文献极少探讨如何通过LLM能力来增强标准求解器。本文提出一种基于代理视角来增强梯度下降算法（GDAs）的新方法。事实上，GDAs在优化和现代机器学习中仍处于核心地位，但其效率往往依赖于脆弱且耗时的超参数调优。我们探究了在已知或可估计梯度的平滑、无约束、单目标标准形式优化问题（SFOP）中，LLM是否能够融入运行中的梯度方法并发挥作用。为此，我们提出了“增强型梯度下降算法”（AGDA），这是一种将LLM作为闭环控制器的机制：它实时监控优化进程，并动态调整学习率调度策略，同时在每次运行中保持优化器家族不变。该设计将LLM置于反馈信息最丰富的环节，从而在不改变数值核心的前提下，显著减少人工试错。在基准函数以及两个著名的深度学习任务上的实验表明，闭环学习率调度策略在性能上可与强大的静态基线方法相媲美，甚至在某些情况下实现更快的收敛速度或更低的损失值。"
  },
  {
    "date": "2026-2-19",
    "title": "Implementation of SPI Protocol with Adaptive Baud Rate and Configurable N-Bit Transmission",
    "authors": "Singam Aruna, Kethavathu Srinivasa Naik, J Divya Harika, Ponnada Divya",
    "publish": "2025 International Conference on Intelligent Innovations in Engineering and Technology (ICIIET)",
    "url": "https://doi.org/10.1109/iciiet65921.2025.11378483",
    "source": "IEEE",
    "abstract": "Serial peripheral interface (SPI) protocol is very common in embedded systems based on its simplicity, speediness and capability of being full-duplex in communication. A better SPI design is introduced in this work which accommodates an amenable baud rate producer and adjustable N -bit data transmission. Adaptive baud rate mechanism adjusts clock frequency dynamically and allows reliable communication among devices with varying speed needs as well as enhancing flexibility of the system. The design also has a programmable word length which can be conveniently used to transmit variable sizes of data in the word which is longer than the traditional 8-bit word format. The implementation is provided in the form of Verilog HDL and it is tested by simulation hence ensuring that it operates correctly in various configurations. This design has rendered the SPI controller applicable in a large variety of devices which includes sensor interfacing, multimedia data transfer, and FPGA-based communication systems, where scalability and flexibility is of pivotal concern.",
    "title_zh": "具有自适应波特率和可配置N位传输的SPI协议实现",
    "abstract_zh": "串行外设接口（SPI）协议因其简洁性、高速性以及全双工通信能力，在基于嵌入式系统的应用中非常普遍。本文提出了一种改进的SPI设计，该设计集成了可灵活配置的波特率发生器和可调节的N位数据传输功能。自适应波特率机制能够动态调整时钟频率，从而实现不同速度需求设备间的可靠通信，并显著提升系统的灵活性。此外，该设计还具备可编程的字长功能，可方便地传输长度超过传统8位字长的可变大小数据。该设计采用Verilog HDL语言实现，并通过仿真测试，确保其在多种配置下均能正确运行。该SPI控制器设计适用于多种应用场景，包括传感器接口、多媒体数据传输以及基于FPGA的通信系统，尤其在对可扩展性和灵活性要求较高的场合具有重要应用价值。"
  },
  {
    "date": "2026-2-19",
    "title": "Table of Content",
    "authors": "N/A",
    "publish": "2025 IEEE International Circuit and System Symposium (ICSyS)",
    "url": "https://doi.org/10.1109/icsys65149.2025.11379895",
    "source": "IEEE",
    "abstract": null,
    "title_zh": "目录",
    "abstract_zh": "None"
  },
  {
    "date": "2026-2-19",
    "title": "FMP3+TECS/Rust: Memory-safe Component Framework for Multi-processor Embedded Systems",
    "authors": "Nao Yoshimura, Hiroshi Oyama, Takuya Azumi",
    "publish": "IEEE Open Journal of the Industrial Electronics Society",
    "url": "https://doi.org/10.1109/ojies.2026.3666358",
    "source": "IEEE",
    "abstract": "In recent years, embedded systems have become indispensable across a wide range of domains, including automotive and IoT applications. The increasing scale and complexity of these systems have intensified the demand in industrial development environments for platforms that ensure reliability, maintainability, and efficiency simultaneously. Conventional programming languages, such as C and C++, offer high performance but suffer from serious issues related to memory safety, resulting in bugs and security vulnerabilities. In contrast, Rust enables safe development by enforcing memory safety at compile time through its ownership and borrowing mechanisms, contributing to improved code quality and maintainability in industrial settings. Component-based development (CBD) supports modularization by units of functionality, enhancing reusability and structural clarity, which facilitates the handling of large-scale and complex industrial systems. Modern embedded applications, which require high processing performance and energy efficiency, increasingly adopt multi-processor architectures to enable parallel execution. This study proposes a Rust-compatible framework that combines the safety features of Rust with the structural advantages of CBD, based on the TOPPERS Embedded Component Systems (TECS) and the multi-processor real-time operating system TOPPERS FMP3. The proposed framework facilitates safe and reusable CBD in practice by designing and automatically generating Rust code that represents the TECS component structure and through optimization of exclusive control while maintaining safety. Evaluation of execution time demonstrates minimal overhead from integration with the multi-processor RTOS, enabling efficient and reliable application development.",
    "title_zh": "FMP3+TECS/Rust：面向多处理器嵌入式系统的内存安全组件框架",
    "abstract_zh": "近年来，嵌入式系统在汽车、物联网等多个领域已成为不可或缺的技术基础。随着系统规模和复杂性的不断增长，工业开发环境对能够同时保障可靠性、可维护性和高效性的平台需求日益迫切。传统的编程语言如C和C++虽然具备高性能，但存在严重的内存安全问题，容易引发程序错误和安全漏洞。相比之下，Rust通过其所有权和借用机制在编译时强制实现内存安全，有效提升了代码质量与可维护性，特别适用于工业级应用。组件化开发（CBD）通过功能单元的模块化设计，增强了代码的可重用性与结构清晰度，有助于应对大规模、复杂工业系统的设计与维护挑战。现代嵌入式应用对处理性能和能效要求越来越高，正越来越多地采用多处理器架构以实现并行执行。本研究提出了一种与Rust兼容的框架，该框架结合了Rust的安全特性与CBD的结构优势，基于TOPPERS嵌入式组件系统（TECS）以及多处理器实时操作系统TOPPERS FMP3。该框架通过设计并自动生成表示TECS组件结构的Rust代码，同时在保障安全的前提下优化独占控制机制，从而在实践中实现安全且可重用的组件化开发。执行时间评估表明，与多处理器实时操作系统集成后引入的开销极小，能够支持高效、可靠的嵌入式应用开发。"
  },
  {
    "date": "2026-2-19",
    "title": "Bridging Security Gaps in Software Requirement Specification through Machine Learning: A Comprehensive Review",
    "authors": "Sachin Bhardwaj, Archana Sahai, Pankaj Kumar",
    "publish": "2025 5th International Conference on Advancement in Electronics &amp;amp; Communication Engineering (AECE)",
    "url": "https://doi.org/10.1109/aece67531.2025.11386685",
    "source": "IEEE",
    "abstract": "This review examines how hybrid learning models can be integrated into the creation of secure Software Requirement Specifications (SRS) to deal with the emerging changes in the field of software engineering. The use of machine learning and deep learning in conjunction with statistical methods through hybrid learning models provides a powerful tool of automating and improving SRS processes. The study highlights how these models help in finding, ranking, and improving requirements while keeping data safe and systems reliable using advanced methods like differential privacy and federated learning. By analyzing existing literature, the review offers conclusions about the transformative impact of hybrid learning models on improving the precision, adaptability, and security of SRS. This work sets the stage for future research aimed at creating more resilient, scalable, and efficient systems for software requirement management.",
    "title_zh": "通过机器学习弥合软件需求规格说明书中的安全漏洞：一项综合评述",
    "abstract_zh": "本研究探讨了如何将混合学习模型融入安全软件需求规格说明书（SRS）的创建过程中，以应对软件工程领域不断涌现的新变化。通过将机器学习与深度学习技术结合统计方法，混合学习模型为自动化和优化SRS流程提供了强大工具。研究强调，这些模型在需求的发现、排序与优化方面发挥重要作用，同时借助差分隐私和联邦学习等先进方法，确保数据安全与系统可靠性。通过对现有文献的分析，本文总结了混合学习模型在提升SRS的精确性、适应性与安全性方面的变革性影响。本研究为未来致力于构建更具韧性、可扩展性和高效性的软件需求管理系统的深入研究奠定了基础。"
  },
  {
    "date": "2026-2-19",
    "title": "Design of Intelligent Translation Optimization Algorithm Based on Large Language Model",
    "authors": "Xiuming Xu, Wenjuan Zhao",
    "publish": "Proceedings of the 2025 International Conference on Digital Society and Intelligent Computing",
    "url": "https://doi.org/10.1145/3788910.3788946",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "基于大语言模型的智能翻译优化算法设计",
    "abstract_zh": "None"
  },
  {
    "date": "2026-2-19",
    "title": "Pungoe Pentest: Orchestration of Web Penetration Testing Tools with Assistance Based on Large Language Model (LLM) Pentest-AI and Retrieval-augmented Generation (RAG)",
    "authors": "Syubbanul Siddiq, R Budiarto Hadiprakoso, Prasetyo Adi Wibowo Putro, Girinoto, Deva Finanda Saputra",
    "publish": "2025 IEEE 2nd International Conference on Cryptography, Informatics, and Cybersecurity (ICoCICs)",
    "url": "https://doi.org/10.1109/icocics68032.2025.11384036",
    "source": "IEEE",
    "abstract": "Web applications are the most vulnerable applications because they are often targeted for security exploits, so good preventive measures are needed. Penetration testing can be used to identify security vulnerabilities, but testing tools are often standalone and Command Line Interface (CLI)-based, limiting accessibility for ordinary users in the vulnerability identification process. This research designs and implements Pungoe Pentest, a web GUI that orchestrates multiple penetration-testing tools. The system centralizes tool invocation, result management, and LLM-assisted analysis. Additionally, this research implements a Large Language Model (LLM) using the Retrieval-Augmented Generation (RAG) method with the Pentest-AI model to assist users in the penetration testing process. With the implementation of RAG, the LLM model is expected to generate context-based recommendations and reduce the risk of hallucinations in model outputs. Evaluation results show that the developed application can detect various vulnerabilities on a web dummy, the implementation of RAG on the Pentest-AI model successfully improves answer accuracy and STS scores compared to before RAG implementation, and SUS testing yields an average SUS score of 83, categorised as Acceptable. The resulting platform, Pungoe Pentest, is expected to serve as an effective tool for both seasoned pentesters and students in the field of web penetration testing.",
    "title_zh": "Pungoe Pentest：基于大语言模型（LLM）渗透测试AI与检索增强生成（RAG）的Web渗透测试工具编排",
    "abstract_zh": "Web应用程序是最容易受到攻击的应用程序，因为它们经常成为安全漏洞的攻击目标，因此需要采取有效的预防措施。渗透测试可用于识别安全漏洞，但现有的测试工具通常为独立运行且基于命令行界面（CLI），这限制了普通用户在漏洞识别过程中的使用。本研究设计并实现了一个名为Pungoe Pentest的Web图形用户界面（GUI），该系统能够协调多个渗透测试工具。该系统集中管理工具调用、结果处理以及基于大语言模型（LLM）的辅助分析。此外，本研究采用检索增强生成（RAG）方法，基于Pentest-AI模型构建了一个大语言模型，以辅助用户完成渗透测试流程。通过引入RAG机制，期望该LLM能够生成基于上下文的建议，降低模型输出中的幻觉风险。评估结果表明，所开发的应用程序能够有效检测Web模拟系统中的多种漏洞；与未使用RAG的版本相比，RAG在Pentest-AI模型上的应用显著提升了回答准确率和语义相似度（STS）得分；用户体验满意度（SUS）测试的平均得分为83，属于“可接受”水平。最终形成的Pungoe Pentest平台，有望成为资深渗透测试人员以及网络安全学习者在Web渗透测试领域中的有效工具。"
  },
  {
    "date": "2026-2-19",
    "title": "A 10-Bit 100-kHz Partially Segmented DAC Design for LCD Gamma Correction Using Open-Source EDA Tools",
    "authors": "Yohanes Stefanus, Astria N. Irfansyah, Hendra Kusuma",
    "publish": "2025 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS)",
    "url": "https://doi.org/10.1109/ispacs68724.2025.11383376",
    "source": "IEEE",
    "abstract": "This paper presents the design of a partially 10-bit segmented digital-to-analog converter (DAC) operating at 100 kHz, intended to control LCD panels and perform gamma correction for full high definition (FHD) resolution at 60 frames per second (FPS). The DAC architecture combines a 6-bit thermometer-coded R-DAC and a 4-bit binary-weighted DACembedded operational amplifier. This architecture was chosen because it offers a significantly smaller chip size than conventional resistor-string DACs (R-DACs) while also providing <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$g_{m}$</tex> modulated voltage interpolation along the gamma-voltage transfer curve. The circuit design was carried out using open-source EDA tools and the SkyWater 130 nm PDK. The maximum value of simulated differential non-linearity (DNL) and integral nonlinearity (INL) for temperature variation are <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathbf{0. 6 6 1}$</tex> LSB and <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathbf{1. 2 9 9}$</tex> LSB at 0° C, respectively. This DAC achieves a spurious-free dynamic range (SFDR) of <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathbf{1 9. 5 4 9 ~ d B}$</tex> and a settling time of <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathbf{2. 2 5} \\boldsymbol{\\mu} \\mathbf{s}$</tex>. The designed DAC occupies an area of <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$13,907.916 \\mu ~\\mathrm{m}^{2}$</tex>, which is <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathbf{2 6. 2 6 2 \\%}$</tex> smaller than a conventional 10-bit R-DAC.",
    "title_zh": "用于LCD伽马校正的10位100 kHz部分分段DAC设计，采用开源EDA工具",
    "abstract_zh": "本文提出了一种工作频率为100 kHz的10位部分分段式数字-模拟转换器（DAC）的设计，旨在控制液晶显示器（LCD）面板，并在每秒60帧（FPS）的全高清（FHD）分辨率下实现伽马校正。该DAC架构结合了6位热编码电阻-电容DAC（R-DAC）与嵌入式运算放大器的4位权值二进制DAC。选择该架构的原因在于，相较于传统的电阻串型DAC（R-DAC），其芯片面积显著减小，同时还能在伽马电压传输曲线上实现$g_{m}$调制的电压插值。电路设计采用开源EDA工具及SkyWater 130 nm PDK完成。在温度变化条件下的仿真结果显示，差分非线性（DNL）和积分非线性（INL）的最大值分别为0.661 LSB和1.299 LSB（在0°C时）。该DAC实现了19.549 dB的无杂散动态范围（SFDR）和2.25 μs的建立时间。所设计的DAC芯片面积为13,907.916 μm²，比传统10位R-DAC小26.262%。"
  },
  {
    "date": "2026-2-19",
    "title": "Autonomous Digital Design Implementation on FPGA Using Large Language Models",
    "authors": "Moulee Sudharsan R Y, Nithish Kumar K, Visalakshi P",
    "publish": "2025 International Conference on Intelligent Innovations in Engineering and Technology (ICIIET)",
    "url": "https://doi.org/10.1109/iciiet65921.2025.11378618",
    "source": "IEEE",
    "abstract": "The increasing complexity of digital systems and the need for rapid prototyping have highlighted the demand for automation in hardware design workflows. This proposed work provides a fully autonomous system for implementing digital designs on Field-Programmable Gate Arrays (FPGAs). The system gets a high-level input describing a digital circuit from the user and makes use of Large Language Model (LLM) to generate the TCL file containing corresponding Verilog HDL code. This TCL file is then passed through a TCL-scripted Vivado workflow, which automates project creation, synthesis, implementation, and bitstream generation without any manual intervention. The proposed method removes the need for users to interact with the Vivado GUI thereby reducing development time and human error. Additionally, the system supports storing the generated files in the GitHub cloud for future references. By bridging AI and EDA toolchains, this research work gives the foundation for a next-generation design-as-code paradigm in hardware development.",
    "title_zh": "基于大语言模型的FPGA自主数字设计实现",
    "abstract_zh": "随着数字系统复杂性的不断增加以及快速原型设计的需求日益增长，硬件设计工作流中对自动化的需求愈发凸显。本文提出了一种完全自主的系统，用于在现场可编程门阵列（FPGA）上实现数字设计。该系统接收用户提供的高层次数字电路描述作为输入，利用大型语言模型（LLM）自动生成包含相应Verilog HDL代码的TCL文件。随后，该TCL文件将通过一个由TCL脚本驱动的Vivado工作流，实现项目创建、综合、实现及比特流生成的全流程自动化，无需任何人工干预。该方法消除了用户与Vivado图形界面交互的需要，从而显著缩短开发时间并减少人为错误。此外，系统还支持将生成的文件存储至GitHub云端，便于日后查阅与管理。通过融合人工智能与电子设计自动化（EDA）工具链，本研究为下一代“设计即代码”（Design-as-Code）硬件开发范式奠定了基础。"
  },
  {
    "date": "2026-2-19",
    "title": "ML-Pump: A Machine Learning Based Bidirectional Prediction Framework for Charge Pump Design Optimization",
    "authors": "Ashutosh Singh, Anuj Grover, Abhishek Jain",
    "publish": "2025 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)",
    "url": "https://doi.org/10.1109/apccas67402.2025.11376707",
    "source": "IEEE",
    "abstract": "Charge Pump circuit design presents significant challenges due to interrelated parameters, nonlinear component behavior, and complex performance trade-offs that traditionally require extensive iterations and simulation time. The proposed ML-Pump, a bidirectional predictive model, tackles these challenges by functioning in both forward and reverse directions. It predicts performance metrics (including output voltage, efficiency, and power) from design parameters (including width, length, capacitance, input voltage, load, etc.), and conversely, determines the optimal design parameters needed to achieve the desired performance characteristics. A single-stage cross-coupled charge pump implemented in 90nm CMOS technology was utilized for training and evaluating several machine learning models (Random Forest, Multi-layer Perceptron regressor (MLPRegressor) and Gradient Boosting) on a comprehensive dataset of approximately 65,000 samples generated from Cadence simulations. The MLP Regressor demonstrated superior performance with mean MSE of 2.09e-02 and MAE of 5.00e-02, outperforming Random Forest and Gradient Boosting algorithms. This bidirectional prediction capability enables designers to instantly evaluate performance impacts of parameter changes without time-consuming EDA tool simulations, significantly accelerating the design optimization process. This framework significantly speeds up the design process by enabling early prediction of performance metrics and design parameters, thereby improving efficiency and circuit performance.",
    "title_zh": "ML-Pump：一种基于机器学习的双向预测框架用于电荷泵设计优化",
    "abstract_zh": "电荷泵电路设计由于参数之间相互关联、器件呈现非线性特性以及性能权衡复杂，传统上需要大量迭代和耗时的仿真。本文提出的ML-Pump是一种双向预测模型，能够同时实现正向和反向预测。该模型可基于设计参数（如晶体管宽长比、电容值、输入电压、负载等）预测性能指标（如输出电压、效率和功率），也可反向推导出实现目标性能所需的最优设计参数。研究采用在90nm CMOS工艺下实现的单级交叉耦合电荷泵电路，利用Cadence仿真生成的约6.5万组数据，对多种机器学习模型（随机森林、多层感知机回归器（MLPRegressor）和梯度提升回归）进行了训练与评估。结果表明，MLP回归器表现最优，平均均方误差（MSE）为2.09e-02，平均绝对误差（MAE）为5.00e-02，优于随机森林和梯度提升算法。该双向预测能力使设计人员无需依赖耗时的EDA工具仿真，即可即时评估参数变更对性能的影响，显著加速了设计优化进程。该框架通过实现性能指标与设计参数的早期预测，大幅提升了设计效率和电路性能。"
  },
  {
    "date": "2026-2-19",
    "title": "ProvAuditChain: A Gas-Efficient On-Chain Provenance Framework for AI-Driven Smart Contract Audits",
    "authors": "George Chidera Akor, Love Allen Chijioke Ahakonye, Jae Min Lee, Dong-Seong Kim",
    "publish": "2025 16th International Conference on Information and Communication Technology Convergence (ICTC)",
    "url": "https://doi.org/10.1109/ictc66702.2025.11388677",
    "source": "IEEE",
    "abstract": "The increasing use of AI-powered tools for smart contract security audits presents a critical challenge: ensuring the integrity and provenance of audit reports. Traditional methods lack cryptographic guarantees linking audit outputs to specific AI models and source code, creating vulnerabilities to tampering and misattribution. To address this, we propose ProvAuditChain, a gas-efficient, hybrid on-chain/off-chain framework that records immutable provenance of AI-driven audit reports on Layer 2 blockchain networks. ProvAuditChain utilizes lightweight smart contracts to securely anchor cryptographically signed audit report hashes on-chain, while storing the complete reports on decentralized IPFS storage. We deploy the system on the Arbitrum Sepolia testnet and benchmark gas consumption, latency, and throughput across 600 audit cycles. Our results demonstrate a stable average gas cost of approximately 173,000 per audit, equivalent to roughly $0.05 USD, alongside a throughput of nearly six audits per minute. These findings confirm the practical viability of ProvAuditChain for integration into automated CI/CD pipelines, providing a scalable foundation for trustworthy AI accountability in decentralized ecosystems.",
    "title_zh": "ProvAuditChain：一种面向AI驱动智能合约审计的高效Gas链上溯源框架",
    "abstract_zh": "随着人工智能驱动工具在智能合约安全审计中的日益普及，一个关键挑战随之而来：确保审计报告的完整性与可追溯性。传统方法缺乏将审计输出与特定AI模型及源代码进行密码学关联的保障，从而容易受到篡改和责任归属错误的威胁。为应对这一问题，我们提出ProvAuditChain——一种高效节省Gas的混合式链上/链下框架，可将AI驱动的审计报告的不可篡改溯源信息记录在Layer 2区块链网络上。ProvAuditChain利用轻量级智能合约，将经过密码学签名的审计报告哈希值安全地锚定在链上，同时将完整的审计报告存储于去中心化的IPFS存储系统中。我们在Arbitrum Sepolia测试网上部署了该系统，并对600次审计周期的Gas消耗、延迟和吞吐量进行了基准测试。结果表明，每次审计的平均Gas成本稳定在约173,000，相当于约0.05美元，吞吐量接近每分钟六次审计。这些成果证实了ProvAuditChain在自动化CI/CD流水线中集成的可行性，为去中心化生态系统中可信的人工智能问责机制提供了可扩展的基础。"
  },
  {
    "date": "2026-2-19",
    "title": "LLM-as-a-Judge for Software Engineering: Literature Review, Vision, and the Road Ahead",
    "authors": "Junda He, Jieke Shi, Terry Yue Zhuo, Christoph Treude, Jiamou Sun, Zhenchang Xing, Xiaoning Du, David Lo",
    "publish": "ACM Transactions on Software Engineering and Methodology",
    "url": "https://doi.org/10.1145/3797276",
    "source": "ACM",
    "abstract": "None",
    "title_zh": "软件工程中的大语言模型作为裁判：文献综述、愿景与未来展望",
    "abstract_zh": "None"
  },
  {
    "date": "2026-2-19",
    "title": "Design Automation for a-IGZO Thin Film Technology Using a Multi-Row Standard Cell Architecture",
    "authors": "Yi-Ting Lin, Yalun Tang, Byeonggon Kang, Iris Hui-Ru Jiang, Kenji Nomura, Yuhwa Lo, Lifu Chang, Bill Lin, Chung-Kuan Cheng",
    "publish": "2025 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)",
    "url": "https://doi.org/10.1109/apccas67402.2025.11377051",
    "source": "IEEE",
    "abstract": "Amorphous-IGZO thin-film transistors (a-IGZO TFTs) have attracted increasing attention for emerging applications in wearable devices and IoT technologies, owing to their compatibility with flexible substrates, low processing temperatures, and energy-efficient operation. However, the lack of p-type devices breaks the symmetry of conventional CMOS logic, and the technology requires a double-gate structure with bias voltage to adjust the I-V characteristics. While previous studies have primarily focused on fabrication and application aspects, the electronic design automation (EDA) perspective remains underexplored. In this paper, we propose the first standard cell synthesis framework for a-IGZO TFTs. The framework features a cell layout architecture that integrates bias voltage into power rail planning and adopts multirow transistor placement to address logic asymmetry. Transistor placement is modeled as an exact cover with colors (XCC) problem and solved using the dancing cells algorithm, while intra-cell routing is formulated with satisfiability modulo theories (SMT) to account for double-gate and multi-rail structures. Experimental results demonstrate the effectiveness of the proposed architecture and methodologies.",
    "title_zh": "基于多行标准单元架构的a-IGZO薄膜技术设计自动化",
    "abstract_zh": "非晶氧化铟镓锌薄膜晶体管（a-IGZO TFTs）因其与柔性基底的兼容性、低温加工工艺以及低功耗运行特性，近年来在可穿戴设备和物联网技术等新兴应用领域受到越来越多的关注。然而，缺乏p型器件破坏了传统CMOS逻辑的对称性，且该技术需要采用双栅结构并施加偏置电压以调节I-V特性。尽管以往研究主要集中在器件制备与应用方面，但电子设计自动化（EDA）视角仍鲜有探索。本文首次提出面向a-IGZO TFTs的标准单元综合框架。该框架采用一种将偏置电压集成到电源布线规划中的单元版图架构，并通过多行晶体管布局策略来应对逻辑不对称问题。晶体管布局被建模为带颜色的精确覆盖（XCC）问题，并利用舞蹈单元算法求解；单元内布线则通过满足模理论（SMT）方法进行建模，以充分考虑双栅结构和多电源轨结构的复杂性。实验结果验证了所提架构与方法的有效性。"
  },
  {
    "date": "2026-2-19",
    "title": "Enhancing the Performance of VLSI Architecture for Arithmetical Application",
    "authors": "Sukruth R, Bore Gowda S. B.",
    "publish": "2025 International Conference on Electronics and Computing, Communication Networking Automation Technologies (ICEC2NT)",
    "url": "https://doi.org/10.1109/icec2nt65402.2025.11379927",
    "source": "IEEE",
    "abstract": "This paper presents an optimized design of a <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$4 \\times 4$</tex> multiplier using pass transistor logic to address the challenges of power consumption, speed, and circuit complexity in VLSI applications. The architecture suggested has a three-step computation procedure, where-in, 16 partial products are computed using logical AND gates and then tree length is reduced using a series of 3 -input and 2 -input binary adders at strategic points. Buffers are incorporated at major points of the design to guarantee signal integrity and signal voltage. The unique full adder circuit, consisting of Gate Diffusion Input (GDI) XOR gates and Pass Transistor Logic (PTL) XOR and MUX cells has low dynamic and quiescent power consumption and an improved performance. The proposed multiplier compares favorably to current designs with smaller power dissipations, critical path delay and a smaller number of transistors thus it is very efficient in terms of low power and high-speed computer applications. This work establishes a robust framework for designing compact and energy-efficient multipliers, providing valuable insights for advanced digital signal processing and portable device implementations.",
    "title_zh": "提升VLSI架构在算术应用中的性能",
    "abstract_zh": "本文提出了一种采用传输门逻辑（Pass Transistor Logic, PTL）的优化4×4乘法器设计，旨在解决VLSI应用中功耗、速度和电路复杂度方面的挑战。所提出的架构采用三步计算流程：首先利用逻辑与门计算16个部分积，随后在关键位置通过一系列三输入和二输入的二进制加法器对树形结构进行压缩，以减少延迟。在设计的关键节点上引入缓冲器，以确保信号完整性及信号电压水平。该设计采用一种独特的全加器电路，由门扩散输入（Gate Diffusion Input, GDI）异或门以及PTL异或门和多路选择器（MUX）单元构成，具有低动态功耗和静态功耗，并显著提升了性能。与现有设计相比，该乘法器在功耗更低、关键路径延迟更短、晶体管数量更少方面表现优异，因此在低功耗、高速计算应用中极为高效。本研究为设计紧凑且能效更高的乘法器建立了一个稳健的框架，为高级数字信号处理及便携式设备的应用提供了宝贵的参考与启示。"
  },
  {
    "date": "2026-2-19",
    "title": "Hybrid LLM and Sentence Transformer Evaluation for ITS Programming Exercises",
    "authors": "Javier Ortega-Morla, Daniel Ferreiro, Alejandro Paz-Lopez, Beatriz Péerez-Sáanchez, Sara Guerreiro-Santalla",
    "publish": "2025 International Conference on Education Technology and Computers (ICETC)",
    "url": "https://doi.org/10.1109/icetc66579.2025.11387358",
    "source": "IEEE",
    "abstract": "This paper explores automatic evaluation and feedback generation for varied student output in programming exercises. To this end, a hybrid approach was implemented, combining Sentence Transformer Models for output classification with General Purpose Large Language Models for generating brief, informative feedback. The solution was tested on a dataset of 1,300 manually labeled student outputs from programming exams at the University of A Coruña. Finally, this system will be integrated into ProgTutor, an Intelligent Tutoring System, to replace the current evaluation method and enable its assessment in large-scale, real-world conditions.",
    "title_zh": "ITS编程练习中混合大语言模型与句子嵌入模型的评估",
    "abstract_zh": "本文探讨了针对编程练习中多样化学生作答的自动评估与反馈生成方法。为此，我们采用了一种混合方法，结合了句子嵌入模型（Sentence Transformer Models）进行输出分类，以及通用大型语言模型（General Purpose Large Language Models）生成简明且具有信息量的反馈。该方案在西班牙拉科鲁尼亚大学编程考试的1300份人工标注学生作答数据集上进行了测试。最终，该系统将被集成到智能辅导系统ProgTutor中，取代现有的评估方法，并在大规模真实场景下实现评估应用。"
  },
  {
    "date": "2026-2-19",
    "title": "QCA Realization of Lightweight ASCON S-Box for Security Critical Applications",
    "authors": "Saravanan P, Parthasarathy R, Mohan Kumar S, Niveditha AR, Kavin M",
    "publish": "2025 International Conference on Intelligent Innovations in Engineering and Technology (ICIIET)",
    "url": "https://doi.org/10.1109/iciiet65921.2025.11379157",
    "source": "IEEE",
    "abstract": "The growing need for ultra-low-power and high-density nanoelectronic circuits in IoT and embedded systems has led to the investigation of Quantum-dot Cellular Automata (QCA) as an alternative to traditional CMOS technology. This research introduces a QCA-based implementation of the core permutation logic S-Box of ASCON–a lightweight cryptographic algorithm that includes authenticated encryption with associated data (AEAD) and hashing functions. ASCON was chosen as one of the winners of the CAESAR competition due to its high security, compact hardware design, and reliability in resource-limited environments such as IoT applications. In the proposed design, Bit-sliced implementation is used to generate the output. This work establishes the basis for a complete 320-bit QCA version of ASCON and offers insights into the challenges involved in implementing cryptographic functions within the QCA logic framework",
    "title_zh": "面向安全关键应用的轻量级ASCON S-Box的QCA实现",
    "abstract_zh": "物联网和嵌入式系统对超低功耗、高密度纳米电子电路的需求日益增长，促使研究人员将量子点细胞自动机（QCA）作为传统CMOS技术的替代方案进行探索。本研究提出了一种基于QCA的ASCON轻量级密码算法核心置换逻辑S-Box的实现方案。ASCON是一种支持带关联数据的认证加密（AEAD）和哈希功能的加密算法，因其高安全性、紧凑的硬件设计以及在资源受限环境（如物联网应用）中的可靠性，被选为CAESAR竞赛的优胜者之一。在所提出的方案中，采用比特切片（Bit-sliced）实现方法生成输出。该工作为构建完整的320位QCA版本ASCON奠定了基础，并深入探讨了在QCA逻辑框架中实现密码函数所面临的关键挑战。"
  },
  {
    "date": "2026-2-19",
    "title": "Finetune-Then-Merge: Democratizing Large Language Model Accessibility Through Efficiency Optimization",
    "authors": "James Che",
    "publish": "2025 3rd International Conference on Foundation and Large Language Models (FLLM)",
    "url": "https://doi.org/10.1109/fllm67465.2025.11391195",
    "source": "IEEE",
    "abstract": "Large Language Models (LLMs) have largely powered recent advancements in AI, from ChatGPT to agentic systems, yet remain inaccessible to large numbers of people without stable access to the internet or high-end hardware. While smaller LLMs can operate in low-resource environments, these models are often prone to hallucination and inaccurate responses, representing tradeoffs between model compute and knowledge. This work presents Finetune-Then-Merge, a novel approach to improving models’ compute-knowledge efficiency, which finetunes multiple specialized expert LLMs and merges them, maintaining specialized knowledge without significantly increasing computational requirements. To validate this approach, Omni, a model merged from four expert models specialized in all subdomains of STEM, was created. Omni achieves state-of-the-art performance in domain knowledge and model efficiency compared to alternative optimization approaches, outperforming its base model by 31.70% in MMLU (Measuring Massive Multitask Language Understanding) STEM and 25.32% across all tested STEM benchmarks. It additionally runs on 4GB of RAM with considerable throughput and achieves optimal compute-knowledge efficiency across alternative models ranging from sizes of 1 billion to 8 billion parameters. Finetune-Then-Merge can also scale across different model architectures and domains, offering a new direction in LLM research towards models available to anyone, anywhere.",
    "title_zh": "微调后合并：通过效率优化实现大语言模型的普惠化",
    "abstract_zh": "大型语言模型（LLMs）在人工智能领域推动了诸多突破性进展，从ChatGPT到智能体系统，但其广泛应用仍受限于稳定互联网接入或高端硬件的获取。尽管较小的LLM可在资源有限的环境中运行，但这些模型往往容易产生幻觉并给出不准确的回答，反映出模型计算能力与知识容量之间的权衡。本文提出一种名为“微调后合并”（Finetune-Then-Merge）的新方法，旨在提升模型在计算与知识效率方面的表现：通过微调多个专业领域的专家型LLM，并将它们融合，从而在不显著增加计算开销的前提下保留专业领域的知识。为验证该方法的有效性，研究团队构建了Omni——一个由四个专注于STEM各子领域的专家模型融合而成的模型。Omni在领域知识和模型效率方面均达到当前最优水平，相较于基线模型，在MMLU（大规模多任务语言理解评估）的STEM任务上性能提升31.70%，在所有测试的STEM基准中平均提升25.32%。此外，Omni仅需4GB内存即可运行，具备较高的吞吐量，并在参数规模介于10亿至80亿的各类模型中实现了最佳的计算-知识效率平衡。该方法还可扩展至不同模型架构和应用领域，为大模型研究开辟了一条新路径——让任何人在任何地方都能使用高效、可靠的AI模型。"
  },
  {
    "date": "2026-2-19",
    "title": "Benchmarking DevSecOps Pipelines: A Performance and Security Analysis of Laravel and CodeIgniter",
    "authors": "Muhammad Faki Raihan, Hermawan Setiawan",
    "publish": "2025 IEEE 2nd International Conference on Cryptography, Informatics, and Cybersecurity (ICoCICs)",
    "url": "https://doi.org/10.1109/icocics68032.2025.11383915",
    "source": "IEEE",
    "abstract": "This study presents a comparative analysis of the performance and implementation of DevSecOps pipelines in two popular PHP frameworks, Laravel and CodeIgniter. To provide empirical data, two functionally identical web applications were developed and tested within a CI/CD pipeline orchestrated by Jenkins. The pipeline integrated SonarQube for Static Application Security Testing (SAST) and OWASP ZAP for Dynamic Application Security Testing (DAST) to automate security evaluations. The primary research goals were to measure pipeline execution time, compare resource consumption (CPU and memory), and analyze the detection effectiveness of the security tools on both frameworks. The results show that CodeIgniter is more efficient (execution time of 152 seconds vs. 167 seconds). However, both automated SAST and DAST tools failed to detect critical vulnerabilities without deep configuration, emphasizing the trade-off between efficiency and implementation effort. The main contribution of this study is empirical evidence demonstrating how a framework's architecture directly influences the trade-off between pipeline efficiency and the implementation effort required for meaningful security analysis, underscoring that automated tools are a baseline that requires intelligent human configuration to be truly effective.",
    "title_zh": "基准测试 DevSecOps 流水线：Laravel 与 CodeIgniter 的性能与安全分析",
    "abstract_zh": "本研究对两种流行PHP框架Laravel与CodeIgniter在DevSecOps流水线中的性能与实现效果进行了对比分析。为获取实证数据，研究人员开发并测试了两个功能完全相同的Web应用，并在Jenkins orchestrat的CI/CD流水线中运行。该流水线集成了SonarQube用于静态应用安全测试（SAST），以及OWASP ZAP用于动态应用安全测试（DAST），以实现安全评估的自动化。研究的主要目标包括：测量流水线执行时间、比较资源消耗（CPU与内存使用量），以及分析安全工具在两个框架上的漏洞检测效果。结果表明，CodeIgniter在效率上更具优势（执行时间分别为152秒 vs. 167秒）。然而，两种自动化SAST与DAST工具在未进行深度配置的情况下均未能检测出关键漏洞，凸显了效率与实施投入之间的权衡。本研究的主要贡献在于提供了实证证据，表明框架的架构设计直接影响流水线效率与实现有效安全分析所需投入的实施成本之间的权衡关系，强调了自动化工具仅能作为基础，必须通过智能的人工配置才能真正发挥效能。"
  }
]